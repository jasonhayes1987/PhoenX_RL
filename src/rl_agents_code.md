# Python Files in /workspaces/RL_Agents/src/app

## __init__.py

```python

```

---

## adaptive_kl.py

```python

class AdaptiveKL():
    """
    Keeps track of a KL penalty coefficient `beta` that is adjusted
    after each update so the observed KL divergence hovers near `target_kl`.
    """
    def __init__(self, initial_beta=1.0, target_kl=0.01,
                 scale_up=2.0, scale_down=0.5,
                 kl_tolerance_high=1.5, kl_tolerance_low=0.5):
        """
        Args:
            initial_beta (float): initial KL penalty
            target_kl (float): desired KL divergence
            scale_up (float): factor by which to increase beta if KL is too high
            scale_down (float): factor by which to reduce beta if KL is too low
            kl_tolerance_high (float): if observed KL > target_kl * kl_tolerance_high,
                                       we consider that "too high"
            kl_tolerance_low (float): if observed KL < target_kl * kl_tolerance_low,
                                      we consider that "too low"
        """
        self.initial_beta = initial_beta
        self.beta = initial_beta
        self.target_kl = target_kl
        self.scale_up = scale_up
        self.scale_down = scale_down
        self.kl_tolerance_high = kl_tolerance_high
        self.kl_tolerance_low = kl_tolerance_low

    def step(self, observed_kl):
        """
        Update beta based on how the observed KL compares to target_kl.
        Typically called after each PPO update (once you can measure KL).
        """
        # If KL is way above target, raise beta
        if observed_kl > self.target_kl * self.kl_tolerance_high:
            self.beta *= self.scale_up
        # If KL is much below target, lower beta
        elif observed_kl < self.target_kl * self.kl_tolerance_low:
            self.beta *= self.scale_down

    def get_beta(self):
        return self.beta
    
    def get_config(self):
        return {
            "initial_beta": self.initial_beta,
            "target_kl": self.target_kl,
            "scale_up": self.scale_up,
            "scale_down": self.scale_down,
            "kl_tolerance_high": self.kl_tolerance_high,
            "kl_tolerance_low": self.kl_tolerance_low
        }
```

---

## buffer.py

```python
import torch as T
import numpy as np
import gymnasium as gym
from env_wrapper import EnvWrapper, GymnasiumWrapper, IsaacSimWrapper
from utils import build_env_wrapper_obj
from torch_utils import get_device
from typing import Optional, Tuple, List, Any, Dict
from collections import defaultdict
import math


class SumTree:
    """
    A binary sum tree for efficient sampling based on priorities.
    """
    def __init__(self, capacity: int, device: T.device):
        self.capacity = capacity
        self.device = get_device(device)
        # Initialize the tree with zeros
        self.tree = T.zeros(2 * capacity - 1, dtype=T.float32, device=self.device)
        self.next_idx = 0
        # self.size = 0
        self.max_priority = T.tensor(1.0, dtype=T.float32, device=self.device)
        # Add tracking for debugging
        self.debug_last_large_priority = None
        self.debug_last_large_priority_idx = None
    
    def update(self, data_indices, priorities):
        """Update the priorities of the given data indices."""
        # Debug large priorities
        if priorities.numel() > 0:
            max_val = T.max(priorities)
            if max_val > 1e6:  # Track suspiciously large priorities
                large_idx = T.argmax(priorities)
                self.debug_last_large_priority = max_val.item()
                self.debug_last_large_priority_idx = data_indices[large_idx].item()
                print(f"WARNING: Large priority detected: {max_val.item():.2e} at buffer index {data_indices[large_idx].item()}")
        
        # Safety check for NaN values
        if T.isnan(priorities).any():
            priorities = T.nan_to_num(priorities, nan=1.0)
            print("WARNING: NaN priorities detected and replaced with 1.0")
        
        # Update max recorded priority if needed (before normalization)
        if priorities.numel() > 0 and not T.isnan(priorities).all():
            new_max = T.max(priorities)
            if new_max > self.max_priority:
                old_max = self.max_priority.item()
                self.max_priority = new_max
                if new_max > old_max * 10:  # Log significant jumps
                    print(f"WARNING: Large max priority increase: {old_max:.2e} -> {new_max.item():.2e}")
        
        # Normalize priorities globally using max_recorded_priority
        # priorities = priorities / self.max_priority
        
        # Debug normalization
        # if T.max(priorities) > 1.0:
        #     print(f"WARNING: Post-normalization priorities > 1.0: max = {T.max(priorities).item():.2e}")
        
        # Compute tree indices (leaf nodes) from data indices
        tree_indices = data_indices + self.capacity - 1
        
        # Update leaf nodes with new priorities
        self.tree[tree_indices] = priorities
        
        # Update parent nodes efficiently without using Python sets/lists
        # Directly update all parents in a bottom-up fashion
        parent_indices = (tree_indices - 1) // 2
        
        # Handle case where parent_indices might be empty
        while parent_indices.numel() > 0:
            # For each parent, calculate the sum of its children
            left_children = 2 * parent_indices + 1
            right_children = left_children + 1
            
            # Update parents with sum of children
            # Handle edge cases where right child might not exist
            right_valid = right_children < len(self.tree)
            self.tree[parent_indices] = self.tree[left_children] + \
                                       T.where(right_valid, self.tree[right_children], 
                                              T.zeros_like(self.tree[right_children]))
            
            # Move up to next level of parents, removing duplicates
            parent_indices = (parent_indices - 1) // 2
            
            # Use unique values but handle potential empty tensor
            if parent_indices.numel() > 0:
                parent_indices = T.unique(parent_indices)
            
            # Stop when we reach the root
            if parent_indices.numel() == 0 or (parent_indices < 0).all():
                break

    def get(self, p_values: T.Tensor) -> Tuple[T.Tensor, T.Tensor]:
        """
        Optimized vectorized batch sampling from the SumTree.
        """
        batch_size = p_values.size(0)
        
        # Pre-allocate space for resulting indices
        tree_indices = T.zeros(batch_size, dtype=T.long, device=self.device)
        
        # Use an iterative approach for batch traversal
        # This is still sequential per sample but avoids Python loop overhead
        @T.jit.script
        def traverse_tree(p_values: T.Tensor, tree: T.Tensor, capacity: int) -> T.Tensor:
            batch_size = p_values.size(0)
            tree_indices = T.zeros(batch_size, dtype=T.long, device=p_values.device)
            
            for i in range(batch_size):
                idx = 0  # Start at root
                p = p_values[i].item()
                
                # Traverse down the tree
                while idx < capacity - 1:  # Not a leaf node
                    left = 2 * idx + 1
                    
                    # If we would access beyond tree bounds, we've reached a leaf
                    if left >= tree.size(0):
                        break
                    
                    left_val = tree[left].item()
                    
                    # Choose direction
                    if p <= left_val:
                        idx = left
                    else:
                        p -= left_val
                        idx = left + 1
                
                tree_indices[i] = idx
            
            return tree_indices
            
        # Traverse tree for each sample in batch
        tree_indices = traverse_tree(p_values, self.tree, self.capacity)
        
        # Convert tree indices to data indices and get priorities
        data_indices = tree_indices - (self.capacity - 1)
        priorities = self.tree[tree_indices]
        
        return data_indices, priorities
    
    @property
    def total_priority(self) -> float:
        """Return the total priority (value at root)."""
        return self.tree[0].item() if self.tree.size(0) > 0 else 0.0

class Buffer():
    """
    Base class for replay buffers.
    """

    def __init__(self, env: EnvWrapper, buffer_size: int, device: Optional[str] = None):
        self.device = get_device(device)
        self.env = env
        self.buffer_size = buffer_size

    def add(self, *args, **kwargs):
        """
        Add a transition to the buffer.
        """
        pass
    
    def sample(self, batch_size: int):
        """
        Sample a batch of transitions from the buffer.

        Args:
            batch_size (int): The number of transitions to sample.

        Returns:
            Tuple: Sampled transitions.
        """
        pass

    def get_config(self) -> Dict[str, Any]:
        """
        Retrieve the configuration of the buffer.

        Returns:
            dict: Configuration details.
        """
        pass

    @classmethod
    def create_instance(cls, buffer_class_name: str, **kwargs) -> 'Buffer':
        """
        Create an instance of the requested buffer class.

        Args:
            buffer_class_name (str): Name of the buffer class.
            kwargs: Parameters for the buffer class.

        Returns:
            Buffer: An instance of the requested buffer class.

        Raises:
            ValueError: If the buffer class is not recognized.
        """
        buffer_classes = {
            "ReplayBuffer": ReplayBuffer,
        }

        if buffer_class_name in buffer_classes:
            return buffer_classes[buffer_class_name](**kwargs)
        else:
            raise ValueError(f"{buffer_class_name} is not a subclass of Buffer")
class ReplayBuffer(Buffer):
    """
    Replay buffer for storing transitions during reinforcement learning.

    Attributes:
        env (EnvWrapper): The environment wrapper associated with the buffer.
        buffer_size (int): Maximum size of the buffer.
        goal_shape (Optional[tuple]): Shape of goals (if used).
        device (str): Device to store the buffer ('cpu' or 'cuda').
    """
    def __init__(
        self,
        env: EnvWrapper,
        buffer_size: int = 100000,
        goal_shape: Optional[Tuple[int]] = None,
        device: Optional[str] = None,
    ):
        """
        Initialize the ReplayBuffer.

        Args:
            env (EnvWrapper): The environment wrapper object.
            buffer_size (int): Maximum size of the buffer.
            goal_shape (Optional[tuple]): Shape of goals, if applicable.
            device (Optional[str]): Device to store buffer data ('cpu' or 'cuda').
        """
        super().__init__(env, buffer_size, device)
        self.goal_shape = goal_shape
        
        # Determine observation space shape
        if isinstance(self.env.single_observation_space, gym.spaces.Dict):
            self._obs_space_shape = self.env.single_observation_space['observation'].shape
        else:
            self._obs_space_shape = self.env.single_observation_space.shape

        #DEBUG
        shape = (buffer_size,) + self._obs_space_shape
        print(f"shape: {shape}")

        self.states = T.zeros(shape, dtype=T.float32, device=self.device)
        self.actions = T.zeros((buffer_size, *self.env.single_action_space.shape), dtype=T.float32, device=self.device)
        self.rewards = T.zeros((buffer_size,), dtype=T.float32, device=self.device)
        self.next_states = T.zeros(shape, dtype=T.float32, device=self.device)
        self.dones = T.zeros((buffer_size,), dtype=T.int8, device=self.device)
        
        if self.goal_shape is not None:
            self.desired_goals = T.zeros((buffer_size, *self.goal_shape), dtype=T.float32, device=self.device)
            self.state_achieved_goals = T.zeros((buffer_size, *self.goal_shape), dtype=T.float32, device=self.device)
            self.next_state_achieved_goals = T.zeros((buffer_size, *self.goal_shape), dtype=T.float32, device=self.device)
        
        self.counter = 0
        self.gen = np.random.default_rng()

    def reset(self) -> None:
        """
        Reset the buffer to all zeros and the counter to zero.
        """
        self.states.zero_()
        self.actions.zero_()
        self.rewards.zero_()
        self.next_states.zero_()
        self.dones.zero_()
        self.counter = 0
        
        if self.goal_shape is not None:
            self.desired_goals.zero_()
            self.state_achieved_goals.zero_()
            self.next_state_achieved_goals.zero_()
        
        
    def add(
        self,
        states: np.ndarray,
        actions: np.ndarray,
        rewards: float,
        next_states: np.ndarray,
        dones: bool,
        state_achieved_goals: Optional[np.ndarray] = None,
        next_state_achieved_goals: Optional[np.ndarray] = None,
        desired_goals: Optional[np.ndarray] = None,
    ) -> None:
        """
        Add a transition to the replay buffer.

        Args:
            states (np.ndarray): Current states.
            actions (np.ndarray): Actions taken.
            rewards (float): Rewards received.
            next_states (np.ndarray): Next states.
            dones (bool): Whether the episode is done.
            state_achieved_goals (Optional[np.ndarray]): Achieved goals in the current state.
            next_state_achieved_goals (Optional[np.ndarray]): Achieved goals in the next state.
            desired_goals (Optional[np.ndarray]): Desired goals.
        """
        batch_size = len(states)
        start_idx = self.counter % self.buffer_size
        end_idx = (self.counter + batch_size) % self.buffer_size

        # Compute indices with wrapping
        if end_idx > start_idx:
            indices = np.arange(start_idx, end_idx)
        else:
            indices = np.concatenate([np.arange(start_idx, self.buffer_size), np.arange(0, end_idx)])

        # Convert lists to numpy arrays and then to tensors in one operation
        self.states[indices] = T.tensor(np.array(states), dtype=T.float32, device=self.device)
        self.actions[indices] = T.tensor(np.array(actions), dtype=T.float32, device=self.device)
        self.rewards[indices] = T.tensor(np.array(rewards), dtype=T.float32, device=self.device)
        self.next_states[indices] = T.tensor(np.array(next_states), dtype=T.float32, device=self.device)
        self.dones[indices] = T.tensor(np.array(dones), dtype=T.int8, device=self.device)

        if self.goal_shape is not None:
            if state_achieved_goals is None or next_state_achieved_goals is None or desired_goals is None:
                raise ValueError("Goal data must be provided when using goals")
            self.state_achieved_goals[indices] = T.tensor(np.array(state_achieved_goals), dtype=T.float32, device=self.device)
            self.next_state_achieved_goals[indices] = T.tensor(np.array(next_state_achieved_goals), dtype=T.float32, device=self.device)
            self.desired_goals[indices] = T.tensor(np.array(desired_goals), dtype=T.float32, device=self.device)

        self.counter += batch_size
        
    def sample(self, batch_size: int) -> Tuple[T.Tensor, ...]:
        """
        Sample a batch of transitions from the replay buffer.

        Args:
            batch_size (int): Number of transitions to sample.

        Returns:
            Tuple[T.Tensor, ...]: Sampled transitions.
        """
        size = min(self.counter, self.buffer_size)
        indices = self.gen.integers(0, size, (batch_size,))
        
        if self.goal_shape is not None:
            return (
                self.states[indices],
                self.actions[indices],
                self.rewards[indices],
                self.next_states[indices],
                self.dones[indices],
                self.state_achieved_goals[indices],
                self.next_state_achieved_goals[indices],
                self.desired_goals[indices],
            )
        else:
            return (
                self.states[indices],
                self.actions[indices],
                self.rewards[indices],
                self.next_states[indices],
                self.dones[indices]
            )
    
    def get_config(self) -> Dict[str, Any]:
        """
        Retrieve the configuration of the replay buffer.

        Returns:
            Dict[str, Any]: Configuration details.
        """
        return {
            'class_name': self.__class__.__name__,
            'config': {
                "env": self.env.to_json(),
                "buffer_size": self.buffer_size,
                "goal_shape": self.goal_shape,
                "device": self.device.type,
            }
        }
    
    def clone(self) -> 'ReplayBuffer':
        """
        Clone the replay buffer.

        Returns:
            ReplayBuffer: A new instance of the replay buffer with the same configuration.
        """
        env = build_env_wrapper_obj(self.env.config)
        return ReplayBuffer(env, self.buffer_size, self.goal_shape, self.device)

class PrioritizedReplayBuffer(ReplayBuffer):
    """
    Prioritized Experience Replay buffer that samples transitions based on TD error.
    Supports both proportional and rank-based prioritization strategies.
    All tensor operations happen on the specified device to minimize data transfers.
    """
    def __init__(
        self,
        env: EnvWrapper,
        buffer_size: int = 100_000,
        alpha: float = 0.6,
        beta_start: float = 0.4,
        beta_iter: int = 100_000,
        beta_update_freq: int = 10,
        priority: str = 'proportional',
        goal_shape: Optional[Tuple[int]] = None,
        epsilon: float = 1e-6,
        device: Optional[str] = None
    ):
        if priority not in ['proportional', 'rank']:
            raise ValueError(f"Invalid priority type: {priority} (must be 'proportional' or 'rank')")

        super().__init__(env, buffer_size, goal_shape, device)
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_iter = beta_iter
        self.priority = priority
        self.goal_shape = goal_shape
        self.epsilon = epsilon
        self.beta_update_freq = beta_update_freq
        self.beta = self.beta_start
        self._total_steps = 0
        
        # Add debug tracking
        self.debug_last_td_error = None
        self.debug_last_priority = None
        self.debug_last_indices = None

        if self.priority == "proportional":
            self.sum_tree = SumTree(buffer_size, self.device)
        else:  # rank-based
            self.priorities = T.zeros(buffer_size, dtype=T.float32, device=self.device)
            self.sorted_indices = None
        
        self.counter = 0

    def add(
        self,
        states: np.ndarray,
        actions: np.ndarray,
        rewards: float,
        next_states: np.ndarray,
        dones: bool,
        state_achieved_goals: Optional[np.ndarray] = None,
        next_state_achieved_goals: Optional[np.ndarray] = None,
        desired_goals: Optional[np.ndarray] = None,
    ) -> None:
        batch_size = len(states)
        start_idx = self.counter % self.buffer_size
        end_idx = (self.counter + batch_size) % self.buffer_size

        if end_idx > start_idx:
            indices = T.arange(start_idx, end_idx, device=self.device)
        else:
            indices = T.cat([T.arange(start_idx, self.buffer_size, device=self.device), 
                             T.arange(0, end_idx, device=self.device)])

        # Add to buffer tensors
        self.states[indices] = T.tensor(states, dtype=T.float32, device=self.device)
        self.actions[indices] = T.tensor(actions, dtype=T.float32, device=self.device)
        self.rewards[indices] = T.tensor(rewards, dtype=T.float32, device=self.device)
        self.next_states[indices] = T.tensor(next_states, dtype=T.float32, device=self.device)
        self.dones[indices] = T.tensor(dones, dtype=T.int8, device=self.device)

        if self.goal_shape is not None:
            if state_achieved_goals is None or next_state_achieved_goals is None or desired_goals is None:
                raise ValueError("Goal data must be provided when using goals")
            self.state_achieved_goals[indices] = T.tensor(state_achieved_goals, dtype=T.float32, device=self.device)
            self.next_state_achieved_goals[indices] = T.tensor(next_state_achieved_goals, dtype=T.float32, device=self.device)
            self.desired_goals[indices] = T.tensor(desired_goals, dtype=T.float32, device=self.device)

        # Set initial priorities (will be normalized in update)
        if self.priority == "proportional":
            priorities = T.ones(len(indices), device=self.device) * self.sum_tree.max_priority
            self.sum_tree.update(indices, priorities)
        else:  # rank-based
            self.priorities[indices] = T.ones(len(indices), device=self.device)
            self.sorted_indices = None

        self.counter += batch_size
        self._total_steps += 1

    def update_beta(self) -> None:
        """Anneal beta param more efficiently"""
        progress = min(self._total_steps / self.beta_iter, 1.0)
        self.beta = self.beta_start + progress * (1.0 - self.beta_start)

    def update_priorities(self, indices: T.Tensor, priorities: T.Tensor) -> None:
        """Updates priorities of sampled transitions"""
        if not isinstance(indices, T.Tensor):
            indices = T.tensor(indices, device=self.device)
        
        if not isinstance(priorities, T.Tensor):
            priorities = T.tensor(priorities, device=self.device)
        
        # Store for debugging
        self.debug_last_td_error = priorities.clone()
        self.debug_last_indices = indices.clone()
            
        # Handle NaN and zero values
        # priorities = T.where(
        #     T.isnan(priorities) | (priorities == 0),
        #     T.ones_like(priorities) * self.epsilon,
        #     priorities
        # )
        
        # Apply minimum epsilon and ensure abs values
        priorities = T.clamp(T.abs(priorities), min=self.epsilon)
        
        # Store processed priorities for debugging
        self.debug_last_priority = priorities.clone()
        
        # Debug large TD errors
        if T.max(priorities) > 1e6:
            max_idx = T.argmax(priorities)
            print(f"WARNING: Large TD error detected in update_priorities:")
            print(f"  Index: {indices[max_idx].item()}")
            print(f"  Original TD error: {self.debug_last_td_error[max_idx].item():.2e}")
            print(f"  Processed priority: {priorities[max_idx].item():.2e}")

        if self.priority == "proportional":
            # # Get current priorities from tree for these indices
            # tree_indices = indices + self.sum_tree.capacity - 1
            # current_priorities = self.sum_tree.tree[tree_indices]
            
            # # Scale new priorities relative to current priorities
            # if T.max(current_priorities) > 0:
            #     scale_factor = T.max(current_priorities) / T.max(priorities)
            #     priorities = priorities * scale_factor
            
            # Apply alpha and update the tree
            self.sum_tree.update(indices, priorities ** self.alpha)
        else:  # rank-based
            self.priorities[indices] = priorities
            self.sorted_indices = None

    def _prepare_rank_based(self) -> None:
        """Sorts priorities for rank-based sampling"""

        if self.sorted_indices is None:
            size = min(self.counter, self.buffer_size)
            if size > 0:
                self.sorted_indices = T.argsort(self.priorities[:size], descending=True)
            else:
                self.sorted_indices = T.tensor([], dtype=T.long, device=self.device)

    def sample(self, batch_size: int) -> Tuple[T.Tensor, ...]:
        """Samples a batch of transitions based on priority - optimized version"""

        # Anneal beta
        if self._total_steps % self.beta_update_freq == 0:
            self.update_beta()
            
        size = min(self.counter, self.buffer_size)
        if size == 0:
            raise ValueError("Cannot sample from empty buffer")

        batch_size = min(batch_size, size)
        
        if self.priority == "proportional":
            # Calculate segment boundaries
            # total_priority = self.sum_tree.tree[0].item() if self.sum_tree.tree.numel() > 0 else 0
            total_priority = self.sum_tree.total_priority
            
            if total_priority <= 0:
                # If tree has no meaningful priorities, fall back to uniform sampling
                indices = T.randint(0, size, (batch_size,), device=self.device)
                weights = T.ones(batch_size, device=self.device)
            else:
                # Vectorized stratified sampling
                segment_size = total_priority / batch_size
                segment_boundaries = T.arange(0, batch_size, device=self.device) * segment_size
                random_offsets = T.rand(batch_size, device=self.device) * segment_size
                p_values = segment_boundaries + random_offsets
                
                # Get indices and priorities in a single batch operation
                indices, priorities = self.sum_tree.get(p_values)
                
                # Ensure indices are valid (in case of numerical errors)
                indices = T.clamp(indices, 0, size - 1)
                
                # Calculate importance sampling weights
                probs = priorities.clamp(min=self.epsilon) / total_priority
                weights = (size * probs) ** (-self.beta)
        else:  # rank-based
            # Prepare ranks for sampling
            self._prepare_rank_based()
            
            # Efficient inverse transform sampling
            u = T.rand(batch_size, device=self.device)
            rank_indices = (u ** (1 / self.alpha) * size).long().clamp(max=size-1)
            
            # Get actual indices from sorted indices
            indices = self.sorted_indices[rank_indices]
            
            # Calculate weights directly
            sample_probs = 1 / ((rank_indices + 1).float() ** self.alpha)
            weights = (size * sample_probs) ** (-self.beta)
        
        # Normalize weights
        if weights.numel() > 0:
            max_weight = weights.max()
            if max_weight > 0:  # Prevent division by zero
                weights = weights / max_weight
        
        # Return batch with weights
        if self.goal_shape is not None:
            return (
                self.states[indices],
                self.actions[indices],
                self.rewards[indices],
                self.next_states[indices],
                self.dones[indices],
                self.state_achieved_goals[indices],
                self.next_state_achieved_goals[indices],
                self.desired_goals[indices],
                weights,
                indices
            )
        else:
            return (
                self.states[indices],
                self.actions[indices],
                self.rewards[indices],
                self.next_states[indices],
                self.dones[indices],
                weights,
                indices
            )

    def get_config(self) -> Dict[str, Any]:
        """Get buffer config."""
        return {
            'class_name': self.__class__.__name__,
            'config': {
                "env": self.env.to_json(),
                "buffer_size": self.buffer_size,
                "alpha": self.alpha,
                "beta_start": self.beta_start,
                "beta_iter": self.beta_iter,
                "beta_update_freq": self.beta_update_freq,
                "priority": self.priority,
                "goal_shape": self.goal_shape,
                "epsilon": self.epsilon,
                "device": self.device.type
            }
        }
    
    def clone(self) -> 'PrioritizedReplayBuffer':
        """Create a new instance with the same configuration."""
        env = build_env_wrapper_obj(self.env.config)
        return PrioritizedReplayBuffer(
            env, 
            self.buffer_size, 
            self.alpha, 
            self.beta_start, 
            self.beta_iter,
            self.beta_update_freq,
            self.priority, 
            self.goal_shape, 
            self.device.type, 
            self.epsilon,
        )

#TODO Dont think shared replay buffer is needed.  If needed, update to EnvWrapper
class SharedReplayBuffer(Buffer):
    def __init__(self, env:gym.Env, buffer_size:int=100000, goal_shape:tuple=None, device='cpu'):
        self.env = env
        self.buffer_size = buffer_size
        self.goal_shape = goal_shape
        self.device = device if device else T.device('cuda' if T.cuda.is_available() else 'cpu')

        # self.lock = manager.Lock()
        self.lock = threading.Lock()

        # set internal attributes
        # get observation space
        if isinstance(self.env.observation_space, gym.spaces.dict.Dict):
            self._obs_space_shape = self.env.observation_space['observation'].shape
        else:
            self._obs_space_shape = self.env.observation_space.shape

        # Create shared data buffers to map ndarrays to
        # States
        # Calculate byte size
        state_byte_size = np.prod(self._obs_space_shape) * np.float32().itemsize
        state_total_bytes = self.buffer_size * state_byte_size
        # Create shared memory block for state and next states
        self.shared_states = shared_memory.SharedMemory(create=True, size=state_total_bytes)
        self.shared_next_states = shared_memory.SharedMemory(create=True, size=state_total_bytes)
        
        # Actions
        # Calculate byte size
        action_byte_size = np.prod(self.env.action_space.shape) * np.float32().itemsize
        action_total_bytes = self.buffer_size * action_byte_size
        # Create Shared memory block for actions
        self.shared_actions = shared_memory.SharedMemory(create=True, size=action_total_bytes)

        # Rewards
        # Calculate byte size
        reward_total_bytes = self.buffer_size * np.float32().itemsize
        self.shared_rewards = shared_memory.SharedMemory(create=True, size=reward_total_bytes)

        # Dones (byte size same as rewards)
        self.shared_dones = shared_memory.SharedMemory(create=True, size=reward_total_bytes)

        # If goal shape provided in constructor, create shared blocks for goals
        if self.goal_shape is not None:
            goal_byte_size = np.prod(self.goal_shape) * np.float32().itemsize
            goal_total_bytes = self.buffer_size * goal_byte_size
            # Create shared memory blocks for desired, state, and next state goals
            self.shared_desired_goals = shared_memory.SharedMemory(create=True, size=goal_total_bytes)
            self.shared_state_achieved_goals = shared_memory.SharedMemory(create=True, size=goal_total_bytes)
            self.shared_next_state_achieved_goals = shared_memory.SharedMemory(create=True, size=goal_total_bytes)

        # Create ndarrays to store data mapped to memory blocks
        self.states = np.ndarray((buffer_size, *self._obs_space_shape), dtype=np.float32, buffer=self.shared_states.buf)
        self.next_states = np.ndarray((buffer_size, *self._obs_space_shape), dtype=np.float32, buffer=self.shared_next_states.buf)
        self.actions = np.ndarray((buffer_size, *env.action_space.shape), dtype=np.float32, buffer=self.shared_actions.buf)
        self.rewards = np.ndarray((buffer_size,), dtype=np.float32, buffer=self.shared_rewards.buf)
        self.dones = np.ndarray((buffer_size,), dtype=np.int8, buffer=self.shared_dones.buf)
        
        # Fill ndarrays with zeros
        self.states.fill(0)
        self.next_states.fill(0)
        self.actions.fill(0)
        self.rewards.fill(0)
        self.dones.fill(0)

        # If using goals, create ndarrays to store data mapped to memory blocks
        if self.goal_shape is not None:
            self.desired_goals = np.ndarray((buffer_size, *self.goal_shape), dtype=np.float32, buffer=self.shared_desired_goals.buf)
            self.state_achieved_goals = np.ndarray((buffer_size, *self.goal_shape), dtype=np.float32, buffer=self.shared_state_achieved_goals.buf)
            self.next_state_achieved_goals = np.ndarray((buffer_size, *self.goal_shape), dtype=np.float32, buffer=self.shared_next_state_achieved_goals.buf)
            # Fill ndarray with zeros
            self.desired_goals.fill(0)
            self.state_achieved_goals.fill(0)
            self.next_state_achieved_goals.fill(0)


        self.counter = 0
        self.gen = np.random.default_rng()
        
    def add(self, state:np.ndarray, action:np.ndarray, reward:float, next_state:np.ndarray, done:bool,
            state_achieved_goal:np.ndarray=None, next_state_achieved_goal:np.ndarray=None, desired_goal:np.ndarray=None):
        """Add a transition to the replay buffer."""
        with self.lock:
            index = self.counter % self.buffer_size
            self.states[index] = state
            self.actions[index] = action
            self.rewards[index] = reward
            self.next_states[index] = next_state
            self.dones[index] = done
            
            if self.goal_shape is not None:
                if desired_goal is None or state_achieved_goal is None or next_state_achieved_goal is None:
                    raise ValueError("Desired goal, state achieved goal, and next state achieved goal must be provided when use_goals is True.")
                self.state_achieved_goals[index] = state_achieved_goal
                self.next_state_achieved_goals[index] = next_state_achieved_goal
                self.desired_goals[index] = desired_goal
        
            self.counter = self.counter + 1
        
    def sample(self, batch_size:int):
        with self.lock:
            size = min(self.counter, self.buffer_size)
            indices = self.gen.integers(0, size, (batch_size,))
            
            if self.goal_shape is not None:
                return (
                    self.states[indices],
                    self.actions[indices],
                    self.rewards[indices],
                    self.next_states[indices],
                    self.dones[indices],
                    self.state_achieved_goals[indices],
                    self.next_state_achieved_goals[indices],
                    self.desired_goals[indices],
                )
            else:
                return (
                    self.states[indices],
                    self.actions[indices],
                    self.rewards[indices],
                    self.next_states[indices],
                    self.dones[indices]
                )
    
    def get_config(self):
        return {
            'class_name': self.__class__.__name__,
            'config': {
                "env": self.env.spec.id,
                "buffer_size": self.buffer_size,
                "goal_shape": self.goal_shape
            }
        }
    
    def clone(self):
        env = gym.make(self.env.spec)
        return ReplayBuffer(
            env,
            self.buffer_size,
            self.goal_shape,
            self.device
        )
    
    def cleanup(self):
        # Close and unlink shared memory blocks
        try:
            if self.shared_states:
                self.shared_states.unlink()
                self.shared_states.close()
                self.shared_states = None
        except FileNotFoundError as e:
            print(f"Shared states already cleaned up: {e}")
        try:
            if self.shared_next_states:
                self.shared_next_states.unlink()
                self.shared_next_states.close()
                self.shared_next_states = None
        except FileNotFoundError as e:
            print(f"Shared next states already cleaned up: {e}")
        try:
            if self.shared_rewards:
                self.shared_rewards.unlink()
                self.shared_rewards.close()
                self.shared_rewards = None
        except FileNotFoundError as e:
            print(f"Shared rewards already cleaned up: {e}")
        try:
            if self.shared_dones:
                self.shared_dones.unlink()
                self.shared_dones.close()
                self.shared_dones = None
        except FileNotFoundError as e:
            print(f"Shared dones already cleaned up: {e}")
        
        if self.goal_shape is not None:
            try:
                if self.shared_desired_goals:
                    self.shared_desired_goals.unlink()
                    self.shared_desired_goals.close()
                    self.shared_desired_goals = None
            except FileNotFoundError as e:
                print(f"Shared desired goals already cleaned up: {e}")
            try:
                if self.shared_state_achieved_goals:
                    self.shared_state_achieved_goals.unlink()
                    self.shared_state_achieved_goals.close()
                    self.shared_state_achieved_goals = None
            except FileNotFoundError as e:
                print(f"Shared state achieved goals already cleaned up: {e}")
            try:
                if self.shared_next_state_achieved_goals:
                    self.shared_next_state_achieved_goals.unlink()
                    self.shared_next_state_achieved_goals.close()
                    self.shared_next_state_achieved_goals = None
            except FileNotFoundError as e:
                print(f"Shared next state achieved goals already cleaned up: {e}")

        print("SharedNormalizer resources have been cleaned up.")

    def __del__(self):
        self.cleanup()
```

---

## cnn_models.py

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class CNN(nn.Module):
    def __init__(self, layers, env):
        super(CNN, self).__init__()
        self.layer_config = layers
        self.env = env

        # Set the device
        device = T.device("cuda" if T.cuda.is_available() else "cpu")
        self.device = device
        
        # Instantiate empty ModuleDict to hold CNN layers
        self.layers = nn.ModuleDict()

        # set the initial input size
        input_size = self.env.observation_space.shape[-1]
        
        # Build the layers
        for i, layer in enumerate(layers):
            for layer_type, params in layer.items():
                self.layers[f'{layer_type}_{i}'] = self._build_layer(layer_type, params, input_size)
                if layer_type == 'conv':
                    input_size = params['out_channels']

        # Add flatten layer to the end
        self.layers['flatten'] = nn.Flatten()

        # Move the model to the specified device
        self.to(self.device)

    def _build_layer(self, layer_type, params, input_size):
        # set the input size in the params dict
        
        if layer_type == 'conv':
            params['in_channels'] = input_size
            return nn.Conv2d(**params)
        elif layer_type == 'pool':
            return nn.MaxPool2d(**params)
        elif layer_type == 'dropout':
            return nn.Dropout(**params)
        elif layer_type == 'batchnorm':
            params['num_features'] = input_size
            return nn.BatchNorm2d(**params)
        elif layer_type == 'relu':
            return nn.ReLU()
        elif layer_type == 'tanh':
            return nn.Tanh()
        else:
            raise ValueError(f"Unsupported layer type: {layer_type}")

    def forward(self, x):
        for layer in self.layers.values():
            x = layer(x)
        return x
    
    def get_config(self):
        return {
            'layers': self.layer_config,
            'env': self.env.spec.id,
        }
    
# class ResNet50():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_v2.ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape, pooling=pooling)

#     def __call__(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNet101():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_v2.ResNet101V2(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNet152():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_v2.ResNet152V2(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS50(Model):
#     def __init__(self, input_shape, pooling):
#         super().__init__()
#         self.model = resnet_rs.ResNetRS50(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS101():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS101(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS152():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS152(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS200():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS200(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS270():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS270(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS350():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS350(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class ResNetRS420():
#     def __init__(self, input_shape, pooling):
#         self.model = resnet_rs.ResNetRS420(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class VGG16():
#     def __init__(self, input_shape, pooling):
#         self.model = VGG16_base(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def __call__(self, inputs):
#         x = self.model(inputs)
#         return x
    
# class VGG19():
#     def __init__(self, input_shape, pooling):
#         self.model = VGG19_base(include_top=False, weights=None, input_shape=input_shape, pooling=pooling)

#     def call(self, inputs):
#         x = self.model(inputs)
#         return x
```

---

## dash_callbacks.py

```python
import os
from pathlib import Path
import subprocess
import multiprocessing
from multiprocessing import Queue
from queue import Empty
import threading
import time
import json
# import logging
from logging_config import logger
import base64
import dash
from dash import html, dcc, dash_table, ctx
from dash.dependencies import Input, Output, State, MATCH, ALL
from dash.exceptions import PreventUpdate
import dash_bootstrap_components as dbc
import numpy as np
import io
import ast

import plotly.graph_objs as go
import plotly.offline as pyo
import plotly.tools as tls
import plotly.express as px



import gymnasium as gym
import gymnasium_robotics as gym_robo
import wandb
# import tensorflow as tf
import pandas as pd

import layouts
import gym_helper
import dash_utils
# from dash_utils import create_wrappers_list, instantiate_envwrapper_obj
# import models
# from models import StochasticDiscretePolicy, StochasticContinuousPolicy, ValueModel, CriticModel, ActorModel
from models import *
# import rl_agents
from rl_agents import Reinforce, ActorCritic, DDPG, PPO, TD3,init_sweep, HER
import wandb_support
from schedulers import ScheduleWrapper
from adaptive_kl import AdaptiveKL
from buffer import ReplayBuffer


# Create a queue to store the formatted data
formatted_data_queue = Queue()

def fetch_data_process(project, sweep_name, shared_data):
    while True:
        try:
            # print("Fetching data from wandb...")
            metrics_data = wandb_support.get_metrics(project, sweep_name)
            logger.debug(f'fetch_data_process metrics data:{metrics_data}')
            formatted_data = wandb_support.format_metrics(metrics_data)
            logger.debug(f'fetch_data_process formatted data:{formatted_data}')
            shared_data['formatted_data'] = formatted_data
            logger.debug(f"fetch_data_process shared_data[formatted_data]:{shared_data['formatted_data']}")
            time.sleep(10)  # Wait before fetching data again
        except Exception as e:
            logger.error(f"Error in fetch_data_process: {str(e)}", exc_info=True)
            time.sleep(5)


def update_heatmap_process(shared_data, hyperparameters, bins, z_score, reward_threshold):
    print(f'update heatmap process shared data: {shared_data}')
    try:
        if 'formatted_data' in shared_data:
            #DEBUG
            formatted_data = shared_data['formatted_data']
            logger.debug(f'update_heatmap_process: formatted_data:{formatted_data}')
            # Convert the JSON string back to a pandas DataFrame
            # formatted_data = pd.read_json(data, orient='split')
            # print(f'formatted data passed to wandb_support: {formatted_data}')
            matrix_data, bin_ranges = wandb_support.calculate_co_occurrence_matrix(formatted_data, hyperparameters, reward_threshold, bins, z_score)
            logger.debug(f'update_heatmap_process: matrix_data:{matrix_data}')
            logger.debug(f'update_heatmap_process: bin_ranges:{bin_ranges}')
            shared_data['matrix_data'] = matrix_data.to_dict(orient='split')
            logger.debug(f"update_heatmap_process: shared_data[matrix_data]:{shared_data['matrix_data']}")
            shared_data['bin_ranges'] = bin_ranges
            logger.debug(f"update_heatmap_process: shared_data[bin_ranges]:{shared_data['bin_ranges']}")
        # time.sleep(5)  # Wait for 5 seconds before updating the heatmap again
    except Exception as e:
        print(f"Error in update_heatmap_process: {str(e)}")
        # time.sleep(5)

# def run_agent(sweep_id, sweep_config, num_sweeps):
#     # from rl_agents import init_sweep
#     print('run agent fired...')
#     wandb.agent(
#         sweep_id,
#         function=lambda: init_sweep(sweep_config),
#         count=num_sweeps,
#         project=sweep_config["project"],
#     )

def run_agent(sweep_id, sweep_config, num_sweeps):
    def agent_process():
        wandb.agent(
            sweep_id,
            function=lambda: init_sweep(sweep_config),
            count=num_sweeps,
            project=sweep_config["project"],
        )
    # Determine how many agents you can run based on your hardware capabilities
    num_processes = min(num_sweeps, multiprocessing.cpu_count())  # or adjust based on GPU capability
    processes = []
    for _ in range(num_processes):
        p = multiprocessing.Process(target=agent_process)
        p.start()
        processes.append(p)
    for p in processes:
        p.join()


def register_callbacks(app, shared_data):
    @app.callback(
            Output('page-content', 'children'),
            Input('url', 'pathname')
    )
    def display_page(page):
        if page == '/':
            return layouts.home(page)
        elif page == '/build-agent':
            return layouts.build_agent(page)
        elif page == '/train-agent':
            return layouts.train_agent(page)
        elif page == '/test-agent':
            return layouts.test_agent(page)
        elif page == '/hyperparameter-search':
            return layouts.hyperparameter_search(page)
        elif page == '/co-occurrence-analysis':
            return layouts.co_occurrence_analysis(page)
        elif page == '/wandb-utils':
            return layouts.wandb_utils(page)
        # Add additional conditions for other pages
        else:
            return '404'

    @app.callback(
        [Output({'type': 'env-dropdown', 'page': MATCH}, 'style'),
        Output({'type': 'env-dropdown', 'page': MATCH}, 'options'),
        Output({'type': 'env-dropdown', 'page': MATCH}, 'placeholder')],
        Input({'type': 'library-select', 'page': MATCH}, 'value')
    )
    def update_env_dropdown(selected_library):
        if selected_library is None:
            return {'display': 'none'}, [], "Select Environment"
        
        if selected_library == 'gymnasium':
            # List of Gymnasium environments
            gym_envs = dash_utils.get_all_gym_envs()
            return {'display': 'block'}, gym_envs, "Select Gymnasium Environment"
        elif selected_library == 'isaacsim':
            # Placeholder for IsaacSim environments
            isaac_envs = ['IsaacAnt', 'IsaacHumanoid']  # Replace with actual IsaacSim envs
            return {'display': 'block'}, [{'label': env, 'value': env} for env in isaac_envs], "Select IsaacSim Environment"
        
        return {'display': 'none'}, [], "Select Environment"
    
    @app.callback(
        Output('agent-parameters-inputs', 'children'),
        Input({'type':'agent-type-dropdown', 'page':'/build-agent'}, 'value'),
    )
    def update_agent_parameters_inputs(agent_type):
        if agent_type:
            return dash_utils.create_agent_parameter_inputs(agent_type)
        else:
            return "Select a model type to configure its parameters."
        
    @app.callback(
        Output({"type":"wrapper-tabs", "page":MATCH}, "children"),
        Input({"type":"gym_wrappers_dropdown", "page":MATCH}, "value")
    )
    def build_wrapper_tabs(selected_wrappers):
        """
        Create a dcc.Tab for each selected wrapper IF it is in WRAPPER_REGISTRY.
        For wrappers not in the registry, skip creating a tab (no extra params).
        """
        #DEBUG
        print('build_wrapper_tabs called')
        if not selected_wrappers:
            return []

        tabs = []
        for wrapper_name in selected_wrappers:
            # Check if its in the registry
            if wrapper_name in dash_utils.WRAPPER_REGISTRY:
                default_params = dash_utils.WRAPPER_REGISTRY[wrapper_name]["default_params"]

                param_components = []
                for param_key, param_val in default_params.items():
                    label_text = param_key.replace("_", " ").capitalize()

                    input_id = {
                        "type": "wrapper-param",
                        "wrapper": wrapper_name,
                        "param": param_key
                    }

                    # Decide how to represent the input
                    if isinstance(param_val, bool):
                        input_comp = dcc.Dropdown(
                            id=input_id,
                            options=[{"label": "True", "value": True}, {"label": "False", "value": False}],
                            value=param_val,
                            clearable=False,
                            style={"width": "100px"}
                        )
                    elif isinstance(param_val, int) or isinstance(param_val, float):
                        input_comp = dcc.Input(
                            id=input_id,
                            type="number",
                            value=param_val
                        )
                    else:
                        # Fallback is text
                        input_comp = dcc.Input(
                            id=input_id,
                            type="text",
                            value=str(param_val)
                        )

                    param_components.append(
                        html.Div([
                            html.Label(label_text),
                            input_comp
                        ], style={"marginBottom": "10px"})
                    )

                tabs.append(
                    dcc.Tab(
                        label=wrapper_name,
                        children=html.Div(
                            children=param_components,
                            style={"marginLeft": "20px"}
                            )
                    )
                )

        return tabs

    @app.callback(
        Output({"type":"wrappers_params_store", "page":MATCH}, "data"),
        Input({"type": "wrapper-param", "wrapper": ALL, "param": ALL}, "value"),
        State({"type": "wrapper-param", "wrapper": ALL, "param": ALL}, "id"),
        State({"type":"wrappers_params_store", "page":MATCH}, "data"),
        # prevent_initial_call=True
    )
    def update_wrapper_params(all_values, all_ids, store):
        """
        When any param input changes in a registry wrapper tab,
        store that new value in wrappers_params_store.
        """
        #DEBUG
        print('update_wrapper_params called')
        print(f'all values:{all_values}')
        print(f'all ids:{all_ids}')
        # triggered_id = ctx.triggered_id
        # if not triggered_id:
        #     raise PreventUpdate

        # # triggered_id has structure:
        # # { "type": "wrapper-param", "wrapper": "<wrapper_name>", "param": "<param_key>" }
        # w_name = triggered_id["wrapper"]
        # p_name = triggered_id["param"]

        # store_data = dict(current_store) if current_store else {}
        # if w_name not in store_data:
        #     store_data[w_name] = {}

        # # all_values is typically a list of a single item
        # new_val = all_values[0] if isinstance(all_values, list) else all_values
        # store_data[w_name][p_name] = new_val

        # #DEBUG
        # print(f'new wrapper params:{store_data}')

        # return store_data
         # Initialize the store if it's None
        store = store or {}

        # Update the store with indexed components
        for value, id_dict in zip(all_values, all_ids):
            if value is not None:
                # Convert the id_dict into a JSON-serializable string using underscores
                key = "_".join(f"{k}:{v}" for k, v in id_dict.items() if k != 'type')
                store[key] = value

        # DEBUG
        # print(f"Updated wrapper params store: {store}")
        return store

    # Callback to add a new layer dropdown
    @app.callback(
        Output({"type": "layer-dropdowns", "model": MATCH, "agent": MATCH}, "children"),
        [Input({"type": "add-layer-btn", "model": MATCH, "agent": MATCH}, "n_clicks")],
        [State({"type": "layer-dropdowns", "model": MATCH, "agent": MATCH}, "children"),
        State({"type": "add-layer-btn", "model": MATCH, "agent": MATCH}, "id"),
        State("agent-params-store", "data")]
    )
    def add_layer_dropdown(n_clicks, children, btn_id, agent_params_store):
        if children is None:
            children = []

        if n_clicks > 0:
            # Use `btn_id` to infer the `model` and `agent` dynamically
            model = btn_id["model"]
            agent = btn_id["agent"]

            dropdown_id = {
                'type': 'layer-type-dropdown',
                'model': model,
                'agent': agent,
                'index': n_clicks
            }

            stored_value = agent_params_store.get(dash_utils.get_key(dropdown_id), None)

            new_dropdown = html.Div(
                id={
                    "type": f"layer-container-{n_clicks}",
                    "model": model,
                    "agent": agent,
                },
                children=[
                    html.Label(f'Layer {n_clicks}', style={'text-decoration': 'underline'}),
                    dcc.Dropdown(
                        id=dropdown_id,
                        options=[
                            {"label": "TransformerEncoderLayer", "value": "transformer_encoder_layer"},
                            {"label": "Dense", "value": "dense"},
                            {"label": "Conv2D", "value": "conv2d"},
                            {"label": "MaxPool2D", "value": "maxpool2d"},
                            {"label": "Dropout", "value": "dropout"},
                            {"label": "BatchNorm2D", "value": "batchnorm2d"},
                            {"label": "LayerNorm", "value": "layernorm"},
                            {"label": "Flatten", "value": "flatten"},
                            {"label": "ReLU", "value": "relu"},
                            {"label": "LeakyReLU", "value": "leakyrelu"},
                            {"label": "Tanh", "value": "tanh"}
                        ],
                        placeholder="Select Layer Type",
                        clearable=False,
                        value=stored_value
                    ),
                    html.Div(
                        id={
                            'type': 'layer-params',
                            'model': model,
                            'agent': agent,
                            'index': n_clicks
                        }
                    )
                ],
                style={'margin-top': '10px', 'margin-left': '20px'}
            )
            children.append(new_dropdown)

        return children
        
    # Callback to display parameters based on layer type selection
    @app.callback(
        Output({'type': 'layer-params', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'children'),
        Input({'type': 'layer-type-dropdown', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value'),
        [State({'type': 'layer-type-dropdown', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'id'),
         State("agent-params-store", "data")
        ],
        prevent_initial_call=True,
    )
    def display_layer_parameters(layer_type, ids, agent_params_store):
        children = []
        params = []
        
        # Generate the key for the `agent_params_store`
        # layer_key = "_".join(f"{k}:{v}" for k, v in ids.items())

        if layer_type == "dense":
            params = [
                dcc.Input(
                    id={
                        'type': 'num-units',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Number of Units',
                    value=agent_params_store.get(dash_utils.get_key(ids, "num-units"), None)
                ),
                dash_utils.create_kernel_input(ids['agent'], ids['model'], ids['index'], agent_params_store),
                dcc.Dropdown(
                    id={
                        'type': 'bias',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    options=[{"label": "True", "value": True}, {"label": "False", "value": False}],
                    placeholder="Bias",
                    value=agent_params_store.get(dash_utils.get_key(ids, "bias"), None)
                )
            ]
        elif layer_type == "conv2d":
            params = [
                dcc.Input(
                    id={
                        'type': 'out-channels',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Out Channels',
                    value=agent_params_store.get(dash_utils.get_key(ids, "out-channels"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'kernel-size',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Kernel Size',
                    value=agent_params_store.get(dash_utils.get_key(ids, "kernel-size"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'stride',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Stride',
                    value=agent_params_store.get(dash_utils.get_key(ids, "stride"), None)
                ),
                dcc.Dropdown(
                    id={
                        'type': 'padding-dropdown',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']},

                    options=[{"label": "Valid", "value": "valid"}, 
                            {"label": "Same", "value": "same"}, 
                            {"label": "Custom", "value": "custom"}
                    ], 
                    placeholder="Padding",
                    value=agent_params_store.get(dash_utils.get_key(ids, "padding-dropdown"), None)
                ),
                html.Div(
                    dcc.Input(
                        id={
                            'type': 'custom-padding',
                            'model': ids['model'],
                            'agent': ids['agent'],
                            'index': ids['index']
                        },
                        type='number',
                        placeholder='Custom Padding',
                        value=agent_params_store.get(dash_utils.get_key(ids, "custom-padding"), None)
                    ),
                    style={'display': 'none'}
                ),
                dash_utils.create_kernel_input(ids['agent'], ids['model'], ids['index'], agent_params_store),
                dcc.Dropdown(
                    id={
                        'type': 'bias',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    options=[{"label": "True", "value": True}, {"label": "False", "value": False}],
                    placeholder="Bias",
                    value=agent_params_store.get(dash_utils.get_key(ids, "bias"), None)
                )
            ]
        elif layer_type == "batchnorm2d":
            params = [
                dcc.Input(
                    id={
                        'type': 'num-features',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Num Features',
                    value=agent_params_store.get(dash_utils.get_key(ids, "num-features"), None)
                )
            ]
        
        elif layer_type == "layernorm":
            params = [
                dcc.Input(
                    id={
                        'type': 'normalized-shape',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Normalized Shape',
                    value=agent_params_store.get(dash_utils.get_key(ids, "normalized-shape"), None)
                )
            ]
        
        elif layer_type == "maxpool2d":
            params = [
                dcc.Input(
                    id={
                        'type': 'kernel-size',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Kernel Size',
                    value=agent_params_store.get(dash_utils.get_key(ids, "kernel-size"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'stride',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Stride',
                    value=agent_params_store.get(dash_utils.get_key(ids, "stride"), None)
                )
            ]
        
        elif layer_type == "dropout":
            params = [
                dcc.Input(
                    id={
                        'type': 'dropout-prob',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Dropout Probability',
                    min=0.1,
                    max=0.9,
                    step=0.1,
                    value=agent_params_store.get(dash_utils.get_key(ids, "dropout-prob"), None)
                )
            ]
        
        elif layer_type == "transformer_encoder_layer":
            params = [
                dcc.Input(
                    id={
                        'type': 'd-model',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Model Dimension (d_model)',
                    value=agent_params_store.get(dash_utils.get_key(ids, "d-model"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'nhead',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Number of Attention Heads (nhead)',
                    value=agent_params_store.get(dash_utils.get_key(ids, "nhead"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'dim-feedforward',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    }, 
                    type='number',
                    placeholder='Feedforward Dimension (dim_feedforward)',
                    value=agent_params_store.get(dash_utils.get_key(ids, "dim-feedforward"), None)
                ),
                dcc.Input(
                    id={
                        'type': 'dropout',
                        'model': ids['model'],
                        'agent': ids['agent'],
                        'index': ids['index']
                    },
                    type='number',
                    placeholder='Dropout',
                    min=0,
                    max=1,
                    step=0.1,
                    value=agent_params_store.get(dash_utils.get_key(ids, "dropout"), None)
                )
            ]
        
        # Wrap parameters with indentation style
        children.append(html.Div(params, style={'margin-top': '10px', 'margin-left': '20px'}))
        
        return children
        # return html.Div(params, style={'margin-top': '10px', 'margin-left': '20px'})
    
    # # Callback to store values when dropdown or input changes
    # @app.callback(
    #     Output({'type': 'layer-values-store', 'model': MATCH, 'agent': MATCH}, "data"),
    #     [Input({'type': ALL, 'model': MATCH, 'agent': MATCH, 'index': ALL}, "value")],
    #     [State({'type': 'layer-values-store', 'model': MATCH, 'agent': MATCH}, "data"),
    #     State({'type': ALL, 'model': MATCH, 'agent': MATCH, 'index': ALL}, "id")]
    # )
    # def store_layer_values(values, stored_values, ids):
    #     # Initialize stored_values if it's None
    #     # stored_values = stored_values[0]

    #     # Loop through ids and values together
    #     for id_dict, value in zip(ids, values):
    #         if value is not None:
    #             # Create a unique key using the 'id' dictionary directly
    #             key = "-".join(f"{k}:{v}" for k, v in id_dict.items())
    #             stored_values[key] = value
    #     #DEBUG
    #     print(f'layer value store:{stored_values}')

    #     return stored_values

    @app.callback(
        Output("agent-params-store", 'data'),
        [
            Input({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'value'),
            Input({'type': ALL, 'model': ALL, 'agent': ALL}, 'value')
        ],
        [
            State("agent-params-store", 'data'),
            State({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'id'),
            State({'type': ALL, 'model': ALL, 'agent': ALL}, 'id')
        ]
    )
    def update_agent_params(indexed_values, non_indexed_values, store, indexed_ids, non_indexed_ids):
        # Initialize the store if it's None
        store = store or {}

        # Update the store with indexed components
        for value, id_dict in zip(indexed_values, indexed_ids):
            if value is not None:
                # Convert the id_dict into a JSON-serializable string using underscores
                key = "_".join(f"{k}:{v}" for k, v in id_dict.items())
                store[key] = value

        # Update the store with non-indexed components
        for value, id_dict in zip(non_indexed_values, non_indexed_ids):
            if value is not None:
                # Convert the id_dict into a JSON-serializable string using underscores
                key = "_".join(f"{k}:{v}" for k, v in id_dict.items())
                store[key] = value

        # DEBUG
        print(f"Updated agent params store: {store}")
        return store
    
    @app.callback(
        Output({"type":"run-params-store", "page":MATCH}, 'data'),
        Input({'type':ALL, 'page':MATCH}, 'value'),
        [
            State({"type":"run-params-store", "page":MATCH}, 'data'),
            State({'type':ALL, 'page':MATCH}, 'id')
        ]
    )
    def update_run_params(values, store, ids):
        # Initialize the store if it's None
        store = store or {}

        # Update the store with non-indexed components
        for value, id_dict in zip(values, ids):
            if value is not None:
                # Convert the id_dict into a JSON-serializable string using underscores
                key = "_".join(f"{k}:{v}" for k, v in id_dict.items())
                store[key] = value

        # DEBUG
        # print(f"Updated run params store: {store}")
        return store

    # Callback to toggle visibility of custom padding input for Conv2D layers
    @app.callback(
        Output({'type': 'custom-padding', 'model': ALL, 'agent': ALL, 'index': ALL}, 'style'),
        Input({'type': 'padding-dropdown', 'model': ALL, 'agent': ALL, 'index': ALL}, 'value')
    )
    def toggle_custom_padding(selected_padding):
        # Show custom padding input only if 'custom' is selected
        styles = [{'display': 'block' if padding == 'custom' else 'none'} for padding in selected_padding]
        return styles
    
    @app.callback(
        Output({'type':'optimizer-options', 'model':MATCH, 'agent':MATCH}, 'children'),
        Input({'type':'optimizer', 'model':MATCH, 'agent':MATCH}, 'value'),
        State({'type':'optimizer-options', 'model':MATCH, 'agent':MATCH}, 'id'),
        prevent_initial_call=True,
    )
    def update_agent_optimizer_params(optimizer, optimizer_id):
        agent_type = optimizer_id['agent']
        model_type = optimizer_id['model']
        return dash_utils.create_optimizer_params_input(agent_type, model_type, optimizer)

        
#     @app.callback(
#     Output({'type': 'units-per-layer', 'model': MATCH, 'agent': MATCH}, 'children'),
#     Input({'type': 'dense-layers', 'model': MATCH, 'agent': MATCH}, 'value'),
#     State({'type': 'dense-layers', 'model': MATCH, 'agent': MATCH}, 'id'),
# )
#     def update_units_per_layer_inputs(num_layers, id):
#         if num_layers is not None:
#             model_type = id['model']
#             agent_type = id['agent']
#             inputs = []
#             for i in range(1, num_layers + 1):
#                 input_id = {
#                     'type': 'layer-units',
#                     'model': model_type,
#                     'agent': agent_type,
#                     'index': i,
#                 }
#                 inputs.append(html.Div([
#                     html.Label(f'Neurons in Hidden Layer {i}', style={'text-decoration': 'underline'}),
#                     dcc.Input(
#                         id=input_id,
#                         type='number',
#                         min=1,
#                         max=1024,
#                         step=1,
#                         value=512,
#                     ),
#                 ]))

#             return inputs
        
        
#     @app.callback(
#     Output({'type': 'layer-types', 'model': MATCH, 'agent': MATCH}, 'children'),
#     Input({'type': 'conv-layers', 'model': MATCH, 'agent': MATCH}, 'value'),
#     State({'type': 'conv-layers', 'model': MATCH, 'agent': MATCH}, 'id'),
# )
#     def update_cnn_layer_type_inputs(num_layers, id):
#         if num_layers is not None:
#             model_type = id['model']
#             agent_type = id['agent']

#             layer_types = []
#             for i in range(1, num_layers + 1):
#                 input_id = {
#                     'type': 'cnn-layer-type',
#                     'model': model_type,
#                     'agent': agent_type,
#                     'index': i,
#                 }
#                 layer_types.append(html.Div([
#                     html.Label(f'Layer Type for Conv Layer {i}', style={'text-decoration': 'underline'}),
#                     dcc.Dropdown(
#                         id=input_id,
#                         options=[
#                             {'label': 'Conv2D', 'value': 'conv'},
#                             {'label': 'MaxPool2D', 'value': 'pool'},
#                             {'label': 'Dropout', 'value': 'dropout'},
#                             {'label': 'BatchNorm2D', 'value': 'batchnorm'},
#                             {'label': 'Relu', 'value':'relu'},
#                             {'label': 'Tanh', 'value': 'tanh'},
#                         ]
#                     ),
#                     html.Div(
#                         id={
#                             'type': 'cnn-layer-type-parameters',
#                             'model': model_type,
#                             'agent': agent_type,
#                             'index': i,
#                         },
#                     )
#                     ])
#                 )

#             return layer_types
        
    
    # @app.callback(
    # Output({'type': 'conv-padding-custom-container', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'style'),
    # Input({'type': 'conv-padding', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value')
    # )
    # def show_hide_custom_padding(padding_value):
    #     if padding_value == 'custom':
    #         return {'display': 'block'}
    #     else:
    #         return {'display': 'none'}
        
    
    # @app.callback(
    # Output({'type': 'conv-padding-custom-container-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'style'),
    # Input({'type': 'conv-padding-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value')
    # )
    # def show_hide_custom_padding_hyperparams(padding_value):
    #     if padding_value == 'custom':
    #         return {'display': 'block'}
    #     else:
    #         return {'display': 'none'}
        
        
    # @app.callback(
    # Output({'type': 'cnn-layer-type-parameters', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'children'),
    # Input({'type': 'cnn-layer-type', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value'),
    # State({'type': 'cnn-layer-type', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'id'),
    # )
    # def update_layer_type_params(layer_type, id):
    #     if layer_type is not None:
    #         model_type = id['model']
    #         agent = id['agent']
    #         index = id['index']

    #         # loop over layer types to create the appropriate parameters
    #         if layer_type == 'conv':
    #             return html.Div([
    #                 html.Label(f'Filters in Conv Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'conv-filters',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=1024,
    #                     step=1,
    #                     value=32,
    #                 ),
    #                 html.Label(f'Kernel Size in Conv Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'conv-kernel-size',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=10,
    #                     step=1,
    #                     value=3,
    #                 ),
    #                 html.Label(f'Kernel Stride in Conv Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'conv-stride',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=10,
    #                     step=1,
    #                     value=3,
    #                 ),
    #                 html.Label(f'Input Padding in Conv Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.RadioItems(
    #                     id={
    #                         'type': 'conv-padding',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     options=[
    #                         {'label': 'Same', 'value': 'same'},
    #                         {'label': 'Valid', 'value': 'valid'},
    #                         {'label': 'Custom', 'value': 'custom'},
    #                     ],
    #                     value='same',  # Default value
    #                 ),
    #                 html.Div(
    #                     [
    #                         html.Label('Custom Padding (pixels)', style={'text-decoration': 'underline'}),
    #                         dcc.Input(
    #                             id={
    #                                 'type': 'conv-padding-custom',
    #                                 'model': model_type,
    #                                 'agent': agent,
    #                                 'index': index,
    #                             },
    #                             type='number',
    #                             min=0,
    #                             max=10,
    #                             step=1,
    #                             value=1,
    #                         ),
    #                     ],
    #                     id={
    #                         'type': 'conv-padding-custom-container',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     style={'display': 'none'},  # Hide initially
    #                 ),
    #                 dcc.Checklist(
    #                     id={
    #                         'type': 'conv-use-bias',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     options=[
    #                         {'label': 'Use Bias', 'value': True},
    #                     ]
    #                 )
    #             ])
    #         if layer_type == 'pool':
    #             return html.Div([
    #                 html.Label(f'Kernel Size of Pooling Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'pool-kernel-size',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=10,
    #                     step=1,
    #                     value=3,
    #                 ),
    #                 html.Label(f'Kernel Stride in Pooling Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'pool-stride',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=10,
    #                     step=1,
    #                     value=3,
    #                 ),
    #             ])
    #         if layer_type == 'batchnorm':
    #             return html.Div([
    #                 html.Label(f'Number of Features for BatchNorm Layer {index} (set to number of input channels)', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'batch-features',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=1,
    #                     max=1024,
    #                     step=1,
    #                     value=32,
    #                 ),
    #             ])
    #         if layer_type == 'dropout':
    #             return html.Div([
    #                 html.Label(f'Probability of Zero-ed Element for Dropout Layer {index}', style={'text-decoration': 'underline'}),
    #                 dcc.Input(
    #                     id={
    #                         'type': 'dropout-prob',
    #                         'model': model_type,
    #                         'agent': agent,
    #                         'index': index,
    #                     },
    #                     type='number',
    #                     min=0.0,
    #                     max=1.0,
    #                     step=0.1,
    #                     value=0.5,
    #                 ),
    #             ])
    
    
    @app.callback(
        Output('actor-critic-config-container', 'children'),
        Input({'type':'agent-type-dropdown', 'page':'/build-agent'}, 'value')
    )
    def update_actor_critic_parameters_inputs(agent_type):
        """Adds additional trace decay options for actor critic model"""
        if agent_type == 'Actor Critic':
            return html.Div([
                html.H3("Actor Critic Configuration"),
                dcc.Input(
                    id={
                        'type':'trace-decay',
                        'model':'policy',
                        'agent':agent_type,
                    },
                    type='number',
                    placeholder="Policy Trace Decay",
                    min=0.0,
                    max=1.0,
                    step=0.01
                ),
                dcc.Input(
                    id={
                        'type':'trace-decay',
                        'model':'value',
                        'agent':agent_type,
                    },
                    type='number',
                    placeholder="Value Trace Decay",
                    min=0.0,
                    max=1.0,
                    step=0.01
                ),
            ])
        else:
            return None
        
    @app.callback(
        Output({'type': 'noise-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'noise-function', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'noise-function', 'model': MATCH, 'agent': MATCH}, 'id'),
    )
    def update_noise_inputs(noise_type, id):
        if noise_type is not None:
            inputs = dash_utils.update_noise_inputs(noise_type, id)
            return inputs
        else:
            return None
        
    # # Callback that updates the placeholder div based on the selected kernel initializer
    # @app.callback(
    #     Output({'type': 'kernel-initializer-options', 'model': MATCH, 'agent': MATCH}, 'children'),
    #     Input({'type': 'kernel-function', 'model': MATCH, 'agent': MATCH}, 'value'),
    #     State({'type': 'kernel-function', 'model': MATCH, 'agent': MATCH}, 'id'),
    #     prevent_initial_call=True
    # )
    # def update_kernel_initializer_options(selected_initializer, initializer_id):
    #     # Use the utility function to get the initializer inputs
    #     return utils.get_kernel_initializer_inputs(selected_initializer, initializer_id)
        
    # Callback to display kernel initializer parameters based on the selected kernel type
    @app.callback(
        Output({'type': 'kernel-params', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'children'),
        Input({'type': 'kernel-init', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value'),
        [State({'type': 'kernel-init', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'id'),
         State("agent-params-store", "data")],
        prevent_initial_call=True
    )
    def display_kernel_parameters(selected_kernel, dropdown_id, agent_params):
        # children = []
        # for (kernel, ids) in zip(selected_kernels, dropdown_ids):
            # params = []
            # if kernel in ["kaiming_normal", "kaiming_uniform"]:
            #     params = [
            #         dcc.Dropdown(
            #             id={
            #                 'type': 'mode',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             options=[
            #                 {"label": "fan in", "value": "fan_in"},
            #                 {"label": "fan out", "value": "fan_out"}
            #             ],
            #             placeholder="Mode",
            #             value=agent_params.get(dash_utils.get_key(ids, 'mode'), None)
            #         )
            #     ]
            # elif kernel in ["xavier_normal", "xavier_uniform"]:
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'gain',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Gain',
            #             min=1.0,
            #             max=3.0,
            #             step=0.1,
            #             value=agent_params.get(dash_utils.get_key(ids, 'gain'), None)
            #         )
            #     ]
            # elif kernel == "truncated_normal":
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'mean',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Mean',
            #             min=0.01,
            #             max=0.99,
            #             step=0.01,
            #             value=agent_params.get(dash_utils.get_key(ids, 'mean'), None)
            #         ),
            #         dcc.Input(
            #             id={
            #                 'type': 'std-dev',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Standard Deviation',
            #             min=0.1,
            #             max=3.0,
            #             step=0.1,
            #             value=agent_params.get(dash_utils.get_key(ids, 'std-dev'), None)
            #         )
            #     ]
            # elif kernel == "uniform":
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'min',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Minimum',
            #             min=0.01,
            #             max=1.0,
            #             step=0.01,
            #             value=agent_params.get(dash_utils.get_key(ids, 'min'), None)
            #         ),
            #         dcc.Input(
            #             id={
            #                 'type': 'max',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Maximum',
            #             min=0.01,
            #             max=1.0,
            #             step=0.01,
            #             value=agent_params.get(dash_utils.get_key(ids, 'max'), None)
            #         )
            #     ]
            # elif kernel == "normal":
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'mean',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             }, 
            #             type='number',
            #             placeholder='Mean',
            #             min=0.01,
            #             max=0.99,
            #             step=0.01,
            #             value=agent_params.get(dash_utils.get_key(ids, 'mean'), None)
            #         ),
            #         dcc.Input(
            #             id={
            #                 'type': 'std-dev',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Standard Deviation',
            #             min=0.1,
            #             max=3.0,
            #             step=0.1,
            #             value=agent_params.get(dash_utils.get_key(ids, 'std-dev'), None)
            #         )
            #     ]
            # elif kernel == "constant":
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'value',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Value',
            #             min=0.01,
            #             max=1.0,
            #             step=0.01,
            #             value=agent_params.get(dash_utils.get_key(ids, 'value'), None)
            #         )
            #     ]
            # elif kernel == "variance_scaling":
            #     params = [
            #         dcc.Input(
            #             id={
            #                 'type': 'scale',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             type='number',
            #             placeholder='Scale',
            #             min=1.0,
            #             max=5.0,
            #             step=0.1,
            #             value=agent_params.get(dash_utils.get_key(ids, 'scale'), None)
            #         ),
            #         dcc.Dropdown(
            #             id={
            #                 'type': 'mode',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             options=[
            #                 {"label": "fan in", "value": "fan_in"},
            #                 {"label": "fan out", "value": "fan_out"},
            #                 {"label": "fan avg", "value": "fan_avg"}
            #             ],
            #             placeholder="Mode",
            #             value=agent_params.get(dash_utils.get_key(ids, 'mode'), None)
            #         ),
            #         dcc.Dropdown(
            #             id={
            #                 'type': 'distribution',
            #                 'model': ids['model'],
            #                 'agent': ids['agent'],
            #                 'index': ids['index'],
            #             },
            #             options=[
            #                 {"label": "truncated normal", "value": "truncated_normal"},
            #                 {"label": "uniform", "value": "uniform"}
            #             ],
            #             placeholder="Distribution",
            #             value=agent_params.get(dash_utils.get_key(ids, 'distribution'), None)
            #         )
            #     ]

            # children.append(html.Div(params, style={'margin-top': '10px', 'margin-bottom': '10px', 'margin-left': '20px'}))
        
        return dash_utils.get_kernel_initializer_inputs(selected_kernel, dropdown_id, agent_params)
    
    @app.callback(
        Output({'type': 'lr-scheduler-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'lr-scheduler', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'lr-scheduler', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_learning_rate_scheduler_options(lr_scheduler, lr_scheduler_id):
        agent_type = lr_scheduler_id['agent']
        model_type = lr_scheduler_id['model']
        return dash_utils.update_scheduler_options('lr', agent_type, model_type, lr_scheduler)

    @app.callback(
        Output({'type': 'noise-scheduler-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'noise-scheduler', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'noise-scheduler', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_noise_scheduler_options(noise_scheduler, noise_scheduler_id):
        agent_type = noise_scheduler_id['agent']
        model_type = noise_scheduler_id['model']
        return dash_utils.update_scheduler_options('noise', agent_type, model_type, noise_scheduler)

    @app.callback(
        Output({'type': 'entropy-scheduler-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'entropy-scheduler', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'entropy-scheduler', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_entropy_scheduler_options(entropy_scheduler, entropy_scheduler_id):
        agent_type = entropy_scheduler_id['agent']
        model_type = entropy_scheduler_id['model']
        return dash_utils.update_scheduler_options('entropy', agent_type, model_type, entropy_scheduler)
    
    @app.callback(
        Output({'type': 'surrogate-clip-scheduler-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'surrogate-clip-scheduler', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'surrogate-clip-scheduler', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_surrogate_loss_clip_scheduler_options(surrogate_clip_scheduler, surrogate_clip_scheduler_id):
        agent_type = surrogate_clip_scheduler_id['agent']
        model_type = surrogate_clip_scheduler_id['model']
        return dash_utils.update_scheduler_options('surrogate-clip', agent_type, model_type, surrogate_clip_scheduler)
    
    # @app.callback(
    #     Output({'type': 'value-clip-scheduler-options', 'model': MATCH, 'agent': MATCH}, 'children'),
    #     Input({'type': 'value-clip-scheduler', 'model': MATCH, 'agent': MATCH}, 'value'),
    #     State({'type': 'value-clip-scheduler', 'model': MATCH, 'agent': MATCH}, 'id'),
    #     prevent_initial_call=True
    # )
    # def update_value_clip_scheduler_options(value_clip_scheduler, value_clip_scheduler_id):
    #     agent_type = value_clip_scheduler_id['agent']
    #     model_type = value_clip_scheduler_id['model']
    #     return dash_utils.update_value_clip_scheduler_options(agent_type, model_type, value_clip_scheduler)
    
    @app.callback(
        Output({'type': 'adaptive-kl-block', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'adaptive-kl', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'adaptive-kl', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_adaptive_kl_options(use_adaptive_kl, adaptive_kl):
        agent_type = adaptive_kl['agent']
        if use_adaptive_kl:
            return dash_utils.create_adaptive_kl_options(agent_type)
    
    @app.callback(
        Output({'type': 'goal-strategy-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'goal-strategy', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'goal-strategy', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_goal_strategy_options(strategy, strategy_id):
        agent_type = strategy_id['agent']
        return dash_utils.update_goal_strategy_options(agent_type, strategy)
    
    @app.callback(
        Output({'type': 'replay-buffer-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'replay-buffer', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'replay-buffer', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_replay_buffer_options(replay_buffer, replay_buffer_id):
        agent_type = replay_buffer_id['agent']
        return dash_utils.update_replay_buffer_options(agent_type, replay_buffer)
    
    
    @app.callback(
        Output({'type': 'goal-strategy-options-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'goal-strategy-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'goal-strategy-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_goal_strategy_hyperparam_options(strategy, strategy_id):
        agent_type = strategy_id['agent']
        options = []
        for strat in strategy:
            options.append(dash_utils.update_goal_strategy_hyperparam_options(agent_type, strat))
        
        return options
    
    @app.callback(
        Output({'type': 'normalize-options', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'normalize-input', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'normalize-input', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_normalize_options(normalize, normalize_id):
        if normalize == 'True':
            agent_type = normalize_id['agent']
            # if agent_type == 'DDPG':
            return dash_utils.create_input_normalizer_options_input(agent_type)
        return html.Div()
    
    @app.callback(
        Output({'type': 'normalize-options-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'normalize-input-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'normalize-input-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_normalize_hyperparam_options(normalize, normalize_id):
        for norm in normalize:
            if norm == 'True':
                agent_type = normalize_id['agent']
                model_type = normalize_id['model']
                # if agent_type == 'DDPG':
                return dash_utils.create_input_normalizer_options_hyperparam_input(agent_type, model_type)
        return html.Div()
    

    @app.callback(
        Output({'type': 'normalize-values-options-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'normalize-values-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'normalize-values-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_normalize_value_hyperparam_options(normalize, normalize_id):
        for norm in normalize:
            if norm == 'True':
                agent_type = normalize_id['agent']
                model_type = normalize_id['model']
                # if agent_type == 'DDPG':
                return dash_utils.create_value_normalizer_options_hyperparam_input(agent_type, model_type)
        return html.Div()

    
        
    @app.callback(
        Output('callback-selection', 'children'),
        Input({'type':'agent-type-dropdown', 'page':'/build-agent'}, 'value'),
        # State('url', 'pathname'),
    )
    def update_callback_inputs(agent_type):
        if agent_type is not None:
            return html.Div([
                html.H3("Callback Selection"),
                dcc.Dropdown(
                    id={
                        'type': 'callback',
                        'page': '/build-agent',
                    },
                    options=[
                        {'label': 'Weights & Biases', 'value': 'Weights & Biases'},
                    ],
                    placeholder='Select Callbacks to Use',
                    clearable=True,
                    multi=True,
                ),
            ])
        else:
            return None
        
    @app.callback(
        Output('build-agent-status', 'children'),
        [Input('build-agent-button', 'n_clicks')],
        [State({'type':'agent-type-dropdown', 'page':'/build-agent'}, 'value'),
         State({'type':'projects-dropdown', 'page': '/build-agent'}, 'value'),
         State({'type':'callback', 'page':'/build-agent'}, 'value'),
         State({'type':'library-select', 'page':'/build-agent'}, 'value'),
         State({'type': 'env-dropdown', 'page': '/build-agent'}, 'value'),
         State('agent-params-store', 'data'),
         State({"type":"gym_wrappers_dropdown", "page":'/build-agent'}, "value"),
         State({"type":"wrappers_params_store", "page":'/build-agent'}, "data"),
         State({'type': 'add-layer-btn', 'model': ALL, 'agent': ALL, 'index': ALL}, 'n_clicks')
        ],
        prevent_initial_call=True,
)
    def build_agent_model(n_clicks, agent_type_dropdown_value, project, callbacks, env_library, env_selection, agent_params, wrappers, wrapper_params, layer_clicks):
        if n_clicks is None or n_clicks < 1:
            raise PreventUpdate
        
        # wrappers_list = dash_utils.create_wrappers_list(wrappers, wrapper_params)
        formatted_wrappers = dash_utils.format_wrappers(wrapper_params)
        #DEBUG
        # print(f'formatted wrappers:{formatted_wrappers}')
        env = dash_utils.instantiate_envwrapper_obj(env_library, env_selection, formatted_wrappers)
        device = agent_params.get(dash_utils.get_key({'type':'device', 'model':'none', 'agent':agent_type_dropdown_value}))
        save_dir = agent_params.get(dash_utils.get_key({'type':'save-dir', 'model':'none', 'agent':agent_type_dropdown_value}))

        # set params if agent is reinforce/actor critic/PPO
        if agent_type_dropdown_value in ["Reinforce", "ActorCritic", "PPO"]:

            policy_learning_rate_schedule = dash_utils.get_scheduler('lr', 'policy', agent_type_dropdown_value, agent_params)
            policy_optimizer = dash_utils.get_optimizer('policy', agent_type_dropdown_value, agent_params)
            policy_layers = dash_utils.format_layers('policy', agent_type_dropdown_value, agent_params)
            policy_output_kernel = dash_utils.format_output_kernel_initializer_config('policy', agent_type_dropdown_value, agent_params)

            if agent_type_dropdown_value == "PPO":
                policy_type = agent_params.get(dash_utils.get_key({'type':'policy-type', 'model':'policy', 'agent':agent_type_dropdown_value}))
                dist = agent_params.get(dash_utils.get_key({'type':'distribution', 'model':'none', 'agent':agent_type_dropdown_value}))

                if policy_type == 'StochasticContinuousPolicy':
                    model = StochasticContinuousPolicy
                    policy_model = model(
                        env=env,
                        layer_config=policy_layers,
                        output_layer_kernel=policy_output_kernel,
                        optimizer_params=policy_optimizer,
                        scheduler_params = policy_learning_rate_schedule,
                        distribution=dist,
                        device=device,
                    )
                
                else:
                    model = StochasticDiscretePolicy
                    policy_model = model(
                        env=env,
                        layer_config=policy_layers,
                        output_layer_kernel=policy_output_kernel,
                        optimizer_params=policy_optimizer,
                        scheduler_params = policy_learning_rate_schedule,
                        distribution=dist,
                        device=device,
                    )
            else:
                model = StochasticDiscretePolicy
                policy_model = model(
                    env=env,
                        layer_config=policy_layers,
                        output_layer_kernel=policy_output_kernel,
                        optimizer_params=policy_optimizer,
                        scheduler_params = policy_learning_rate_schedule,
                        distribution="categorical",
                        device=device,
                )

            value_learning_rate_schedule = dash_utils.get_scheduler('lr', 'value', agent_type_dropdown_value, agent_params)
            value_optimizer = dash_utils.get_optimizer('value', agent_type_dropdown_value, agent_params)
            value_layers = dash_utils.format_layers('value', agent_type_dropdown_value, agent_params)
            value_output_kernel = dash_utils.format_output_kernel_initializer_config('value', agent_type_dropdown_value, agent_params)
            
            value_model = ValueModel(
                env=env,
                layer_config=value_layers,
                output_layer_kernel=value_output_kernel,
                optimizer_params=value_optimizer,
                scheduler_params=value_learning_rate_schedule,
                device=device,
            )

            discount = agent_params.get(dash_utils.get_key({'type':'discount', 'model':'none', 'agent':agent_type_dropdown_value}))
            
            if agent_type_dropdown_value == "Reinforce":

                agent = Reinforce(
                    env=env,
                    policy_model=policy_model,
                    value_model=value_model,
                    discount=discount,
                    callbacks=dash_utils.get_callbacks(callbacks, project),
                    save_dir=os.path.join(os.getcwd(), save_dir),
                    device=device
                )

            elif agent_type_dropdown_value == "ActorCritic":
                policy_trace_decay = agent_params.get(dash_utils.get_key({'type':'trace-decay', 'model':'policy', 'agent':agent_type_dropdown_value}))
                value_trace_decay = agent_params.get(dash_utils.get_key({'type':'trace-decay', 'model':'value', 'agent':agent_type_dropdown_value}))

                agent = ActorCritic(
                    env=env,
                    policy_model=policy_model,
                    value_model=value_model,
                    discount=discount,
                    policy_trace_decay=policy_trace_decay,
                    value_trace_decay=value_trace_decay,
                    callbacks=dash_utils.get_callbacks(callbacks, project),
                    save_dir=os.path.join(os.getcwd(), save_dir),
                )

            elif agent_type_dropdown_value == "PPO":
                
                gae_coeff = agent_params.get(dash_utils.get_key({'type':'advantage-coeff', 'model':'none', 'agent':agent_type_dropdown_value}))
                policy_clip = agent_params.get(dash_utils.get_key({'type':'surrogate-clip', 'model':'policy', 'agent':agent_type_dropdown_value}))
                policy_clip_schedule = ScheduleWrapper(dash_utils.get_scheduler('surrogate-clip', 'policy', agent_type_dropdown_value, agent_params))
                value_clip = agent_params.get(dash_utils.get_key({'type':'surrogate-clip', 'model':'value', 'agent':agent_type_dropdown_value}))
                value_clip_schedule = ScheduleWrapper(dash_utils.get_scheduler('surrogate-clip', 'value', agent_type_dropdown_value, agent_params))
                value_loss_coeff = agent_params.get(dash_utils.get_key({'type':'value-model-coeff', 'model':'value', 'agent':agent_type_dropdown_value}))
                entropy_coeff = agent_params.get(dash_utils.get_key({'type':'entropy-coeff', 'model':'none', 'agent':agent_type_dropdown_value}))
                entropy_schedule = ScheduleWrapper(dash_utils.get_scheduler('entropy', 'none', agent_type_dropdown_value, agent_params))
                kl_coeff = agent_params.get(dash_utils.get_key({'type':'kl-coeff', 'model':'none', 'agent':agent_type_dropdown_value}))
                kl_adapter_params = dash_utils.get_kl_adapter('none', agent_type_dropdown_value, agent_params)
                if kl_adapter_params is None:
                    kl_adapter = None
                else:
                    kl_adapter = AdaptiveKL(**kl_adapter_params)
                normalize_advs = agent_params.get(dash_utils.get_key({'type':'norm-adv', 'model':'none', 'agent':'PPO'}), False)
                normalize_values = agent_params.get(dash_utils.get_key({'type':'norm-values', 'model':'none', 'agent':'PPO'}), False)
                val_norm_clip = agent_params.get(dash_utils.get_key({'type':'norm-clip', 'model':'none', 'agent':'PPO'}), np.inf)
                policy_grad_clip = agent_params.get(dash_utils.get_key({'type':'grad-clip', 'model':'policy', 'agent':'PPO'}), np.inf)
                value_grad_clip = agent_params.get(dash_utils.get_key({'type':'grad-clip', 'model':'value', 'agent':'PPO'}), np.inf)
                reward_clip = agent_params.get(dash_utils.get_key({'type':'reward-clip', 'model':'none', 'agent':'PPO'}), np.inf)
                
                agent = PPO(
                    env=env,
                    policy_model=policy_model,
                    value_model=value_model,
                    discount=discount,
                    gae_coefficient=gae_coeff,
                    policy_clip=policy_clip,
                    policy_clip_schedule=policy_clip_schedule,
                    value_clip=value_clip,
                    value_clip_schedule=value_clip_schedule,
                    value_loss_coefficient=value_loss_coeff,
                    entropy_coefficient=entropy_coeff,
                    entropy_schedule=entropy_schedule,
                    kl_coefficient=kl_coeff,
                    kl_adapter=kl_adapter,
                    normalize_advantages=normalize_advs,
                    normalize_values=normalize_values,
                    value_normalizer_clip=val_norm_clip,
                    policy_grad_clip=policy_grad_clip,
                    value_grad_clip=value_grad_clip,
                    reward_clip=reward_clip,
                    callbacks = dash_utils.get_callbacks(callbacks, project),
                    save_dir = os.path.join(os.getcwd(), save_dir),
                    device=device,

                )

        elif agent_type_dropdown_value in ["DDPG", "TD3", "HER_DDPG", "HER_TD3"]:

            actor_learning_rate_schedule = dash_utils.get_scheduler('lr', 'actor', agent_type_dropdown_value, agent_params)
            actor_optimizer = dash_utils.get_optimizer('actor', agent_type_dropdown_value, agent_params)
            actor_layers = dash_utils.format_layers('actor', agent_type_dropdown_value, agent_params)
            actor_output_kernel = dash_utils.format_output_kernel_initializer_config('actor', agent_type_dropdown_value, agent_params)

            # Create actor model
            actor_model = ActorModel(
                env=env,
                layer_config=actor_layers,
                output_layer_kernel=actor_output_kernel,
                optimizer_params=actor_optimizer,
                scheduler_params=actor_learning_rate_schedule,
                device=device,
            )
            
            # Set critic params

            critic_learning_rate_schedule = dash_utils.get_scheduler('lr', 'critic', agent_type_dropdown_value, agent_params)
            critic_optimizer = dash_utils.get_optimizer('critic', agent_type_dropdown_value, agent_params)
            critic_state_layers = dash_utils.format_layers('critic-state', agent_type_dropdown_value, agent_params)
            critic_merged_layers = dash_utils.format_layers('critic-merged', agent_type_dropdown_value, agent_params)
            critic_output_kernel = dash_utils.format_output_kernel_initializer_config('critic', agent_type_dropdown_value, agent_params)
           
            critic_model = CriticModel(
                env=env,
                state_layers=critic_state_layers,
                merged_layers=critic_merged_layers,
                output_layer_kernel=critic_output_kernel,
                optimizer_params=critic_optimizer,
                scheduler_params=critic_learning_rate_schedule,
                device=device,
            )

            # Set DDPG params
            discount = agent_params.get(dash_utils.get_key({'type':'discount', 'model':'none', 'agent':agent_type_dropdown_value}))
            tau = agent_params.get(dash_utils.get_key({'type':'tau', 'model':'none', 'agent':agent_type_dropdown_value}))
            epsilon = agent_params.get(dash_utils.get_key({'type':'epsilon-greedy', 'model':'none', 'agent':agent_type_dropdown_value}))
            replay_buffer = dash_utils.get_replay_buffer(env, agent_type_dropdown_value, 'none', agent_params)
            batch_size = agent_params.get(dash_utils.get_key({'type':'batch-size', 'model':'none', 'agent':agent_type_dropdown_value}))
            noise = agent_params.get(dash_utils.get_key({'type':'noise-function', 'model':'actor', 'agent':agent_type_dropdown_value}))
            noise = dash_utils.create_noise_object(env, model_type='actor', agent_type=agent_type_dropdown_value, agent_params=agent_params)
            noise_schedule = ScheduleWrapper(dash_utils.get_scheduler('noise', 'actor', agent_type_dropdown_value, agent_params))
            normalize_inputs = agent_params.get(dash_utils.get_key({'type':'normalize-input', 'model':'none', 'agent':agent_type_dropdown_value}))

            clip_value = agent_params.get(dash_utils.get_key({'type':'clip-value', 'model':'none', 'agent':agent_type_dropdown_value}))

            warmup = agent_params.get(dash_utils.get_key({'type':'warmup', 'model':'none', 'agent':agent_type_dropdown_value}))
            
            if agent_type_dropdown_value == "DDPG":
                agent = DDPG(
                    env = env,
                    actor_model = actor_model,
                    critic_model = critic_model,
                    discount = discount,
                    tau = tau,
                    action_epsilon = epsilon,
                    replay_buffer = replay_buffer,
                    batch_size = batch_size,
                    noise = noise,
                    noise_schedule = noise_schedule,
                    normalize_inputs = normalize_inputs,
                    normalizer_clip = clip_value,
                    warmup = warmup,
                    callbacks = dash_utils.get_callbacks(callbacks, project),
                    save_dir = os.path.join(os.getcwd(), save_dir),
                    device=device,
                )


            elif agent_type_dropdown_value == "TD3":
                # Set TD3 params
                target_noise = agent_params.get(dash_utils.get_key({'type':'noise-function', 'model':'target-actor', 'agent':agent_type_dropdown_value}))
                target_noise = dash_utils.create_noise_object(env, model_type='target-actor', agent_type=agent_type_dropdown_value, agent_params=agent_params)
                target_noise_schedule = ScheduleWrapper(dash_utils.get_scheduler('noise', 'target-actor', agent_type_dropdown_value, agent_params))
                target_noise_clip = agent_params.get(dash_utils.get_key({'type':'target-noise-clip', 'model':'target-actor', 'agent':agent_type_dropdown_value}))
                actor_delay = agent_params.get(dash_utils.get_key({'type':'actor-update-delay', 'model':'actor', 'agent':agent_type_dropdown_value}))
            
                agent = TD3(
                    env = env,
                    actor_model = actor_model,
                    critic_model = critic_model,
                    discount = discount,
                    tau = tau,
                    action_epsilon = epsilon,
                    replay_buffer = replay_buffer,
                    batch_size = batch_size,
                    noise = noise,
                    noise_schedule = noise_schedule,
                    target_noise = target_noise,
                    target_noise_schedule = target_noise_schedule,
                    target_noise_clip = target_noise_clip,
                    actor_update_delay = actor_delay,
                    normalize_inputs = normalize_inputs,
                    normalizer_clip = clip_value,
                    warmup = warmup,
                    callbacks = dash_utils.get_callbacks(callbacks, project),
                    save_dir = os.path.join(os.getcwd(), save_dir),
                    device=device,
                )

            elif agent_type_dropdown_value in ["HER_DDPG", "HER_TD3"]:

                # set HER specific hyperparams
                replay_buffer = dash_utils.get_replay_buffer(env, agent_type_dropdown_value, 'none', agent_params)
                strategy = agent_params.get(dash_utils.get_key({'type':'goal-strategy', 'model':'none', 'agent':agent_type_dropdown_value}))
                num_goals = agent_params.get(dash_utils.get_key({'type':'future-goals', 'model':'none', 'agent':agent_type_dropdown_value}))
                tolerance = agent_params.get(dash_utils.get_key({'type':'goal-tolerance', 'model':'none', 'agent':agent_type_dropdown_value}))
                normalizer_clip = agent_params.get(dash_utils.get_key({'type':'clip-value', 'model':'none', 'agent':agent_type_dropdown_value}))
                save_dir = agent_params.get(dash_utils.get_key({'type':'save-dir', 'model':'none', 'agent':agent_type_dropdown_value}))
                
                if agent_type_dropdown_value == "HER_DDPG":
                    # Create DDPG agent
                    algo = DDPG(
                        env = env,
                        actor_model = actor_model,
                        critic_model = critic_model,
                        discount = discount,
                        tau = tau,
                        action_epsilon = epsilon,
                        replay_buffer = replay_buffer,
                        batch_size = batch_size,
                        noise = noise,
                        noise_schedule = noise_schedule,
                        normalize_inputs = normalize_inputs,
                        normalizer_clip = clip_value,
                        warmup = warmup,
                        callbacks = dash_utils.get_callbacks(callbacks, project),
                        save_dir = os.path.join(os.getcwd(), save_dir),
                        device=device,
                    )

                elif agent_type_dropdown_value == "HER_TD3":
                    target_noise = agent_params.get(dash_utils.get_key({'type':'noise-function', 'model':'target-actor', 'agent':agent_type_dropdown_value}))
                    target_noise = dash_utils.create_noise_object(env, model_type='target-actor', agent_type=agent_type_dropdown_value, agent_params=agent_params)
                    target_noise_schedule = ScheduleWrapper(dash_utils.get_scheduler('noise', 'target-actor', agent_type_dropdown_value, agent_params))
                    target_noise_clip = agent_params.get(dash_utils.get_key({'type':'target-noise-clip', 'model':'target-actor', 'agent':agent_type_dropdown_value}))
                    actor_delay = agent_params.get(dash_utils.get_key({'type':'actor-update-delay', 'model':'actor', 'agent':agent_type_dropdown_value}))
                    # Create TD3 agent
                    algo = TD3(
                        env = env,
                        actor_model = actor_model,
                        critic_model = critic_model,
                        discount = discount,
                        tau = tau,
                        action_epsilon = epsilon,
                        replay_buffer = replay_buffer,
                        batch_size = batch_size,
                        noise = noise,
                        noise_schedule = noise_schedule,
                        target_noise = target_noise,
                        target_noise_schedule = target_noise_schedule,
                        target_noise_clip = target_noise_clip,
                        actor_update_delay = actor_delay,
                        normalize_inputs = normalize_inputs,
                        normalizer_clip = clip_value,
                        warmup = warmup,
                        callbacks = dash_utils.get_callbacks(callbacks, project),
                        save_dir = os.path.join(os.getcwd(), save_dir),
                        device=device,
                    )
                

                # create HER object
                agent = HER(
                    agent=algo,
                    strategy=strategy,
                    tolerance=tolerance,
                    num_goals=num_goals,
                    normalizer_clip=normalizer_clip,
                    device=device,
                    save_dir = os.path.join(os.getcwd(), save_dir),
                )
            
        # save agent
        agent.save()

        # source_dir = os.path.join(os.getcwd(), 'assets')  # Directory containing the agent's files
        # output_zip = os.path.join('assets', 'agent_model.zip')  # Path to save the ZIP archive
        # # Package the agent files into a ZIP archive
        # utils.zip_agent_files(source_dir, output_zip)


        return html.Div([
        dbc.Alert("Agent built successfully!", color="success")
        # html.P("Agent built successfully."),
        # html.A('Download Agent Model', href='/assets/agent_model.zip', download='agent_model.zip'),
        ])
    

    @app.callback(
        Output({'type':'wandb-login-container', 'page':'/build-agent'}, 'children'),
        Input({'type':'callback', 'page':'/build-agent'}, 'value'),
        # State({'type':'callback', 'page':'/build-agent'}, 'id'),
        # prevent_initial_call=True,
)
    def show_wandb_login(callbacks):
        if callbacks is not None:
            # Flatten the list if it's a list of lists, which can happen with ALL
            flattened_values = [item for sublist in callbacks for item in (sublist if isinstance(sublist, list) else [sublist])]
            if 'Weights & Biases' in flattened_values:
                return dash_utils.create_wandb_login('/build-agent')
            else:
                return html.Div()
        
    @app.callback(
        Output({'type':'wandb-login-feedback', 'page':MATCH}, 'children'),
        Input({'type':'wandb-login', 'page':MATCH}, 'n_clicks'),
        State({'type':'wandb-api-key', 'page':MATCH}, 'value'),
        State('url', 'pathname'),
        prevent_initial_call=True,
    )
    def login_to_wandb(n_clicks, api_key, page):
        if not n_clicks or n_clicks < 1:
            raise PreventUpdate

        # Attempt to log in to Weights & Biases
        try:
            wandb.login(key=api_key)
            return dbc.Alert("Login successful!", color="success", is_open=True,
                             id={'type':'wandb-login-success', 'page':page})
        
        # html.Div(
        #             "Logged in to Weights & Biases successfully.", 
        #             style={'color': 'green'},
        #             id={'type':'wandb-login-success', 'page':page},
        #         ),
                
        except (ValueError, ConnectionError) as e:
            return html.Div(f"Failed to log in to Weights & Biases: {e}", style={'color': 'red'})
        
    @app.callback(
        Output({'type': 'hyperparameter-inputs', 'page':MATCH}, 'hidden'),
        Input({'type': 'wandb-login-success', 'page':MATCH}, 'is_open'),
        State('url', 'pathname'),
        prevent_initial_call=True,
    )
    def show_hyperparam_inputs(wandb_login, page):
            # print(wandb_login)
            return not wandb_login

    @app.callback(
        Output({'type':'project-selection-container', 'page':MATCH}, 'children'),
        Input({'type':'wandb-login-success', 'page':MATCH}, 'is_open'),
        State('url', 'pathname'),
        prevent_initial_call=True,
    )
    def show_project_selection(wandb_login, page):
        
        if wandb_login:
            return dash_utils.create_wandb_project_dropdown(page)
    
    @app.callback(
        [Output({'type':'env-description', 'page':MATCH}, 'children'),
        Output({'type':'env-gif', 'page':MATCH}, 'src'),
        Output({'type':'gym-params', 'page':MATCH}, 'children')],
        [Input({'type':'env-dropdown', 'page':MATCH}, 'value')],
        State({'type':'env-dropdown', 'page':MATCH}, 'id'),
        prevent_initial_call=True,
    )
    def update_env_info(env_name, id):
        if env_name:
            description, gif_url = dash_utils.get_env_data(env_name)
            gym_params = {}
            if id['page'] != "/build-agent":
                gym_params = dash_utils.generate_gym_extra_params_container(env_name)
            return description, gif_url, gym_params
        return "", "", gym_params  # Default empty state

    @app.callback(
        Output({'type':'hidden-div', 'page':'/train-agent'}, 'data'),
        Input({'type':'start', 'page':'/train-agent'}, 'n_clicks'),
        # State({'type':'start', 'page':'/train-agent'}, 'id'),
        State({'type':'agent-store', 'page':'/train-agent'}, 'data'),
        State({'type':'run-params-store', 'page':'/train-agent'}, 'data'),
        # State({'type':'env-dropdown', 'page':'/train-agent'}, 'value'),
        # State({'type':'num-episodes', 'page':'/train-agent'}, 'value'),
        # State({'type':'render-option', 'page':'/train-agent'}, 'value'),
        # State({'type':'render-freq', 'page':'/train-agent'}, 'value'),
        # State({'type':'epochs', 'page':'/train-agent'}, 'value'),
        # State({'type':'cycles', 'page':'/train-agent'}, 'value'),
        # State({'type':'learning-cycles', 'page':'/train-agent'}, 'value'),
        # State({'type':'num-timesteps', 'page':'/train-agent'}, 'value'),
        # State({'type':'traj-length', 'page':'/train-agent'}, 'value'),
        # State({'type':'batch-size', 'page':'/train-agent'}, 'value'),
        # State({'type':'learning-epochs', 'page':'/train-agent'}, 'value'),
        # State({'type':'num-envs', 'page':'/train-agent'}, 'value'),
        # State({'type':'mpi', 'page':'/train-agent'}, 'value'),
        # State({'type':'workers', 'page':'/train-agent'}, 'value'),
        # State({'type':'load-weights', 'page':'/train-agent'}, 'value'),
        # State({'type':'seed', 'page':'/train-agent'}, 'value'),
        # State({'type':'run-number', 'page':'/train-agent'}, 'value'),
        # State({'type':'num-runs', 'page':'/train-agent'}, 'value'),
        # State({'type':'save-dir', 'page':'/train-agent'}, 'value'),
        # prevent_initial_call=True,
    )
    def train_agent(n_clicks, agent_data, run_data):
        # clear metrics in storage
        if os.path.exists('/workspaces/RL_Agents/pytorch/src/app/assets/training_data.json'):
            # Remove the file
            os.remove('/workspaces/RL_Agents/pytorch/src/app/assets/training_data.json')
        if n_clicks > 0 and agent_data and run_data:
            # Create an empty dict for train_config.json
            train_config = {}

            # Update the configuration with render settings
            # Loop over each entry of run_data and format into train_config
            for key, value in run_data.items():
                train_config[dash_utils.get_param_from_key(key)] = value
            #DEBUG
            print(f'train config:{train_config}')

            # Save the updated configuration to a train config file
            train_config_path = agent_data["save_dir"] + '/train_config.json'
            with open(train_config_path, 'w') as f:
                json.dump(train_config, f)

            # Set the config path of the agent
            agent_config_path = agent_data["save_dir"] + '/config.json'

            # clear the renders in the train folder
            # if train_config["render_freq"] > 0:
            if os.path.exists(agent_data["save_dir"] + '/renders/training'):
                dash_utils.delete_renders(agent_data["save_dir"] + '/renders/training')

            script_path = 'train.py'
            run_command = f"python {script_path} --agent_config {agent_config_path} --train_config {train_config_path}"
            subprocess.Popen(run_command, shell=True)

        raise PreventUpdate
    
    @app.callback(
        Output({'type':'hidden-div', 'page':'/test-agent'}, 'data'),
        Input({'type':'start', 'page':'/test-agent'}, 'n_clicks'),
        # State({'type':'start', 'page':'/test-agent'}, 'id'),
        State({'type':'agent-store', 'page':'/test-agent'}, 'data'),
        State({'type':'run-params-store', 'page':'/test-agent'}, 'data'),
        # State({'type':'env-dropdown', 'page':'/test-agent'}, 'value'),
        # State({'type':'num-episodes', 'page':'/test-agent'}, 'value'),
        # State({'type':'num-envs', 'page':'/test-agent'}, 'value'),
        # State({'type':'render-option', 'page':'/test-agent'}, 'value'),
        # State({'type':'render-freq', 'page':'/test-agent'}, 'value'),
        # State({'type':'load-weights', 'page':'/test-agent'}, 'value'),
        # State({'type':'seed', 'page':'/test-agent'}, 'value'),
        # State({'type':'run-number', 'page':'/test-agent'}, 'value'),
        # State({'type':'num-runs', 'page':'/test-agent'}, 'value'),
        # prevent_initial_call=True,
    )
    def test_agent(n_clicks, agent_data, run_data):

        # clear metrics in storage
        if os.path.exists('/workspaces/RL_Agents/pytorch/src/app/assets/testing_data.json'):
            # Remove the file
            os.remove('/workspaces/RL_Agents/pytorch/src/app/assets/testing_data.json')
        try:
            if n_clicks > 0 and agent_data and run_data:
                
                # Create empty dict for test_config.json
                test_config = {}
                # # Update the configuration with render settings
                # Loop over each entry of run_data and format into train_config
                for key, value in run_data.items():
                    test_config[dash_utils.get_param_from_key(key)] = value
                #DEBUG
                print(f'test config:{test_config}')

                # Delete renders if any in folder
                if os.path.exists(agent_data['save_dir'] + '/renders'):
                    dash_utils.delete_renders(agent_data['save_dir'] + '/renders')

                # Save the updated configuration to a file
                test_config_path = agent_data['save_dir'] + '/test_config.json'
                with open(test_config_path, 'w') as f:
                    json.dump(test_config, f)

                # Set the config path of the agent
                agent_config_path = agent_data['save_dir'] + '/config.json'
                
                script_path = 'test.py'
                run_command = f"python {script_path} --agent_config {agent_config_path} --test_config {test_config_path}"
                subprocess.Popen(run_command, shell=True)

            raise PreventUpdate

        except KeyError as e:
            print(f"KeyError: {str(e)}")
            # Handle the case when a required key is missing in agent_data
            # You can choose to raise an exception, return an error message, or take appropriate action

        except FileNotFoundError as e:
            print(f"FileNotFoundError: {str(e)}")
            # Handle the case when the specified file or directory is not found
            # You can choose to raise an exception, return an error message, or take appropriate action

        except subprocess.SubprocessError as e:
            print(f"SubprocessError: {str(e)}")
            # Handle the case when there is an error executing the subprocess
            # You can choose to raise an exception, return an error message, or take appropriate action

        except Exception as e:
            print(f"An unexpected error occurred: {str(e)}")
            # Handle any other unexpected exceptions
            # You can choose to raise an exception, return an error message, or take appropriate action

        # if n_clicks > 0:
        #     # clear the renders in the train folder
        #     agent_type = agent_data['agent_type']
        #     if agent_type == "HER":
        #         agent_type = agent_data['agent']['agent_type']
        #     if os.path.exists(f'assets/models/{agent_type}/renders/testing'):
        #         utils.delete_renders(f"assets/models/{agent_type}/renders/testing")
            
        #     # Use the agent_data['save_dir'] to load agent
        #     if agent_data:  # Check if agent_data is not empty
        #         render = 'RENDER' in render_option

        #         # Update the configuration with render settings
        #         agent_data['num_episodes'] = num_episodes
        #         agent_data['render'] = render
        #         agent_data['render_freq'] = render_freq
        #         agent_data['load_weights'] = load_weights
                
        #         # Save the updated configuration to a file
        #         config_path = agent_data['save_dir'] + '/test_config.json'
        #         with open(config_path, 'w') as f:
        #             json.dump(agent_data, f)

        #         script_path = 'test.py'
        #         run_command = f"python {script_path} {config_path}"
        #         subprocess.Popen(run_command, shell=True)

        # raise PreventUpdate
            
    
    @app.callback(
    Output({'type':'agent-store', 'page':MATCH}, 'data'),
    Output({'type':'output-agent-load', 'page':MATCH}, 'children'),
    Input({'type':'upload-agent-config', 'page':MATCH}, 'contents'),
    State({'type':'upload-agent-config', 'page':MATCH}, 'id'),
    prevent_initial_call=True,
    )
    def store_agent(contents, upload_id):
        # for content, page in zip(contents, id):
        if contents is not None:
            _, encoded = contents.split(',')
            decoded = base64.b64decode(encoded)           
            config = json.loads(decoded.decode('utf-8'))
            
            success_message = html.Div([
            dbc.Alert("Config loaded successfully", color="success")
            ])
            
            return config, success_message
    
        return {}, "Please upload a file.", {'display': 'none'}

    @app.callback(
        Output({'type': 'run-options', 'page': MATCH}, 'children'),
        Input({'type': 'agent-store', 'page': MATCH}, 'data'),
        State({'type': 'agent-store', 'page': MATCH}, 'id'),
        prevent_initial_call=True,
    )
    def update_run_options(agent_data, data_id):
        agent_type = agent_data['agent_type']
        page = data_id['page']
        
        # Return the dynamically created dash components
        return dash_utils.update_run_options(agent_type, page)
    
    # @app.callback(
    # Output({'type': 'her-options', 'page': '/train-agent'}, 'style'),
    # Input({'type':'agent-store', 'page': '/train-agent'}, 'data'),
    # prevent_initial_call = True,
    # )
    # def update_her_options(agent_data):

    #     if agent_data['agent_type'] == 'HER':
    #         her_options_style = {'display': 'block'}
    #     else:
    #         her_options_style = {'display': 'none'}
        
    #     return her_options_style
    
    # @app.callback(
    # Output({'type': 'ppo-options', 'page': '/train-agent'}, 'style'),
    # Input({'type':'agent-store', 'page': '/train-agent'}, 'data'),
    # prevent_initial_call = True,
    # )
    # def update_ppo_options(agent_data):

    #     if agent_data['agent_type'] == 'PPO':
    #         ppo_options_style = {'display': 'block'}
    #     else:
    #         ppo_options_style = {'display': 'none'}
        
    #     return ppo_options_style
    
    # @app.callback(
    # Output({'type': 'episode-option', 'page': '/test-agent'}, 'style'),
    # Input({'type':'agent-store', 'page': '/test-agent'}, 'data'),
    # prevent_initial_call = True,
    # )
    # def update_ppo_test_options(agent_data):

    #     if agent_data['agent_type'] == 'PPO':
    #         episode_option_style = {'display': 'block'}
    #     else:
    #         episode_option_style = {'display': 'none'}
        
    #     return episode_option_style
    
    # @app.callback(
    # Output({'type': 'mpi-options', 'page': '/train-agent'}, 'style'),
    # Input({'type':'agent-store', 'page': '/train-agent'}, 'data'),
    # prevent_initial_call = True,
    # )
    # def update_mpi_options(agent_data):

    #     if agent_data['agent_type'] in ['DDPG', 'TD3', 'HER']:
    #         mpi_options_style = {'display': 'block'}
        
    #     else:
    #         mpi_options_style = {'display': 'none'}
        
    #     return mpi_options_style
    
    @app.callback(
    Output({'type': 'mpi-options', 'page': '/hyperparameter-search'}, 'style'),
    Input({'type':'agent-type-selector', 'page': '/hyperparameter-search'}, 'value'),
    prevent_initial_call = True,
    )
    def update_mpi_hyperparam_options(agent_types):

        print(f'agent types: {agent_types}')

        if any(agent in ['DDPG', 'TD3', 'HER_DDPG'] for agent in agent_types):
        # if agent_types:
            print(f'hyperparam options true')
            mpi_options = {'display': 'block'}
        
        else:
            print(f'hyperparam options false')
            mpi_options = {'display': 'none'}
        
        print(f'mpi options: {mpi_options}')
        return mpi_options
    
    @app.callback(
    Output({'type': 'workers', 'page': MATCH}, 'style'),
    Input({'type':'mpi', 'page': MATCH}, 'value'),
    prevent_initial_call = True,
    )
    def update_workers_option(use_mpi):

        if use_mpi:
            workers_style = {'display': 'block'}
        else:
            workers_style = {'display': 'none'}
        
        return workers_style
    
    @app.callback(
    Output({'type': 'sweep-options', 'page': '/hyperparameter-search'}, 'style'),
    Input({'type':'device', 'model': 'none', 'agent':ALL}, 'value'),
    State('url', 'pathname'),
    prevent_initial_call = True,
    )
    def update_sweep_option(device, page):

        if page == '/hyperparameter-search':

            if any([d=='cpu' for d in device]):
                options = {'display': 'block'}
            else:
                options = {'display': 'none'}
            
            return options
    
    @app.callback(
    Output({'type': 'clamp-value', 'model':MATCH, 'agent':MATCH}, 'style'),
    Input({'type':'clamp-output', 'model':MATCH, 'agent':MATCH}, 'value'),
    # prevent_initial_call = True,
    )
    def update_clamp_options(clamp_output):

        if clamp_output:
            clamp_options_style = {'display': 'block'}
        else:
            clamp_options_style = {'display': 'none'}
        
        return clamp_options_style
    
    @app.callback(
    Output({'type': 'grad-clip-block', 'model':MATCH, 'agent':MATCH}, 'style'),
    Input({'type':'clip-grad', 'model':MATCH, 'agent':MATCH}, 'value'),
    # prevent_initial_call = True,
    )
    def update_grad_clip_options(clip_grad):

        if clip_grad:
            clip_options_style = {'display': 'block', 'margin-left': '20px'}
        else:
            clip_options_style = {'display': 'none', 'margin-left': '20px'}
        
        return clip_options_style
    
    @app.callback(
    Output({'type':'render-block', 'page':MATCH}, 'style'),  
    Input({'type':'render-option', 'page':MATCH}, 'value'),
    # prevent_initial_call = True,
    )
    def toggle_render_freq(render_option):
        if render_option:
            return {'display': 'block', 'margin-left': '10px'}
        return {'display': 'none'}
    
    @app.callback(
    Output({'type':'clip-rewards-block', 'model':MATCH, 'agent':MATCH}, 'style'),  
    Input({'type':'clip-rewards', 'model':MATCH, 'agent':MATCH}, 'value'),
    # prevent_initial_call = True,
    )
    def toggle_reward_clip(clip_reward_option):
        if clip_reward_option:
            return {'display': 'block', 'margin-left': '10px'}
        return {'display': 'none'}
    

    # @app.callback(
    #     Output({'type':'storage', 'page':MATCH}, 'data'),
    #     [Input({'type':'interval-component', 'page':MATCH}, 'n_intervals'),
    #     Input({'type':'start', 'page':MATCH}, 'n_clicks')],  # Add the train button as an input
    #     [State({'type':'storage', 'page':MATCH}, 'data'),
    #     State({'type':'agent-store', 'page':MATCH}, 'data'),
    #     # State({'type':'num-episodes', 'page':MATCH}, 'value'),
    #     # State({'type':'num-timesteps', 'page':MATCH}, 'value'),
    #     # State({'type':'epochs', 'page':MATCH}, 'value'),
    #     # State({'type':'cycles', 'page':MATCH}, 'value'),
    #     State('url', 'pathname')],
    #     prevent_initial_call=True,
    # )
    # def update_data(n, n_clicks, storage_data, agent_config, pathname):
    #     ctx = dash.callback_context

    #     # Determine which input fired the callback
    #     if not ctx.triggered:
    #         raise PreventUpdate
    #     trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]

    #     # Clear data if the train button was clicked
    #     if trigger_id == '{"page":"/train-agent","type":"start"}':
    #         print('train start trigger')
    #         storage_data = {'data': {}, 'progress': 0, 'status': 'Training Started...'}
    #         print(f'storage_data:{storage_data}')
    #         return storage_data
        
    #     elif trigger_id == '{"page":"/test-agent","type":"start"}':
    #         storage_data = {'data': {}, 'progress': 0, 'status': 'Testing Started...'}
    #         return storage_data
        
    #     # Proceed with regular data update if the interval component fired the callback
    #     # if trigger_id == f"{'type':'interval-component', 'page':MATCH}":
    #     else:
    #         # if num_episodes is not None or num_timesteps is not None:
    #         #     if pathname == '/train-agent':
    #         #         file_name = 'training_data.json'
    #         #         process = 'Training'
    #         #     elif pathname == '/test-agent':
    #         #         file_name = 'testing_data.json'
    #         #         process = 'Testing'
    #         #     else:
    #         #         raise PreventUpdate

    #         #     # Read the latest training progress data from the JSON file
    #         #     try:
    #         #         with open(Path("assets") / file_name, 'r') as f:
    #         #             data = json.load(f)
    #         #     except (FileNotFoundError, json.JSONDecodeError):
    #         #         data = {}

    #         #     # If the new data dict isn't empty, update storage data
    #         #     if data:
    #         #         # Determine agent type to correctly calculate num_episodes and progress
    #         #         if agent_config['agent_type'] == 'HER':
    #         #             num_episodes = num_epochs * num_cycles * num_episodes
    #         #         storage_data['data'] = data
    #         #         if agent_config['agent_type'] in ['Reinforce', 'ActorCritic', 'DDPG', 'TD3']:
    #         #             storage_data['progress'] = round(data['episode'] / num_episodes, ndigits=2)
    #         #         elif agent_config['agent_type'] == 'PPO':
    #         #             if pathname == "/train-agent":
    #         #                 storage_data['progress'] = round(data['episode'] / num_timesteps, ndigits=2)
    #         #             elif pathname == "/test-agent":
    #         #                 storage_data['progress'] = round(data['episode'] / num_episodes, ndigits=2)
                    
    #         #         # Update status
    #         #         if storage_data['progress'] == 1.0:
    #         #             storage_data['status'] = f"{process} Completed"
    #         #         else:
    #         #             storage_data['status'] = f"{process} in Progress..."
            
    #         return storage_data
    
    
    @app.callback(
        Output({'type':'status', 'page':MATCH}, 'children'),
        Input({'type':'interval-component', 'page':MATCH}, 'n_intervals'),
        State({'type':'storage', 'page':MATCH}, 'data'),
        State({'type':'start', 'page':MATCH}, 'n_clicks'),
    )
    def update_status_and_progress(n, storage_data, start_clicks):
        if start_clicks > 0:
            # Extract status message
            status_message = storage_data.get('status', "Status not found.")

            # Extract progress and calculate percentage
            progress = storage_data.get('progress', 0) * 100  # Assuming progress is a fraction

            # Format data metrics for display
            data_metrics = storage_data.get('data', {})
            metrics_display = html.Ul([html.Li(f"{key}: {value}") for key, value in data_metrics.items()])

            # Create progress bar component
            progress_bar = dbc.Progress(value=progress, max=100, striped=True, animated=True, style={"margin": "20px 0"})

            # Combine all components to be displayed in a single div
            combined_display = html.Div([
                html.P(status_message),
                metrics_display,
                progress_bar
            ])

            return combined_display
        
        PreventUpdate
    
    # @app.callback(
    #     Output({'type':'video-carousel-store', 'page':MATCH}, 'data'),
    #     [Input({'type':'interval-component', 'page':MATCH}, 'n_intervals')],
    #     State({'type':'interval-component', 'page':MATCH}, 'id'),
    #     State({'type':'start', 'page':MATCH}, 'n_clicks'),
    #     State({'type':'storage', 'page':MATCH}, 'data'),
    # )
    # def update_video_carousel(n_intervals, interval_id, start_clicks, video_storage):
    #     if start_clicks > 0:
    #         video_filenames = utils.get_video_files(interval_id['page'])
    #         #DEBUG
    #         # print(f'video filenames: {video_filenames}')
    #         # check if video_filenames is not empty
    #         video_storage['video_list'] = video_filenames
            
    #         return video_storage
    
    #     PreventUpdate
    @app.callback(
        Output({'type': 'video-carousel-store', 'page': MATCH}, 'data'),
        Output({'type': 'video-carousel', 'page': MATCH}, 'children'),
        Output({'type': 'video-filename', 'page': MATCH}, 'children'),
        [Input({'type': 'prev-btn', 'page': MATCH}, 'n_clicks'),
        Input({'type': 'next-btn', 'page': MATCH}, 'n_clicks'),
        Input({'type': 'interval-component', 'page': MATCH}, 'n_intervals')],
        [State({'type': 'video-carousel-store', 'page': MATCH}, 'data'),
        State({'type': 'interval-component', 'page': MATCH}, 'id'),
        State({'type': 'start', 'page': MATCH}, 'n_clicks')],
        State({'type': 'video-carousel', 'page': MATCH}, 'id'),
        State({'type':'agent-store', 'page':MATCH}, 'data'),
        prevent_initial_call=True
    )
    def update_video_data(prev_clicks, next_clicks, n_intervals, data, interval_id, start_clicks, carousel_id, agent_data):
        # check if start has been clicked before trying to update        
        ctx = dash.callback_context
        triggered_id, prop = ctx.triggered[0]['prop_id'].split('.')
        #DEBUG
        # print(f'trigger id: {triggered_id}')

        # Initial load or automatic update triggered by the interval component
        if 'interval-component' in triggered_id and start_clicks > 0:
            video_filenames = dash_utils.get_video_files(interval_id['page'], agent_data)
            if video_filenames:
                data['video_list'] = video_filenames
                # Optionally reset current video to 0 or keep it as is
                # data['current_video'] = 0
                # video_files = data['video_list']
                current_video = data.get('current_video', 0)
                current_filename = video_filenames[current_video]
                video_item = dash_utils.generate_video_items([current_filename], carousel_id['page'], agent_data)[0]
                
                #DEBUG
                # print(f'data: {data}')
                return data, video_item, current_filename
            else:
                raise PreventUpdate

        # Navigation button clicks
        elif 'prev-btn' in triggered_id or 'next-btn' in triggered_id:
            current_video = data.get('current_video', 0)
            video_list = data.get('video_list', [])
            #DEBUG
            # print(f'current video: {current_video}')
            # print(f'video_list: {video_list}')
            
            if 'prev-btn' in triggered_id:
                current_video = max(0, current_video - 1)
            elif 'next-btn' in triggered_id:
                current_video = min(len(video_list) - 1, current_video + 1)

            data['current_video'] = current_video
            #DEBUG
            # print(f'current video: {current_video}')

            video_files = data['video_list']
            current_filename = video_files[current_video]
            video_item = dash_utils.generate_video_items([current_filename], carousel_id['page'], agent_data)[0]

            return data, video_item, current_filename

        raise PreventUpdate

    # Callback to update video display based on navigation
    # @app.callback(
    #     Output({'type': 'video-carousel', 'page': MATCH}, 'children'),
    #     Output({'type': 'video-filename', 'page': MATCH}, 'children'),
    #     [Input({'type': 'prev-btn', 'page': MATCH}, 'n_clicks'),
    #     Input({'type': 'next-btn', 'page': MATCH}, 'n_clicks')],
    #     [State({'type': 'video-carousel-store', 'page': MATCH}, 'data'),
    #     State({'type': 'video-carousel', 'page': MATCH}, 'id')],
    #     prevent_initial_call=True
    # )
    # def update_video(n_clicks_prev, n_clicks_next, video_storage, id):
    #     ctx = dash.callback_context

    #     if not ctx.triggered:
    #         button_id = 'No clicks yet'
    #     else:
    #         button_id = ctx.triggered[0]['prop_id'].split('.')[0]

    #     video_files = video_storage['video_list']
    #     current_video = video_storage['current_video']
    #     current_filename = video_files[current_video]

        # if 'prev-btn' in button_id:
        #     current_video = max(0, current_video - 1)
        # elif 'next-btn' in button_id:
        #     current_video = min(len(video_files) - 1, current_video + 1)

        # video_storage['current_video']=current_video
        # video_item = utils.generate_video_items([video_files[current_video]], id['page'])[0]

        # return video_item, current_filename

    # Callback to enable/disable navigation buttons
    @app.callback(
        [Output({'type': 'prev-btn', 'page': MATCH}, 'disabled'),
        Output({'type': 'next-btn', 'page': MATCH}, 'disabled')],
        [Input({'type': 'video-carousel-store', 'page': MATCH}, 'data')],
        [State({'type': 'video-carousel', 'page': MATCH}, 'id')]
    )
    def toggle_navigation_buttons(data, id):
        video_files = data['video_list']
        current_video = data['current_video']

        prev_disabled = current_video <= 0
        next_disabled = current_video >= len(video_files) - 1

        return prev_disabled, next_disabled
        
        ## HYPERPARAMETER SEARCH CALLBACKS
    
    @app.callback(
        Output('agent-options-tabs', 'children'),
        Output('agent-sweep-options', 'children'),
        Input({'type':'agent-type-selector', 'page':'/hyperparameter-search'}, 'value')
    )
    def update_hyperparam_inputs(agent_type):
        # This function updates the inputs based on selected agent types
        tabs = []
        # for agent_type in selected_agent_types:
        if agent_type == 'Reinforce':
            tabs.append(dash_utils.create_reinforce_hyperparam_input(agent_type))
        
        elif agent_type == 'ActorCritic':
            tabs.append(dash_utils.create_actor_critic_hyperparam_input(agent_type))
        
        elif agent_type == 'DDPG':
            tabs.append(dash_utils.create_ddpg_hyperparam_input(agent_type))

        elif agent_type == 'TD3':
            tabs.append(dash_utils.create_td3_hyperparam_input(agent_type))

        elif agent_type == 'HER_DDPG':
            tabs.append(dash_utils.create_her_ddpg_hyperparam_input(agent_type))

        elif agent_type == 'PPO':
            tabs.append(dash_utils.create_ppo_hyperparam_input(agent_type))
        
        options = dash_utils.create_agent_sweep_options(agent_type)
        return tabs, options
    

    @app.callback(
    Output({'type': 'optimizer-options-hyperparams', 'model': MATCH, 'agent': MATCH}, 'children'),
    Input({'type': 'optimizer-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
    State({'type': 'optimizer-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
    prevent_initial_call=True
    )
    def update_optimizer_options_hyperparams(optimizers, optimizer_id):
        tabs = []
        for optimizer in optimizers:
            if optimizer == 'Adam':
                tab = dcc.Tab(
                    label='Adam',
                    children=[
                        html.Div([
                            html.Label("Weight Decay", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'adam-weight-decay-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                               options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            )
                        ])
                    ]
                )
            elif optimizer == 'Adagrad':
                tab = dcc.Tab(
                    label='Adagrad',
                    children=[
                        html.Div([
                            html.Label("Weight Decay", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'adagrad-weight-decay-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            ),
                            html.Label("Learning Rate Decay", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'adagrad-lr-decay-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            )
                        ])
                    ]
                )
            elif optimizer == 'RMSprop':
                tab = dcc.Tab(
                    label='RMSprop',
                    children=[
                        html.Div([
                            html.Label("Weight Decay", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'rmsprop-weight-decay-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            ),
                            html.Label("Momentum", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'rmsprop-momentum-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            )
                        ])
                    ]
                )
            elif optimizer == 'SGD':
                tab = dcc.Tab(
                    label='SGD',
                    children=[
                        html.Div([
                            html.Label("Weight Decay", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'sgd-weight-decay-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            ),
                            html.Label("Momentum", style={'text-decoration': 'underline'}),
                            dcc.Dropdown(
                                id=
                                {
                                    'type': 'sgd-momentum-hyperparam',
                                    'model': optimizer_id['model'],
                                    'agent': optimizer_id['agent']
                                },
                                options=[
                                    {'label': '0.0', 'value': 0.0},
                                    {'label': '0.1', 'value': 0.1},
                                    {'label': '0.2', 'value': 0.2},
                                    {'label': '0.3', 'value': 0.3},
                                    {'label': '0.4', 'value': 0.4},
                                    {'label': '0.5', 'value': 0.5},
                                    {'label': '0.6', 'value': 0.6},
                                    {'label': '0.7', 'value': 0.7},
                                    {'label': '0.8', 'value': 0.8},
                                    {'label': '0.9', 'value': 0.9},
                                    {'label': '0.99', 'value': 0.99},
                                    {'label': '1.0', 'value': 1.0},
                                ],
                                multi=True,
                            )
                        ])
                    ]
                )
            
            tabs.append(tab)
        
        return dcc.Tabs(tabs)
    

    @app.callback(
        Output({'type': 'hidden-layers-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'hidden-layers-slider', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'hidden-layers-slider', 'model': MATCH, 'agent': MATCH}, 'id'),
    )
    def update_hidden_layers_hyperparam_inputs(num_layers, id):
        if num_layers is not None:
            model_type = id['model']
            agent_type = id['agent']

            return dbc.Tabs(dash_utils.generate_layer_hyperparam_component(agent_type, model_type, num_layers[-1]))

    @app.callback(
        Output({'type': 'layer-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'children'),
        Input({'type': 'layer-type-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value'),
        State({'type': 'layer-type-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'id'),
    )
    def update_layer_hyperparam_inputs(layer_types, id):
        if layer_types is not None:
            model_type = id['model']
            agent_type = id['agent']
            layer_num = id['index']
            tabs = dash_utils.generate_layer_hyperparam_tab(agent_type, model_type, layer_num, layer_types)
            # inputs = []
            # for layer_type in layer_types:
            #     if layer_type == 'dense':
            #         inputs.append(
            #             utils.generate_layer_hyperparam_tab(agent_type, model_type, layer_num)
            #         )

            # return [item for sublist in tabs for item in (sublist if isinstance(sublist, list) else [sublist])]
            return tabs

    @app.callback(
        Output({'type': 'cnn-layer-types-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'cnn-layers-slider-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'cnn-layers-slider-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
    )
    def update_hyperparam_units_types_inputs(num_layers, layer_id):
        if num_layers is not None:
            model_type = layer_id['model']
            agent_type = layer_id['agent']
            layer_types = []
            for i in range(1, num_layers[1] + 1):
                layer_types.append(
                    dash_utils.generate_cnn_layer_type_hyperparam_component(agent_type, model_type, i)
                )

            return layer_types
        

    @app.callback(
    Output({'type': 'cnn-layer-type-parameters-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'children'),
    Input({'type': 'cnn-layer-type-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'value'),
    State({'type': 'cnn-layer-type-hyperparam', 'model': MATCH, 'agent': MATCH, 'index': MATCH}, 'id'),
    prevent_initial_call=True
    )
    def update_layer_type_params_hyperparams(layer_types, layer_id):
        if layer_types is not None:
            layer_params = []
            for layer_type in layer_types:
                model = layer_id['model']
                agent = layer_id['agent']
                index = layer_id['index']

                layer_params.append(dash_utils.generate_cnn_layer_parameters_hyperparam_component(layer_type, agent, model, index))
            
        return layer_params

 
    @app.callback(
        Output({'type': 'kernel-options-tabs', 'model':MATCH, 'agent':MATCH, 'index':MATCH}, 'children'),
        Output({'type': 'kernel-options-header' , 'model':MATCH, 'agent':MATCH, 'index':MATCH}, 'hidden'),
        Input({'type': 'kernel-function-hyperparam', 'model':MATCH, 'agent':MATCH, 'index':MATCH}, 'value'),
        State({'type': 'kernel-function-hyperparam', 'model':MATCH, 'agent':MATCH, 'index':MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_kernel_hyperparam_options(kernel_functions, id):
        model_type = id['model']
        agent_type = id['agent']
        layer_num = id['index']

        tabs = dash_utils.generate_kernel_options_hyperparam_component(agent_type, model_type, layer_num, kernel_functions)

        # Hide header if no kernel options
        hide_header = True if not kernel_functions else False

        return tabs, hide_header
    
    @app.callback(
        Output({'type': 'entropy-block', 'model': 'none', 'agent': MATCH},'style'),
        Output({'type': 'kl-block', 'model': 'none', 'agent': MATCH},'style'),
        Output({'type': 'lambda-block', 'model': 'none', 'agent': MATCH},'style'),
        Input({'type':'loss-type', 'model': 'none', 'agent': MATCH}, 'value'),
        prevent_initial_call=True
    )
    def update_ppo_loss_options(loss_type):
        if loss_type == 'kl':
            kl_block = {'display': 'block'}
            ep_block = {'display': 'none'}
            lm_block = {'display': 'none'}
            return ep_block, kl_block, lm_block
        elif loss_type == 'clipped':
            kl_block = {'display': 'none'}
            ep_block = {'display': 'block'}
            lm_block = {'display': 'none'}
            return ep_block, kl_block, lm_block
        kl_block = {'display': 'block'}
        ep_block = {'display': 'block'}
        lm_block = {'display': 'block'}
        return ep_block, kl_block, lm_block

    @app.callback(
        Output({'type': 'norm-clip-block', 'model': 'none', 'agent': MATCH},'style'),
        Input({'type':'norm-values', 'model': 'none', 'agent': MATCH}, 'value'),
        prevent_initial_call=True
    )
    def update_norm_clip_options(norm_values):
        if norm_values:
            norm_block = {'margin-left': '10px', 'display': 'block'}
            return norm_block
        
        norm_block = {'display': 'none'}
        return norm_block
    
    @app.callback(
        Output({'type': 'norm-clip-hyperparam-block', 'model': MATCH, 'agent': MATCH},'style'),
        Input({'type':'normalize-values-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        prevent_initial_call=True
    )
    def update_norm_clip_hyperparam_options(norm_values):
        if any(norm_values):
            norm_block = {'margin-left': '10px', 'display': 'block'}
            return norm_block
        
        norm_block = {'display': 'none'}
        return norm_block
    
    @app.callback(
        Output('her-options-hyperparam', 'hidden'),
        Input({'type':'agent-type-selector', 'page':'/hyperparameter-search'}, 'value'),
        prevent_initial_call=True
    )
    def update_her_hyperparam_options(agent_types):
        if 'HER_DDPG' in agent_types:
            return False
        return True
    
    @app.callback(
        Output({'type': 'noise-options-tabs', 'model': MATCH, 'agent': MATCH}, 'children'),
        Output({'type': 'noise-options-header' , 'model': MATCH, 'agent': MATCH}, 'hidden'),
        Input({'type': 'noise-function-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'noise-function-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
        prevent_initial_call=True
    )
    def update_noise_hyperparam_options(noise_functions, id):
        model_type = id['model']
        agent_type = id['agent']

        tabs = dash_utils.generate_noise_options_hyperparams_component(agent_type, model_type, noise_functions)

        # Hide header if no kernel options
        hide_header = True if not noise_functions else False

        return tabs, hide_header

    @app.callback(
        Output({'type': 'lr-scheduler-tabs-hyperparam', 'model': MATCH, 'agent': MATCH}, 'children'),
        Input({'type': 'lr-scheduler-hyperparam', 'model': MATCH, 'agent': MATCH}, 'value'),
        State({'type': 'lr-scheduler-hyperparam', 'model': MATCH, 'agent': MATCH}, 'id'),
    )
    def update_lr_scheduler_tabs_hyperparam(lr_schedulers, lr_schedulers_id):
        agent_type = lr_schedulers_id['agent']
        model_type = lr_schedulers_id['model']
        #DEBUG
        print(f'agent:{agent_type}, model:{model_type}')
        return dash_utils.update_lr_scheduler_hyperparam_options(agent_type, model_type, lr_schedulers)

    @app.callback(
        Output({'type':'hidden-div-hyperparam', 'page':'/hyperparameter-search'}, 'children'),
        Input({'type':'start', 'page':'/hyperparameter-search'}, 'n_clicks'),
        State({'type':'agent-store', 'page':'/hyperparameter-search'}, 'data'),
        State({'type':'storage', 'page':'/hyperparameter-search'}, 'data'),
        State('search-type', 'value'),
        State({'type': 'projects-dropdown', 'page': '/hyperparameter-search'}, 'value'),
        State('sweep-name', 'value'),
        State('goal-metric', 'value'),
        State('goal-type', 'value'),
        State({'type':'library-select', 'page':'/hyperparameter-search'}, 'value'),
        State({'type': 'env-dropdown', 'page': '/hyperparameter-search'}, 'value'),
        State({'type':'gym-params', 'page':'/hyperparameter-search'}, 'children'),
        State({"type":"gym_wrappers_dropdown", "page":'/build-agent'}, "value"),
        State({'type':'agent-type-selector', 'page':'/hyperparameter-search'}, 'value'),
        State('num-sweeps', 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL}, 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL}, 'id'),
        State({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'id'),
        prevent_initial_call=True
    )
    def begin_sweep(num_clicks, agent_data, data, method, project, sweep_name, metric_name, metric_goal, env_library, env, env_params, env_wrappers, agent, num_sweeps, all_values, all_ids, all_indexed_values, all_indexed_ids):
        print('begin sweep callback fired...')
        try:
            if num_clicks > 0:
                # extract any additional gym env params
                params = dash_utils.extract_gym_params(env_params)
                print('gym params extracted')
                # Check if wandb config is uploaded and if so, use it
                if agent_data:
                    print('sweep config detected')
                    sweep_config = agent_data
                    print('uploaded sweep config set')
                else:
                    sweep_config = dash_utils.create_wandb_config(
                        method,
                        project,
                        sweep_name,
                        metric_name,
                        metric_goal,
                        env_library,
                        env,
                        params,
                        env_wrappers,
                        agent,
                        all_values,
                        all_ids,
                        all_indexed_values,
                        all_indexed_ids
                    )

                if sweep_config:  # Check if sweep_config is not empty
                    # Create an empty dict for sweep_config.json
                    # train_config = {}
                    # Build train config
                    # train_config = utils.build_train_config(agent, sweep_config, num_sweeps)
                    
                
                    # Save the updated configuration to a train config file
                    # os.makedirs('sweep', exist_ok=True)
                    # train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')
                    # with open(train_config_path, 'w') as f:
                    #     json.dump(train_config, f)

                    # Save the sweep config and set the sweep config path
                    sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')
                    with open(sweep_config_path, 'w') as f:
                        json.dump(sweep_config, f)
                    print('saved sweep config')

                    # Construct and run the MPI command
                    # command = [
                    #     'mpiexec', '-np', str(num_workers), 'python', 'init_sweep.py',
                    #     '--sweep_config', sweep_config_path
                    #     # '--train_config', train_config_path
                    # ]

                    # Construct and run the sweep.py script
                    # print('constructing terminal command...')
                    # command = [
                    #     'python', 'sweep.py',
                    #     '--sweep_config', sweep_config_path,
                    #     '--num_sweeps', str(num_sweeps),
                    # ]
                    # print('command constructed')

                    # subprocess.Popen(command)
                    sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config["project"])

                    # num_sweep_agents = train_config['num_agents'] if train_config['num_agents'] is not None else 1
                    # print(f'num sweep agents:{num_sweep_agents}')

                    # if num_sweep_agents > 1:
                    #     processes = []
                    #     for agent in range(num_sweep_agents):
                    #         p = multiprocessing.Process(target=run_agent, args=(sweep_id, sweep_config, train_config))
                    #         p.start()
                    #         processes.append(p)

                    #     for p in processes:
                    #         p.join()
                    
                    # else:
                    run_agent(sweep_id, sweep_config, num_sweeps)
                    


        except KeyError as e:
            logger.error(f"KeyError in W&B stream handling: {e}")
        except Exception as e:
            logger.error(f"An error occurred: {e}")
                
                
                

                # command = ['python', 'sweep.py']

                
                # subprocess.Popen(command)
            

        raise PreventUpdate


    @app.callback(
        Output({'type':'hyperparameter-selector', 'page':'/hyperparameter-search'}, 'options'),
        Input('update-hyperparam-selector', 'n_intervals'),
        State({'type':'start', 'page':'/hyperparameter-search'}, 'n_clicks'),
        State({'type':'projects-dropdown', 'page':'/hyperparameter-search'}, 'value'),
        State('sweep-name', 'value'),
        prevent_initial_call=True,
    )
    def update_hyperparameter_list(n_intervals, num_clicks, project_name, sweep_id):
        if num_clicks > 0:
            # print('update hyperparameter list')
            hyperparameters = wandb_support.fetch_sweep_hyperparameters_single_run(project_name, sweep_id)
            # print(f'retrieved params: {hyperparameters}')
            # print('')
            # task = tasks.test_task.delay()
            return [{'label': hp, 'value': hp} for hp in hyperparameters]
        else:
            return []
        
            
    @app.callback(
        Output({'type':'heatmap-data-store', 'page':'/hyperparameter-search'}, 'data'),
        Input('heatmap-store-data-interval', 'n_intervals'),
        State({'type':'heatmap-data-store', 'page':'/hyperparameter-search'}, 'data'),
    )
    def update_heatmap_data(n, data):
        if 'matrix_data' not in shared_data:
            raise PreventUpdate
        matrix_data = shared_data.get('matrix_data')
        bin_ranges = shared_data.get('bin_ranges')
        if matrix_data:
            # print(f'heatmap data: {data}')
            new_data = {'matrix_data': matrix_data, 'bin_ranges': bin_ranges}
            print(f'new data:{new_data}')
            return new_data
        else:
            return None
        
    
    @app.callback(
        Output({'type':'heatmap-container', 'page':'/hyperparameter-search'}, 'children'),
        Output({'type':'legend-container', 'page':'/hyperparameter-search'}, 'children'),
        Input({'type':'heatmap-data-store', 'page':'/hyperparameter-search'}, 'data')
    )
    def update_heatmap_container(data):
        heatmap, bar_chart = dash_utils.update_heatmap(data)
        if heatmap is None:
            return dash.no_update
        return dcc.Graph(figure=heatmap), dcc.Graph(figure=bar_chart)
    
    @app.callback(
        Output({'type':'heatmap-placeholder', 'page':'/hyperparameter-search'}, 'style'),
        Input({'type':'heatmap-container', 'page':'/hyperparameter-search'}, 'children')
    )
    def toggle_placeholder(heatmap):
        if heatmap is None:
            return {'display': 'block'}
        return {'display': 'none'}
        
    @app.callback(
        Output('hidden-div-fetch-process', 'children'),
        Input({'type':'start', 'page':'/hyperparameter-search'}, 'n_clicks'),
        State({'type':'agent-store', 'page':'/hyperparameter-search'}, 'data'),
        State({'type': 'projects-dropdown', 'page': '/hyperparameter-search'}, 'value'),
        State('sweep-name', 'value'),
    )
    def start_data_fetch_processes(n_clicks, sweep_config, project, sweep_name):
        if n_clicks > 0:
            # Check if sweep config loaded and use project and sweep name from there if so
            if sweep_config is not None:
                project = sweep_config['project']
                sweep_name = sweep_config['name']
            # Create and start the fetch_data_process
            # print('start data fetch process called')
            fetch_data_thread = threading.Thread(target=fetch_data_process, args=(project, sweep_name, shared_data))
            fetch_data_thread.start()

        return None

    @app.callback(
        Output('hidden-div-matrix-process', 'children'),
        Input('start-matrix-process-interval', 'n_intervals'),
        State({'type':'hyperparameter-selector', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'bin-slider', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'z-score-checkbox', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'reward-threshold', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'start', 'page':'/hyperparameter-search'}, 'n_clicks'),
    )
    def start_matrix_process(n, hyperparameters, bins, zscore_option, reward_threshold, n_clicks):
    # Create and start the update_heatmap_process
        if n_clicks > 0:
            z_score = 'zscore' in zscore_option
            # print('start matrix process callback called')
            # print(f'hyperparameters passed to start matrix callback: {hyperparameters}')
            update_heatmap_thread = threading.Thread(target=update_heatmap_process, args=(shared_data, hyperparameters, bins, z_score, reward_threshold))
            update_heatmap_thread.start()
        
        return None


    @app.callback(
        Output({'type':'sweeps-dropdown', 'page':'/co-occurrence-analysis'}, 'options'),
        Input({'type': 'projects-dropdown', 'page': '/co-occurrence-analysis'}, 'value'),
        prevent_initial_call=True,
    )
    def update_sweeps_dropdown(project):
        if project is not None:
            sweep_names = wandb_support.get_sweeps_from_name(project)
            return [{'label': name, 'value': name} for name in sweep_names]
        
    @app.callback(
        Output({'type':'heatmap-data-store', 'page':'/co-occurrence-analysis'}, 'data'),
        Output({'type':'output-data-upload', 'page':'/co-occurrence-analysis'}, 'children'),
        Input({'type':'sweep-data-button', 'page': '/co-occurrence-analysis'}, 'n_clicks'),
        State({'type':'projects-dropdown', 'page': '/co-occurrence-analysis'}, 'value'),
        State({'type':'sweeps-dropdown', 'page': '/co-occurrence-analysis'}, 'value'),
        State({'type':'heatmap-data-store', 'page':'/co-occurrence-analysis'}, 'data'),
        prevent_initial_call=True,
    )
    def get_sweep_data(n_clicks, project, sweeps, co_occurrence_data):
        try:
            if n_clicks > 0:
                dfs = []
                for sweep in sweeps:
                    metrics_data = wandb_support.get_metrics(project, sweep)
                    logger.debug(f'get_sweep_data: metrics data: {metrics_data}')
                    formatted_data = wandb_support.format_metrics(metrics_data)
                    logger.debug(f'get_sweep_data: formatted data: {formatted_data}')
                    dfs.append(formatted_data)
                data = pd.concat(dfs, ignore_index=True)
                data_json = data.to_json(orient='split')
                co_occurrence_data['formatted_data'] = data_json
                logger.debug(f'get_sweep_data: co_occurrence_data[formatted_data]: {co_occurrence_data["formatted_data"]}')
                # create a Div containing a success message to return
                success_message = html.Div([
                    dbc.Alert("Data Loaded.", color="success")
                ])
                return co_occurrence_data, success_message

            return None
        except Exception as e:
            logger.error(f"Error in get_sweep_data: {e}", exc_info=True)
    
    @app.callback(
        Output({'type':'hyperparameter-selector', 'page':'/co-occurrence-analysis'}, 'options'),
        Input({'type':'sweeps-dropdown', 'page':'/co-occurrence-analysis'}, 'value'),
        State({'type':'projects-dropdown', 'page':'/co-occurrence-analysis'}, 'value'),
        prevent_initial_call=True,
    )
    def update_co_occurance_hyperparameter_dropdown(sweeps, project):
        hyperparameters = wandb_support.fetch_sweep_hyperparameters_single_run(project, sweeps[0])

        return [{'label': hp, 'value': hp} for hp in hyperparameters]
    
    # @app.callback(
    #     Output({'type':'hyperparameter-selector', 'page':'/hyperparameter-search'}, 'options'),
    #     Input({'type':'sweeps-dropdown', 'page':'/hyperparameter-search'}, 'value'),
    #     State({'type':'projects-dropdown', 'page':'/hyperparameter-search'}, 'value'),
    #     prevent_initial_call=True,
    # )
    # def update_wandb_sweep_hyperparameter_dropdown(sweeps, project):
    #     hyperparameters = wandb_support.fetch_sweep_hyperparameters_single_run(project, sweeps[0])

    #     return [{'label': hp, 'value': hp} for hp in hyperparameters]


    @app.callback(
        Output({'type':'heatmap-container', 'page':'/co-occurrence-analysis'}, 'children'),
        Output({'type':'legend-container', 'page':'/co-occurrence-analysis'}, 'children'),
        Input({'type':'heatmap-data-store', 'page':'/co-occurrence-analysis'}, 'data'),
        Input({'type':'hyperparameter-selector', 'page':'/co-occurrence-analysis'}, 'value'),
        Input({'type':'bin-slider', 'page':'/co-occurrence-analysis'}, 'value'),
        Input({'type':'z-score-checkbox', 'page':'/co-occurrence-analysis'}, 'value'),
        Input({'type':'reward-threshold', 'page':'/co-occurrence-analysis'}, 'value'),
        prevent_initial_call=True,
    )
    def update_co_occurrence_graphs(data, hyperparameters, bins, zscore_option, reward_threshold):
        try:
            logger.debug(f'update_co_occurrence_graphs: data: {data}')
            # f_data = data['formatted_data']
            # Convert the JSON string back to a pandas DataFrame
            formatted_data = pd.read_json(data['formatted_data'], orient='split')
            logger.debug(f"update_co_occurrence_graphs: formatted_data dataframe: {formatted_data}")
            z_score = 'zscore' in zscore_option
            matrix_data, bin_ranges = wandb_support.calculate_co_occurrence_matrix(formatted_data, hyperparameters, reward_threshold, bins, z_score)
            data['matrix_data'] = matrix_data.to_dict(orient='split')
            logger.debug(f'update_co_occurrence_graphs: matrix data: {data["matrix_data"]}')
            data['bin_ranges'] = bin_ranges
            logger.debug(f'update_co_occurrence_graphs: bin_ranges: {data["bin_ranges"]}')
            heatmap, bar_chart = dash_utils.update_heatmap(data)
            
            return dcc.Graph(figure=heatmap), dcc.Graph(figure=bar_chart)
        except Exception as e:
            logger.error(f"Error in update_co_occurrence_graphs: {e}", exc_info=True)

    @app.callback(
    Output("download-wandb-config", "data"),
    [Input("download-wandb-config-button", "n_clicks")],
    [
        State('search-type', 'value'),
        State({'type': 'projects-dropdown', 'page': '/hyperparameter-search'}, 'value'),
        State('sweep-name', 'value'),
        State('goal-metric', 'value'),
        State('goal-type', 'value'),
        State({'type':'library-select', 'page':'/hyperparameter-search'}, 'value'),
        State({'type': 'env-dropdown', 'page': '/hyperparameter-search'}, 'value'),
        State({'type':'gym-params', 'page':'/hyperparameter-search'}, 'children'),
        State({"type":"gym_wrappers_dropdown", "page":'/build-agent'}, "value"),
        State({'type':'agent-type-selector', 'page':'/hyperparameter-search'}, 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL}, 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL}, 'id'),
        State({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'value'),
        State({'type': ALL, 'model': ALL, 'agent': ALL, 'index': ALL}, 'id'),
    ],
    prevent_initial_call=True,
    )
    def download_wandb_config(num_clicks, method, project, sweep_name, metric_name, metric_goal, env_library, env, env_params, env_wrappers, agent_selection, all_values, all_ids, all_indexed_values, all_indexed_ids):
        # extract any additional gym env params
        params = dash_utils.extract_gym_params(env_params)

        if num_clicks > 0:
            wandb_config = dash_utils.create_wandb_config(
                method,
                project,
                sweep_name,
                metric_name,
                metric_goal,
                env_library,
                env,
                params,
                env_wrappers,
                agent_selection,
                all_values,
                all_ids,
                all_indexed_values,
                all_indexed_ids
            )
            config_json = json.dumps(wandb_config, indent=4)

            return dict(content=config_json, filename="wandb_config.json")
        
    @app.callback(
    Output("download-sweep-config", "data"),
    [Input("download-sweep-config-button", "n_clicks")],
    [
        State('num-sweeps', 'value'),
        State('num-episodes', 'value'),
        State('num-epochs', 'value'),
        State('num-cycles', 'value'),
        State('num-updates', 'value'),
        State({'type':'mpi', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'workers', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'num-sweep-agents', 'page':'/hyperparameter-search'}, 'value'),
        State({'type':'seed', 'page':'/hyperparameter-search'}, 'value'),
    ],
    prevent_initial_call=True,
    )
    def download_train_config(n_clicks, num_sweeps, num_episodes, num_epochs, num_cycles, num_updates, use_mpi, num_workers, num_agents, seed):
        if n_clicks > 0:
            # Create an empty dict for sweep_config.json
            sweep_config = {}
            # Add config options to run_config
            sweep_config['num_sweeps'] = num_sweeps
            sweep_config['num_episodes'] = num_episodes
            sweep_config['seed'] = seed if seed is not None else None

            # Add MPI config if not None else None
            sweep_config['use_mpi'] = use_mpi if use_mpi is not None else None
            sweep_config['num_workers'] = num_workers if num_workers is not None else None
            sweep_config['num_agents'] = num_agents if num_agents is not None else None
            
            # Update additional settings for HER agent
            sweep_config['num_epochs'] = num_epochs if num_epochs is not None else None
            sweep_config['num_cycles'] = num_cycles if num_cycles is not None else None
            sweep_config['num_updates'] = num_updates if num_updates is not None else 1

            config_json = json.dumps(sweep_config, indent=4)

            return dict(content=config_json, filename="sweep_config.json")
```

---

## dash_utils.py

```python
from zipfile import ZipFile
import os
import re
from natsort import natsorted
import requests
from pathlib import Path
import shutil
from dash import html, dcc
import dash_bootstrap_components as dbc
import plotly.express as px
import plotly.graph_objects as go
import re

import wandb
import wandb_support
# import ray.tune as tune
import gymnasium as gym
import gymnasium.wrappers as base_wrappers
import gymnasium_robotics as gym_robo
# import tensorflow as tf
import numpy as np

import rl_agents
import models
from models import StochasticContinuousPolicy, StochasticDiscretePolicy, ActorModel, ValueModel, CriticModel
import rl_callbacks
from env_wrapper import GymnasiumWrapper, IsaacSimWrapper
from noise import *
from buffer import *

# Wrappers NOT in the user dropdown
EXCLUSION_LIST = {
    "RecordEpisodeStatistics",
    "RenderCollection",
    "RecordVideo",
    "HumanRendering",
    "DelayObservation",
    "MaxAndSkipObservation",
    "TransformAction",
    "ClipAction",
    "RescaleAction",
    "TransformObservation",
    "FilterObservation",
    "FlattenObservation",
    "ReshapeObservation",
    "RescaleObservation",
    "DtypeObservation",
    "AddRenderObservation",
    "Autoreset",
    "JaxToNumpy",
    "JaxToTorch",
    "NumpyToTorch",
    "OrderEnforcing",
    "PassiveEnvChecker",
    "vector"
}

# Wrappers that require extra parameter inputs:
WRAPPER_REGISTRY = {
    "AtariPreprocessing": {
        "cls": base_wrappers.AtariPreprocessing,
        "default_params": {
            "frame_skip": 1,
            "grayscale_obs": True,
            "scale_obs": True
        }
    },
    "TimeLimit": {
        "cls": base_wrappers.TimeLimit,
        "default_params": {
            "max_episode_steps": 1000
        }
    },
    "TimeAwareObservation": {
        "cls": base_wrappers.TimeAwareObservation,
        "default_params": {
            "flatten": False,
            "normalize_time": False
        }
    },
    "FrameStackObservation": {
        "cls": base_wrappers.FrameStackObservation,
        "default_params": {
            "stack_size": 4
        }
    },
    "ResizeObservation": {
        "cls": base_wrappers.ResizeObservation,
        "default_params": {
            "shape": 84
        }
    }
}

def camel_to_words(name: str) -> str:
    """
    Convert CamelCase to spaced words with each word capitalized.
    E.g. 'AtariPreprocessing' -> 'Atari Preprocessing'
    """
    # Insert a space before any capital letter that isn't the first character
    spaced = re.sub(r'(?<!^)(?=[A-Z])', ' ', name)
    # Now split on spaces and capitalize each token
    words = [word.capitalize() for word in spaced.split()]
    # Join them back into a single string
    return ' '.join(words)

def get_wrappers_dropdown_options():
    """
    Returns a sorted list of *all* wrappers from base_wrappers.__all__,
    minus those in EXCLUSION_LIST, for the user to select from.
    """
    all_wrappers = set(base_wrappers.__all__)
    valid_wrappers = all_wrappers - EXCLUSION_LIST
    return sorted(valid_wrappers)

# def create_wrappers_list(selected_wrappers, user_params):
#     """
#     Given a list of selected wrapper names and a dictionary of user-overridden
#     parameters, return a list of wrapper factory functions.

#     Any wrapper found in WRAPPER_REGISTRY uses user_params if provided.
#     Any wrapper not in WRAPPER_REGISTRY is applied with no arguments.
#     """
#     #DEBUG
#     if not selected_wrappers:
#         return []

#     wrappers_list = []

#     for w_name in selected_wrappers:
#         # If wrapper is in registry, it may have parameter overrides
#         if w_name in WRAPPER_REGISTRY:
#             wrapper_cls = WRAPPER_REGISTRY[w_name]["cls"]
#             default_params = WRAPPER_REGISTRY[w_name]["default_params"]
#             override_params = user_params.get(w_name, {}) if user_params else {}
#             final_params = {**default_params, **override_params}

#             # Example: special case for ResizeObservation
#             if w_name == "ResizeObservation" and isinstance(final_params.get("shape", None), int):
#                 side = final_params["shape"]
#                 final_params["shape"] = (side, side)

#             def wrapper_factory(env, cls=wrapper_cls, fparams=final_params):
#                 return cls(env, **fparams)

#         else:
#             # It's NOT in WRAPPER_REGISTRY => no-arg wrapper
#             wrapper_cls = getattr(base_wrappers, w_name, None)
#             # If the user only picks from a valid filtered list, wrapper_cls should not be None.
#             if wrapper_cls is None:
#                 # If somehow the user selected an invalid or excluded wrapper, skip or log
#                 continue

#             def wrapper_factory(env, cls=wrapper_cls):
#                 return cls(env)

#         wrappers_list.append(wrapper_factory)

#     return wrappers_list

def format_wrappers(wrapper_store):
    wrappers_dict = {}
    for key, value in wrapper_store.items():
        # Split the key into wrapper type and parameter name
        parts = key.split('_param:')
        # print(f'parts:{parts}')
        wrapper_type = parts[0].split('wrapper:')[1]
        # print(f'wrapper_type:{wrapper_type}')
        param_name = parts[1]
        # print(f'param name:{param_name}')
        
        # If the wrapper type already exists in the dictionary, append to its params
        if wrapper_type not in wrappers_dict:
            wrappers_dict[wrapper_type] = {'type': wrapper_type, 'params': {}}
        
        wrappers_dict[wrapper_type]['params'][param_name] = value
    
    # Convert the dictionary to a list of dictionaries
    formatted_wrappers = list(wrappers_dict.values())
    
    return formatted_wrappers

def get_key(id_dict, param=None):
    """
    Generate a key for the params_store dictionary based on the id_dict and an optional parameter.

    Args:
        id_dict (dict): The dictionary containing keys like 'type', 'model', 'agent', and optionally 'index'.
        param (str): An optional parameter name to append to the key.

    Returns:
        str: The formatted key as a string.
    """
    if param:
        #DEBUG
        base_key = "_".join(f"{k}:{v}" for k, v in list(id_dict.items())[1:])
        # print(f"key=type:{param}_{base_key}")
        return f"type:{param}_{base_key}"
    else:
        base_key = "_".join(f"{k}:{v}" for k, v in list(id_dict.items()))
        return base_key

def get_param_from_key(key):
    return key.split("_")[0].split(":")[-1].replace("-","_")

def get_specific_value(all_values, all_ids, id_type, model_type, agent_type):
    #DEBUG
    # print(f'get_specific_value fired...')
    #DEBUG
    # print(f'all values: {all_values}')
    # print(f'all ids: {all_ids}')
    for id_dict, value in zip(all_ids, all_values):
        # Check if this id dictionary matches the criteria
        #DEBUG
        # print(f'checking id_dict: {id_dict} and value: {value}')
        if id_dict.get('type') == id_type and id_dict.get('model') == model_type and id_dict.get('agent') == agent_type:
            #DEBUG
            # print(f"Found value {value} for {id_type} {model_type} {agent_type}")
            return value
    # Return None or some default value if not found
    return None

def get_specific_value_id(all_values, all_ids, value_type, model_type, agent_type, index):
    #DEBUG
    # print(f'get_specific_value id fired...')
    # print(f'all_values: {all_values}')
    # print(f'all_ids: {all_ids}')
    for id_dict, value in zip(all_ids, all_values):
        # print(f'checking id_dict: {id_dict} and value: {value}')
        if 'index' in id_dict.keys():
            # Check if this id dictionary matches the criteria
            if id_dict.get('type') == value_type and id_dict.get('model') == model_type and id_dict.get('agent') == agent_type and id_dict.get('index') == index:
                #DEBUG
                # print(f"Found value {value} for {value_type} {model_type} {agent_type}")
                return value
    # Return None or some default value if not found
    return None

def create_noise_object(env, model_type, agent_type, agent_params):
    noise_type = agent_params.get(get_key({'type':'noise-function', 'model':model_type, 'agent':agent_type}))

    if noise_type == "Normal":
        return Noise.create_instance(
            noise_class_name=noise_type,
            shape=env.action_space.shape,
            mean=agent_params.get(get_key({'type':'normal-mean', 'model':model_type, 'agent':agent_type})),
            stddev=agent_params.get(get_key({'type':'normal-stddv', 'model':model_type, 'agent':agent_type})),
            device=agent_params.get(get_key({'type':'device', 'model':model_type, 'agent':agent_type})),
        )

    elif noise_type == "Uniform":
        return Noise.create_instance(
            noise_class_name=noise_type,
            shape=env.action_space.shape,
            minval=agent_params.get(get_key({'type':'uniform-min', 'model':model_type, 'agent':agent_type})),
            maxval=agent_params.get(get_key({'type':'uniform-max', 'model':model_type, 'agent':agent_type})),
            device=agent_params.get(get_key({'type':'device', 'model':model_type, 'agent':agent_type})),
        )

    elif noise_type == "Ornstein-Uhlenbeck":
        return Noise.create_instance(
            noise_class_name=noise_type,
            shape=env.action_space.shape,
            mean=agent_params.get(get_key({'type':'ou-mean', 'model':model_type, 'agent':agent_type})),
            theta=agent_params.get(get_key({'type':'ou-theta', 'model':model_type, 'agent':agent_type})),
            sigma=agent_params.get(get_key({'type':'ou-sigma', 'model':model_type, 'agent':agent_type})),
            dt=agent_params.get(get_key({'type':'ou-dt', 'model':model_type, 'agent':agent_type})),
            device=agent_params.get(get_key({'type':'device', 'model':model_type, 'agent':agent_type})),
        )

def format_layers(model, agent, agent_params):
    # DEBUG
    print(f'Agent params: {agent_params}')
    
    layer_config = []

    # Extract keys related to the specified model and agent
    relevant_keys = {
        key: value for key, value in agent_params.items()
        if f"model:{model}" in key and f"agent:{agent}" in key
    }

    # Determine the indices of layers for the given model
    layer_indices = set(
        int(key.split("_")[-1].split(":")[-1])  # Extract the index
        for key in relevant_keys.keys()
        if "index:" in key
    )

    # DEBUG
    # print(f'Layer indices: {layer_indices}')

    # Iterate over each layer index and construct the layer configuration
    for index in sorted(layer_indices):
        if index > 0:
            layer_entry = {}

            # Construct keys for the current index
            type_key = f"type:layer-type-dropdown_model:{model}_agent:{agent}_index:{index}"
            if type_key in relevant_keys:
                layer_entry["type"] = relevant_keys[type_key]

            # Handle layers with parameters (e.g., dense, conv2d)
            if layer_entry["type"] == "dense":
                layer_entry["params"] = format_dense_layer(model, agent, index, relevant_keys)

            elif layer_entry["type"] == "conv2d":
                layer_entry["params"] = format_cnn_layer(model, agent, index, relevant_keys)

            elif layer_entry["type"] == "layernorm":
                layer_entry["params"] = format_layernorm_layer(model, agent, index, relevant_keys)


            # Handle layers with no params
            elif layer_entry["type"] in ["relu", "tanh", "flatten"]:
                layer_entry.pop("params", None)  # Remove params if present

            # Append the structured layer entry
            layer_config.append(layer_entry)
            # DEBUG
            print(f'Layer entry added: {layer_entry}')

    print(f'Final layer config: {layer_config}')
    return layer_config

def format_dense_layer(model, agent, index, keys):
    units_key = f"type:num-units_model:{model}_agent:{agent}_index:{index}"
    kernel_key = f"type:kernel-init_model:{model}_agent:{agent}_index:{index}"
    kernel_params_key = f"type:kernel-params_model:{model}_agent:{agent}_index:{index}"
    bias_key = f"type:bias_model:{model}_agent:{agent}_index:{index}"

    params = {
        "units": keys.get(units_key, None),
        "kernel": keys.get(kernel_key, "default"),
        "kernel params": keys.get(kernel_params_key, {}),
        "bias": keys.get(bias_key, True)
    }

    kernel_type = params["kernel"]
    KERNEL_PARAMS_MAP = get_kernel_params_map()
    if kernel_type in KERNEL_PARAMS_MAP:
        for param in KERNEL_PARAMS_MAP[kernel_type]:
            param_key = f"type:{param}_model:{model}_agent:{agent}_index:{index}"
            if param_key in keys:
                params["kernel params"][param] = keys[param_key]
    return params

def format_layernorm_layer(model, agent, index, keys):
    normalized_shape_key = f"type:normalized-shape_model:{model}_agent:{agent}_index:{index}"
    return {
        "normalized_shape": keys.get(normalized_shape_key, None)
    }

def format_cnn_layer(model, agent, index, keys):
    out_channels_key = f"type:out-channels_model:{model}_agent:{agent}_index:{index}"
    kernel_size_key = f"type:kernel-size_model:{model}_agent:{agent}_index:{index}"
    stride_key = f"type:stride_model:{model}_agent:{agent}_index:{index}"
    padding_key = f"type:padding-dropdown_model:{model}_agent:{agent}_index:{index}"
    bias_key = f"type:bias_model:{model}_agent:{agent}_index:{index}"
    kernel_key = f"type:kernel-init_model:{model}_agent:{agent}_index:{index}"
    kernel_params_key = f"type:kernel-params_model:{model}_agent:{agent}_index:{index}"

    params = {
        "out_channels": keys.get(out_channels_key, 32),
        "kernel_size": keys.get(kernel_size_key, 2),
        "stride": keys.get(stride_key, 1),
        "padding": keys.get(padding_key, 0),
        "bias": keys.get(bias_key, True),
        "kernel": keys.get(kernel_key, "default"),
        "kernel params": keys.get(kernel_params_key, {}),
    }
    return params


#TODO Don't think I need this function anymore
# def format_cnn_layers(all_values, all_ids, all_indexed_values, all_indexed_ids, model_type, agent_type):
#     layers = []
#     # Get num CNN layers for model type
#     num_cnn_layers = get_specific_value(
#         all_values=all_values,
#         all_ids=all_ids,
#         id_type='conv-layers',
#         model_type=model_type,
#         agent_type=agent_type,
#     )
#     #DEBUG
#     # print(f'num_cnn_layers: {num_cnn_layers}')

#     # Loop through num of CNN layers
#     for index in range(1, num_cnn_layers+1):
#         # Get the layer type
#         layer_type = get_specific_value_id(
#             all_values=all_indexed_values,
#             all_ids=all_indexed_ids,
#             value_type='cnn-layer-type',
#             model_type=model_type,
#             agent_type=agent_type,
#             index=index
#         )
#         #DEBUG
#         # print(f'layer_type: {layer_type}')

#         # Parse layer types to set params
#         if layer_type == "conv":
#             params = {}
#             params['out_channels'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='conv-filters',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             params['kernel_size'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='conv-kernel-size',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             params['stride'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='conv-stride',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             padding = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='conv-padding',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             if padding == "custom":
#                 params['padding'] = get_specific_value_id(
#                     all_values=all_indexed_values,
#                     all_ids=all_indexed_ids,
#                     value_type='conv-padding-custom',
#                     model_type=model_type,
#                     agent_type=agent_type,
#                     index=index
#                 )

#             else:
#                 params['padding'] = padding


#             params['bias'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='conv-use-bias',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             # Append to layers list
#             layers.append({layer_type: params})
#             continue
        
#         elif layer_type == "batchnorm":
#             params = {}
#             params['num_features'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='batch-features',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             layers.append({layer_type: params})
#             continue
        
#         elif layer_type == "pool":
#             params = {}
#             params['kernel_size'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='pool-kernel-size',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             params['stride'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='pool-stride',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             layers.append({layer_type: params})
#             continue

#         elif layer_type == 'dropout':
#             params = {}
#             params['p'] = get_specific_value_id(
#                 all_values=all_indexed_values,
#                 all_ids=all_indexed_ids,
#                 value_type='dropout-prob',
#                 model_type=model_type,
#                 agent_type=agent_type,
#                 index=index
#             )
#             layers.append({layer_type: params})
#             continue

#         elif layer_type == 'relu':
#             params = {}
#             layers.append({layer_type: params})

#         elif layer_type == 'tanh':
#             params = {}
#             layers.append({layer_type: params})

#         else:
#             raise ValueError(f'Layer type {layer_type} not supported')
        
#     return layers




def get_projects():
    """Returns a list of projects from the W&B API."""

    api = wandb.Api()

    return [project.name for project in api.projects()]


def get_callbacks(callbacks, project):
    
    callback_objects = {
        "Weights & Biases": rl_callbacks.WandbCallback(project),
    }
    #DEBUG
    # print(f'callbacks: {callbacks}')

    callbacks_list = [rl_callbacks.DashCallback("http://127.0.0.1:8050")]
    # for id_dict, value in zip(all_ids, all_values):
    #     if id_dict.get('type') == 'callback':
    #         #DEBUG
    #         print(f'callback type found: {id_dict, value}')
    #         # Ensure value is a list before iterating over it
    #         selected_callbacks = value if isinstance(value, list) else [value] # make value variable a list
    #         #DEBUG
    #         print(f'selected_callbacks: {selected_callbacks}')
    #         for callback_name in selected_callbacks:
    #             #DEBUG
    #             print(f'callback_name: {callback_name}')
    #             if callback_name in callbacks:
    #                 #DEBUG
    #                 print(f'matched callback: {callback_name}')
    #                 callbacks_list.append(callbacks[callback_name])

    for callback in callbacks:
        if callback in callback_objects:
            callbacks_list.append(callback_objects[callback])

    #DEBUG
    # print(f'callback list: {callbacks_list}')
    return callbacks_list


def zip_agent_files(source_dir, output_zip):
    # Print the directory being zipped and list its contents
    # print(f"Zipping contents of: {source_dir}")
    files_found = os.listdir(source_dir)
    # print("Files found:", files_found)
    
    if not files_found:
        # print("No files found to zip. Exiting zip operation.")
        return
    
    with ZipFile(output_zip, 'w') as zipf:
        for root, dirs, files in os.walk(source_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # Ensure the file exists before adding it
                if os.path.isfile(file_path):
                    zipf.write(file_path, os.path.relpath(file_path, source_dir))
                    # print(f"Added {file} to ZIP archive.")
                # else:
                    # print(f"Skipped {file}, not found or inaccessible.")


def load(agent_data, env_name):
    # check if the env name in agent data matches env_name var
    # check if the agent_data has the key 'agent'
    if 'agent' in agent_data:
        # if 'agent' key exists, use agent_data['agent']['env'] as the env
        agent_env = agent_data['agent']['env']
    else:
        # if 'agent' key doesn't exist, use agent_data['env'] as the env
        agent_env = agent_data['env']
    
    
    if agent_env == env_name:
        #DEBUG
        # print('env name matches!')
        # Load the agent
        return rl_agents.load_agent_from_config(agent_data)

    # else (they don't match) change params to match new environment action space
    # check what the agent type is to update params accordingly
    if agent_data['agent_type'] == 'Reinforce' or agent_data['agent_type'] == 'ActorCritic':
        env=gym.make(env_name)
        
        policy_optimizer = agent_data['policy_model']['optimizer']['class_name']
        
        value_optimizer = agent_data['value_model']['optimizer']['class_name']

        policy_layers = [(units, activation, initializer) for (units, activation, initializer) in agent_data['policy_model']['dense_layers']]

        ##DEBUG
        # print("Policy layers:", policy_layers)
        
        value_layers = [(units, activation, initializer) for (units, activation, initializer) in agent_data['value_model']['dense_layers']]

        ##DEBUG
        # print("Value layers:", value_layers)

        policy_model = models.StochasticDiscretePolicy(
                env=env,
                dense_layers=policy_layers,
                optimizer=policy_optimizer,
                learning_rate=agent_data['learning_rate'],
            )
        value_model = models.ValueModel(
            env=env,
            dense_layers=value_layers,
            optimizer=value_optimizer,
            learning_rate=agent_data['learning_rate'],
        )
        
        if agent_data['agent_type'] == "Reinforce":
            
            agent = rl_agents.Reinforce(
                env=env,
                policy_model=policy_model,
                value_model=value_model,
                discount=agent_data['discount'],
                callbacks = [rl_callbacks.load(callback['class_name'], callback['config']) for callback in agent_data['callbacks']],
                save_dir=agent_data['save_dir'],
            )
        elif agent_data['agent_type'] == "ActorCritic":
            
            agent = rl_agents.ActorCritic(
                env=gym.make(env),
                policy_model=policy_model,
                value_model=value_model,
                discount=agent_data['discount'],
                policy_trace_decay=agent_data['policy_trace_decay'],
                value_trace_decay=agent_data['policy_trace_decay'],
                callbacks = [rl_callbacks.load(callback['class_name'], callback['config']) for callback in agent_data['callbacks']],
                save_dir=agent_data['save_dir'],
            )
    elif agent_data['agent_type'] == "DDPG":
        # set defualt gym environment in order to build policy and value models and save
        env=gym.make(env_name)

        # set actor and critic model params
        actor_optimizer = agent_data['actor_model']['optimizer']['class_name']
        critic_optimizer = agent_data['critic_model']['optimizer']['class_name']
        
        actor_layers = [(units, activation, initializer) for (units, activation, initializer) in agent_data['actor_model']['dense_layers']]

        ##DEBUG
        # print("Actor layers:", actor_layers)
        
        critic_state_layers = [(units, activation, initializer) for (units, activation, initializer) in agent_data['critic_model']['state_layers']]
        ##DEBUG
        # print("Critic state layers:", critic_state_layers)
        critic_merged_layers = [(units, activation, initializer) for (units, activation, initializer) in agent_data['critic_model']['merged_layers']]
        ##DEBUG
        # print("Critic merged layers:", critic_merged_layers)

        actor_model = models.ActorModel(
            env=env,
            dense_layers=actor_layers,
            learning_rate=agent_data['actor_model']['learning_rate'],
            optimizer=actor_optimizer
        )
        ##DEBUG
        # print("Actor model:", actor_model.get_config())
        
        critic_model = models.CriticModel(
            env=env,
            state_layers=critic_state_layers,
            merged_layers=critic_merged_layers,
            learning_rate=agent_data['critic_model']['learning_rate'],
            optimizer=critic_optimizer
        )
        ##DEBUG
        # print("Critic model:", critic_model.get_config())
        agent = rl_agents.DDPG(
            env=env,
            actor_model=actor_model,
            critic_model=critic_model,
            discount=agent_data['discount'],
            tau=agent_data['tau'],
            replay_buffer=helper.ReplayBuffer(env, 100000),
            batch_size=agent_data['batch_size'],
            noise=helper.Noise.create_instance(agent_data["noise"]["class_name"], **agent_data["noise"]["config"]),
            callbacks = [rl_callbacks.load(callback['class_name'], callback['config']) for callback in agent_data['callbacks']],
            save_dir=agent_data['save_dir'],
        )

        #TODO: ADD IF HER
    
    return agent

# def train_model(agent_data, env_name, num_episodes, render, render_freq, num_epochs=None, num_cycles=None, num_updates=None, workers=None):  
#     # print('Training agent...')
#     # agent = rl_agents.load_agent_from_config(save_dir)
#     agent = load(agent_data, env_name)
#     # print('Agent loaded.')
#     if agent_data['agent_type'] == "HER":
#         agent.train(num_epochs, num_cycles, num_episodes, num_updates, render, render_freq)
#     else:
#         agent.train(num_episodes, render, render_freq)
#     # print('Training complete.')

# def test_model(agent_data, env_name, num_episodes, render, render_freq):  
#     # print('Testing agent...')
#     # agent = rl_agents.load_agent_from_config(save_dir)
#     agent = load(agent_data, env_name)
#     # print('Agent loaded.')
#     agent.test(num_episodes, render, render_freq)
#     # print('Testing complete.')

def delete_renders(folder_path):
    # Iterate over the files in the folder
    for filename in os.listdir(folder_path):
        # Check if the file has a .mp4 or .meta.json extension
        if filename.endswith(".mp4") or filename.endswith(".meta.json"):
            # Construct the full file path
            file_path = os.path.join(folder_path, filename)
            # Remove the file
            os.remove(file_path)

def get_video_files(page, agent_data):

    # get correct model folder to look in for renders
    agent_type = agent_data['agent_type']
    if agent_type == "HER":
        agent_type = agent_data['agent']['agent_type']
    
    # check if path exists
    if os.path.exists(f'assets/models/{agent_type}/renders'):
        try:
            if page == "/train-agent":
                # Get video files from training renders folder
                return natsorted([f for f in os.listdir(Path(f"assets/models/{agent_type}/renders/training")) if f.endswith('.mp4')])
            elif page == "/test-agent":
                # Get video files from testing renders folder
                return natsorted([f for f in os.listdir(Path(f"assets/models/{agent_type}/renders/testing")) if f.endswith('.mp4')])
        except Exception as e:
            print(f"Failed to get video files: {e}")

def reset_agent_status_data(data):
    data['data'] = None
    data['progress'] = 0.0
    data['status'] = "Pending"
    
    return data


## LAYOUT COMPONENT GENERATORS

# Function to generate carousel items from video paths
def generate_video_items(video_files, page, agent_data):
    if page == "/train-agent":
        folder = 'training'
    elif page == "/test-agent":
        folder = 'testing'
    else:
        raise ValueError(f"Invalid page {page}")
    
    # determine agent type to determine folder to look in for renders folder
    agent_type = agent_data['agent_type']
    if agent_type == "HER":
        agent_type = agent_data['agent']['agent_type']
    return [
        html.Video(src=f'assets/models/{agent_type}/renders/{folder}/{video_file}', controls=True,
                   style={'width': '100%', 'height': 'auto'},
                   id={
                       'type':'video',
                       'page':page,
                       'index':index
                   })
        for index, video_file in enumerate(natsorted(video_files))
    ]



# Component for file upload
def upload_component(page):
    return dcc.Upload(
        id={
            'type': 'upload-agent-config',
            'page': page,
        },
        children=html.Div([
            'Drag and Drop or ',
            html.A('Select Files')
        ]),
        style={
            'width': '100%',
            'height': '60px',
            'lineHeight': '60px',
            'borderWidth': '1px',
            'borderStyle': 'dashed',
            'borderRadius': '5px',
            'textAlign': 'center',
            'margin': '10px'
        },
        multiple=False
    )

def instantiate_envwrapper_obj(library:str, env_id:str, wrappers:list = None):
    # Instantiates an EnvWrapper object for an env of library
    if library == 'gymnasium':
        env = gym.make(env_id)
        print(f'env spec:{env.spec}')
        return GymnasiumWrapper(env.spec, wrappers)
        
    elif library == 'isaacsim':
        # Placeholder for IsaacSim environments
        pass

# Environment dropdown components
def env_dropdown_component(page):
    allowed_wrappers = get_wrappers_dropdown_options()
    layout = html.Div([
        html.H2("Select Environment"),
        dcc.Dropdown(
            id={
                'type': 'library-select',
                'page': page,
            },
            options=[
                {'label': 'Gymnasium', 'value': 'gymnasium'},
                {'label': 'IsaacSim', 'value': 'isaacsim'}
            ],
            value=None,
            placeholder="Select Environment Library"
        ),
        dcc.Dropdown(
            id={
                'type': 'env-dropdown',
                'page': page,
            },
            options=[],
            placeholder="Select Gym Environment",
            style={'display': 'none'},
        ),
        html.H2("Select Wrappers"),
        dcc.Dropdown(
            id={
                'type': 'gym_wrappers_dropdown',
                'page': page,
            },
            options=[{"label": camel_to_words(w), "value": w} for w in allowed_wrappers],
            multi=True,
            placeholder="Select one or more wrappers..."
        ),
        dcc.Tabs(id={"type":"wrapper-tabs", "page":page}, children=[]),

        # This store will hold user param overrides for those wrappers in the registry
        dcc.Store(id={"type":"wrappers_params_store", "page":page}, data={}),
    ])
    print(f'layer:{layout}')
    return layout

def get_all_gym_envs():
    """
    Returns a list of the latest gym environment versions, excluding:
      - Certain directories (phys2d/, tabular/, etc.).
      - Atari games that contain 'Deterministic', 'NoFrameskip', or '-ram' in their IDs.
      - All older versions if there's a newer version available.
    """
    # 1) Directories to exclude entirely:
    exclude = ["Gym", "phys2d/", "tabular/"]

    # 2) Collect all env specs, skipping excluded directories
    all_specs = [
        spec for spec in gym.envs.registry.values()
        if not any(spec.id.startswith(ex) for ex in exclude)
    ]

    # 3) Exclude Atari variants with 'Deterministic', 'NoFrameskip', or '-ram'
    #    If you ONLY want to exclude these for Atari envs, check if env is "Atari-like".
    #    In practice, though, those suffixes only appear for Atari anyway.
    def is_unwanted_atari_variant(env_id: str) -> bool:
        return (
            "Deterministic" in env_id
            or "NoFrameskip" in env_id
            or "-ram" in env_id
        )
    
    filtered_specs = []
    for spec in all_specs:
        # skip if it has the "unwanted Atari variant" substrings
        if is_unwanted_atari_variant(spec.id):
            continue
        filtered_specs.append(spec)

    # 4) Now parse the final list to group them by (base_name) -> version
    def parse_env_id(env_id: str):
        """
        e.g., "ALE/Breakout-v5" -> ("Breakout", 5)
              "Breakout-v4"     -> ("Breakout", 4)
              "CartPole-v1"     -> ("CartPole", 1)
              "ALE/Crossbow-v5" -> ("Crossbow", 5)
        """
        # Remove the directory prefix if it exists: "ALE/Breakout-v5" -> "Breakout-v5"
        final_name = env_id.split("/")[-1]
        if "-v" in final_name:
            base_name, version_str = final_name.rsplit("-v", 1)
            try:
                version = int(version_str)
            except ValueError:
                version = None
        else:
            base_name = final_name
            version = None
        return base_name, version

    # Group by base_name
    grouped = {}
    for spec in filtered_specs:
        base_name, version = parse_env_id(spec.id)
        grouped.setdefault(base_name, []).append((spec.id, version))

    # Pick the environment that has the highest version for each base_name
    def version_or_neg1(v):
        return v if v is not None else -1

    latest_envs = []
    for base_name, items in grouped.items():
        best_item = max(items, key=lambda x: version_or_neg1(x[1]))  # max by version
        latest_envs.append(best_item[0])  # just the env_id

    # print(latest_envs)

    # Sort so the list is stable and predictable
    return sorted(latest_envs)

def get_env_data(env_name):
    env_data = {
    "CartPole-v1": {
        "description": "Similar to CartPole-v0 but with a different set of parameters for a harder challenge.",
        "gif_url": "https://gymnasium.farama.org/_images/cart_pole.gif",
    },
    "MountainCar-v0": {
        "description": "A car is on a one-dimensional track between two mountains; the goal is to drive up the mountain on the right.",
        "gif_url": "https://gymnasium.farama.org/_images/mountain_car.gif",
    },
    "MountainCarContinuous-v0": {
        "description": "A continuous control version of the MountainCar environment.",
        "gif_url": "https://gymnasium.farama.org/_images/mountain_car.gif",
    },
    "Pendulum-v1": {
        "description": "Control a frictionless pendulum to keep it upright.",
        "gif_url": "https://gymnasium.farama.org/_images/pendulum.gif",
    },
    "Acrobot-v1": {
        "description": "A 2-link robot that swings up to reach a given height.",
        "gif_url": "https://gymnasium.farama.org/_images/acrobot.gif",
    },
    "LunarLander-v2": {
        "description": "Lunar Lander description",
        "gif_url": "https://gymnasium.farama.org/_images/lunar_lander.gif",
    },
    "LunarLanderContinuous-v2": {
        "description": "A continuous control version of the LunarLander environment.",
        "gif_url": "https://gymnasium.farama.org/_images/lunar_lander.gif",
    },
    "BipedalWalker-v3": {
        "description": "Control a two-legged robot to walk through rough terrain without falling.",
        "gif_url": "https://gymnasium.farama.org/_images/bipedal_walker.gif",
    },
    "BipedalWalkerHardcore-v3": {
        "description": "A more challenging version of BipedalWalker with harder terrain and obstacles.",
        "gif_url": "https://gymnasium.farama.org/_images/bipedal_walker.gif",
    },
    "CarRacing-v2": {
        "description": "A car racing environment where the goal is to complete a track as quickly as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/car_racing.gif",
    },
    "Blackjack-v1": {
        "description": "A classic Blackjack card game environment.",
        "gif_url": "https://gymnasium.farama.org/_images/blackjack1.gif",
    },
    "FrozenLake-v1": {
        "description": "Navigate a grid world to reach a goal without falling into holes, akin to crossing a frozen lake.",
        "gif_url": "https://gymnasium.farama.org/_images/frozen_lake.gif",
    },
    "FrozenLake8x8-v1": {
        "description": "An 8x8 version of the FrozenLake environment, providing a larger and more complex grid.",
        "gif_url": "https://gymnasium.farama.org/_images/frozen_lake.gif",
    },
    "CliffWalking-v0": {
        "description": "A grid-based environment where the agent must navigate cliffs to reach a goal.",
        "gif_url": "https://gymnasium.farama.org/_images/cliff_walking.gif",
    },
    "Taxi-v3": {
        "description": "A taxi must pick up and drop off passengers at designated locations.",
        "gif_url": "https://gymnasium.farama.org/_images/taxi.gif",
    },
    "Reacher-v4": {
        "description": "Control a robotic arm to reach a target location",
        "gif_url": "https://gymnasium.farama.org/_images/reacher.gif",
    },
    "Reacher-v5": {
        "description": "Control a robotic arm to reach a target location",
        "gif_url": "https://gymnasium.farama.org/_images/reacher.gif",
    },
    "Pusher-v4": {
        "description": "A robot arm needs to push objects to a target location.",
        "gif_url": "https://gymnasium.farama.org/_images/pusher.gif",
    },
    "Pusher-v5": {
        "description": "A robot arm needs to push objects to a target location.",
        "gif_url": "https://gymnasium.farama.org/_images/pusher.gif",
    },
    "InvertedPendulum-v4": {
        "description": "Balance a pendulum in the upright position on a moving cart",
        "gif_url": "https://gymnasium.farama.org/_images/inverted_pendulum.gif",
    },
    "InvertedPendulum-v5": {
        "description": "Balance a pendulum in the upright position on a moving cart",
        "gif_url": "https://gymnasium.farama.org/_images/inverted_pendulum.gif",
    },
    "InvertedDoublePendulum-v4": {
        "description": "A more complex version of the InvertedPendulum with two pendulums to balance.",
        "gif_url": "https://gymnasium.farama.org/_images/inverted_double_pendulum.gif",
    },
    "InvertedDoublePendulum-v5": {
        "description": "A more complex version of the InvertedPendulum with two pendulums to balance.",
        "gif_url": "https://gymnasium.farama.org/_images/inverted_double_pendulum.gif",
    },
    "HalfCheetah-v4": {
        "description": "Control a 2D cheetah robot to make it run as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/half_cheetah.gif",
    },
    "HalfCheetah-v5": {
        "description": "Control a 2D cheetah robot to make it run as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/half_cheetah.gif",
    },
    "Hopper-v4": {
        "description": "Make a two-dimensional one-legged robot hop forward as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/hopper.gif",
    },
    "Hopper-v5": {
        "description": "Make a two-dimensional one-legged robot hop forward as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/hopper.gif",
    },
    "Swimmer-v4": {
        "description": "Control a snake-like robot to make it swim through water.",
        "gif_url": "https://gymnasium.farama.org/_images/swimmer.gif",
    },
    "Swimmer-v5": {
        "description": "Control a snake-like robot to make it swim through water.",
        "gif_url": "https://gymnasium.farama.org/_images/swimmer.gif",
    },
    "Walker2d-v4": {
        "description": "A bipedal robot walking simulation aiming to move forward as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/walker2d.gif",
    },
    "Walker2d-v5": {
        "description": "A bipedal robot walking simulation aiming to move forward as fast as possible.",
        "gif_url": "https://gymnasium.farama.org/_images/walker2d.gif",
    },
    "Ant-v4": {
        "description": "Control a four-legged robot to explore a terrain.",
        "gif_url": "https://gymnasium.farama.org/_images/ant.gif",
    },
    "Ant-v5": {
        "description": "Control a four-legged robot to explore a terrain.",
        "gif_url": "https://gymnasium.farama.org/_images/ant.gif",
    },
    "Humanoid-v4": {
        "description": "A two-legged humanoid robot that learns to walk and balance.",
        "gif_url": "https://gymnasium.farama.org/_images/humanoid.gif",
    },
    "Humanoid-v5": {
        "description": "A two-legged humanoid robot that learns to walk and balance.",
        "gif_url": "https://gymnasium.farama.org/_images/humanoid.gif",
    },
    "HumanoidStandup-v4": {
        "description": "The goal is to make a humanoid stand up from a prone position.",
        "gif_url": "https://gymnasium.farama.org/_images/humanoid_standup.gif",
    },
    "HumanoidStandup-v5": {
        "description": "The goal is to make a humanoid stand up from a prone position.",
        "gif_url": "https://gymnasium.farama.org/_images/humanoid_standup.gif",
    },
    "FetchReach-v2": {
        "description": "The goal in the environment is for a manipulator to move the end effector to a randomly \
            selected position in the robots workspace.",
        "gif_url": "https://robotics.farama.org/_images/reach.gif",
    },
    "FetchSlide-v2": {
        "description": "The task in the environment is for a manipulator hit a puck in order to reach a target \
            position on top of a long and slippery table.",
        "gif_url": "https://robotics.farama.org/_images/slide.gif",
    },
    "FetchPickAndPlace-v2": {
        "description": "The goal in the environment is for a manipulator to move a block to a target position on \
            top of a table or in mid-air.",
        "gif_url": "https://robotics.farama.org/_images/pick_and_place.gif",
    },
    "FetchPush-v2": {
        "description": "The goal in the environment is for a manipulator to move a block to a target position on \
            top of a table by pushing with its gripper.",
        "gif_url": "https://robotics.farama.org/_images/push.gif",
    },
    "HandReach-v1": {
        "description": "The goal of the task is for the fingertips of the hand to reach a predefined \
            target Cartesian position.",
        "gif_url": "https://robotics.farama.org/_images/reach1.gif",
    },
    "HandManipulateBlockRotateZ-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. There is a random target rotation \
            around the z axis of the block. No target position. Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlockRotateZ_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockRotateZ_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockRotateParallel-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. There is a random target rotation around \
            the z axis of the block and axis-aligned target rotations for the x and y axes. \
            No target position. Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlockRotateParallel_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockRotateParallel_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockRotateXYZ-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. There is a random target rotation for all \
            axes of the block. No target position. Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlockRotateXYZ_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockRotateXYZ_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlockFull-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. There is a Random target rotation \
            for all axes of the block. Random target position. Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlock-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. Rewards are Sparse",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlockDense-v1": {
        "description": "In this task a block is placed on the palm of the hand. The task is to then \
            manipulate the block such that a target pose is achieved. Rewards are Dense",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block.gif",
    },
    "HandManipulateBlock_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateBlock_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateBlock environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_block_touch_sensors.gif",
    },
    "HandManipulateEgg-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEggDense-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            Rewards are dense.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEggRotate-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            There is a random target rotation for all axes of the egg. No target position. \
            Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEggRotateDense-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            There is a random target rotation for all axes of the egg. No target position. \
            Rewards are dense.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEggFull-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            There is a random target rotation for all axes of the egg. Random target position. \
            Rewards are sparse.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEggFullDense-v1": {
        "description": "The task to be solved is very similar to that in the HandManipulateBlock \
            environment, but in this case an egg-shaped object is placed on the palm of the hand. \
            The task is to then manipulate the object such that a target pose is achieved. \
            There is a random target rotation for all axes of the egg. Random target position. \
            Rewards are dense.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg.gif",
    },
    "HandManipulateEgg_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggRotate_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            No target position. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggFull_ContinuousTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            Random target position. Rewards are sparse. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEgg_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggRotate_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            No target position. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggFull_BooleanTouchSensors-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            Random target position. Rewards are sparse. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEgg_ContinuousTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are dense. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggRotate_ContinuousTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            No target position. Rewards are dense. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggFull_ContinuousTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            Random target position. Rewards are dense. Continuous action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEgg_BooleanTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. Rewards are dense. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggRotate_BooleanTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            No target position. Rewards are dense. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
    "HandManipulateEggFull_BooleanTouchSensorsDense-v1": {
        "description": "The task to be solved is the same as in the HandManipulateEgg environment. \
            However, in this case the environment observation also includes tactile sensory information. \
            This is achieved by placing a total of 92 MuJoCo touch sensors in the palm and finger \
            phalanxes of the hand. There is a random target rotation for all axes of the egg. \
            Random target position. Rewards are dense. Discrete action space.",
        "gif_url": "https://robotics.farama.org/_images/manipulate_egg_touch_sensors.gif",
    },
}
    description = env_data[env_name]['description']
    gif_url = env_data[env_name]['gif_url']

    return description, gif_url

def update_run_options(agent_type, page):
    if agent_type == 'PPO':
        return create_ppo_run_options(page)
    elif agent_type == 'HER':
        return create_her_run_options(page)
    elif agent_type == 'Reinforce':
        return create_reinforce_run_options(page)
    elif agent_type == 'ActorCritic':
        return create_actor_critic_run_options(page)
    elif agent_type == 'DDPG':
        return create_ddpg_run_options(page)
    elif agent_type == 'TD3':
        return create_td3_run_options(page)
    elif agent_type == 'HER':
        return create_her_run_options(page)
    else:
        # return some default or "unknown agent type" message
        return html.Div([
            html.P(f"No options found for agent type: {agent_type}")
        ])


def create_ppo_run_options(page):
    return html.Div(
        [
            create_num_timesteps_component(page),
            create_trajectory_length_component(page),
            create_batch_size_component(page),
            create_learning_epochs_component(page),
            *create_common_run_components(page)
        ],
    ),

def create_reinforce_run_options(page):
    components = []
    components.append(create_num_episodes_component(page))
    if page == '/train-agent':
        components.append(create_batch_size_component(page))
    
    return html.Div([
        *components,
        *create_common_run_components(page)
    ])

def create_actor_critic_run_options(page):
    components = []
    components.append(create_num_episodes_component(page))
    
    return html.Div([
        *components,
        *create_common_run_components(page)
    ])

def create_ddpg_run_options(page):
    components = []
    components.append(create_num_episodes_component(page))
    
    return html.Div([
        *components,
        *create_common_run_components(page)
    ])

def create_td3_run_options(page):
    components = []
    components.append(create_num_episodes_component(page))
    
    return html.Div([
        *components,
        *create_common_run_components(page)
    ])

def create_her_run_options(page):
    components = []
    components.append(create_num_epochs_component(page))
    components.append(create_num_cycles_component(page))
    components.append(create_num_episodes_component(page))
    components.append(create_learning_epochs_component(page))
    return html.Div([
        *components,
        *create_common_run_components(page)
    ])

def create_num_timesteps_component(page):
    return dcc.Input(
                id={
                    'type': 'num-timesteps',
                    'page': page,
                },
                type='number',
                placeholder="Number of Timesteps",
                min=1,
            )
def create_trajectory_length_component(page):
    return dcc.Input(
                id={
                    'type': 'traj-length',
                    'page': page,
                },
                type='number',
                placeholder="Trajectories Length (timesteps)",
                min=1,
            )

def create_batch_size_component(page):
    return dcc.Input(
                id={
                    'type': 'batch-size',
                    'page': page,
                },
                type='number',
                placeholder="Batch Size",
                min=1,
            )

def create_learning_epochs_component(page):
    return dcc.Input(
                id={
                    'type': 'learning-epochs',
                    'page': page,
                },
                type='number',
                placeholder="Learning Epochs",
                min=1,
            )

def create_num_episodes_component(page):
    return dcc.Input(
                    id={
                        'type': 'num-episodes',
                        'page': page,
                    },
                    type='number',
                    placeholder="Number of Episodes",
                    min=1,
                )

def create_num_envs_component(page):
    return dcc.Input(
            id={
                'type': 'num-envs',
                'page': page,
            },
            type='number',
            placeholder="Number of Envs",
            min=1,
        )

def create_load_weights_component(page):
    return dcc.Checklist(
            options=[
                {'label': 'Load Weights', 'value': True}
            ],
            id={
                'type': 'load-weights',
                'page': page,
            },
        )

def create_render_episode_component(page):
    return html.Div([
        html.Label('Render Episodes'),
        dcc.RadioItems(
            id={
                'type': 'render-option',
                'page': page,
            },
            options=[
                {'label': 'True', 'value': True},
                {'label': 'False', 'value': False},
            ],
            value=False,
            style={'margin-left': '10px'},
        ),
        html.Div(
            id = {
                'type':'render-block',
                'page':page,
            },
            children = [
                html.Label('Render Frequency'),
                dcc.Input(
                    id={
                        'type': 'render-freq',
                        'page': page,
                    },
                    type='number',
                    placeholder="Every 'n' Episodes",
                    min=1,
                )
            ],
            style={'margin-left': '10px', 'display':'none'}
        )
    ])

def create_num_epochs_component(page):
    return dcc.Input(
            id={
                'type': 'num-epochs',
                'page': page,
            },
            type='number',
            placeholder="Number of Epochs",
            min=1,
        )

def create_num_cycles_component(page):
    return dcc.Input(
            id={
                'type': 'num-cycles',
                'page': page,
            },
            type='number',
            placeholder="Number of Cycles",
            min=1,
        )
        

def create_common_run_components(page):
    common = []
    # if page == '/train-agent':
    common.append(create_num_envs_component(page))
    common.append(create_seed_component(page))
    common.append(create_load_weights_component(page))
    common.append(create_render_episode_component(page))
    # Create hidden div to serve as dummy output for train callback
    hidden = html.Div(
            id={
                'type':'hidden-div',
                'page':page,
            },
            style={'display': 'none'}
        )
    common.append(hidden)

    return common

# Training settings component
# def run_agent_settings_component(page, agent_type=None):
#     return html.Div([
#         html.Div(
#             id={
#                 'type': 'her-options',
#                 'page': page,
#             },
#             style={'display': 'none'},
#             children=[
#                 dcc.Input(
#                     id={
#                         'type': 'epochs',
#                         'page': page,
#                     },
#                     type='number',
#                     placeholder="Number of Epochs",
#                     min=1,
#                 ),
#                 dcc.Input(
#                     id={
#                         'type': 'cycles',
#                         'page': page,
#                     },
#                     type='number',
#                     placeholder="Number of Cycles",
#                     min=1,
#                 ),
#                 dcc.Input(
#                     id={
#                         'type': 'learning-cycles',
#                         'page': page,
#                     },
#                     type='number',
#                     placeholder="Number of Learning Cycles",
#                     min=1,
#                 ),
#             ]
#         ),
#         html.Div(
#             id={
#                 'type':'hidden-div',
#                 'page':page,
#             },
#             style={'display': 'none'}
#         ),
#     ])

# custom carousel for video playback
# def video_carousel_component(page, video_files=[]):
#     return html.Div([
#         html.Div(id={'type':'video-carousel', 'page':page}, children=[],
#                  style={'display': 'flex', 'overflowX': 'scroll', 'justifyContent': 'center', 'alignItems': 'center'}),
#         html.Div([
#             html.Button('Prev', id='prev-btn', n_clicks=0, style={'marginRight': '10px'}),
#             html.Button('Next', id='next-btn', n_clicks=0),
#         ], style={'textAlign': 'center', 'marginTop': '20px'}),
#     ], style={'width': '60%', 'margin': 'auto'})
def video_carousel_component(page):
    return html.Div([
        dcc.Store(id={'type': 'video-carousel-store', 'page': page}, data={'current_video': 0, 'video_list':[]}),
        html.Div(id={'type': 'video-carousel', 'page': page}),
        html.Div(id={'type': 'video-filename', 'page': page}, style={'textAlign': 'center', 'margin': '10px 0'}),
        html.Div([
            html.Button('Prev', id={'type': 'prev-btn', 'page': page}, n_clicks=0, style={'marginRight': '10px'}),
            html.Button('Next', id={'type': 'next-btn', 'page': page}, n_clicks=0),
        ], style={'textAlign': 'center', 'marginTop': '20px'}),
    ], style={'width': '60%', 'margin': 'auto'})

# AGENT HYPERPARAM COMPONENTS #

def create_device_input(agent_type):
    return html.Div(
        [
            # html.Label('Computation Device', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'device',
                    'model':'none',
                    'agent':agent_type
                },
                options=[
                    {'label': 'CPU', 'value': 'cpu'},
                    {'label': 'CUDA', 'value': 'cuda'},
                ],
                placeholder='Computation Device'
            )
        ]
    )

def create_save_dir_input(agent_type):
    return html.Div(
        [
            html.Label('Save Directory:', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'save-dir',
                    'model':'none',
                    'agent':agent_type
                },
                type='text',
                placeholder='path/to/model'
            )
        ]
    )


def get_kernel_initializer_inputs(selected_initializer, initializer_id, agent_params):
    # Dictionary mapping the initializer names to the corresponding function
    #DEBUG
    print(f'selected initializer:{selected_initializer}, initializer_id:{initializer_id}')
    initializer_input_creators = {
        "variance_scaling": create_variance_scaling_inputs,
        "constant": create_constant_initializer_inputs,
        "normal": create_normal_initializer_inputs,
        "uniform": create_uniform_initializer_inputs,
        "truncated_normal": create_truncated_normal_initializer_inputs,
        "xavier_uniform": create_xavier_uniform_initializer_inputs,
        "xavier_normal": create_xavier_normal_initializer_inputs,
        "kaiming_uniform": create_kaiming_uniform_initializer_inputs,
        "kaiming_normal": create_kaiming_normal_initializer_inputs,
    }
    
    # Call the function associated with the selected_initializer,
    # or return an empty html.Div() if not found
    if selected_initializer in initializer_input_creators:
        # return initializer_input_creators.get(selected_initializer, lambda: html.Div())(initializer_id)
        return initializer_input_creators.get(selected_initializer)(initializer_id, agent_params)
    elif selected_initializer not in ['ones', 'zeros', 'orthogonal', 'default']:
        raise ValueError(f"{selected_initializer} not in initializer input creator dict")

def create_kaiming_normal_initializer_inputs(initializer_id, agent_params):
    """Component for kaiming uniform initializer hyperparameters"""
    # return html.Div(
    #     id={
    #         'type': 'kernel-params',
    #         'model': initializer_id['model'],
    #         'agent': initializer_id['agent']
    #         },
    children=[
        html.Label('Mode', style={'text-decoration': 'underline'}),
        dcc.Dropdown(
            id={
                'type':'mode',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
                },
            options=[
                    {'label': 'fan in', 'value': 'fan_in'},
                    {'label': 'fan out', 'value': 'fan_out'},
                ],
            value=agent_params.get(get_key(initializer_id, 'mode'), 'fan_in'),
        ),
        html.Hr(),
    ]
    return children
    


def create_kaiming_uniform_initializer_inputs(initializer_id, agent_params):
    """Component for kaiming uniform initializer hyperparameters"""
    children=[
        html.Label('Mode', style={'text-decoration': 'underline'}),
        dcc.Dropdown(
            id={
                'type':'mode',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
                },
            options=[
                    {'label': 'fan in', 'value': 'fan_in'},
                    {'label': 'fan out', 'value': 'fan_out'},
                ],
            value=agent_params.get(get_key(initializer_id, 'mode'), 'fan_in'),
        ),
        html.Hr(),
    ]
    return children
                    


def create_xavier_normal_initializer_inputs(initializer_id, agent_params):
    """Component for xavier uniform initializer hyperparameters"""
    children=[
        html.Label('Gain', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
                'type':'gain',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
            },
            type='number',
            min=1.0,
            max=3.0,
            step=1.0,
            value=agent_params.get(get_key(initializer_id, 'gain'), 1.0)
        ),
        html.Hr(),
    ]
    return children



def create_xavier_uniform_initializer_inputs(initializer_id, agent_params):
    """Component for xavier uniform initializer hyperparameters"""
    children=[
        html.Label('Gain', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
                'type':'gain',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
            },
            type='number',
            min=1.0,
            max=3.0,
            step=1.0,
            value=agent_params.get(get_key(initializer_id, 'gain'), 1.0)
        ),
        html.Hr(),
    ]
    return children

def create_truncated_normal_initializer_inputs(initializer_id, agent_params):
    """Component for truncated normal initializer hyperparameters"""
    children=[
        html.Label('Mean', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'mean',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=0.00,
            max=1.00,
            step=0.01,
            value=agent_params.get(get_key(initializer_id, 'mean'), 0.00)
        ),

        html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'std',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=0.01,
            max=3.00,
            step=0.01,
            value=agent_params.get(get_key(initializer_id, 'std'), 1.00)
        ),
        html.Hr(),
    ]
    return children


def create_uniform_initializer_inputs(initializer_id, agent_params):
    """Component for random uniform initializer hyperparameters"""
    children=[
        html.Label('Minimum', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'a',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=-1.000,
            max=1.000,
            step=0.0001,
            value=agent_params.get(get_key(initializer_id, 'a'), -1.000)
        ),

        html.Label('Maximum', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'b',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=-1.000,
            max=1.000,
            step=0.0001,
            value=agent_params.get(get_key(initializer_id, 'b'), 1.000)
        ),
        html.Hr(),
    ]
    return children


def create_normal_initializer_inputs(initializer_id, agent_params):
    """Component for random normal initializer hyperparameters"""
    children=[
        html.Label('Mean', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'mean',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=-1.00,
            max=1.00,
            step=0.01,
            value=agent_params.get(get_key(initializer_id, 'mean'), 0.0)
        ),

        html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'std',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=0.01,
            max=2.00,
            step=0.01,
            value=agent_params.get(get_key(initializer_id, 'std'), 1.0)
        ),
        html.Hr(),
    ]
    return children


def create_constant_initializer_inputs(initializer_id, agent_params):
    """Component for constant initializer hyperparameters"""
    children=[
        html.Label('Value', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'val',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=0.001,
            max=0.99,
            step=0.001,
            value=agent_params.get(get_key(initializer_id, 'val'), 1.0)
        ),
        html.Hr(),
    ]
    return children


def create_variance_scaling_inputs(initializer_id, agent_params):
    """Component for variance scaling initializer hyperparameters"""
    children=[
        html.Label('Scale', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={
            'type':'scale',
            'model':initializer_id['model'],
            'agent':initializer_id['agent'],
            'index':initializer_id['index'],
            },
            type='number',
            min=1.0,
            max=5.0,
            step=1.0,
            value=agent_params.get(get_key(initializer_id, 'scale'), 2.0)
        ),
        
        html.Label('Mode', style={'text-decoration': 'underline'}),
        dcc.Dropdown(
            id={
                'type':'mode',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
            },
            options=[{'label': mode, 'value': mode} for mode in ['fan_in', 'fan_out', 'fan_avg']],
            placeholder="Mode",
            value=agent_params.get(get_key(initializer_id, 'mode'), 'fan_in')
        ),
        
        html.Label('Distribution', style={'text-decoration': 'underline'}),
        dcc.Dropdown(
            id={
                'type':'distribution',
                'model':initializer_id['model'],
                'agent':initializer_id['agent'],
                'index':initializer_id['index'],
            },
            options=[{'label': dist, 'value': dist} for dist in ['truncated_normal', 'uniform']],
            placeholder="Distribution",
            value=agent_params.get(get_key(initializer_id, 'distribution'), 'truncated_normal')
        ),
        html.Hr(),
    ]
    return children

def format_output_kernel_initializer_config(model_type, agent_type, agent_params):
    """Returns an initializer object based on initializer component values"""

    initializer_type = agent_params.get(get_key({'type':'kernel-init', 'model':model_type, 'agent':agent_type, 'index':0}))

    # create empty dictionary to store initializer config params
    config = {}
    # Iterate over initializer_type params list and get values
    if initializer_type not in ['zeros', 'ones', 'default']: 
        for param in get_kernel_params_map()[initializer_type]:
            config[param] = agent_params.get(get_key({'type':param, 'model':model_type, 'agent':agent_type, 'index':0}))
    
    # Get distribution type in order to format output correctly
    distribution = agent_params.get(get_key({'type':'distribution', 'model':'none', 'agent':agent_type}), None)
    # format
    if distribution is None or distribution == 'categorical':
        initializer_config = [{'type':'dense', 'params':{'kernel':initializer_type, 'kernel params':config}}]
    # elif distribution == 'categorical':
    #     initializer_config = [{'type':'dense', 'params':{'kernel':initializer_type, 'kernel params':config}}]
    elif distribution in ['beta','normal']:
        initializer_config = [{'type':'dense', 'params':{'kernel':initializer_type, 'kernel params':config}},
                              {'type':'dense', 'params':{'kernel':initializer_type, 'kernel params':config}}]

    return initializer_config

def create_discount_factor_input(agent_type):
    return html.Div(
        [
            html.Label('Discount Factor', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'discount',
                    'model':'none',
                    'agent':agent_type
                },
                type='number',
                min=0.01,
                max=0.99,
                step=0.01,
                value=0.99
            )
        ]
    )

def create_advantage_coeff_input(agent_type):
    return html.Div(
        [
            html.Label('Advantage Coefficient', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'advantage-coeff',
                    'model':'none',
                    'agent':agent_type
                },
                type='number',
                min=0.01,
                max=0.99,
                step=0.01,
                value=0.95
            )
        ]
    )

def create_surrogate_loss_clip_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Loss Clip', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'surrogate-clip',
                    'model':model_type,
                    'agent':agent_type
                },
                type='number',
                min=0.01,
                max=0.99,
                step=0.01,
                value=0.2,
            ),
            create_scheduler_input('surrogate-clip', model_type, agent_type)

        ]
    )

# def create_surrogate_loss_clip_scheduler_input(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label(f'{model_type.capitalize()} Clip Scheduler', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'surrogate-clip-scheduler',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 options=[{'label': i, 'value': i.lower()} for i in ['Step', 'Exponential', 'CosineAnnealing', 'Linear', 'None']],
#                 placeholder=f"{model_type.capitalize()} Clip Scheduler",
#             ),
#             html.Div(
#                 id={
#                     'type':'surrogate-clip-scheduler-options',
#                     'model':model_type,
#                     'agent':agent_type,
#                 }
#             )
#         ]
#     )

# def update_surrogate_loss_clip_scheduler_options(agent_type, model_type, surr_clip_scheduler):
#     if surr_clip_scheduler == 'step':
#         return surrogate_loss_clip_step_scheduler_options(agent_type, model_type)
#     elif surr_clip_scheduler == 'exponential':
#         return surrogate_loss_clip_exponential_scheduler_options(agent_type, model_type)
#     elif surr_clip_scheduler == 'cosineannealing':
#         return surrogate_loss_clip_cosineannealing_scheduler_options(agent_type, model_type)
#     elif surr_clip_scheduler == 'linear':
#         return surrogate_loss_clip_linear_scheduler_options(agent_type, model_type)
#     return html.Div()

# def surrogate_loss_clip_cosineannealing_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('T max (max iters)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-t-max',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=10000,
#                 step=1,
#                 value=1000,
#             ),
#             html.Label('Eta min (min LR)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-eta-min',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.000001,
#                 max=0.1,
#                 step=0.000001,
#                 value=0.0001,
#             ),
#         ]
#     )

# def surrogate_loss_clip_exponential_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def surrogate_loss_clip_step_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Step Size', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-step-size',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1000,
#                 step=1,
#                 value=100,
#             ),
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def surrogate_loss_clip_linear_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Start Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-start-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=1.0,
#                 step=0.01,
#                 value=1.0,
#             ),
#             html.Label('End Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-end-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.0001,
#                 max=1.0000,
#                 step=0.0001,
#                 value=0.0010,
#             ),
#             html.Label('Total Iterations', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'surrogate-clip-total-iters',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1e8,
#                 step=1,
#                 value=1e3,
#             ),
#         ]
#     )

# def get_surrogate_loss_clip_scheduler(model_type, agent_type, agent_params):

#     scheduler = agent_params.get(get_key({'type':'surrogate-clip-scheduler', 'model':model_type, 'agent':agent_type}))
#     if scheduler == 'none':
#         return None
#     params = {}
#     if scheduler == 'step':
#         params['step_size'] = agent_params.get(get_key({'type':'surrogate-clip-step-size', 'model':model_type, 'agent':agent_type}))
#         params['gamma'] = agent_params.get(get_key({'type':'surrogate-clip-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'exponential':
#         params['gamma'] = agent_params.get(get_key({'type':'surrogate-clip-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'cosineannealing':
#         params['T_max'] = agent_params.get(get_key({'type':'surrogate-clip-t-max', 'model':model_type, 'agent':agent_type}))
#         params['eta_min'] = agent_params.get(get_key({'type':'surrogate-clip-eta-min', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'linear':
#         params['start_factor'] = agent_params.get(get_key({'type':'surrogate-clip-start-factor', 'model':model_type, 'agent':agent_type}))
#         params['end_factor'] = agent_params.get(get_key({'type':'surrogate-clip-end-factor', 'model':model_type, 'agent':agent_type}))
#         params['total_iters'] = agent_params.get(get_key({'type':'surrogate-clip-total-iters', 'model':model_type, 'agent':agent_type}))

#     return {'type':scheduler, 'params':params}

# def create_value_clip_input(agent_type):
#     return html.Div(
#         [
#             html.Label('Value Loss Clip', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip',
#                     'model':'value',
#                     'agent':agent_type
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.2,
#             ),
#             create_value_clip_scheduler_input(agent_type)

#         ]
#     )

# def create_value_clip_scheduler_input(agent_type):
#     return html.Div(
#         [
#             html.Label('Value Clip Scheduler', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'value-clip-scheduler',
#                     'model':'value',
#                     'agent':agent_type,
#                 },
#                 options=[{'label': i, 'value': i.lower()} for i in ['Step', 'Exponential', 'CosineAnnealing', 'Linear', 'None']],
#                 placeholder="Value Clip Scheduler",
#             ),
#             html.Div(
#                 id={
#                     'type':'value-clip-scheduler-options',
#                     'model':'value',
#                     'agent':agent_type,
#                 }
#             )
#         ]
#     )

# def update_value_clip_scheduler_options(agent_type, model_type, lr_scheduler):
#     if lr_scheduler == 'step':
#         return value_clip_step_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'exponential':
#         return value_clip_exponential_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'cosineannealing':
#         return value_clip_cosineannealing_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'linear':
#         return value_clip_linear_scheduler_options(agent_type, model_type)
#     return html.Div()

# def value_clip_cosineannealing_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('T max (max iters)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-t-max',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=10000,
#                 step=1,
#                 value=1000,
#             ),
#             html.Label('Eta min (min LR)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-eta-min',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.000001,
#                 max=0.1,
#                 step=0.000001,
#                 value=0.0001,
#             ),
#         ]
#     )

# def value_clip_exponential_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def value_clip_step_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Step Size', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-step-size',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1000,
#                 step=1,
#                 value=100,
#             ),
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def value_clip_linear_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Start Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-start-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=1.0,
#                 step=0.01,
#                 value=1.0,
#             ),
#             html.Label('End Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-end-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=1.00,
#                 step=0.01,
#                 value=0.01,
#             ),
#             html.Label('Total Iterations', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'value-clip-total-iters',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1e8,
#                 step=1,
#                 value=1e3,
#             ),
#         ]
#     )

# def get_value_clip_scheduler(model_type, agent_type, agent_params):

#     scheduler = agent_params.get(get_key({'type':'value-clip-scheduler', 'model':model_type, 'agent':agent_type}))
#     if scheduler == 'none':
#         return None
#     params = {}
#     if scheduler == 'step':
#         params['step_size'] = agent_params.get(get_key({'type':'value-clip-step-size', 'model':model_type, 'agent':agent_type}))
#         params['gamma'] = agent_params.get(get_key({'type':'value-clip-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'exponential':
#         params['gamma'] = agent_params.get(get_key({'type':'value-clip-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'cosineannealing':
#         params['T_max'] = agent_params.get(get_key({'type':'value-clip-t-max', 'model':model_type, 'agent':agent_type}))
#         params['eta_min'] = agent_params.get(get_key({'type':'value-clip-eta-min', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'linear':
#         params['start_factor'] = agent_params.get(get_key({'type':'value-clip-start-factor', 'model':model_type, 'agent':agent_type}))
#         params['end_factor'] = agent_params.get(get_key({'type':'value-clip-end-factor', 'model':model_type, 'agent':agent_type}))
#         params['total_iters'] = agent_params.get(get_key({'type':'value-clip-total-iters', 'model':model_type, 'agent':agent_type}))

#     return {'type':scheduler, 'params':params}

def create_grad_clip_input(agent_type, model_type):
    return html.Div(
        [
            # html.Label('Normalize Advantage', style={'text-decoration': 'underline'}),
            dcc.Checklist(
                id={'type':f'clip-grad',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': f'Clip {model_type.capitalize()} Gradient', 'value': True},
                ],
                # value=False
                style={'display':'inline-block'}
            ),
            html.Div(
                id = {
                    'type':f'grad-clip-block',
                    'model':model_type,
                    'agent':agent_type,
                },
                children = [
                    html.Label(f'{model_type.capitalize()} Gradient Clip'),
                    dcc.Input(
                        id={
                            'type': 'grad-clip',
                            'model': model_type,
                            'agent': agent_type
                        },
                        type='number',
                        min=0.01,
                        max=999.0,
                        value=999.0,
                        step=0.01,
                    )
                ],
                style={'display':'none', 'margin-left': '20px'}
            )
        ]
    )

def create_entropy_input(model_type, agent_type):
    return html.Div(
        [
            html.Label('Entropy Coefficient', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'entropy-coeff',
                    'model':model_type,
                    'agent':agent_type
                },
                type='number',
                min=0.000,
                max=0.990,
                step=0.0001,
                value=0.001,
            ),
            create_scheduler_input('entropy', model_type, agent_type)
        ]
    )

def create_scheduler_input(name, model_type, agent_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':f'{name}-scheduler',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i.lower()} for i in ['Step', 'Exponential', 'CosineAnnealing', 'Linear', 'None']],
                placeholder=f"{name.capitalize()} Scheduler",
            ),
            html.Div(
                id={
                    'type':f'{name}-scheduler-options',
                    'model':model_type,
                    'agent':agent_type,
                }
            )
        ]
    )

def update_scheduler_options(name, agent_type, model_type, lr_scheduler):
    if lr_scheduler == 'step':
        return step_scheduler_options(name, agent_type, model_type)
    elif lr_scheduler == 'exponential':
        return exponential_scheduler_options(name, agent_type, model_type)
    elif lr_scheduler == 'cosineannealing':
        return cosineannealing_scheduler_options(name, agent_type, model_type)
    elif lr_scheduler == 'linear':
        return linear_scheduler_options(name, agent_type, model_type)
    return html.Div()

def cosineannealing_scheduler_options(name, agent_type, model_type):
    return html.Div(
        [
            html.Label('T max (max iters)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-t-max',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=10000,
                step=1,
                value=1000,
            ),
            html.Label('Eta min (min LR)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-eta-min',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.000001,
                max=0.1,
                step=0.000001,
                value=0.0001,
            ),
        ]
    )


def exponential_scheduler_options(name, agent_type, model_type):
    return html.Div(
        [
            html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-gamma',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.01,
                max=0.99,
                step=0.01,
                value=0.99,
            ),
        ]
    )

def step_scheduler_options(name, agent_type, model_type):
    return html.Div(
        [
            html.Label('Step Size', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-step-size',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=1000,
                step=1,
                value=100,
            ),
            html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-gamma',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.01,
                max=0.99,
                step=0.01,
                value=0.99,
            ),
        ]
    )

def linear_scheduler_options(name, agent_type, model_type):
    return html.Div(
        [
            html.Label('Start Factor', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-start-factor',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.01,
                max=1.0,
                step=0.01,
                value=1.0,
            ),
            html.Label('End Factor', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-end-factor',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.01,
                max=1.00,
                step=0.01,
                value=0.01,
            ),
            html.Label('Total Iterations', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'{name}-total-iters',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=1e8,
                step=1,
                value=1e3,
            ),
        ]
    )

def get_scheduler(name, model_type, agent_type, agent_params):

    scheduler = agent_params.get(get_key({'type':f'{name}-scheduler', 'model':model_type, 'agent':agent_type}))
    if scheduler == 'none':
        return None
    params = {}
    if scheduler == 'step':
        params['step_size'] = agent_params.get(get_key({'type':f'{name}-step-size', 'model':model_type, 'agent':agent_type}))
        params['gamma'] = agent_params.get(get_key({'type':f'{name}-gamma', 'model':model_type, 'agent':agent_type}))
    elif scheduler == 'exponential':
        params['gamma'] = agent_params.get(get_key({'type':f'{name}-gamma', 'model':model_type, 'agent':agent_type}))
    elif scheduler == 'cosineannealing':
        params['T_max'] = agent_params.get(get_key({'type':f'{name}-t-max', 'model':model_type, 'agent':agent_type}))
        params['eta_min'] = agent_params.get(get_key({'type':f'{name}-eta-min', 'model':model_type, 'agent':agent_type}))
    elif scheduler == 'linear':
        params['start_factor'] = agent_params.get(get_key({'type':f'{name}-start-factor', 'model':model_type, 'agent':agent_type}))
        params['end_factor'] = agent_params.get(get_key({'type':f'{name}-end-factor', 'model':model_type, 'agent':agent_type}))
        params['total_iters'] = agent_params.get(get_key({'type':f'{name}-total-iters', 'model':model_type, 'agent':agent_type}))

    return {'type':scheduler, 'params':params}

def create_kl_coeff_input(agent_type):
    return html.Div(
        [
            html.Label('KL Coefficient', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'kl-coeff',
                    'model':'none',
                    'agent':agent_type
                },
                type='number',
                min=0.00,
                max=10.00,
                step=0.01,
                value=3.00,
            ),
            html.Div(
                [
                    html.Label('Use Adaptive KL'),
                    dcc.RadioItems(
                        id={
                            'type': 'adaptive-kl',
                            'model': 'none',
                            'agent': agent_type,
                        },
                        options=[
                            {'label': 'True', 'value': True},
                            {'label': 'False', 'value': False},
                        ],
                        value=False,
                        style={'margin-left': '10px'},
                    ),
                    html.Div(
                        id = {
                            'type':'adaptive-kl-block',
                            'model':'none',
                            'agent':agent_type,
                        },
                        style={'margin-left': '20px'}
                    )
                ],
            )
        ]
    )

def create_adaptive_kl_options(agent_type):
    return html.Div([
        html.Label('Target KL'),
        dcc.Input(
            id={
                'type': 'adaptive-kl-target-kl',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0.0,
            max=1.0,
            value=0.01,
            step=0.0001,
        ),
        html.Label('Scale Up'),
        dcc.Input(
            id={
                'type': 'adaptive-kl-scale-up',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0.0,
            max=3.0,
            value=2.0,
            step=0.1,
        ),
        html.Label('Scale Down'),
        dcc.Input(
            id={
                'type': 'adaptive-kl-scale-down',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0.0,
            max=3.0,
            value=0.5,
            step=0.1,
        ),
        html.Label('High Tolerance'),
        dcc.Input(
            id={
                'type': 'adaptive-kl-tolerance-high',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0.0,
            max=3.0,
            value=1.5,
            step=0.1,
        ),
        html.Label('Low Tolerance'),
        dcc.Input(
            id={
                'type': 'adaptive-kl-tolerance-low',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0.0,
            max=3.0,
            value=0.5,
            step=0.1,
        )
    ])

def get_kl_adapter(model_type, agent_type, agent_params):
    adapt_kl = agent_params.get(get_key({'type':'adaptive-kl', 'model':model_type, 'agent':agent_type}))
    if not adapt_kl:
        return None
    params = {}
    params['initial_beta'] = agent_params.get(get_key({'type':'kl-coeff', 'model':model_type, 'agent':agent_type}))
    params['target_kl'] = agent_params.get(get_key({'type':'adaptive-kl-target-kl', 'model':model_type, 'agent':agent_type}))
    params['scale_up'] = agent_params.get(get_key({'type':'adaptive-kl-scale-up', 'model':model_type, 'agent':agent_type}))
    params['scale_down'] = agent_params.get(get_key({'type':'adaptive-kl-scale-down', 'model':model_type, 'agent':agent_type}))
    params['kl_tolerance_high'] = agent_params.get(get_key({'type':'adaptive-kl-tolerance-high', 'model':model_type, 'agent':agent_type}))
    params['kl_tolerance_low'] = agent_params.get(get_key({'type':'adaptive-kl-tolerance-low', 'model':model_type, 'agent':agent_type}))

    #DEBUG
    print(f'kl adapter params:{params}')

    return params

def create_value_model_coeff_input(agent_type):
    return html.Div(
        [
            html.Label('Value Model Coefficient', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'value-model-coeff',
                    'model':'value',
                    'agent':agent_type
                },
                type='number',
                min=0.01,
                max=2.00,
                step=0.01,
                value=0.5,
            )
        ]
    )

# def create_ppo_loss_type_input(agent_type):
#     return html.Div(
#         [
#             html.Label('Loss Type', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'loss-type',
#                     'model':'none',
#                     'agent':agent_type,
#                 },
#                 options=[{'label': i, 'value': i.lower()} for i in ["KL", "Clipped", "Hybrid"]],
#                 placeholder="select loss type"
#             ),
#             html.Div(
#                 id={
#                     'type':'entropy-block',
#                     'model':'none',
#                     'agent':agent_type,
#                 },
#                 children = [
#                     html.Label('Entropy Coefficient', style={'text-decoration': 'underline'}),
#                     dcc.Input(
#                     id={
#                         'type':'entropy-value',
#                         'model':'none',
#                         'agent':agent_type,
#                     },
#                     type='number',
#                     # placeholder="Clamp Value",
#                     min=0.001,
#                     max=1.00,
#                     step=0.001,
#                     # value=0.01,
#                     ),
#                 ],
#                 style={'display': 'none'},
#             ),
#             html.Div(
#                 id={
#                     'type':'kl-block',
#                     'model':'none',
#                     'agent':agent_type,
#                 },
#                 children = [
#                     html.Label('KL Coefficient', style={'text-decoration': 'underline'}),
#                     dcc.Input(
#                     id={
#                         'type':'kl-value',
#                         'model':'none',
#                         'agent':agent_type,
#                     },
#                     type='number',
#                     # placeholder="Clamp Value",
#                     min=0.01,
#                     max=5.00,
#                     step=0.01,
#                     ),
#                 ],
#                 style={'display': 'none'},
#             ),
#             html.Div(
#                 id={
#                     'type':'lambda-block',
#                     'model':'none',
#                     'agent':agent_type,
#                 },
#                 children = [
#                     html.Label('Lambda Coefficient', style={'text-decoration': 'underline'}),
#                     dcc.Input(
#                     id={
#                         'type':'lambda-value',
#                         'model':'none',
#                         'agent':agent_type,
#                     },
#                     type='number',
#                     # placeholder="Clamp Value",
#                     min=0.00,
#                     max=1.00,
#                     step=0.01,
#                     # value=0.00,
#                     ),
#                 ],
#                 style={'display': 'none'},
#             ),
#         ]
#     )



def create_normalize_advantage_input(agent_type):
    return html.Div(
        [
            html.Label('Normalize Advantage'),
            dcc.RadioItems(
                id={
                    'type': 'norm-adv',
                    'model': 'none',
                    'agent': agent_type,
                },
                options=[
                    {'label': 'True', 'value': True},
                    {'label': 'False', 'value': False},
                ],
                value=True,
                style={'margin-left': '10px'},
            ),
        ]
    )

def create_normalize_values_input(agent_type):
    return html.Div(
        [
            html.Label('Normalize Values'),
            dcc.RadioItems(
                id={
                    'type': 'norm-values',
                    'model': 'none',
                    'agent': agent_type,
                },
                options=[
                    {'label': 'True', 'value': True},
                    {'label': 'False', 'value': False},
                ],
                value=False,
                style={'margin-left': '10px'},
            ),
            html.Div(
                id = {
                    'type':'norm-clip-block',
                    'model':'none',
                    'agent':agent_type,
                },
                children = [
                    html.Label('Norm Clip'),
                    dcc.Input(
                        id={
                            'type': 'norm-clip',
                            'model': 'none',
                            'agent': agent_type
                        },
                        type='number',
                        min=0.1,
                        max=10.0,
                        value=5.0,
                        step=0.1,
                    )
                ],
                style={'margin-left': '10px', 'display':'none'}
            )
        ]
    )

def create_clip_rewards_input(agent_type):
    return html.Div(
        [
            html.Label('Clip Rewards'),
            dcc.RadioItems(
                id={
                    'type': 'clip-rewards',
                    'model': 'none',
                    'agent': agent_type,
                },
                options=[
                    {'label': 'True', 'value': True},
                    {'label': 'False', 'value': False},
                ],
                value=True,
                style={'margin-left': '10px'},
            ),
            html.Div(
                id = {
                    'type':'clip-rewards-block',
                    'model':'none',
                    'agent':agent_type,
                },
                children = [
                    html.Label('Reward Clip'),
                    dcc.Input(
                        id={
                            'type': 'reward-clip',
                            'model': 'none',
                            'agent': agent_type
                        },
                        type='number',
                        min=0.1,
                        max=10.0,
                        value=1.0,
                        step=0.1,
                    )
                ],
                style={'margin-left': '10px', 'display':'none'}
            )
        ]
    )

def create_distribution_input(agent_type):
    return html.Div(
        [
            html.Label('Probability Distribution', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
            id={
                'type':'distribution',
                'model':'none',
                'agent':agent_type,
                },
                options=[{'label': i.capitalize(), 'value': i} for i in ["beta", "normal", "categorical"]],
                placeholder="select distribution"
            ),
        ]
    )

def create_warmup_input(agent_type):
    return html.Div([
        html.Label('Warmup Period (Steps)'),
        dcc.Input(
            id={
                'type': 'warmup',
                'model': 'none',
                'agent': agent_type
            },
            type='number',
            min=0,
            max=10000,
            value=0,
            step=500,
        )
    ])

def create_tau_input(agent_type):
    return html.Div(
        [
            html.Label('Tau', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'tau',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=0.001,
                max=0.999,
                value=0.005,
                step=0.001,
            ),
        ]
    )

def create_batch_size_input(agent_type):
    return html.Div(
        [
            html.Label('Batch Size', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'batch-size',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=1024,
                step=1,
                value=64,
            ),
        ]
    )

def create_noise_function_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
            id={
                'type':'noise-function',
                'model':model_type,
                'agent':agent_type,
                },
                options=[{'label': i, 'value': i} for i in ["Ornstein-Uhlenbeck", "Normal", "Uniform"]],
                placeholder="Noise Function"
            ),
            html.Div(
                id={
                    'type':'noise-options',
                    'model':model_type,
                    'agent':agent_type,
                }
            ),
        ]
    )

def update_noise_inputs(noise_type, id):
    agent_type = id['agent']
    model_type = id['model']
    if noise_type == "Ornstein-Uhlenbeck":
        inputs = html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'ou-mean',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.0,
            ),
            html.Label('Mean Reversion', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'ou-sigma',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.15,
            ),
            html.Label('Volatility', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'ou-theta',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.2,
            ),
            html.Label('Time Delta', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'ou-dt',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=1.0,
            ),
        ])

    elif noise_type == "Normal":
        inputs = html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'normal-mean',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.0,
            ),
            html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'normal-stddv',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=1.0,
            ),
        ])

    elif noise_type == "Uniform":
        inputs = html.Div([
            html.Label('Minimum Value', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'uniform-min',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.1,
            ),
            html.Label('Maximum Value', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'uniform-max',
                    'model':model_type,
                    'agent': agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=1.0,
            ),
        ])
    return inputs

def create_target_noise_clip_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Target Noise Clip', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'target-noise-clip',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.01,
                max=0.9,
                step=0.01,
                value=0.5,
            ),
        ]
    )

def create_actor_delay_input(agent_type):
    return html.Div(
        [
            html.Label('Actor Update Delay (steps)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'actor-update-delay',
                    'model':'actor',
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=10,
                step=1,
                value=2,
            ),
        ]
    )

def create_convolution_layers_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Convolution Layers', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'conv-layers',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0,
                max=20,
                step=1,
                value=0,
            ),
            html.Div(
                id={
                    'type':'layer-types',
                    'model':model_type,
                    'agent':agent_type,
                }
            ),
        ]
    )

def create_dense_layers_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Dense Layers', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'dense-layers',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0,
                max=10,
                step=1,
                value=2,
            ),
            html.Div(
                id={
                    'type':'units-per-layer',
                    'model':model_type,
                    'agent':agent_type,
                }
            ),
        ]
    )

def create_kernel_input(agent_type, model_type, index, stored_values=None, key=None):

    #DEBUG
    print({
    'type': 'kernel-init',
    'model': model_type,
    'agent': agent_type,
    'index': index
    })
    
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type': 'kernel-init',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                    options=[
                        {"label": x, "value": x.replace(" ", "_")} for x in 
                        ["kaiming uniform", "kaiming normal", "xavier uniform", "xavier normal", "truncated normal", 
                        "uniform", "normal", "constant", "ones", "zeros", "variance scaling", "orthogonal", "default"
                        ]
                    ],
                placeholder="Kernel Initialization",
                value=stored_values.get(key) if stored_values else None,
            ),
            html.Div(
                id={
                    'type': 'kernel-params',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index
                }
            ),
        ]
    )

def get_kernel_params_map():
    return {
        "kaiming_normal": ["mode"],
        "kaiming_uniform": ["mode"],
        "xavier_normal": ["gain"],
        "xavier_uniform": ["gain"],
        "truncated_normal": ["mean", "std-dev"],
        "uniform": ["a", "b"],
        "normal": ["mean", "std-dev"],
        "constant": ["value"],
        "variance_scaling": ["scale", "mode", "distribution"]
    }

def create_activation_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'activation-function',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i} for i in ['relu', 'tanh', 'sigmoid']],
                placeholder="Activation Function",
            ),
        ]
    )

def create_optimizer_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'optimizer',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i} for i in ['Adam', 'SGD', 'RMSprop', 'Adagrad']],
                placeholder="Optimizer",
            ),
            html.Div(
                id=
                {
                    'type':'optimizer-options',
                    'model':model_type,
                    'agent':agent_type,
                },
            ),
        ]
    )

def create_optimizer_params_input(agent_type, model_type, optimizer):
    if optimizer == 'Adam':
        return html.Div([
            html.Label("Weight Decay", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'adam-weight-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            )
        ])
    
    elif optimizer == 'Adagrad':
        return html.Div([
            html.Label("Weight Decay", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'adagrad-weight-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            ),
            html.Label("Learning Rate Decay", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'adagrad-lr-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            )
        ])
    
    elif optimizer == 'RMSprop':
        return html.Div([
            html.Label("Weight Decay", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'rmsprop-weight-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            ),
            html.Label("Momentum", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'rmsprop-momentum',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            )
        ])
    
    elif optimizer == 'SGD':
        return html.Div([
            html.Label("Weight Decay", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'sgd-weight-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            ),
            html.Label("Momentum", style={'text-decoration': 'underline', 'margin-left': '20px'}),
            dcc.Input(
                id=
                {
                    'type':'sgd-momentum',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.01,
            )
        ])
    
def get_optimizer(model_type, agent_type, agent_params):
    """Returns formatted dict of optimizer from params dict

    Args:
        model_type (str): model type the optimizer belongs to
        agent_type (str): agent type the optimizer belongs to
        params (dict): dict of all parameters belonging to the agent (from dcc.Store(id='agent-params-store')

    Returns:
        dict: dictionary of all {param:value} pairs belonging to the passed optimizer for the model of the agent
    """

    optimizer = agent_params.get(get_key({'type':'optimizer', 'model':model_type, 'agent':agent_type}))
    learning_rate_constant = agent_params.get(get_key({'type':'learning-rate-const', 'model':model_type, 'agent':agent_type}))
    learning_rate_exp = agent_params.get(get_key({'type':'learning-rate-exp', 'model':model_type, 'agent':agent_type}))
    learning_rate = learning_rate_constant * 10**learning_rate_exp

    # instantiate empty dict to store params
    params = {'lr':learning_rate}

    if optimizer == 'Adam':
        weight_decay = agent_params.get(get_key({'type':'adam-weight-decay', 'model':model_type, 'agent':agent_type}))
        params['weight_decay'] = weight_decay

    elif optimizer == 'Adagrad':
        weight_decay = agent_params.get(get_key({'type':'adagrad-weight-decay', 'model':model_type, 'agent':agent_type}))
        lr_decay = agent_params.get(get_key({'type':'adagrad-lr-decay', 'model':model_type, 'agent':agent_type}))
        params['weight_decay'] = weight_decay
        params['lr_decay'] = lr_decay

    elif optimizer == 'RMSprop':
        weight_decay = agent_params.get(get_key({'type':'rmsprop-weight-decay', 'model':model_type, 'agent':agent_type}))
        momentum = agent_params.get(get_key({'type':'rmsprop-momentum', 'model':model_type, 'agent':agent_type}))
        params['weight_decay'] = weight_decay
        params['momentum'] = momentum

    elif optimizer == 'SGD':
        weight_decay = agent_params.get(get_key({'type':'sgd-weight-decay', 'model':model_type, 'agent':agent_type}))
        momentum = agent_params.get(get_key({'type':'sgd-momentum', 'model':model_type, 'agent':agent_type}))
        params['weight_decay'] = weight_decay
        params['momentum'] = momentum
    
    else:
        raise ValueError(f"{optimizer} not found in utils.get_optimizer_params")
    
    return {'type':optimizer, 'params':params}

def create_learning_rate_constant_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Learning Rate Constant', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'learning-rate-const',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=1.0,
                max=9.0,
                step=0.1,
                value=1.0,
            ),
        ]
    )
    
def create_learning_rate_exponent_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Learning Rate Exponent(10^x)', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'learning-rate-exp',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=-9,
                max=-1,
                step=1,
                value=-4,
            ),
        ]
    )

# def create_lr_scheduler_input(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Learning Rate Scheduler', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'lr-scheduler',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 options=[{'label': i, 'value': i.lower()} for i in ['Step', 'Exponential', 'CosineAnnealing', 'Linear', 'None']],
#                 placeholder="Learning Rate Scheduler",
#             ),
#             html.Div(
#                 id={
#                     'type':'lr-scheduler-options',
#                     'model':model_type,
#                     'agent':agent_type,
#                 }
#             )
#         ]
#     )

# def update_lr_scheduler_options(agent_type, model_type, lr_scheduler):
#     if lr_scheduler == 'step':
#         return lr_step_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'exponential':
#         return lr_exponential_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'cosineannealing':
#         return lr_cosineannealing_scheduler_options(agent_type, model_type)
#     elif lr_scheduler == 'linear':
#         return lr_linear_scheduler_options(agent_type, model_type)
#     return html.Div()

# def lr_cosineannealing_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('T max (max iters)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-t-max',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=10000,
#                 step=1,
#                 value=1000,
#             ),
#             html.Label('Eta min (min LR)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-eta-min',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.000001,
#                 max=0.1,
#                 step=0.000001,
#                 value=0.0001,
#             ),
#         ]
#     )


# def lr_exponential_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def lr_step_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Step Size', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-step-size',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1000,
#                 step=1,
#                 value=100,
#             ),
#             html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-gamma',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.01,
#                 max=0.99,
#                 step=0.01,
#                 value=0.99,
#             ),
#         ]
#     )

# def lr_linear_scheduler_options(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Start Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-start-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.0001,
#                 max=1.0000,
#                 step=0.0001,
#                 value=1.0,
#             ),
#             html.Label('End Factor', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-end-factor',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=0.0001,
#                 max=1.0000,
#                 step=0.0001,
#                 value=0.0010,
#             ),
#             html.Label('Total Iterations', style={'text-decoration': 'underline'}),
#             dcc.Input(
#                 id={
#                     'type':'lr-total-iters',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 type='number',
#                 min=1,
#                 max=1e8,
#                 step=1,
#                 value=1e3,
#             ),
#         ]
#     )

# def get_lr_scheduler(model_type, agent_type, agent_params):

#     scheduler = agent_params.get(get_key({'type':'lr-scheduler', 'model':model_type, 'agent':agent_type}))
#     if scheduler == 'none':
#         return None
#     params = {}
#     if scheduler == 'step':
#         params['step_size'] = agent_params.get(get_key({'type':'lr-step-size', 'model':model_type, 'agent':agent_type}))
#         params['gamma'] = agent_params.get(get_key({'type':'lr-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'exponential':
#         params['gamma'] = agent_params.get(get_key({'type':'lr-gamma', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'cosineannealing':
#         params['T_max'] = agent_params.get(get_key({'type':'lr-t-max', 'model':model_type, 'agent':agent_type}))
#         params['eta_min'] = agent_params.get(get_key({'type':'lr-eta-min', 'model':model_type, 'agent':agent_type}))
#     elif scheduler == 'linear':
#         params['start_factor'] = agent_params.get(get_key({'type':'lr-start-factor', 'model':model_type, 'agent':agent_type}))
#         params['end_factor'] = agent_params.get(get_key({'type':'lr-end-factor', 'model':model_type, 'agent':agent_type}))
#         params['total_iters'] = agent_params.get(get_key({'type':'lr-total-iters', 'model':model_type, 'agent':agent_type}))

#     return {'type':scheduler, 'params':params}

def create_goal_strategy_input(agent_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'goal-strategy',
                    'model':'none',
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i.lower()} for i in ['Future', 'Final', 'None']],
                placeholder="Goal Strategy",
            ),
            html.Div(
                id={
                    'type':'goal-strategy-options',
                    'model':'none',
                    'agent':agent_type,
                }
            )
        ]
    )

def create_replay_buffer_input(agent_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'replay-buffer',
                    'model':'none',
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i.lower()} for i in ['PER', 'Standard']],
                placeholder="Replay Buffer",
            ),
            html.Div(
                id={
                    'type':'replay-buffer-options',
                    'model':'none',
                    'agent':agent_type,
                }
            )
        ]
    )

def update_replay_buffer_options(agent_type, replay_buffer):
    if replay_buffer == 'per':
        return per_replay_buffer_options(agent_type)
    return standard_replay_buffer_options(agent_type)

def per_replay_buffer_options(agent_type):
    return html.Div(
        [
            html.Label('Buffer Size', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'buffer-size',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1000,
                max=1000000,
                step=1000,
                value=100000,
            ),
            html.Label('Priority Exponent', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'per-priority-exponent',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.5,
            ),
            html.Label('Beta Start', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'per-beta-start',    
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.5,
            ),
            html.Label('Beta Iter', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'per-beta-iter',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1000,
                max=1000000,
                step=1000,
                value=100000,
            ),
            html.Label('Update Frequency', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'per-update-freq',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=1000,
                step=1,
                value=100,
            ),
        ]
    )

def standard_replay_buffer_options(agent_type):
    return html.Div(
        [
            html.Label('Buffer Size', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'buffer-size',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1000,
                max=1000000,
                step=1000,
                value=100000,
            ),
        ]
    )

def get_replay_buffer(env, agent_type, model_type, agent_params):
    replay_buffer = agent_params.get(get_key({'type':'replay-buffer', 'model':model_type, 'agent':agent_type}))
    device = agent_params.get(get_key({'type':'device', 'model':model_type, 'agent':agent_type}))
    buffer_size = agent_params.get(get_key({'type':'buffer-size', 'model':model_type, 'agent':agent_type}))
    #DEBUG
    print(f'replay_buffer: {replay_buffer}')
    print(f"buffer_size: {buffer_size}")
    print(f"device: {device}")
    if agent_type in ['HER_DDPG', 'HER_TD3']:
        goal_shape = (env.observation_space['desired_goal'].shape[-1],)
        #DEBUG
        print(f"goal_shape: {goal_shape}")
    else:
        goal_shape = None

    if replay_buffer == 'per':
        priority_exponent = agent_params.get(get_key({'type':'per-priority-exponent', 'model':model_type, 'agent':agent_type}))
        beta_start = agent_params.get(get_key({'type':'per-beta-start', 'model':model_type, 'agent':agent_type}))
        beta_iter = agent_params.get(get_key({'type':'per-beta-iter', 'model':model_type, 'agent':agent_type}))
        update_freq = agent_params.get(get_key({'type':'per-update-freq', 'model':model_type, 'agent':agent_type}))
        return PrioritizedReplayBuffer(env, buffer_size, priority_exponent, beta_start, beta_iter, goal_shape=goal_shape, update_freq=update_freq, device=device)
    return ReplayBuffer(env, buffer_size, goal_shape, device)

def update_goal_strategy_options(agent_type, goal_strategy):
    if goal_strategy == 'future':
        return future_goal_strategy_options(agent_type)
    return html.Div()

def future_goal_strategy_options(agent_type):
    return html.Div(
        [
            html.Label('Number of Future Goals', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'future-goals',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=10,
                step=1,
                value=4,
            ),
        ]
    )

def create_tolerance_input(agent_type):
    return html.Div(
        [
            html.Label('Goal Tolerance', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'goal-tolerance',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=0.001,
                max=10.000,
                step=0.001,
                value=0.05,
            ),
        ]
    )

def create_input_normalizer_options_input(agent_type):
    return html.Div(
        [
            html.Label(f'Minimum/Maximum Clip Value', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'clip-value',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=1,
                max=100,
                step=1,
                value=5,
            ),
        ]
    )

def create_input_normalizer_input(agent_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-input',
                    'model':'none',
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i} for i in ['True', 'False']],
                placeholder="Normalize Input",
            ),
            html.Div(
                id={
                    'type':'normalize-options',
                    'model':'none',
                    'agent':agent_type,
                }
            )
        ]
    )

def create_normalize_layers_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-layers',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i=='True'} for i in ['True', 'False']],
                placeholder="Normalize Layers",
            ),
        ]
    )

def create_clamp_output_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'clamp-output',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i=='True'} for i in ['True', 'False']],
                placeholder="Clamp Output",
            ),
            dcc.Input(
                id={
                    'type':'clamp-value',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                placeholder="Clamp Value",
                min=0.01,
                step=0.01,
                style={'display': 'none'},
            ),
        ]
    )

def create_epsilon_greedy_input(agent_type):
    return html.Div(
        [
            html.Label(f'Epsilon Greedy', style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':f'epsilon-greedy',
                    'model':'none',
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.01,
                value=0.2,
            ),
        ]
    )


def create_trace_decay_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f"{model_type.capitalize()} Trace Decay", style={'text-decoration': 'underline'}),
            dcc.Input(
                id={
                    'type':'trace-decay',
                    'model':model_type,
                    'agent':agent_type,
                },
                type='number',
                min=0.0,
                max=1.0,
                step=0.1,
                value=0.9,
            ),
        ]
    )

def create_add_layer_button(agent_type, model_type):
    return html.Div([
        # dcc.Store(
        #     id={
        #         'type': 'layer-values-store',
        #         'model': model_type,
        #         'agent': agent_type,
        #     },
        #     data={}
        # ),
        html.Div(
            id={
                'type':'layer-dropdowns',
                'model':model_type,
                'agent':agent_type,
                }),
        html.Button("Add Layer",
            id={
                'type':'add-layer-btn',
                'model':model_type,
                'agent':agent_type,
            },
            n_clicks=0,
            style={'margin-top': '10px', 'margin-left': '20px'}
        )
    ])
    
def create_policy_model_type_input(agent_type):
    return html.Div(
        [
            html.Label("Policy Type", style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'policy-type',
                    'model':'policy',
                    'agent':agent_type,
                },
                options=[{'label': 'Stochastic Continuous', 'value': 'StochasticContinuousPolicy'},
                         {'label': 'Stochastic Discrete', 'value': 'StochasticDiscretePolicy'},
                         ],
                placeholder="Policy Model Type",
            ),
        ]
    )

def create_policy_model_input(agent_type):
    return html.Div(
        [
            html.H3("Policy Model Configuration"),
            # create_dense_layers_input(agent_type, 'policy'),
            # html.Label("Hidden Layers Kernel Initializers"),
            # create_kernel_input(agent_type, 'policy-hidden'),
            create_add_layer_button(agent_type, 'policy'),
            html.Label("Output Layer Kernel Initializer"),
            create_kernel_input(agent_type, 'policy', 0),
            # create_activation_input(agent_type, 'policy'),
            create_optimizer_input(agent_type, 'policy'),
            create_learning_rate_constant_input(agent_type, 'policy'),
            create_learning_rate_exponent_input(agent_type, 'policy'),
            create_scheduler_input('lr', 'policy', agent_type)
        ]
    )


def create_value_model_input(agent_type):
    return html.Div(
        [
            html.H3("Value Model Configuration"),
            # create_dense_layers_input(agent_type, 'value'),
            # html.Label("Hidden Layers Kernel Initializers"),
            # create_kernel_input(agent_type, 'value-hidden'),
            create_add_layer_button(agent_type, 'value'),
            html.Label("Output Layer Kernel Initializer"),
            create_kernel_input(agent_type, 'value', 0),
            # create_activation_input(agent_type, 'value'),
            create_optimizer_input(agent_type, 'value'),
            create_learning_rate_constant_input(agent_type, 'value'),
            create_learning_rate_exponent_input(agent_type, 'value'),
            create_scheduler_input('lr', 'value', agent_type)
        ]
    )
    

def create_actor_model_input(agent_type):
    return html.Div(
        [
            html.H3("Actor Model Configuration"),
            # create_dense_layers_input(agent_type, 'value'),
            # html.Label("Hidden Layers Kernel Initializers"),
            # create_kernel_input(agent_type, 'value-hidden'),
            create_add_layer_button(agent_type, 'actor'),
            html.Label("Output Layer Kernel Initializer"),
            create_kernel_input(agent_type, 'actor', 0),
            # create_activation_input(agent_type, 'value'),
            create_optimizer_input(agent_type, 'actor'),
            create_learning_rate_constant_input(agent_type, 'actor'),
            create_learning_rate_exponent_input(agent_type, 'actor'),
            create_scheduler_input('lr', 'actor', agent_type)
        ]
    )


def create_critic_model_input(agent_type):
    return html.Div(
        [
            html.H3("Critic State Layers Configuration"),
            create_add_layer_button(agent_type, 'critic-state'),
            html.H3("Critic State/Action Layers Configuration"),
            create_add_layer_button(agent_type, 'critic-merged'),
            html.Label("Output Layer Kernel Initializer"),
            create_kernel_input(agent_type, 'critic', 0),
            # create_activation_input(agent_type, 'value'),
            create_optimizer_input(agent_type, 'critic'),
            create_learning_rate_constant_input(agent_type, 'critic'),
            create_learning_rate_exponent_input(agent_type, 'critic'),
            create_scheduler_input('lr', 'critic', agent_type)
        ]
    )


def create_reinforce_parameter_inputs(agent_type):
    """Adds inputs for REINFORCE Agent"""
    return html.Div(
        id=f'{agent_type}-inputs',
        children=[
            dbc.Tabs([
                # Tab 1 Agent Parameters
                dbc.Tab(
                    label="Agent Parameters",
                    children=[
                        create_discount_factor_input(agent_type),
                    ]
                ),

                # Tab 2 Policy Model
                dbc.Tab(
                    label="Policy Model",
                    children=[
                        create_policy_model_input(agent_type),
                    ]
                ),

                # Tab 3 Value Model
                dbc.Tab(
                    label="Value Model",
                    children=[
                        create_value_model_input(agent_type),
                    ]
                ),

                # Tab 4 Agent Options
                dbc.Tab(
                    label="Agent Options",
                    children=[
                        create_device_input(agent_type),
                        create_save_dir_input(agent_type),
                    ]
                ),
            ])
        ]
    )

def create_actor_critic_parameter_inputs(agent_type):
    return html.Div(
        id=f'{agent_type}-inputs',
        children=[
            dbc.Tabs([
                # Tab 1 Agent Parameters
                dbc.Tab(
                    label="Agent Parameters",
                    children=[
                        create_discount_factor_input(agent_type),
                        create_trace_decay_input(agent_type, 'policy'),
                        create_trace_decay_input(agent_type, 'value'),
                    ]
                ),

                # Tab 2 Policy Model
                dbc.Tab(
                    label="Policy Model",
                    children=[
                        create_policy_model_input(agent_type),
                    ]
                ),

                # Tab 3 Value Model
                dbc.Tab(
                    label="Value Model",
                    children=[
                        create_value_model_input(agent_type),
                    ]
                ),

                # Tab 4 Agent Options
                dbc.Tab(
                    label="Agent Options",
                    children=[
                        create_device_input(agent_type),
                        create_save_dir_input(agent_type),
                    ]
                ),
            ])
        ]
    )

def create_ddpg_parameter_inputs(agent_type):
    """Adds inputs for DDPG Agent"""
    return html.Div(
            id=f'{agent_type}-inputs',
            children=[
                dbc.Tabs([
                    # Tab 1 Agent Parameters
                    dbc.Tab(
                        label="Agent Parameters",
                        children=[
                            create_replay_buffer_input(agent_type),
                            create_discount_factor_input(agent_type),
                            create_tau_input(agent_type),
                            create_epsilon_greedy_input(agent_type),
                            create_batch_size_input(agent_type),
                            html.Label("Actor Noise"),
                            create_noise_function_input(agent_type, 'actor'),
                            create_scheduler_input('noise', 'actor', agent_type),
                            create_input_normalizer_input(agent_type),
                            create_warmup_input(agent_type),
                        ]
                    ),

                    # Tab 2 Policy Model
                    dbc.Tab(
                        label="Actor Model",
                        children=[
                            create_actor_model_input(agent_type),
                        ]
                    ),

                    # Tab 3 Value Model
                    dbc.Tab(
                        label="Critic Model",
                        children=[
                            create_critic_model_input(agent_type),
                        ]
                    ),

                    # Tab 4 Agent Options
                    dbc.Tab(
                        label="Agent Options",
                        children=[
                            create_device_input(agent_type),
                            create_save_dir_input(agent_type),
                        ]
                    ),
                ])
            ]
        )

def create_td3_parameter_inputs(agent_type):
    """Adds inputs for TD3 Agent"""
    return html.Div(
            id=f'{agent_type}-inputs',
            children=[
                dbc.Tabs([
                    # Tab 1 Agent Parameters
                    dbc.Tab(
                        label="Agent Parameters",
                        children=[
                            create_discount_factor_input(agent_type),
                            create_tau_input(agent_type),
                            create_epsilon_greedy_input(agent_type),
                            create_batch_size_input(agent_type),
                            html.Label("Actor Noise"),
                            create_noise_function_input(agent_type, 'actor'),
                            create_scheduler_input('noise', 'actor', agent_type),
                            html.Label("Target Actor Noise"),
                            create_noise_function_input(agent_type, 'target-actor'),
                            create_scheduler_input('noise', 'target-actor', agent_type),
                            create_target_noise_clip_input(agent_type, 'target-actor'),
                            create_input_normalizer_input(agent_type),
                            create_warmup_input(agent_type),
                        ]
                    ),

                    # Tab 2 Policy Model
                    dbc.Tab(
                        label="Actor Model",
                        children=[
                            create_actor_model_input(agent_type),
                            create_actor_delay_input(agent_type),
                        ]
                    ),

                    # Tab 3 Value Model
                    dbc.Tab(
                        label="Critic Model",
                        children=[
                            create_critic_model_input(agent_type),
                        ]
                    ),

                    # Tab 4 Agent Options
                    dbc.Tab(
                        label="Agent Options",
                        children=[
                            create_device_input(agent_type),
                            create_save_dir_input(agent_type),
                        ]
                    ),
                ])
            ]
        )


def create_her_ddpg_parameter_inputs(agent_type):
    """Adds inputs for Hindsight Experience Replay w/DDPG Agent"""
    return html.Div(
            id=f'{agent_type}-inputs',
            children=[
                dbc.Tabs([
                    # Tab 1 Agent Parameters
                    dbc.Tab(
                        label="Agent Parameters",
                        children=[
                            create_replay_buffer_input(agent_type),
                            create_goal_strategy_input(agent_type),
                            create_tolerance_input(agent_type),
                            create_discount_factor_input(agent_type),
                            create_tau_input(agent_type),
                            create_epsilon_greedy_input(agent_type),
                            create_batch_size_input(agent_type),
                            html.Label("Actor Noise"),
                            create_noise_function_input(agent_type, 'actor'),
                            create_scheduler_input('noise', 'actor', agent_type),
                            create_input_normalizer_input(agent_type),
                            create_warmup_input(agent_type),
                        ]
                    ),

                    # Tab 2 Policy Model
                    dbc.Tab(
                        label="Actor Model",
                        children=[
                            create_actor_model_input(agent_type),
                        ]
                    ),

                    # Tab 3 Value Model
                    dbc.Tab(
                        label="Critic Model",
                        children=[
                            create_critic_model_input(agent_type),
                        ]
                    ),

                    # Tab 4 Agent Options
                    dbc.Tab(
                        label="Agent Options",
                        children=[
                            create_device_input(agent_type),
                            create_save_dir_input(agent_type),
                        ]
                    ),
                ])
            ]
        )

def create_her_td3_parameter_inputs(agent_type):
    """Adds inputs for Hindsight Experience Replay w/TD3 Agent"""
    return html.Div(
            id=f'{agent_type}-inputs',
            children=[
                dbc.Tabs([
                    # Tab 1 Agent Parameters
                    dbc.Tab(
                        label="Agent Parameters",
                        children=[
                            create_goal_strategy_input(agent_type),
                            create_tolerance_input(agent_type),
                            create_discount_factor_input(agent_type),
                            create_tau_input(agent_type),
                            create_epsilon_greedy_input(agent_type),
                            create_batch_size_input(agent_type),
                            html.Label("Actor Noise"),
                            create_noise_function_input(agent_type, 'actor'),
                            create_scheduler_input('noise', 'actor', agent_type),
                            html.Label("Target Actor Noise"),
                            create_noise_function_input(agent_type, 'target-actor'),
                            create_scheduler_input('noise', 'target-actor', agent_type),
                            create_target_noise_clip_input(agent_type, 'target-actor'),
                            create_input_normalizer_input(agent_type),
                            create_warmup_input(agent_type),
                        ]
                    ),

                    # Tab 2 Policy Model
                    dbc.Tab(
                        label="Actor Model",
                        children=[
                            create_actor_model_input(agent_type),
                            create_actor_delay_input(agent_type),
                        ]
                    ),

                    # Tab 3 Value Model
                    dbc.Tab(
                        label="Critic Model",
                        children=[
                            create_critic_model_input(agent_type),
                        ]
                    ),

                    # Tab 4 Agent Options
                    dbc.Tab(
                        label="Agent Options",
                        children=[
                            create_device_input(agent_type),
                            create_save_dir_input(agent_type),
                        ]
                    ),
                ])
            ]
        )

def create_ppo_parameter_inputs(agent_type):
    """Adds inputs for PPO Agent"""
    return html.Div(
        id=f'{agent_type}-inputs',
        children=[
            dbc.Tabs([
                # Tab 1 Agent Parameters
                dbc.Tab(
                    label="Agent Parameters",
                    children=[
                        create_distribution_input(agent_type),
                        create_discount_factor_input(agent_type),
                        create_advantage_coeff_input(agent_type),
                        create_entropy_input(agent_type),
                        create_kl_coeff_input(agent_type),
                        create_normalize_advantage_input(agent_type),
                        create_normalize_values_input(agent_type),
                        create_clip_rewards_input(agent_type),
                    ]
                ),

                # Tab 2 Policy Model
                dbc.Tab(
                    label="Policy Model",
                    children=[
                        create_policy_model_type_input(agent_type),
                        create_surrogate_loss_clip_input(agent_type, 'policy'),
                        create_grad_clip_input(agent_type, 'policy'),
                        create_policy_model_input(agent_type),
                    ]
                ),

                # Tab 3 Value Model
                dbc.Tab(
                    label="Value Model",
                    children=[
                        create_surrogate_loss_clip_input(agent_type, 'value'),
                        create_value_model_coeff_input(agent_type),
                        create_grad_clip_input(agent_type, 'value'),
                        create_value_model_input(agent_type),
                    ]
                ),

                # Tab 4 Agent Options
                dbc.Tab(
                    label="Agent Options",
                    children=[
                        create_device_input(agent_type),
                        create_save_dir_input(agent_type),
                    ]
                ),
            ])
        ]
    )

def create_agent_parameter_inputs(agent_type):
    """Component for agent hyperparameters"""
    if agent_type == 'Reinforce':
        return create_reinforce_parameter_inputs(agent_type)

    elif agent_type == 'ActorCritic':
        return create_actor_critic_parameter_inputs(agent_type)

    elif agent_type == 'DDPG':
        return create_ddpg_parameter_inputs(agent_type)
    
    elif agent_type == 'TD3':
        return create_td3_parameter_inputs(agent_type)
    
    elif agent_type == 'HER_DDPG':
        return create_her_ddpg_parameter_inputs(agent_type)
    
    elif agent_type == 'HER_TD3':
        return create_her_td3_parameter_inputs(agent_type)
    
    elif agent_type == 'PPO':
        return create_ppo_parameter_inputs(agent_type)

    else:
        return html.Div("Select a model type to configure its parameters.")
    
## HYPERPARAMETER SEARCH FUNCTIONS

def create_reinforce_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            generate_learning_rate_hyperparam_component(agent_type, 'none'),
            generate_discount_hyperparam_component(agent_type, 'none'),
            dcc.Tabs([
                dcc.Tab([
                    generate_hidden_layer_hyperparam_component(agent_type, 'policy'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'policy'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'policy'),
                    generate_optimizer_hyperparam_component(agent_type, 'policy'),
                ],
                label="Policy Model"),
                dcc.Tab([
                    generate_hidden_layer_hyperparam_component(agent_type, 'value'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'value'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'value'),
                    generate_optimizer_hyperparam_component(agent_type, 'value'),
                ],
                label="Value Model"),
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def create_actor_critic_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            # utils.generate_actor_critic_hyperparam_component(),
            # html.H3('Actor Critic Hyperparameters'),
            generate_learning_rate_hyperparam_component(agent_type, 'none'),
            generate_discount_hyperparam_component(agent_type, 'none'),
            dcc.Tabs([
                dcc.Tab([
                    # html.H4("Policy Model Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'policy'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'policy'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'policy'),
                    generate_optimizer_hyperparam_component(agent_type, 'policy'),
                    generate_trace_decay_hyperparam_componenent(agent_type, 'policy'),
                ],
                label="Policy Model"),
                dcc.Tab([
                    # html.H4("Value Model Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'value'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'value'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'value'),
                    generate_optimizer_hyperparam_component(agent_type, 'value'),
                    generate_trace_decay_hyperparam_componenent(agent_type, 'value'),
                ],
                label="Value Model")
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def create_ddpg_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            create_device_input(agent_type),
            generate_discount_hyperparam_component(agent_type, 'none'),
            generate_tau_hyperparam_componenent(agent_type, 'none'),
            create_epsilon_greedy_hyperparam_input(agent_type, 'none'),
            generate_batch_hyperparam_componenent(agent_type, 'none'),
            generate_noise_hyperparam_componenent(agent_type, 'none'),
            create_replay_buffer_size_hyperparam_component(agent_type, 'none'),
            create_input_normalizer_hyperparam_input(agent_type, 'none'),
            generate_warmup_hyperparam_input(agent_type, 'none'),
            html.Hr(),
            dcc.Tabs([
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'actor'),
                    generate_cnn_layer_hyperparam_component(agent_type, 'actor'),
                    generate_hidden_layer_hyperparam_component(agent_type, 'actor'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'actor'),
                    generate_optimizer_hyperparam_component(agent_type, 'actor'),
                    create_normalize_layers_hyperparam_input(agent_type, 'actor'),
                ],
                label='Actor Model'),
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'critic'),
                    html.Hr(),
                    generate_cnn_layer_hyperparam_component(agent_type, 'critic'),
                    html.H4("Critic State Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-state'),
                    html.Hr(),
                    html.H4("Critic Merged (State + Action) Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-merged'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'critic'),
                    generate_optimizer_hyperparam_component(agent_type, 'critic'),
                    create_normalize_layers_hyperparam_input(agent_type, 'critic'),
                ],
                label='Critic Model')
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def create_td3_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            create_device_input(agent_type),
            generate_discount_hyperparam_component(agent_type, 'none'),
            generate_tau_hyperparam_componenent(agent_type, 'none'),
            create_epsilon_greedy_hyperparam_input(agent_type, 'none'),
            generate_batch_hyperparam_componenent(agent_type, 'none'),
            generate_noise_hyperparam_componenent(agent_type, 'none'),
            create_replay_buffer_size_hyperparam_component(agent_type, 'none'),
            generate_target_action_noise_stddev_hyperparam_component(agent_type, 'none'),
            generate_target_action_noise_clip_hyperparam_component(agent_type, 'none'),
            generate_actor_update_delay_hyperparam_component(agent_type, 'none'),
            create_input_normalizer_hyperparam_input(agent_type, 'none'),
            generate_warmup_hyperparam_input(agent_type, 'none'),
            html.Hr(),
            dcc.Tabs([
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'actor'),
                    generate_cnn_layer_hyperparam_component(agent_type, 'actor'),
                    generate_hidden_layer_hyperparam_component(agent_type, 'actor'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'actor'),
                    generate_optimizer_hyperparam_component(agent_type, 'actor'),
                    create_normalize_layers_hyperparam_input(agent_type, 'actor'),
                    # create_clamp_output_hyperparam_input(agent_type, 'actor'),
                ],
                label='Actor Model'),
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'critic'),
                    html.Hr(),
                    generate_cnn_layer_hyperparam_component(agent_type, 'critic'),
                    html.H4("Critic State Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-state'),
                    html.Hr(),
                    html.H4("Critic Merged (State + Action) Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-merged'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'critic'),
                    generate_optimizer_hyperparam_component(agent_type, 'critic'),
                    create_normalize_layers_hyperparam_input(agent_type, 'critic'),
                ],
                label='Critic Model')
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def create_her_ddpg_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            create_device_input(agent_type),
            create_goal_strategy_hyperparam_input(agent_type, 'none'),
            create_tolerance_hyperparam_input(agent_type, 'none'),
            generate_discount_hyperparam_component(agent_type, 'none'),
            generate_tau_hyperparam_componenent(agent_type, 'none'),
            create_epsilon_greedy_hyperparam_input(agent_type, 'none'),
            generate_batch_hyperparam_componenent(agent_type, 'none'),
            generate_noise_hyperparam_componenent(agent_type, 'none'),
            create_replay_buffer_size_hyperparam_component(agent_type, 'none'),
            html.H6("Input Normalizers"),
            create_input_normalizer_hyperparam_input(agent_type, 'none'),
            html.Hr(),
            # Actor config
            dcc.Tabs([
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'actor'),
                    generate_cnn_layer_hyperparam_component(agent_type, 'actor'),
                    generate_hidden_layer_hyperparam_component(agent_type, 'actor'),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'actor-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'actor'),
                    generate_optimizer_hyperparam_component(agent_type, 'actor'),
                    create_normalize_layers_hyperparam_input(agent_type, 'actor'),
                    # create_clamp_output_hyperparam_input(agent_type, 'actor'),
                ],
                label='Actor Model'),
                # Critic config
                dcc.Tab([
                    generate_learning_rate_hyperparam_component(agent_type, 'critic'),
                    html.Hr(),
                    generate_cnn_layer_hyperparam_component(agent_type, 'critic'),
                    html.H4("Critic State Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-state'),
                    html.Hr(),
                    html.H4("Critic Merged (State + Action) Input Layer Configuration"),
                    generate_hidden_layer_hyperparam_component(agent_type, 'critic-merged'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-hidden', 'Hidden Layers'),
                    html.Hr(),
                    generate_kernel_initializer_hyperparam_component(agent_type, 'critic-output', 'Output Layer'),
                    html.Hr(),
                    generate_activation_function_hyperparam_component(agent_type, 'critic'),
                    generate_optimizer_hyperparam_component(agent_type, 'critic'),
                    create_normalize_layers_hyperparam_input(agent_type, 'critic'),
                ],
                label='Critic Model')
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def create_ppo_hyperparam_input(agent_type):
    return dcc.Tab([
        html.Div([
            create_device_input(agent_type),
            generate_discount_hyperparam_component(agent_type, 'none'),
            create_advantage_coeff_hyperparam_input(agent_type, 'none'),
            create_entropy_coeff_hyperparam_input(agent_type, 'none'),
            create_kl_coeff_hyperparam_input(agent_type, 'none'),
            create_advantage_normalizer_hyperparam_input(agent_type, 'none'),
            create_reward_clip_hyperparam_input(agent_type, 'none'),
            html.Hr(),
            # Actor config
            dcc.Tabs([
                dcc.Tab(
                    html.Div(
                        [
                            create_model_type_hyperparam_input(agent_type, 'policy'),
                            create_distribution_hyperparam_input(agent_type, 'policy'),
                            create_surrogate_loss_clip_hyperparam_input(agent_type, 'policy'),
                            create_grad_clip_hyperparam_input(agent_type, 'policy'),
                            create_learning_rate_constant_hyperparam_input(agent_type, 'policy'),
                            create_learning_rate_exponent_hyperparam_input(agent_type, 'policy'),
                            # generate_cnn_layer_hyperparam_component(agent_type, 'policy'),
                            generate_hidden_layer_hyperparam_component(agent_type, 'policy'),
                            # generate_kernel_initializer_hyperparam_component(agent_type, 'policy-hidden', 'Hidden Layers'),
                            html.Hr(),
                            html.H5(f'Output Layer', style={'margin-right': '10px'}),
                            generate_kernel_initializer_hyperparam_component(agent_type, 'policy', 'output'),
                            html.Hr(),
                            # generate_activation_function_hyperparam_component(agent_type, 'policy'),
                            generate_optimizer_hyperparam_component(agent_type, 'policy'),
                            # create_normalize_layers_hyperparam_input(agent_type, 'policy'),
                            # create_clamp_output_hyperparam_input(agent_type, 'actor'),
                            create_learning_rate_scheduler_hyperparam_input(agent_type, 'policy')
                        ],
                        style={'padding-left': '10px', 'padding-bottom': '20px'}
                    ),
                    label='Policy Model'),
                # Critic config
                dcc.Tab(
                    html.Div(
                        [
                            create_loss_coeff_hyperparam_input(agent_type, 'value'),
                            create_surrogate_loss_clip_hyperparam_input(agent_type, 'value'),
                            create_grad_clip_hyperparam_input(agent_type, 'value'),
                            create_learning_rate_constant_hyperparam_input(agent_type, 'value'),
                            create_learning_rate_exponent_hyperparam_input(agent_type, 'value'),
                            # generate_cnn_layer_hyperparam_component(agent_type, 'critic'),
                            generate_hidden_layer_hyperparam_component(agent_type, 'value'),
                            # generate_kernel_initializer_hyperparam_component(agent_type, 'value-hidden', 'Hidden Layers'),
                            html.Hr(),
                            html.H5(f'Output Layer', style={'margin-right': '10px'}),
                            generate_kernel_initializer_hyperparam_component(agent_type, 'value', 'output'),
                            html.Hr(),
                            # generate_activation_function_hyperparam_component(agent_type, 'value'),
                            generate_optimizer_hyperparam_component(agent_type, 'value'),
                            html.H6("Value Normalizer"),
                            create_value_normalizer_hyperparam_input(agent_type, 'value'),
                            # create_normalize_layers_hyperparam_input(agent_type, 'critic'),
                            create_learning_rate_scheduler_hyperparam_input(agent_type, 'value')
                        ],
                        style={'padding-left': '10px', 'padding-bottom': '20px'}
                    ),
                    label='Value Model')
            ]),
            create_save_dir_input(agent_type),
        ])
    ],
    label=agent_type)

def generate_learning_rate_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Learning Rate'),
        dcc.Dropdown(
            id={
                'type': 'learning-rate-slider',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    {'label': '10^-2', 'value': 1e-2},
                    {'label': '10^-3', 'value': 1e-3},
                    {'label': '10^-4', 'value': 1e-4},
                    {'label': '10^-5', 'value': 1e-5},
                    {'label': '10^-6', 'value': 1e-6},
                    {'label': '10^-7', 'value': 1e-7},
            ],
            multi=True,
        )             
    ])

def create_learning_rate_constant_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Learning Rate Constant', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'learning-rate-const-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in range(0,10)],
                multi=True,
            ),
        ]
    )

def create_learning_rate_exponent_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Learning Rate Exponent(10^x)', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'learning-rate-exp-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in range(-9,0)],
                multi=True,
            ),
        ]
    )

# def create_learning_rate_scheduler_hyperparam_input(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label('Learning Rate Exponent(10^x)', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'learning-rate-exp-hyperparam',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 options=[{'label': str(i), 'value': i} for i in range(-9,0)],
#                 multi=True,
#             ),
#         ]
#     )

def create_advantage_coeff_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Advantage Coefficient', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'advantage-coeff-hyperparam',
                    'model':model_type,
                    'agent':agent_type
                },
                options=[{'label': str(i), 'value': i} for i in np.arange(0.0, 1.0, 0.05).round(2)],
                multi=True,
            )
        ]
    )

def create_surrogate_loss_clip_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Surrogate Loss Clip', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'surrogate-clip-hyperparam',
                    'model':model_type,
                    'agent':agent_type
                },
                options=[{'label': str(i), 'value': i} for i in np.arange(0.0, 1.0, 0.05).round(2)],
                multi=True,
            )
        ]
    )

def create_entropy_coeff_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Entropy Coefficient', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'entropy-coeff-hyperparam',
                    'model':model_type,
                    'agent':agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.0001', 'value': 0.0001},
                    {'label': '0.0005', 'value': 0.0005},
                    {'label': '0.001', 'value': 0.001},
                    {'label': '0.005', 'value': 0.005},
                    {'label': '0.01', 'value': 0.01},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.1', 'value': 0.1},
                
                ],
                multi=True,
            )
        ]
    )

def create_kl_coeff_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('KL Coefficient', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'kl-coeff-hyperparam',
                    'model':model_type,
                    'agent':agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                
                ],
                multi=True,
            )
        ]
    )

def create_model_type_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Model Type', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'model-type-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i.title(), 'value': i} for i in ['stochastic discrete', 'stochastic continuous']],
                multi=True,
            ),
        ]
    )

def create_distribution_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Distribution', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'distribution-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i.title(), 'value': i} for i in ['beta', 'normal', 'categorical']],
                
                multi=True,
            ),
        ]
    )

def create_reward_clip_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Reward Clip', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'reward-clip-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [1,2,3,4,5,10,20,30,40,50,100,'Infinity']],
                
                multi=True,
            ),
        ]
    )

def generate_discount_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Discount'),
        dcc.Dropdown(
            id={
                'type': 'discount-slider',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '0.99', 'value': 0.99},
                    {'label': '1.0', 'value': 1.0},
            ],
            multi=True,
        )
    ])

def generate_warmup_hyperparam_input(agent_type, model_type):
    return html.Div([
        html.H5('Warmup Period (Steps)'),
        dcc.Dropdown(
            id={
                'type': 'warmup-slider-hyperparam',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    
                    {'label': '500', 'value': 500},
                    {'label': '1000', 'value': 1000},
                    {'label': '5000', 'value': 5000},
                    {'label': '10000', 'value': 10000},
            ],
            multi=True,
        )
    ])

def generate_actor_update_delay_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Actor Update Delay (Steps)'),
        dcc.Dropdown(
            id={
                'type': 'actor-update-delay-slider-hyperparam',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    {'label': '1', 'value': 1},
                    {'label': '2', 'value': 2},
                    {'label': '3', 'value': 3},
                    {'label': '4', 'value': 4},
                    {'label': '5', 'value': 5},
                    {'label': '6', 'value': 6},
                    {'label': '7', 'value': 7},
                    {'label': '8', 'value': 8},
                    {'label': '9', 'value': 9},
                    {'label': '10', 'value': 10},
                    {'label': '20', 'value': 20},
                    {'label': '30', 'value': 30},
                    {'label': '40', 'value': 40},
                    {'label': '50', 'value': 50},
                    {'label': '60', 'value': 60},
                    {'label': '70', 'value': 70},
                    {'label': '80', 'value': 80},
                    {'label': '90', 'value': 90},
                    {'label': '100', 'value': 100},
            ],
            multi=True,
        )
    ])

def generate_target_action_noise_stddev_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Target Action Noise Stddev'),
        dcc.Dropdown(
            id={
                'type': 'target-action-noise-stddev-slider-hyperparam',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
            ],
            multi=True,
        )
    ])

def generate_target_action_noise_clip_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Target Action Noise Clip'),
        dcc.Dropdown(
            id={
                'type': 'target-action-noise-clip-slider-hyperparam',
                'model': model_type,
                'agent': agent_type
            },
            options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
            ],
            multi=True,
        )
    ])

def generate_cnn_layer_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('CNN Layers'),
        dcc.RangeSlider(
            id={
                'type': 'cnn-layers-slider-hyperparam',
                'model': model_type,
                'agent': agent_type
            },
            min=0,
            max=20,
            value=[1, 5],  # Default range
            marks={0: "0", 10: "10", 20: "20"},
            step=1,  # anchor slider to marks
            # pushable=1,  # allow pushing the slider,
            allowCross=True,  # allow selecting one value
            tooltip={"placement": "bottom", "always_visible": True}
        ),
        html.Div(id={
            'type':'cnn-layer-types-hyperparam',
            'model':model_type,
            'agent':agent_type
        })
    ])

def generate_cnn_layer_type_hyperparam_component(agent_type, model_type, index):
    
        input_id = {
            'type': 'cnn-layer-type-hyperparam',
            'model': model_type,
            'agent': agent_type,
            'index': index,
        }
        return html.Div([
            html.Label(f'Layer Type for Conv Layer {index}', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id=input_id,
                options=[
                    {'label': 'Conv2D', 'value': 'conv'},
                    {'label': 'MaxPool2D', 'value': 'pool'},
                    {'label': 'Dropout', 'value': 'dropout'},
                    {'label': 'BatchNorm2D', 'value': 'batchnorm'},
                    {'label': 'Relu', 'value':'relu'},
                    {'label': 'Tanh', 'value': 'tanh'},
                ],
                multi=True,
            ),
            html.Div(
                id={
                    'type': 'cnn-layer-type-parameters-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
            ),
            html.Hr(),
            ])

def generate_cnn_layer_parameters_hyperparam_component(layer_type, agent_type, model_type, index):
    # loop over layer types to create the appropriate parameters
    if layer_type == 'conv':
        return html.Div([
            html.Label(f'Filters in Conv Layer {index}', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'conv-filters-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                options=[{'label': i, 'value': i} for i in [8, 16, 32, 64, 128, 256, 512, 1024]],
                multi=True,
            ),
            html.Label(f'Kernel Size in Conv Layer {index}', style={'text-decoration': 'underline'}),
            dcc.RangeSlider(
                id={
                    'type': 'conv-kernel-size-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                min=1,
                max=10,
                step=1,
                value=[2, 4],
                marks={1:'1', 10:'10'},
                allowCross=True,
                tooltip={"placement": "bottom", "always_visible": True},
            ),
            html.Label(f'Kernel Stride in Conv Layer {index}', style={'text-decoration': 'underline'}),
            dcc.RangeSlider(
                id={
                    'type': 'conv-stride-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                min=1,
                max=10,
                step=1,
                value=[2, 4],
                marks={1:'1', 10:'10'},
                allowCross=True,
                tooltip={"placement": "bottom", "always_visible": True},
            ),
            html.Label(f'Input Padding in Conv Layer {index}', style={'text-decoration': 'underline'}),
            dcc.RadioItems(
                id={
                    'type': 'conv-padding-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                options=[
                    {'label': 'Same', 'value': 'same'},
                    {'label': 'Valid', 'value': 'valid'},
                    {'label': 'Custom', 'value': 'custom'},
                ],
            ),
            html.Div(
                [
                    html.Label('Custom Padding (pixels)', style={'text-decoration': 'underline'}),
                    dcc.RangeSlider(
                        id={
                            'type': 'conv-padding-custom-hyperparam',
                            'model': model_type,
                            'agent': agent_type,
                            'index': index,
                        },
                        min=0,
                        max=10,
                        step=1,
                        value=[1,3],
                        marks={0:'0', 10:'10'},
                        allowCross=True,
                        tooltip={"placement": "bottom", "always_visible": True},
                    ),
                ],
                id={
                    'type': 'conv-padding-custom-container-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                style={'display': 'none'},  # Hide initially
            ),
            html.Hr(),
            html.Label('Use Bias Term', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'conv-use-bias-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                options=[
                    {'label': 'True', 'value': True},
                    {'label': 'False', 'value': False},
                ],
                multi=True,
            ),
            html.Hr(),
        ])
    if layer_type == 'pool':
        return html.Div([
            html.Label(f'Kernel Size of Pooling Layer {index}', style={'text-decoration': 'underline'}),
            dcc.RangeSlider(
                id={
                    'type': 'pool-kernel-size-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                min=1,
                max=10,
                step=1,
                value=[2, 4],
                marks={1:'1', 10:'10'},
                allowCross=True,
                tooltip={"placement": "bottom", "always_visible": True},
            ),
            html.Label(f'Kernel Stride in Pooling Layer {index}', style={'text-decoration': 'underline'}),
            dcc.RangeSlider(
                id={
                    'type': 'pool-stride-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                min=1,
                max=10,
                step=1,
                value=[2,4],
                marks={1:'1', 10:'10'},
                allowCross=True,
                tooltip={"placement": "bottom", "always_visible": True},
            ),
            html.Hr(),
        ])
    if layer_type == 'batchnorm':
        return html.Div([
            html.Label(f'Number of Features for BatchNorm Layer {index} Set by Previous Layer Input Channels', style={'text-decoration': 'underline'}),
            # dcc.RangeSlider(
            #     id={
            #         'type': 'batch-features-hyperparam',
            #         'model': model_type,
            #         'agent': agent_type,
            #         'index': index,
            #     },
            #     min=1,
            #     max=1024,
            #     step=1,
            #     value=32,
            #     marks={1:'1', 1024:'1024'},
            #     tooltip={"placement": "bottom", "always_visible": True},
            # ),
        ])
    if layer_type == 'dropout':
        return html.Div([
            html.Label(f'Probability of Zero-ed Element for Dropout Layer {index}', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'dropout-prob-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': index,
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
            ],
            multi=True,
            ),
            html.Hr(),
        ])


def generate_hidden_layer_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Hidden Layers'),
        dcc.RangeSlider(
            id={
                'type': 'hidden-layers-slider',
                'model': model_type,
                'agent': agent_type
            },
            min=0,
            max=10,
            value=[1, 2],  # Default range
            marks={0: "0", 5: "5", 10: "10"},
            step=1,  # anchor slider to marks
            # pushable=1,  # allow pushing the slider,
            allowCross=True,  # allow selecting one value
            tooltip={"placement": "bottom", "always_visible": True}
        ),
        html.Div(id={
            'type':'hidden-layers-hyperparam',
            'model':model_type,
            'agent':agent_type
        })
    ])

def generate_layer_hyperparam_component(agent_type, model_type, num_layers):
    tabs = []
    for layer_num in range(1, num_layers + 1):
        tab = dcc.Tab([
            generate_layer_type_hyperparam_component(agent_type, model_type, layer_num)
        ],
        label=f'Layer {layer_num}'
        )
        # Append tab to tabs array
        tabs.append(tab)
    
    return tabs

def generate_layer_type_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.Div(
                [
                    html.H6(f'Layer Type', style={'margin-right': '10px'}),
                    dcc.Dropdown(
                        id={
                            'type': f'layer-type-hyperparam',
                            'model': model_type,
                            'agent': agent_type,
                            'index': layer_num,
                        },
                        options=[{'label': i.title(), 'value': i} for i in ['transformer encoder', 'dense', 'conv2d', 'maxpool2d', 'dropout', 'batchnorm2d', 'flatten', 'relu', 'tanh']],
                        multi=True,
                        style={'width': '200px'}
                    ),
                ],
                style={'width':'100px', 'display': 'flex', 'alignItems': 'center'}
            ),
            html.Div(
                id={
                    'type': 'layer-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num,
                },
                style={'margin-left': '10px'}
            ),
        ]
    )

def generate_layer_hyperparam_tab(agent_type, model_type, layer_num, layer_types):
    tabs = []
    for layer_type in layer_types:
        if layer_type == 'dense':
            tab = dcc.Tab(
                [item for item in generate_dense_layer_hyperparam_component(agent_type, model_type, layer_num)],
            label=f'{layer_type.title()}'
            )
        elif layer_type == 'conv2d':
            tab = dcc.Tab(
                [item for item in generate_conv_layer_hyperparam_component(agent_type, model_type, layer_num)],
            label=f'{layer_type.title()}'
            )
        tabs.append(tab)
    
    return dbc.Tabs(tabs)

def generate_dense_layer_hyperparam_component(agent_type, model_type, layer_num):
    params = []
    params.append(generate_hidden_units_per_layer_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_dense_bias_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_kernel_initializer_hyperparam_component(agent_type, model_type, layer_num))

    return params

def generate_hidden_units_per_layer_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Neurons'),
            dcc.Dropdown(
                id={
                    'type': f'layer-units-slider',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': i, 'value': i} for i in [8, 16, 32, 64, 128, 256, 512, 1024]],
                multi=True,
                style={'width':'200px'}
            ),
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_dense_bias_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Use Bias', style={'margin-right': '10px'}),
            dcc.Dropdown(
            id={
                'type': 'dense-bias-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num,
            },
            options=[{"label": "True", "value": True}, {"label": "False", "value": False}],
            multi=True,
            style={'width': '200px'},
            )
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_kernel_initializer_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div([
        html.Div(
            id={
                'type': 'kernel-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num,
            },
            style={'display': 'flex', 'alignItems': 'center'},
            children=[
                html.H6(f'Kernel Function', style={'margin-right': '10px'}),
                dcc.Dropdown(
                    id={
                        'type': 'kernel-function-hyperparam',
                        'model': model_type,
                        'agent': agent_type,
                        'index': layer_num,
                    },
                    options=[
                        {'label': "Default", 'value': "default"},
                        {'label': "Constant", 'value': "constant"},
                        {'label': "Xavier Uniform", 'value': "xavier_uniform"},
                        {'label': "Xavier Normal", 'value': "xavier_normal"},
                        {'label': "Kaiming Uniform", 'value': "kaiming_uniform"},
                        {'label': "Kaiming Normal", 'value': "kaiming_normal"},
                        {'label': "Zeros", 'value': "zeros"},
                        {'label': "Ones", 'value': "ones"},
                        {'label': "Uniform", 'value': "uniform"},
                        {'label': "Normal", 'value': "normal"},
                        {'label': "Truncated Normal", 'value': "truncated_normal"},
                        {'label': "Variance Scaling", 'value': "variance_scaling"},
                    ],
                    placeholder="Kernel Function",
                    multi=True,
                    style={'width':'200px'}
                ),
            ],
        ),
        html.Div(
            id={
                'type': 'hyperparam-kernel-options',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num,
            },
            style={'margin-left': '10px'},
            children=[
                html.H6(
                    id={
                        'type': 'kernel-options-header',
                        'model': model_type,
                        'agent': agent_type,
                        'index': layer_num,
                    },
                    children=['Kernel Options'],
                    hidden=True
                ),
                dcc.Tabs(
                    id={
                        'type': 'kernel-options-tabs',
                        'model': model_type,
                        'agent': agent_type,
                        'index': layer_num,
                    },
                ),
            ]
        )
    ])

def generate_kernel_options_hyperparam_component(agent_type, model_type, layer_num, selected_initializers):
    # Dictionary mapping the initializer names to the corresponding function
    kernel_input_creators = {
        "variance_scaling": generate_variance_scaling_hyperparam_inputs,
        "constant": generate_constant_kernel_hyperparam_inputs,
        "normal": generate_normal_kernel_hyperparam_inputs,
        "uniform": generate_uniform_kernel_hyperparam_inputs,
        "truncated_normal": generate_truncated_normal_kernel_hyperparam_inputs,
        "kaiming_normal": generate_kaiming_normal_hyperparam_inputs,
        "kaiming_uniform": generate_kaiming_uniform_hyperparam_inputs,
        "xavier_normal": generate_xavier_normal_hyperparam_inputs,
        "xavier_uniform": generate_xavier_uniform_hyperparam_inputs,
    }

    tabs = [] # empty list for adding tabs for each initializer in selected initializers
    
    for initializer in selected_initializers:
        if initializer in kernel_input_creators:
            #DEBUG
            print(f'generate kernel options hyperparam component: initializer {initializer} found')
            tabs.append(
                kernel_input_creators.get(initializer)(agent_type, model_type, layer_num)
            )
        
    return tabs


def generate_xavier_uniform_hyperparam_inputs(agent_type, model_type, layer_num):
    """Component for xavier uniform initializer hyperparameters"""
    return dcc.Tab([
        html.Div(
            id={
                'type': 'kernel-params-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num
                },
            children=[
                html.Label('Gain', style={'text-decoration': 'underline'}),
                dcc.Dropdown(
                    id={
                        'type':'xavier-uniform-gain-hyperparam',
                        'model':model_type,
                        'agent':agent_type,
                        'index': layer_num
                    },
                    options=[
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                    ],
                     multi=True,
                ),
                html.Hr(),
            ],
        )
        ],
        label='Xavier Uniform'
    )


def generate_xavier_normal_hyperparam_inputs(agent_type, model_type, layer_num):
    """Component for xavier normal initializer hyperparameters"""
    return dcc.Tab([
        html.Div(
            id={
                'type': 'kernel-params-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num
                },
            children=[
                html.Label('Gain', style={'text-decoration': 'underline'}),
                dcc.Dropdown(
                    id={
                        'type':'xavier-normal-gain-hyperparam',
                        'model':model_type,
                        'agent':agent_type,
                        'index': layer_num
                    },
                    options=[
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                    ],
                     multi=True,
                ),
                html.Hr(),
            ],
        )
        ],
        label="Xavier Normal"
    )

def generate_kaiming_uniform_hyperparam_inputs(agent_type, model_type, layer_num):
    """Component for kaiming uniform initializer sweep hyperparameters"""
    return dcc.Tab([
        html.Div(
            id={
                'type': 'kernel-params-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num
                },
            children=[
                html.Label('Mode', style={'text-decoration': 'underline'}),
                dcc.Dropdown(
                    id={
                        'type':'kaiming-uniform-mode-hyperparam',
                        'model':model_type,
                        'agent':agent_type,
                        'index': layer_num
                        },
                    options=[
                            {'label': 'fan in', 'value': 'fan_in'},
                            {'label': 'fan out', 'value': 'fan_out'},
                        ],
                    multi=True,
                ),
                html.Hr(),
            ]
        )
        ],
        label='Kaiming Uniform'
    )

def generate_kaiming_normal_hyperparam_inputs(agent_type, model_type, layer_num):
    """Component for kaiming normal initializer sweep hyperparameters"""
    return dcc.Tab([
        html.Div(
            id={
                'type': 'kernel-params-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num
                },
            children=[
                html.Label('Mode', style={'text-decoration': 'underline'}),
                dcc.Dropdown(
                    id={
                        'type':'kaiming-normal-mode-hyperparam',
                        'model':model_type,
                        'agent':agent_type,
                        'index': layer_num
                        },
                    options=[
                            {'label': 'fan in', 'value': 'fan_in'},
                            {'label': 'fan out', 'value': 'fan_out'},
                        ],
                    multi=True,
                ),
                html.Hr(),
            ]
        )
    ],
    label='Kaiming Normal'
    )

def generate_variance_scaling_hyperparam_inputs(agent_type, model_type, layer_num):
    return dcc.Tab([
        html.Div([
            html.Label('Scale', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'variance-scaling-scale-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                ],
                multi=True,
            ),
            html.Label('Mode', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'variance-scaling-mode-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': mode, 'value': mode} for mode in ['fan_in', 'fan_out', 'fan_avg']],
                placeholder="Mode",
                multi=True
            ),
            html.Label('Distribution', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'variance-scaling-distribution-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': dist, 'value': dist} for dist in ['truncated normal', 'uniform']],
                placeholder="Distribution",
                multi=True
            ),
        ])
    ],
    label='Variance Scaling')

def generate_constant_kernel_hyperparam_inputs(agent_type, model_type, layer_num):
    return dcc.Tab([
        html.Div([
            html.Label('Value', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'constant-value-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.001', 'value': 0.001},
                    {'label': '0.002', 'value': 0.002},
                    {'label': '0.003', 'value': 0.003},
                    {'label': '0.004', 'value': 0.004},
                    {'label': '0.005', 'value': 0.005},
                    {'label': '0.006', 'value': 0.006},
                    {'label': '0.007', 'value': 0.007},
                    {'label': '0.008', 'value': 0.008},
                    {'label': '0.009', 'value': 0.009},
                    {'label': '0.01', 'value': 0.01},
                    {'label': '0.02', 'value': 0.02},
                    {'label': '0.03', 'value': 0.03},
                    {'label': '0.04', 'value': 0.04},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.06', 'value': 0.06},
                    {'label': '0.07', 'value': 0.07},
                    {'label': '0.08', 'value': 0.08},
                    {'label': '0.09', 'value': 0.09},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                ],
                multi=True,
            ),
        ])
    ],
    label='Constant')

def generate_normal_kernel_hyperparam_inputs(agent_type, model_type, layer_num):
    return dcc.Tab([
        html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'random-normal-mean-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
            html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'random-normal-stddev-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ])
    ],
    label='Random Normal')

def generate_uniform_kernel_hyperparam_inputs(agent_type, model_type, layer_num):
    return dcc.Tab([
        html.Div([ 
            html.Label('Minimum', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'random-uniform-minval-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '-0.1', 'value': -0.1},
                    {'label': '-0.2', 'value': -0.2},
                    {'label': '-0.3', 'value': -0.3},
                    {'label': '-0.4', 'value': -0.4},
                    {'label': '-0.5', 'value': -0.5},
                    {'label': '-0.6', 'value': -0.6},
                    {'label': '-0.7', 'value': -0.7},
                    {'label': '-0.8', 'value': -0.8},
                    {'label': '-0.9', 'value': -0.9},
                    {'label': '-1.0', 'value': -1.0},
                ],
                multi=True,
            ),
            html.Label('Maximum', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'random-uniform-maxval-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ]),
    ],
    label='Random Uniform')

def generate_truncated_normal_kernel_hyperparam_inputs(agent_type, model_type, layer_num):
    return dcc.Tab([
        html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'truncated-normal-mean-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
            html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'truncated-normal-stddev-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ])
    ],
    label='Truncated Normal')

def generate_conv_layer_hyperparam_component(agent_type, model_type, layer_num):
    params = [] 
    params.append(generate_out_channels_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_kernel_size_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_stride_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_padding_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_conv_bias_hyperparam_component(agent_type, model_type, layer_num))
    params.append(generate_kernel_initializer_hyperparam_component(agent_type, model_type, layer_num))

    return params

def generate_out_channels_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Out Channels'),
            dcc.Dropdown(
                id={
                    'type': 'out-channels-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': i, 'value': i} for i in [8, 16, 32, 64, 128, 256, 512, 1024]],
                multi=True,
                style={'width':'200px'}
            ),
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_kernel_size_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Kernel Size'),
            dcc.Dropdown(
                id={
                    'type': 'kernel-size-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': i, 'value': i} for i in range(1, 11)],
                multi=True,
                style={'width':'200px'}
            ),
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_stride_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Stride'),
            dcc.Dropdown(
                id={
                    'type': 'stride-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': i, 'value': i} for i in range(1,6)],
                multi=True,
                style={'width':'200px'}
            ),
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_padding_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Padding'),
            dcc.Dropdown(
                id={
                    'type': 'padding-hyperparam',
                    'model': model_type,
                    'agent': agent_type,
                    'index': layer_num
                },
                options=[{'label': i, 'value': i} for i in range(6)],
                multi=True,
                style={'width':'200px'}
            ),
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_conv_bias_hyperparam_component(agent_type, model_type, layer_num):
    return html.Div(
        [
            html.H6(f'Use Bias', style={'margin-right': '10px'}),
            dcc.Dropdown(
            id={
                'type': 'conv-bias-hyperparam',
                'model': model_type,
                'agent': agent_type,
                'index': layer_num,
            },
            options=[{"label": "True", "value": True}, {"label": "False", "value": False}],
            multi=True,
            style={'width': '200px'},
            )
        ],
        style={'display': 'flex', 'alignItems': 'center'}
    )

def generate_activation_function_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Activation Function'),
        dcc.Dropdown(
            id={
                'type': 'activation-function-hyperparam',
                'model': model_type,
                'agent': agent_type,
            },
            options=[{'label': i, 'value': i} for i in ['relu', 'tanh', 'sigmoid']],
            placeholder="Activation Function",
            multi=True,
        ),
    ])

def generate_optimizer_hyperparam_component(agent_type, model_type):
    return html.Div([
        html.H5('Optimizer'),
        dcc.Dropdown(
            id={
                'type': 'optimizer-hyperparam',
                'model': model_type,
                'agent': agent_type,
            },
            options=[{'label': i, 'value': i} for i in ['Adam', 'SGD', 'RMSprop', 'Adagrad']],
            placeholder="Optimizer",
            multi=True,
        ),
        html.Div(
            id=
            {
                'type':'optimizer-options-hyperparams',
                'model': model_type,
                'agent': agent_type,
            }
        )
    ])

def generate_trace_decay_hyperparam_componenent(agent_type, model_type):
    return html.Div([
        html.H5(f"{model_type.capitalize()} Trace Decay"),
        dcc.Dropdown(
            id={
                'type': 'trace-decay-hyperparam',
                'model': model_type,
                'agent': agent_type,
            },
            options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '0.99', 'value': 0.99},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
        ),
    ])

def generate_tau_hyperparam_componenent(agent_type, model_type):
    return html.Div([
        html.H5("Tau"),
        dcc.Dropdown(
            id={
                'type': 'tau-hyperparam',
                'model': model_type,
                'agent': agent_type,
            },
            options=[
                    {'label': '0.001', 'value': 0.001},
                    {'label': '0.005', 'value': 0.005},
                    {'label': '0.01', 'value': 0.01},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.5', 'value': 0.5},
                ],
                multi=True,
        ),
    ])

def generate_batch_hyperparam_componenent(agent_type, model_type):
    return html.Div([
        html.H5("Batch Size"),
        dcc.Dropdown(
            id={
                'type': 'batch-size-hyperparam',
                'model': model_type,
                'agent': agent_type,
            },
            options=[{'label': i, 'value': i} for i in [8, 16, 32, 64, 128, 256, 512, 1024]],
            placeholder="Batch Sizes",
            multi=True,
        ),
    ])

def generate_noise_hyperparam_componenent(agent_type, model_type):
    return html.Div([
        html.H5("Noise"),
        dcc.Dropdown(
            id={
                'type': 'noise-function-hyperparam',
                'model': model_type,
                'agent': agent_type,
                },
                options=[{'label': i, 'value': i} for i in ["Ornstein-Uhlenbeck", "Normal", "Uniform"]],
                placeholder="Noise Function",
                multi=True,
            ),
        html.Div(
            id={
                'type':'noise-options-hyperparam',
                'model':model_type,
                'agent':agent_type
            },
            children=[
                html.H6(
                    id={
                        'type': 'noise-options-header',
                        'model': model_type,
                        'agent': agent_type
                    },
                    children=['Noise Options'],
                    hidden=True
                ),
                dcc.Tabs(
                    id={
                        'type': 'noise-options-tabs',
                        'model': model_type,
                        'agent': agent_type
                    }
                ),
            ]
        )
    ])




def generate_noise_options_hyperparams_component(agent_type, model_type, noise_functions):
    # Dictionary mapping the initializer names to the corresponding function
    noise_input_creators = {
        "Ornstein-Uhlenbeck": generate_OU_noise_hyperparam_inputs,
        "Uniform": generate_uniform_noise_hyperparam_inputs,
        "Normal": generate_normal_noise_hyperparam_inputs,        
    }


    tabs = [] # empty list for adding tabs for each initializer in selected initializers
    
    for noise in noise_functions:
        if noise in noise_input_creators:
            tabs.append(
                noise_input_creators.get(noise)(agent_type, model_type)
            )
        
    return tabs

def generate_OU_noise_hyperparam_inputs(agent_type, model_type):
    return dcc.Tab([
        html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'ou-mean-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
            html.Label('Mean Reversion', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'ou-theta-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
            html.Label('Volatility', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'ou-sigma-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ])
    ],
    label='Ornstein-Uhlenbeck')

def generate_uniform_noise_hyperparam_inputs(agent_type, model_type):
    return dcc.Tab([
        html.Div([
            html.Label('Minimum Value', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'uniform-min-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '-0.1', 'value': -0.1},
                    {'label': '-0.2', 'value': -0.2},
                    {'label': '-0.3', 'value': -0.3},
                    {'label': '-0.4', 'value': -0.4},
                    {'label': '-0.5', 'value': -0.5},
                    {'label': '-0.6', 'value': -0.6},
                    {'label': '-0.7', 'value': -0.7},
                    {'label': '-0.8', 'value': -0.8},
                    {'label': '-0.9', 'value': -0.9},
                    {'label': '-1.0', 'value': -1.0},
                ],
                multi=True,
            ),
            html.Label('Maximum Value', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'uniform-max-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ])
    ],
    label='Uniform')

def generate_normal_noise_hyperparam_inputs(agent_type, model_type):
    return dcc.Tab([
        html.Div([
            html.Label('Mean', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'normal-mean-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
            html.Label('Standard Deviation', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type': 'normal-stddev-hyperparam',
                    'model': model_type,
                    'agent': agent_type
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.15', 'value': 0.15},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.25', 'value': 0.25},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.35', 'value': 0.35},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.45', 'value': 0.45},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.55', 'value': 0.55},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.65', 'value': 0.65},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.75', 'value': 0.75},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.85', 'value': 0.85},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '0.95', 'value': 0.95},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ])
    ],
    label='Normal')

def create_replay_buffer_hyperparam_component(agent_type, model_type):
    pass

def create_replay_buffer_size_hyperparam_component(agent_type, model_type):
    return html.Div(
        [
            html.Label('Buffer Size', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'buffer-size-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '100,000', 'value': 100_000},
                    {'label': '500,000', 'value': 500_000},
                    {'label': '1,000,000', 'value': 1_000_000}
                    ],
                placeholder="Replay Buffer Size",
                multi=True,
            ),
        ]
    )

def create_goal_strategy_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'goal-strategy-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i.lower()} for i in ['Future', 'Final', 'None']],
                placeholder="Goal Strategy",
                multi=True,
            ),
            html.Div(
                id={
                    'type':'goal-strategy-options-hyperparam',
                    'model':'none',
                    'agent':agent_type,
                }
            )
        ]
    )

def create_tolerance_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Goal Tolerance', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'goal-tolerance-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.15', 'value': 0.15},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.25', 'value': 0.25},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.35', 'value': 0.35},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.45', 'value': 0.45},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.55', 'value': 0.55},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.65', 'value': 0.65},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.75', 'value': 0.75},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.85', 'value': 0.85},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '0.95', 'value': 0.95},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ]
    )

def create_epsilon_greedy_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'Epsilon Greedy', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':f'epsilon-greedy-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                ],
                multi=True,
            ),
        ]
    )

def create_input_normalizer_options_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'Minimum/Maximum Clip Value', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'norm-clip-value-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.0', 'value': 0.0},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                    {'label': '4.0', 'value': 4.0},
                    {'label': '5.0', 'value': 5.0},
                    {'label': '6.0', 'value': 6.0},
                    {'label': '7.0', 'value': 7.0},
                    {'label': '8.0', 'value': 8.0},
                    {'label': '9.0', 'value': 9.0},
                    {'label': '10.0', 'value': 10.0},
                ],
                multi=True,
            ),
        ]
    )

def create_input_normalizer_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-input-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i} for i in ['True', 'False']],
                placeholder="Normalize Input",
                multi=True,
            ),
            html.Div(
                id={
                    'type':'normalize-options-hyperparam',
                    'model':'none',
                    'agent':agent_type,
                }
            )
        ]
    )

# def create_value_normalizer_hyperparam_input(agent_type, model_type):
#     return html.Div(
#         [
#             dcc.Dropdown(
#                 id={
#                     'type':'normalize-values-hyperparam',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 options=[{'label': i, 'value': i} for i in ['True', 'False']],
#                 placeholder="Normalize Values",
#                 multi=True,
#             ),
#             html.Div(
#                 id={
#                     'type':'normalize-values-options-hyperparam',
#                     'model':model_type,
#                     'agent':agent_type,
#                 }
#             )
#         ]
#     )

# def create_value_normalizer_options_hyperparam_input(agent_type, model_type):
#     return html.Div(
#         [
#             html.Label(f'Minimum/Maximum Clip Value', style={'text-decoration': 'underline'}),
#             dcc.Dropdown(
#                 id={
#                     'type':'value-norm-clip-value-hyperparam',
#                     'model':model_type,
#                     'agent':agent_type,
#                 },
#                 options=[
#                     {'label': '0.0', 'value': 0.0},
#                     {'label': '0.1', 'value': 0.1},
#                     {'label': '0.2', 'value': 0.2},
#                     {'label': '0.3', 'value': 0.3},
#                     {'label': '0.4', 'value': 0.4},
#                     {'label': '0.5', 'value': 0.5},
#                     {'label': '0.6', 'value': 0.6},
#                     {'label': '0.7', 'value': 0.7},
#                     {'label': '0.8', 'value': 0.8},
#                     {'label': '0.9', 'value': 0.9},
#                     {'label': '1.0', 'value': 1.0},
#                     {'label': '2.0', 'value': 2.0},
#                     {'label': '3.0', 'value': 3.0},
#                     {'label': '4.0', 'value': 4.0},
#                     {'label': '5.0', 'value': 5.0},
#                     {'label': '6.0', 'value': 6.0},
#                     {'label': '7.0', 'value': 7.0},
#                     {'label': '8.0', 'value': 8.0},
#                     {'label': '9.0', 'value': 9.0},
#                     {'label': '10.0', 'value': 10.0},
#                     {'label': 'Infinity', 'value': 'infinity'},
#                 ],
#                 multi=True,
#             ),
#         ]
#     )

def create_grad_clip_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Gradient Clip', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'grad-clip-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                    {'label': '2.0', 'value': 2.0},
                    {'label': '3.0', 'value': 3.0},
                    {'label': '4.0', 'value': 4.0},
                    {'label': '5.0', 'value': 5.0},
                    {'label': '10.0', 'value': 10.0},
                    {'label': '20.0', 'value': 20.0},
                    {'label': '30.0', 'value': 30.0},
                    {'label': '40.0', 'value': 40.0},
                    {'label': '50.0', 'value': 50.0},
                    {'label': 'Infinity', 'value': 'infinity'},
                ],
                multi=True,
            ),
        ]
    )

def create_loss_coeff_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label(f'{model_type.capitalize()} Loss Coefficient', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'loss-coeff-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0', 'value': 1.0},
                    {'label': '1.1', 'value': 1.1},
                    {'label': '1.2', 'value': 1.2},
                    {'label': '1.3', 'value': 1.3},
                    {'label': '1.4', 'value': 1.4},
                    {'label': '1.5', 'value': 1.5},
                    {'label': '1.6', 'value': 1.6},
                    {'label': '1.7', 'value': 1.7},
                    {'label': '1.8', 'value': 1.8},
                    {'label': '1.9', 'value': 1.9},
                    {'label': '2.0', 'value': 2.0},
                ],
                multi=True,
            ),
        ]
    )

def create_advantage_normalizer_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-advantage-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [True, False]],
                placeholder="Normalize Advantages",
                multi=True,
            ),
        ]
    )

def create_value_normalizer_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-values-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [True, False]],
                placeholder="Normalize Values",
                multi=True,
            ),
            html.Div(
                id = {
                    'type':'norm-clip-hyperparam-block',
                    'model':model_type,
                    'agent':agent_type,
                },
                children = [
                    html.Label('Norm Clip'),
                    dcc.Dropdown(
                        id={
                            'type': 'norm-values-clip-hyperparam',
                            'model': model_type,
                            'agent': agent_type
                        },
                        options=[{'label': str(i), 'value': i} for i in [0.1, 0.5, 1.0, 5.0, 10.0]],
                    )
                ],
                style={'margin-left': '10px', 'display':'none'}
            )
        ]
    )

def create_normalize_layers_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'normalize-layers-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i=='True'} for i in ['True', 'False']],
                placeholder="Normalize Layers",
                multi=True,
            ),
        ]
    )

def create_clamp_output_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            dcc.Dropdown(
                id={
                    'type':'clamp-value-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[
                    {'label': '0.01', 'value': 0.01},
                    {'label': '0.02', 'value': 0.02},
                    {'label': '0.03', 'value': 0.03},
                    {'label': '0.04', 'value': 0.04},
                    {'label': '0.05', 'value': 0.05},
                    {'label': '0.06', 'value': 0.06},
                    {'label': '0.07', 'value': 0.07},
                    {'label': '0.08', 'value': 0.08},
                    {'label': '0.09', 'value': 0.09},
                    {'label': '0.1', 'value': 0.1},
                    {'label': '0.2', 'value': 0.2},
                    {'label': '0.3', 'value': 0.3},
                    {'label': '0.4', 'value': 0.4},
                    {'label': '0.5', 'value': 0.5},
                    {'label': '0.6', 'value': 0.6},
                    {'label': '0.7', 'value': 0.7},
                    {'label': '0.8', 'value': 0.8},
                    {'label': '0.9', 'value': 0.9},
                    {'label': '1.0 (no clamp)', 'value': 1.0}, # Equals no clamp
                ],
                placeholder="Clamp Value",
                multi=True,
            ),
        ]
    )

def update_goal_strategy_hyperparam_options(agent_type, goal_strategy):
    if goal_strategy == 'future':
        return future_goal_strategy_hyperparam_options(agent_type)
    return html.Div()

def future_goal_strategy_hyperparam_options(agent_type):
    return html.Div(
        [
            html.Label('Number of Future Goals', style={'text-decoration': 'underline'}),
            dcc.RangeSlider(
                id={
                    'type':'future-goals-hyperparam',
                    'model':'none',
                    'agent':agent_type,
                },
                min=1,
                max=10,
                step=1,
                value=[4,8],  # Default position
                marks={1:'1', 10:'10'},
                tooltip={"placement": "bottom", "always_visible": True},
                included=False,
                allowCross=True,
            ),
        ]
    )

def create_learning_rate_scheduler_hyperparam_input(agent_type, model_type):
    return html.Div(
        [
            html.Label('Learning Rate Scheduler', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-scheduler-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': i, 'value': i.lower()} for i in ['Step', 'Exponential', 'CosineAnnealing', 'None']],
                multi=True,
                placeholder="Select Learning Rate Scheduler(s)",
            ),
            html.Div(
                id={
                    'type':'lr-scheduler-tabs-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                }
            )
        ]
    )

def update_lr_scheduler_hyperparam_options(agent_type, model_type, lr_schedulers):
    #DEBUG
    print(f'lr_schedulers:{lr_schedulers}')
    if not lr_schedulers:
        return html.Div()

    tabs = []
    # tab_contents = []

    for scheduler in lr_schedulers:
        tab_label = scheduler.title()
        #DEBUG
        print(f'tab_label:{tab_label}')
        # tabs.append(dcc.Tab(label=tab_label, value=scheduler))

        if scheduler == 'step':
            tab_contents = lr_step_scheduler_hyperparam_options(agent_type, model_type)
            tab_label = 'Step'
        elif scheduler == 'exponential':
            tab_contents = lr_exponential_scheduler_hyperparam_options(agent_type, model_type)
            tab_label = 'Exponential'
        elif scheduler == 'cosineannealing':
            tab_contents = lr_cosineannealing_scheduler_hyperparam_options(agent_type, model_type)
            tab_label = 'Cosine Annealing'

        tab = dcc.Tab(
            label=tab_label,
            children=tab_contents
        )

        tabs.append(tab)

    return dcc.Tabs(
        id={
            'type': 'lr-scheduler-tabs-options-hyperparam',
            'model': model_type,
            'agent': agent_type,
        },
        value=lr_schedulers[0] if lr_schedulers else None,
        children=tabs,
        style={'margin-top': '10px'}
    )

def lr_cosineannealing_scheduler_hyperparam_options(agent_type, model_type):
    return html.Div(
        [
            html.Label('T max (max iters)', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-t-max-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [100,500,1000,5000,10000,50000,100000]],
                multi=True,
                placeholder="Select T-Max Value(s)",

            ),
            html.Label('Eta min (min LR)', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-eta-min-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [0.001,0.0001,0.00001,0.000001]],
                multi=True,
                placeholder="Select Eta Min Value(s)",
            ),
        ]
    )

def lr_exponential_scheduler_hyperparam_options(agent_type, model_type):
    return html.Div(
        [
            html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-gamma-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]],
                multi=True,
                placeholder="Select Gamma Value(s)",
            ),
        ]
    )

def lr_step_scheduler_hyperparam_options(agent_type, model_type):
    return html.Div(
        [
            html.Label('Step Size', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-step-size-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [0.001, 0.0001, 0.00001, 0.000001]],
                multi=True,
                placeholder="Select Step Size Value(s)",
            ),
            html.Label('Gamma (decay)', style={'text-decoration': 'underline'}),
            dcc.Dropdown(
                id={
                    'type':'lr-gamma-hyperparam',
                    'model':model_type,
                    'agent':agent_type,
                },
                options=[{'label': str(i), 'value': i} for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]],
                multi=True,
                placeholder="Select Gamma Value(s)",
            ),
        ]
    )

def create_seed_component(page):
    # return html.Div([
        # html.Label('Seed', style={'text-decoration': 'underline'}),
    return dcc.Input(
            id={
                'type':'seed',
                'page': page,
            },
            type='number',
            min=1,
            placeholder="Blank for random"
        )
    # ])

## WEIGHTS AND BIASES FUNCTIONS
def create_wandb_login(page):
    return html.Div([
        dcc.Input(
            id={
                'type': 'wandb-api-key',
                'page': page,
            },
            type='password',
            placeholder='WandB API Key',
            value=''
        ),
        html.Button(
            'Login',
            id={
                'type': 'wandb-login',
                'page': page,
            },
            n_clicks=0,
        ),
        html.Div(id={
            'type': 'wandb-login-feedback',
            'page': page,
        })
    ])

def create_wandb_project_dropdown(page):
    projects = get_projects()
    return html.Div([
            # html.Label("Select a Project:"),
            dcc.Dropdown(
            id={'type':'projects-dropdown', 'page':page},
            options=[{'label': project, 'value': project} for project in projects],
            placeholder="Select a W&B Project",
            className="dropdown-field"
            )
        ])

def create_sweeps_dropdown(page):
    return html.Div([
        dcc.Dropdown(
            id={'type':'sweeps-dropdown', 'page':page},
            options=[],
            multi=True,
            placeholder="Select a W&B Sweep",
            )
    ])

def format_wandb_config_param(config, param_name, all_values, all_ids, dash_id, model, agent, index=None, is_range=False):
    """
    Formats a parameter for the Weights & Biases (wandb) config depending on its structure:
    single value, range (min/max), or list of values.

    Args:
        config (dict): The wandb config dictionary.
        param_name (str): Name of the parameter to format.
        all_values (list): List of all parameter values.
        all_ids (list): List of all parameter IDs.
        dash_id (str): The specific parameter ID to look up.
        model (str): The model name.
        agent (str): The agent name.
        index (int): The index number.
        is_range (bool): if parameter should be configured between a min/max range
        
    Returns:
        dict: The updated wandb config dictionary.
    """
    if index:
        value = get_specific_value_id(all_values, all_ids, dash_id, model, agent, index)
    else:
        value = get_specific_value(all_values, all_ids, dash_id, model, agent)

    if is_range:
        # Check if the range has a single value or a min/max range
        if value[0] == value[1]:
            param = {"value": value[0]}
        else:
            param = {"min": value[0], "max": value[1]}

    else:
        # Check if it's a single value or a list of values
        if isinstance(value, list):
            param = {"values": value}
        else:
            param = {"value": value}

    if index:
        config["parameters"][f"{agent}_{model}_{index}_{param_name}"] = param
    else:
        config["parameters"][f"{agent}_{model}_{param_name}"] = param

    return config

def format_wandb_optimizer_options(config, param_name, all_values, all_ids, model, agent):
    for value in config["parameters"][f"{agent}_{model}_{param_name}"]['values']:
        if value == 'Adam':
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_weight_decay"] = {"values": get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', model, agent)}

        elif value == 'Adagrad':
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_weight_decay"] = {"values": get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', model, agent)}
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_lr_decay"] = {"values": get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', model, agent)}

        elif value == 'RMSprop':
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_weight_decay"] = {"values": get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', model, agent)}
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_momentum"] = {"values": get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', model, agent)}

        elif value == 'SGD':
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_weight_decay"] = {"values": get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', model, agent)}
            config["parameters"][f"{agent}_{model}_{param_name}_{value}_momentum"] = {"values": get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', model, agent)}
    return config
    
def format_wandb_kernel(config, all_indexed_values, all_indexed_ids, model, agent, layer):
    if isinstance(layer, int):
        layer_types = get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-type-hyperparam', model, agent, layer)
        for layer_type in layer_types:
            if layer_type in ['dense', 'conv2d']:
                # Add selected kernels for layer to sweep config
                config = format_wandb_config_param(config, "kernel", all_indexed_values, all_indexed_ids, 'kernel-function-hyperparam', model, agent, layer)
                # Add params for each selected kernel to sweep config
                config = format_wandb_kernel_options(config, all_indexed_values, all_indexed_ids, model, agent, layer)
    else:
        # Output layer kernels
        config = format_wandb_config_param(config, "kernel", all_indexed_values, all_indexed_ids, "kernel-function-hyperparam", model, agent, 'output')
        # Output layer kernel options
        config = format_wandb_kernel_options(config, all_indexed_values, all_indexed_ids, model, agent, 'output')

    return config

def format_wandb_kernel_options(config, all_values, all_ids, model, agent, layer_num):
    for kernel in get_specific_value_id(all_values, all_ids, 'kernel-function-hyperparam', model, agent, layer_num):
        if kernel != 'default':
            # initialize empty config dictionary for parameters
            param = {}

            if kernel == "constant":
                config = format_wandb_config_param(config, "kernel", all_values, all_ids, 'constant-value-hyperparam', model, agent, layer_num)

            elif kernel == "variance_scaling":
                # scale
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_scale"] = {"values": get_specific_value_id(all_values, all_ids, 'variance-scaling-scale-hyperparam', model, agent, layer_num)}

                # mode
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_mode"] = {"values": get_specific_value_id(all_values, all_ids, 'variance-scaling-mode-hyperparam', model, agent, layer_num)}

                # distribution
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_distribution"] = {"values": get_specific_value_id(all_values, all_ids, 'variance-scaling-distribution-hyperparam', model, agent, layer_num)}

            elif kernel == "uniform":
                # maxval
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_maxval"] = {"values": get_specific_value_id(all_values, all_ids, 'random-uniform-maxval-hyperparam', model, agent, layer_num)}

                # minval
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_minval"] = {"values": get_specific_value_id(all_values, all_ids, 'random-uniform-minval-hyperparam', model, agent, layer_num)}

            elif kernel == "normal":
                # mean
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_mean"] = {"values": get_specific_value_id(all_values, all_ids, 'random-normal-mean-hyperparam', model, agent, layer_num)}

                # stddev
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_stddev"] = {"values": get_specific_value_id(all_values, all_ids, 'random-normal-stddev-hyperparam', model, agent, layer_num)}
    
            elif kernel == "truncated_normal":
                # mean
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_mean"] = {"values": get_specific_value_id(all_values, all_ids, 'truncated-normal-mean-hyperparam', model, agent, layer_num)}

                # stddev
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_stddev"] = {"values": get_specific_value_id(all_values, all_ids, 'truncated-normal-stddev-hyperparam', model, agent, layer_num)}

            elif kernel == "xavier_uniform":
                # gain
                config["parameters"][f"{agent}_{kernel}_{layer_num}_{model}_gain"] = {"values": get_specific_value_id(all_values, all_ids, 'xavier-uniform-gain-hyperparam', model, agent, layer_num)}

            elif kernel == "xavier_normal":
                # gain
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_gain"] = {"values": get_specific_value_id(all_values, all_ids, 'xavier-normal-gain-hyperparam', model, agent, layer_num)}

            elif kernel == "kaiming_uniform":
                # mode
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_mode"] = {"values": get_specific_value_id(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', model, agent, layer_num)}

            elif kernel == "kaiming_normal":
                # mode
                config["parameters"][f"{agent}_{model}_{layer_num}_{kernel}_mode"] = {"values": get_specific_value_id(all_values, all_ids, 'kaiming-normal-mode-hyperparam', model, agent, layer_num)}

            else:
                if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                    "uniform", "normal", "truncated_normal", "variance_scaling"]:
                    raise ValueError(f"Unknown kernel: {kernel}")

    return config

def format_wandb_lr_scheduler(config, all_values, all_ids, model, agent):
    schedulers = get_specific_value(all_values, all_ids, 'lr-scheduler-hyperparam', model, agent)
    for scheduler in schedulers:
        config = format_wandb_config_param(config, "scheduler", all_values, all_ids, 'lr-scheduler-hyperparam', model, agent)
        config = format_wandb_lr_scheduler_options(config, all_values, all_ids, model, agent, scheduler)

    return config

def format_wandb_lr_scheduler_options(config, all_values, all_ids, model, agent, scheduler):
    if scheduler == 'step':
        config = format_wandb_steplr_options(config, all_values, all_ids, model, agent)
    elif scheduler == 'exponential':
        config = format_wandb_exponentiallr_options(config, all_values, all_ids, model, agent)
    elif scheduler == 'cosineannealing':
        config = format_wandb_cosineannealinglr_options(config, all_values, all_ids, model, agent)
    
    return config

def format_wandb_steplr_options(config, all_values, all_ids, model, agent):
    config = format_wandb_config_param(config, "step_size", all_values, all_ids, 'lr-step-size-hyperparam', model, agent)
    config = format_wandb_config_param(config, "gamma", all_values, all_ids, 'lr-gamma-hyperparam', model, agent)

def format_wandb_exponentiallr_options(config, all_values, all_ids, model, agent):
    config = format_wandb_config_param(config, "step_size", all_values, all_ids, 'lr-gamma-hyperparam', model, agent)

def format_wandb_cosineannealinglr_options(config, all_values, all_ids, model, agent):
    config = format_wandb_config_param(config, "step_size", all_values, all_ids, 'lr-t-max-hyperparam', model, agent)
    config = format_wandb_config_param(config, "step_size", all_values, all_ids, 'lr-eta-min-hyperparam', model, agent)

def format_wandb_model_layers(config, all_values, all_ids, all_indexed_values, all_indexed_ids, model, agent):
    # Get num layers in model
    num_layers = get_specific_value(all_values, all_ids, 'hidden-layers-slider', model, agent)[1]
    # Add num_layers to wandb config
    config = format_wandb_config_param(config, 'num_layers', all_values, all_ids, 'hidden-layers-slider', model, agent, index=None, is_range=True)
    for layer in range(1, num_layers + 1):
        # Get layer types
        layer_types = get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-type-hyperparam', model, agent, layer)
        # Add layer_types to wandb config
        config = format_wandb_config_param(config, 'layer_types', all_indexed_values, all_indexed_ids, 'layer-type-hyperparam', model, agent, layer)
        # Loop through each layer type to assign correct params
        for layer_type in layer_types:
            if layer_type == 'dense':
                config = format_wandb_config_param(config, 'num_units', all_indexed_values, all_indexed_ids, 'layer-units-slider', model, agent, layer)
                config = format_wandb_config_param(config, 'bias', all_indexed_values, all_indexed_ids, 'dense-bias-hyperparam', model, agent, layer)
                config = format_wandb_kernel(config, all_indexed_values, all_indexed_ids, model, agent, layer)
            elif layer_type == 'cnn':
                config = format_wandb_config_param(config, 'out_channels', all_indexed_values, all_indexed_ids, 'layer-units-slider', model, agent, layer)

    return config


def format_wandb_layer_units(config, param_name, all_values, all_ids, all_indexed_values, all_indexed_ids, id, model, agent):
    print(f'num layers:{get_specific_value(all_values, all_ids, id, model, agent)}')
    for i in range(1, get_specific_value(all_values, all_ids, id, model, agent)[1] + 1):
        config["parameters"][agent]["parameters"][f"{model}_layer_{i}_{param_name}"] = {
            "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-units-slider', model, agent, i)   
        }
    return config


def create_wandb_config(method, project, sweep_name, metric_name, metric_goal, env_library, env, env_params, env_wrappers, agent, all_values, all_ids, all_indexed_values, all_indexed_ids):
    #DEBUG
    # print(f'create wandb config fired...')
    sweep_config = {
        "method": method,
        "project": project,
        "name": sweep_name,
        "metric": {"name": metric_name, "goal": metric_goal},
        "parameters": {
            "env_library": {"value": env_library},
            "env_id": {"value": env},
            **{f'env_{param}': {"value":value} for param, value in env_params.items()},
            "env_wrappers": {"values": env_wrappers},
            "model_type": {"value": agent},
        },
            
    }
    # set base config for each agent type
    # for agent in agent_selection:
        # Initialize the dictionary for the agent if it doesn't exist

    # if agent not in sweep_config["parameters"]:
    #     sweep_config["parameters"][agent] = {}
    
    if agent in ["DDPG", "TD3"]:
        sweep_config["parameters"][agent]["parameters"] = {}

        # actor learning rate
        value_range = get_specific_value(all_values, all_ids, 'learning-rate-slider', 'actor', agent)
        config = {"values": value_range}
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_learning_rate"] = config
        
        # critic learning rate
        value_range = get_specific_value(all_values, all_ids, 'learning-rate-slider', 'critic', agent)
        config = {"values": value_range}
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_learning_rate"] = config
        
        # discount
        value_range = get_specific_value(all_values, all_ids, 'discount-slider', 'none', agent)
        config = {"values": value_range}
        sweep_config["parameters"][agent]["parameters"][f"{agent}_discount"] = config
        
        # tau
        value_range = get_specific_value(all_values, all_ids, 'tau-hyperparam', 'none', agent)
        config = {"values": value_range}
        sweep_config["parameters"][agent]["parameters"][f"{agent}_tau"] = config
        
        # epsilon
        value_range = get_specific_value(all_values, all_ids, 'epsilon-greedy-hyperparam', 'none', agent)
        config = {"values": value_range}
        sweep_config["parameters"][agent]["parameters"][f"{agent}_epsilon_greedy"] = config

        # warmup
        sweep_config["parameters"][agent]["parameters"][f"{agent}_warmup"] = \
            {"values": get_specific_value(all_values, all_ids, 'warmup-slider-hyperparam', 'none', agent)}

        # normalize input
        sweep_config["parameters"][agent]["parameters"][f"{agent}_normalize_input"] = \
            {"values": get_specific_value(all_values, all_ids, 'normalize-input-hyperparam', 'none', agent)}

        # normalize input options
        # for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_normalize_input"]['values']:
        #     if value == 'True':
        #         value_range = get_specific_value(all_values, all_ids, 'norm-clip-value-hyperparam', 'none', agent)
        #         config = {"values": value_range}
        #     sweep_config["parameters"][agent]["parameters"][f"{agent}_normalize_clip"] = config

        value_range = get_specific_value(all_values, all_ids, 'norm-clip-value-hyperparam', 'none', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_normalizer_clip"] = config

        # replay buffer size
        sweep_config["parameters"][agent]["parameters"][f"{agent}_replay_buffer_size"] = \
            {"values": get_specific_value(all_values, all_ids, 'buffer-size-hyperparam', 'none', agent)}

        # Get Device
        value_range = get_specific_value(all_values, all_ids, 'device', 'none', agent)
        sweep_config["parameters"][agent]["parameters"][f"{agent}_device"] = {"value": value_range}
        
        # actor cnn layers
        value_range = get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'actor', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_num_cnn_layers"] = config

        # actor num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'actor', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_num_layers"] = config

        # actor activation
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_activation"] = \
            {"values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'actor', agent)}
        #DEBUG
        # print(f'DDPG actor activation set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_activation"]}')

        # actor hidden layers kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-hidden', agent)}
        #DEBUG
        # print(f'DDPG actor kernel initializer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_kernel_initializer"]}')

        # actor hidden kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-hidden', agent):
            if kernel != 'default':
                if f"{agent}_actor_hidden_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'actor-hidden', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'actor-hidden', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}


                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_{kernel}"]["parameters"] = config


        # actor output layer kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-output', agent)}

        # actor output kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-output', agent):
            if kernel != 'default':
                if f"{agent}_actor_output_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'actor-output', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'actor-output', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}


                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_{kernel}"]["parameters"] = config

        # actor optimizer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"] = \
            {"values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'actor', agent)}
        #DEBUG
        print(f'DDPG actor optimizer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"]}')

        # Actor optimizer options
        for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"]['values']:
            sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer_{value}_options"] = {'parameters': {}}
            config = {}
            if value == 'Adam':
                value_range = get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

            elif value == 'Adagrad':
                value_range = get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', 'actor', agent)
                config[f'{value}_lr_decay'] = {"values": value_range}

            elif value == 'RMSprop':
                value_range = get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', 'actor', agent)
                config[f'{value}_momentum'] = {"values": value_range}

            elif value == 'SGD':
                value_range = get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', 'actor', agent)
                config[f'{value}_momentum'] = {"values": value_range}
                
            sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer_{value}_options"]['parameters'] = config
                
        # actor normalize layers
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_normalize_layers"] = \
            {"values": get_specific_value(all_values, all_ids, 'normalize-layers-hyperparam', 'actor', agent)}

        # critic cnn layers
        value_range = get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'critic', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_num_cnn_layers"] = config
        #DEBUG
        # print(f'DDPG critic cnn layers set to {config}')


        # critic state num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-state', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_state_num_layers"] = config
        #DEBUG
        # print(f'DDPG critic state num layers set to {config}')

        # critic merged num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-merged', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_merged_num_layers"] = config
        #DEBUG
        # print(f'DDPG critic merged num layers set to {config}')

        # critic activation
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_activation"] = \
            {"values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'critic', agent)}
        #DEBUG
        # print(f'DDPG critic activation set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_activation"]}')

        # critic hidden layers kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-hidden', agent)}
        #DEBUG
        # print(f'DDPG critic kernel initializer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_kernel_initializer"]}')

        # critic hidden kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-hidden', agent):
            if kernel != 'default':
                if f"{agent}_critic_hidden_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'critic-hidden', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'critic-hidden', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_{kernel}"]["parameters"] = config

        # critic output layer kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-output', agent)}

        # critic output kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-output', agent):
            if kernel != "default":
                if f"{agent}_critic_output_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'critic-output', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'critic-output', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_{kernel}"]["parameters"] = config

        # critic optimizer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"] = \
            {"values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'critic', agent)}
        #DEBUG
        # print(f'DDPG critic optimizer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"]}')

        # Critic optimizer options
        for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"]['values']:
            sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer_{value}_options"] = {'parameters': {}}
            config = {}
            if value == 'Adam':
                value_range = get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

            elif value == 'Adagrad':
                value_range = get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', 'critic', agent)
                config[f'{value}_lr_decay'] = {"values": value_range}

            elif value == 'RMSprop':
                value_range = get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', 'critic', agent)
                config[f'{value}_momentum'] = {"values": value_range}

            elif value == 'SGD':
                value_range = get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', 'critic', agent)
                config[f'{value}_momentum'] = {"values": value_range}
                
            sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer_{value}_options"]['parameters'] = config

        # critic normalize layers
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_normalize_layers"] = \
            {"values": get_specific_value(all_values, all_ids, 'normalize-layers-hyperparam', 'critic', agent)}
        
        # replay buffer ## NOT NEEDED
        # sweep_config["parameters"][agent]["parameters"][f"{agent}_replay_buffer"] = {"values": ["ReplayBuffer"]}
        #DEBUG
        # print(f'DDPG replay buffer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_replay_buffer"]}')

        # batch size
        sweep_config["parameters"][agent]["parameters"][f"{agent}_batch_size"] = \
            {"values": get_specific_value(all_values, all_ids, 'batch-size-hyperparam', 'none', agent)}
        #DEBUG
        # print(f'DDPG batch size set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_batch_size"]}')

        # noise
        sweep_config["parameters"][agent]["parameters"][f"{agent}_noise"] = \
            {"values": get_specific_value(all_values, all_ids, 'noise-function-hyperparam', 'none', agent)}
        #DEBUG
        # print(f'DDPG noise set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_noise"]}')

        # noise parameter options
        for noise in get_specific_value(all_values, all_ids, 'noise-function-hyperparam', 'none', agent):
            # Initialize the dictionary for the agent if it doesn't exist
            if f"{agent}_noise_{noise}" not in sweep_config["parameters"][agent]["parameters"]:
                sweep_config["parameters"][agent]["parameters"][f"{agent}_noise_{noise}"]={"parameters":{}}
            
            # initialize empty config dictionary for parameters
            config = {}
            
            if noise == "Ornstein-Uhlenbeck":
                # mean
                value_range = get_specific_value(all_values, all_ids, 'ou-mean-hyperparam', 'none', agent)
                config["mean"] = {"values": value_range}
                
                # theta
                value_range = get_specific_value(all_values, all_ids, 'ou-theta-hyperparam', 'none', agent)
                config["theta"] = {"values": value_range}

                # sigma
                value_range = get_specific_value(all_values, all_ids, 'ou-sigma-hyperparam', 'none', agent)
                config["sigma"] = {"values": value_range}
                
            elif noise == "Normal":
                # mean
                value_range = get_specific_value(all_values, all_ids, 'normal-mean-hyperparam', 'none', agent)
                config["mean"] = {"values": value_range}

                # stddev
                value_range = get_specific_value(all_values, all_ids, 'normal-stddev-hyperparam', 'none', agent)
                config["stddev"] = {"values": value_range}

            elif noise == "Uniform":
                # minval
                value_range = get_specific_value(all_values, all_ids, 'uniform-min-hyperparam', 'none', agent)
                config["minval"] = {"values": value_range}

                # maxval
                value_range = get_specific_value(all_values, all_ids, 'uniform-max-hyperparam', 'none', agent)
                config["maxval"] = {"values": value_range}

            sweep_config["parameters"][agent]["parameters"][f"{agent}_noise_{noise}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG noise set to {config}')

        
        #DEBUG
        # print(f'DDPG critic kernel set to {config}')

        # CNN layer params
        # Actor CNN layers
        for i in range(1, get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'actor', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"actor_cnn_layer_{i}_{agent}"] = {"parameters":{}}
            config = {}
            config[f"{agent}_actor_cnn_layer_{i}_types"] = {"values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'cnn-layer-type-hyperparam', 'actor', agent, i)}

            # loop through each type in CNN layer and get the parameters to add to the sweep config
            for value in config[f"{agent}_actor_cnn_layer_{i}_types"]["values"]:
                if value == "conv":
                    config[f"{agent}_actor_cnn_layer_{i}_conv_filters"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-filters-hyperparam', 'actor', agent, i)
                    }

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-kernel-size-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-stride-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_strides"] = {"min": value_range[0], "max": value_range[1]}

                    config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {
                        "value": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-hyperparam', 'actor', agent, i)
                    }

                    if config[f"{agent}_actor_cnn_layer_{i}_conv_padding"]["value"] == 'custom':
                        value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-custom-hyperparam', 'actor', agent, i)
                        if value_range[0] == value_range[1]:
                            config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {"value": value_range[0]}
                        else:
                            config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {"min": value_range[0], "max": value_range[1]}
                        
                        # val_config["conv_padding"]["parameters"] = pad_config
                    
                    config[f"{agent}_actor_cnn_layer_{i}_conv_bias"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-use-bias-hyperparam', 'actor', agent, i)
                    }
                
                if value == "pool":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-kernel-size-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-stride-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_strides"] = {"min": value_range[0], "max": value_range[1]}

                if value == "dropout":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'dropout-prob-hyperparam', 'actor', agent, i)
                    config[f"{agent}_actor_cnn_layer_{i}_dropout_prob"] = {"values": value_range}

                # config["parameters"] = val_config

            sweep_config["parameters"][agent]["parameters"][f"actor_cnn_layer_{i}_{agent}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG actor CNN layers set to {config}')

        # Critic CNN layers
        for i in range(1, get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'critic', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_cnn_layer_{i}_{agent}"] = {"parameters":{}}
            config = {}
            config[f"{agent}_critic_cnn_layer_{i}_types"] = {"values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'cnn-layer-type-hyperparam', 'critic', agent, i)}

            # loop through each type in CNN layer and get the parameters to add to the sweep config
            for value in config[f"{agent}_critic_cnn_layer_{i}_types"]["values"]:
                if value == "conv":
                    config[f"{agent}_critic_cnn_layer_{i}_conv_filters"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-filters-hyperparam', 'critic', agent, i)
                    }

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-kernel-size-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-stride-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_strides"] = {"min": value_range[0], "max": value_range[1]}

                    config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {
                        "value": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-hyperparam', 'critic', agent, i)
                    }

                    if config[f"{agent}_critic_cnn_layer_{i}_conv_padding"]["value"] == 'custom':
                        value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-custom-hyperparam', 'critic', agent, i)
                        if value_range[0] == value_range[1]:
                            config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {"value": value_range[0]}
                        else:
                            config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {"min": value_range[0], "max": value_range[1]}
                    
                    config[f"{agent}_critic_cnn_layer_{i}_conv_bias"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-use-bias-hyperparam', 'critic', agent, i)
                    }
                
                if value == "pool":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-kernel-size-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-stride-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_strides"] = {"min": value_range[0], "max": value_range[1]}

                if value == "dropout":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'dropout-prob-hyperparam', 'critic', agent, i)
                    config[f"{agent}_critic_cnn_layer_{i}_dropout_prob"] = {"values": value_range}

            sweep_config["parameters"][agent]["parameters"][f"critic_cnn_layer_{i}_{agent}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG critic CNN layers set to {config}')

        # layer units
        # actor layer units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'actor', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"actor_units_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'actor', agent)   
            }
        # critic state units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-state', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_units_state_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'critic-state', agent)
            }
        # critic merged units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-merged', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_units_merged_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'critic-merged', agent)
            }

        # Add save dir
        sweep_config["parameters"][agent]["parameters"][f"{agent}_save_dir"] = \
            {"value": get_specific_value(all_values, all_ids, 'save-dir', 'none', agent)}
        
        if agent == "TD3":
            # Add target action stddev
            sweep_config["parameters"][agent]["parameters"][f"{agent}_target_action_stddev"] = \
                {"values": get_specific_value(all_values, all_ids, 'target-action-noise-stddev-slider-hyperparam', 'none', agent)}
            
            # Add target action clip
            sweep_config["parameters"][agent]["parameters"][f"{agent}_target_action_clip"] = \
                {"values": get_specific_value(all_values, all_ids, 'target-action-noise-clip-slider-hyperparam', 'none', agent)}
            
            # Add actor update delay
            sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_update_delay"] = \
                {"values": get_specific_value(all_values, all_ids, 'actor-update-delay-slider-hyperparam', 'none', agent)}


    if agent == "HER_DDPG":
        sweep_config["parameters"][agent]["parameters"] = {}

        # actor learning rate
        value_range = get_specific_value(all_values, all_ids, 'learning-rate-slider', 'actor', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_learning_rate"] = config
        
        # critic learning rate
        value_range = get_specific_value(all_values, all_ids, 'learning-rate-slider', 'critic', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_learning_rate"] = config
        
        # goal strategy
        sweep_config["parameters"][agent]["parameters"][f"{agent}_goal_strategy"] = \
            {"values": get_specific_value(all_values, all_ids, 'goal-strategy-hyperparam', 'none', agent)}

        # number of goals
        for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_goal_strategy"]['values']:
            if value == 'future':
                value_range = get_specific_value(all_values, all_ids, 'future-goals-hyperparam', 'none', agent)
                if value_range[0] == value_range[1]:
                    config = {"value": value_range[0]}
                else:
                    config = {"min": value_range[0], "max": value_range[1]}

            sweep_config["parameters"][agent]["parameters"][f"{agent}_num_goals"] = config
        
        # goal tolerance
        value_range = get_specific_value(all_values, all_ids, 'goal-tolerance-hyperparam', 'none', agent)
        config = {"values": value_range}

        sweep_config["parameters"][agent]["parameters"][f"{agent}_goal_tolerance"] = config

        # discount
        value_range = get_specific_value(all_values, all_ids, 'discount-slider', 'none', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_discount"] = config
        
        # tau
        value_range = get_specific_value(all_values, all_ids, 'tau-hyperparam', 'none', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_tau"] = config
        
        # epsilon
        value_range = get_specific_value(all_values, all_ids, 'epsilon-greedy-hyperparam', 'none', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_epsilon_greedy"] = config

        # normalize input options
        value_range = get_specific_value(all_values, all_ids, 'norm-clip-value-hyperparam', 'none', agent)
        config = {"values": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_normalizer_clip"] = config

        # Get Device
        value_range = get_specific_value(all_values, all_ids, 'device', 'none', agent)
        sweep_config["parameters"][agent]["parameters"][f"{agent}_device"] = {"value": value_range}
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_normalizer_clip"] = config

        # actor cnn layers
        value_range = get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'actor', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_num_cnn_layers"] = config

        # actor num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'actor', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_num_layers"] = config

        # actor activation
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_activation"] = \
            {"values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'actor', agent)}
        #DEBUG
        # print(f'DDPG actor activation set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_activation"]}')

        # actor hidden layer kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-hidden', agent)}
        #DEBUG
        # print(f'DDPG actor kernel initializer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_kernel_initializer"]}')

        # actor hidden layer kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-output', agent)}

        # actor optimizer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"] = \
            {"values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'actor', agent)}
        #DEBUG
        print(f'DDPG actor optimizer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"]}')

        # Actor optimizer options
        for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer"]['values']:
            sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer_{value}_options"] = {'parameters': {}}
            config = {}
            if value == 'Adam':
                value_range = get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

            elif value == 'Adagrad':
                value_range = get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', 'actor', agent)
                config[f'{value}_lr_decay'] = {"values": value_range}

            elif value == 'RMSprop':
                value_range = get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', 'actor', agent)
                config[f'{value}_momentum'] = {"values": value_range}

            elif value == 'SGD':
                value_range = get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', 'actor', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', 'actor', agent)
                config[f'{value}_momentum'] = {"values": value_range}
                
            sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_optimizer_{value}_options"]['parameters'] = config
                
        # actor normalize layers
        sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_normalize_layers"] = \
            {"values": get_specific_value(all_values, all_ids, 'normalize-layers-hyperparam', 'actor', agent)}

        # actor clamp output
        # sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_clamp_output"] = \
        #     {"values": get_specific_value(all_values, all_ids, 'clamp-value-hyperparam', 'actor', agent)}

        # critic cnn layers
        value_range = get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'critic', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_num_cnn_layers"] = config
        #DEBUG
        # print(f'DDPG critic cnn layers set to {config}')


        # critic state num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-state', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_state_num_layers"] = config
        #DEBUG
        # print(f'DDPG critic state num layers set to {config}')

        # critic merged num layers
        value_range = get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-merged', agent)
        if value_range[0] == value_range[1]:
            config = {"value": value_range[0]}
        else:
            config = {"min": value_range[0], "max": value_range[1]}           
        
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_merged_num_layers"] = config
        #DEBUG
        # print(f'DDPG critic merged num layers set to {config}')

        # critic activation
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_activation"] = \
            {"values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'critic', agent)}
        #DEBUG
        # print(f'DDPG critic activation set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_activation"]}')

        # critic hidden kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-hidden', agent)}
        #DEBUG
        # print(f'DDPG critic kernel initializer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_kernel_initializer"]}')

        # critic output kernel initializer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_initializer"] = \
            {"values": get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-output', agent)}

        # critic optimizer
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"] = \
            {"values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'critic', agent)}
        #DEBUG
        # print(f'DDPG critic optimizer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"]}')

        # Critic optimizer options
        for value in sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer"]['values']:
            sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer_{value}_options"] = {'parameters': {}}
            config = {}
            if value == 'Adam':
                value_range = get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

            elif value == 'Adagrad':
                value_range = get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', 'critic', agent)
                config[f'{value}_lr_decay'] = {"values": value_range}

            elif value == 'RMSprop':
                value_range = get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', 'critic', agent)
                config[f'{value}_momentum'] = {"values": value_range}

            elif value == 'SGD':
                value_range = get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', 'critic', agent)
                config[f'{value}_weight_decay'] = {"values": value_range}

                value_range = get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', 'critic', agent)
                config[f'{value}_momentum'] = {"values": value_range}
                
            sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_optimizer_{value}_options"]['parameters'] = config

        # critic normalize layers
        sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_normalize_layers"] = \
            {"values": get_specific_value(all_values, all_ids, 'normalize-layers-hyperparam', 'critic', agent)}
        
        
        # replay buffer size
        sweep_config["parameters"][agent]["parameters"][f"{agent}_replay_buffer_size"] = \
            {"values": get_specific_value(all_values, all_ids, 'buffer-size-hyperparam', 'none', agent)}
        #DEBUG
        # print(f'DDPG replay buffer set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_replay_buffer"]}')

        # batch size
        sweep_config["parameters"][agent]["parameters"][f"{agent}_batch_size"] = \
            {"values": get_specific_value(all_values, all_ids, 'batch-size-hyperparam', 'none', agent)}
        #DEBUG
        # print(f'DDPG batch size set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_batch_size"]}')

        # noise
        sweep_config["parameters"][agent]["parameters"][f"{agent}_noise"] = \
            {"values": get_specific_value(all_values, all_ids, 'noise-function-hyperparam', 'none', agent)}
        #DEBUG
        # print(f'DDPG noise set to {sweep_config["parameters"][agent]["parameters"][f"{agent}_noise"]}')

        # noise parameter options
        for noise in get_specific_value(all_values, all_ids, 'noise-function-hyperparam', 'none', agent):
            # Initialize the dictionary for the agent if it doesn't exist
            if f"{agent}_noise_{noise}" not in sweep_config["parameters"][agent]["parameters"]:
                sweep_config["parameters"][agent]["parameters"][f"{agent}_noise_{noise}"]={"parameters":{}}
            
            # initialize empty config dictionary for parameters
            config = {}
            
            if noise == "Ornstein-Uhlenbeck":
                # mean
                value_range = get_specific_value(all_values, all_ids, 'ou-mean-hyperparam', 'none', agent)
                config["mean"] = {"values": value_range}
                
                # theta
                value_range = get_specific_value(all_values, all_ids, 'ou-theta-hyperparam', 'none', agent)
                config["theta"] = {"values": value_range}

                # sigma
                value_range = get_specific_value(all_values, all_ids, 'ou-sigma-hyperparam', 'none', agent)
                config["sigma"] = {"values": value_range}
                
            elif noise == "Normal":
                # mean
                value_range = get_specific_value(all_values, all_ids, 'normal-mean-hyperparam', 'none', agent)
                config["mean"] = {"values": value_range}

                # stddev
                value_range = get_specific_value(all_values, all_ids, 'normal-stddev-hyperparam', 'none', agent)
                config["stddev"] = {"values": value_range}
            
            elif noise == "Uniform":
                # minval
                value_range = get_specific_value(all_values, all_ids, 'uniform-min-hyperparam', 'none', agent)
                config["minval"] = {"values": value_range}

                # maxval
                value_range = get_specific_value(all_values, all_ids, 'uniform-max-hyperparam', 'none', agent)
                config["maxval"] = {"values": value_range}

            sweep_config["parameters"][agent]["parameters"][f"{agent}_noise_{noise}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG noise set to {config}')

        # kernel options       
        # actor hidden kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-hidden', agent):
            if kernel != 'default':
                if f"{agent}_actor_hidden_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'actor-hidden', agent)
                    config["value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'actor-hidden', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'actor-hidden', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'actor-hidden', agent)
                    config["stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}


                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'actor-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_hidden_kernel_{kernel}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG actor kernel set to {config}')

        # actor output kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'actor-output', agent):
            if kernel != 'default':
                if f"{agent}_actor_output_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'actor-output', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'actor-output', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}


                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'actor-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_actor_output_kernel_{kernel}"]["parameters"] = config

        # critic hidden kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-hidden', agent):
            if kernel != 'default':
                if f"{agent}_critic_hidden_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'critic-hidden', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'critic-hidden', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'critic-hidden', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_hidden_kernel_{kernel}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG critic kernel set to {config}')

        # critic output kernel options
        for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'critic-output', agent):
            if kernel != "default":
                if f"{agent}_critic_output_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
                    sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_{kernel}"]={"parameters":{}}

                # initialize empty config dictionary for parameters
                config = {}

                if kernel == "constant":
                    value_range = get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_value"] = {"values": value_range}
    
                elif kernel == "variance_scaling":
                    # scale
                    value_range = get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_scale"] = {"values": value_range}

                    # mode
                    config[f"{kernel}_mode"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'critic-output', agent)}

                    # distribution
                    config[f"{kernel}_distribution"] = {"values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'critic-output', agent)}

                elif kernel == "uniform":
                    # maxval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_maxval"] = {"values": value_range}

                    # minval
                    value_range = get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_minval"] = {"values": value_range}

                elif kernel == "normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}
        
                elif kernel == "truncated_normal":
                    # mean
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mean"] = {"values": value_range}

                    # stddev
                    value_range = get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_stddev"] = {"values": value_range}

                elif kernel == "xavier_uniform":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-uniform-gain-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "xavier_normal":
                    # gain
                    value_range = get_specific_value(all_values, all_ids, 'xavier-normal-gain-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_gain"] = {"values": value_range}

                elif kernel == "kaiming_uniform":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                elif kernel == "kaiming_normal":
                    # mode
                    values = get_specific_value(all_values, all_ids, 'kaiming-normal-mode-hyperparam', 'critic-output', agent)
                    config[f"{kernel}_mode"] = {"values": values}

                    
                else:
                    if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform", "kaiming_normal", "zeros", "ones", \
                        "uniform", "normal", "truncated_normal", "variance_scaling"]:
                        raise ValueError(f"Unknown kernel: {kernel}")
                    
                sweep_config["parameters"][agent]["parameters"][f"{agent}_critic_output_kernel_{kernel}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG critic kernel set to {config}')

        # CNN layer params
        # Actor CNN layers
        for i in range(1, get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'actor', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"actor_cnn_layer_{i}_{agent}"] = {"parameters":{}}
            config = {}
            config[f"{agent}_actor_cnn_layer_{i}_types"] = {"values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'cnn-layer-type-hyperparam', 'actor', agent, i)}

            # loop through each type in CNN layer and get the parameters to add to the sweep config
            for value in config[f"{agent}_actor_cnn_layer_{i}_types"]["values"]:
                if value == "conv":
                    config[f"{agent}_actor_cnn_layer_{i}_conv_filters"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-filters-hyperparam', 'actor', agent, i)
                    }

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-kernel-size-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-stride-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_conv_strides"] = {"min": value_range[0], "max": value_range[1]}

                    config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {
                        "value": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-hyperparam', 'actor', agent, i)
                    }

                    if config[f"{agent}_actor_cnn_layer_{i}_conv_padding"]["value"] == 'custom':
                        value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-custom-hyperparam', 'actor', agent, i)
                        if value_range[0] == value_range[1]:
                            config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {"value": value_range[0]}
                        else:
                            config[f"{agent}_actor_cnn_layer_{i}_conv_padding"] = {"min": value_range[0], "max": value_range[1]}
                        
                        # val_config["conv_padding"]["parameters"] = pad_config
                    
                    config[f"{agent}_actor_cnn_layer_{i}_conv_bias"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-use-bias-hyperparam', 'actor', agent, i)
                    }
                
                if value == "pool":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-kernel-size-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-stride-hyperparam', 'actor', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_actor_cnn_layer_{i}_pool_strides"] = {"min": value_range[0], "max": value_range[1]}

                if value == "dropout":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'dropout-prob-hyperparam', 'actor', agent, i)
                    config[f"{agent}_actor_cnn_layer_{i}_dropout_prob"] = {"values": value_range}

                # config["parameters"] = val_config

            sweep_config["parameters"][agent]["parameters"][f"actor_cnn_layer_{i}_{agent}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG actor CNN layers set to {config}')

        # Critic CNN layers
        for i in range(1, get_specific_value(all_values, all_ids, 'cnn-layers-slider-hyperparam', 'critic', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_cnn_layer_{i}_{agent}"] = {"parameters":{}}
            config = {}
            config[f"{agent}_critic_cnn_layer_{i}_types"] = {"values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'cnn-layer-type-hyperparam', 'critic', agent, i)}

            # loop through each type in CNN layer and get the parameters to add to the sweep config
            for value in config[f"{agent}_critic_cnn_layer_{i}_types"]["values"]:
                if value == "conv":
                    config[f"{agent}_critic_cnn_layer_{i}_conv_filters"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-filters-hyperparam', 'critic', agent, i)
                    }

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-kernel-size-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-stride-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_conv_strides"] = {"min": value_range[0], "max": value_range[1]}

                    config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {
                        "value": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-hyperparam', 'critic', agent, i)
                    }

                    if config[f"{agent}_critic_cnn_layer_{i}_conv_padding"]["value"] == 'custom':
                        value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-padding-custom-hyperparam', 'critic', agent, i)
                        if value_range[0] == value_range[1]:
                            config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {"value": value_range[0]}
                        else:
                            config[f"{agent}_critic_cnn_layer_{i}_conv_padding"] = {"min": value_range[0], "max": value_range[1]}
                    
                    config[f"{agent}_critic_cnn_layer_{i}_conv_bias"] = {
                        "values": get_specific_value_id(all_indexed_values, all_indexed_ids, 'conv-use-bias-hyperparam', 'critic', agent, i)
                    }
                
                if value == "pool":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-kernel-size-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_kernel_size"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_kernel_size"] = {"min": value_range[0], "max": value_range[1]}

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'pool-stride-hyperparam', 'critic', agent, i)
                    if value_range[0] == value_range[1]:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_strides"] = {"value": value_range[0]}
                    else:
                        config[f"{agent}_critic_cnn_layer_{i}_pool_strides"] = {"min": value_range[0], "max": value_range[1]}

                if value == "dropout":

                    value_range = get_specific_value_id(all_indexed_values, all_indexed_ids, 'dropout-prob-hyperparam', 'critic', agent, i)
                    config[f"{agent}_critic_cnn_layer_{i}_dropout_prob"] = {"values": value_range}

            sweep_config["parameters"][agent]["parameters"][f"critic_cnn_layer_{i}_{agent}"]["parameters"] = config
        #DEBUG
        # print(f'DDPG critic CNN layers set to {config}')

        # layer units
        # actor layer units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'actor', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"actor_units_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'actor', agent)   
            }
        # critic state units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-state', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_units_state_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'critic-state', agent)
            }
        # critic merged units
        for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'critic-merged', agent)[1] + 1):
            sweep_config["parameters"][agent]["parameters"][f"critic_units_merged_layer_{i}_{agent}"] = {
                "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'critic-merged', agent)
            }

        # Add save dir
        sweep_config["parameters"][agent]["parameters"][f"{agent}_save_dir"] = \
            {"value": get_specific_value(all_values, all_ids, 'save-dir', 'none', agent)}

    if agent == "PPO":
        # sweep_config["parameters"][agent]["parameters"] = {}

        # Policy learning rate constant
        sweep_config = format_wandb_config_param(sweep_config, "learning_rate_constant", all_values, all_ids, 'learning-rate-const-hyperparam', 'policy', agent)

        # Policy learning rate exponent
        sweep_config = format_wandb_config_param(sweep_config, "learning_rate_exponent", all_values, all_ids, 'learning-rate-exp-hyperparam', 'policy', agent)
        
        # Value learning rate constant
        sweep_config = format_wandb_config_param(sweep_config, "learning_rate_constant", all_values, all_ids, 'learning-rate-const-hyperparam', 'value', agent)

        # Value learning rate exponent
        sweep_config = format_wandb_config_param(sweep_config, "learning_rate_exponent", all_values, all_ids, 'learning-rate-exp-hyperparam', 'value', agent)

        # Distribution
        sweep_config = format_wandb_config_param(sweep_config, "distribution", all_values, all_ids, "distribution-hyperparam", 'policy', agent)

        # Discount
        sweep_config = format_wandb_config_param(sweep_config, "discount", all_values, all_ids, 'discount-slider', 'none', agent)

        # Reward clip
        sweep_config = format_wandb_config_param(sweep_config, "reward_clip", all_values, all_ids, "reward-clip-hyperparam", 'none', agent)

        # Advantage Coeff
        sweep_config = format_wandb_config_param(sweep_config, "advantage", all_values, all_ids, 'advantage-coeff-hyperparam', 'none', agent)

        # Model type
        sweep_config = format_wandb_config_param(sweep_config, "model_type", all_values, all_ids, 'model-type-hyperparam', 'policy', agent)
        
        # Policy Surrogate Clip
        sweep_config = format_wandb_config_param(sweep_config, "clip_range", all_values, all_ids, 'surrogate-clip-hyperparam', 'policy', agent)

        # Policy Grad Clip
        sweep_config = format_wandb_config_param(sweep_config, "grad_clip", all_values, all_ids, 'grad-clip-hyperparam', 'policy', agent)

        # Entropy Coeff
        sweep_config = format_wandb_config_param(sweep_config, "entropy", all_values, all_ids, 'entropy-coeff-hyperparam', 'none', agent)

        # KL Coeff
        sweep_config = format_wandb_config_param(sweep_config, "kl", all_values, all_ids, 'kl-coeff-hyperparam', 'none', agent)

        # Normalize Advantage
        sweep_config = format_wandb_config_param(sweep_config, "normalize_advantage", all_values, all_ids, 'normalize-advantage-hyperparam', 'none', agent)

        # Normalize Values
        sweep_config = format_wandb_config_param(sweep_config, "normalize_values", all_values, all_ids, 'normalize-values-hyperparam', 'value', agent)

        # Normalize Value Clip
        sweep_config = format_wandb_config_param(sweep_config, "normalize_values_clip", all_values, all_ids, 'norm-values-clip-hyperparam', 'value', agent)

        # Get Device
        sweep_config = format_wandb_config_param(sweep_config, "device", all_values, all_ids, 'device', 'none', agent)

        # Policy num layers
        # sweep_config = format_wandb_config_param(sweep_config, "num_layers", all_values, all_ids, 'hidden-layers-slider', 'policy', agent, is_range=True)

        # Policy layers
        sweep_config = format_wandb_model_layers(sweep_config, all_values, all_ids, all_indexed_values, all_indexed_ids, 'policy', agent)
        
        # policy output layer kernel initializers
        sweep_config = format_wandb_kernel(sweep_config, all_indexed_values, all_indexed_ids, 'policy', agent, 'output')

        # Policy activation
        # sweep_config = format_wandb_config_param(sweep_config, "policy_activation", all_values, all_ids, 'activation-function-hyperparam', 'policy', agent)

        # Policy optimizer
        sweep_config = format_wandb_config_param(sweep_config, "optimizer", all_values, all_ids, 'optimizer-hyperparam', 'policy', agent)

        # Policy optimizer options
        sweep_config = format_wandb_optimizer_options(sweep_config, "optimizer", all_values, all_ids, "policy", agent)

        # Policy learning rate scheduler
        sweep_config = format_wandb_lr_scheduler(sweep_config, all_values, all_ids, "policy", agent)

        # Value num layers
        # sweep_config = format_wandb_config_param(sweep_config, "num_layers", all_values, all_ids, 'hidden-layers-slider', 'value', agent, is_range=True)

        # Value activation
        # sweep_config = format_wandb_config_param(sweep_config, "value_activation", all_values, all_ids, 'activation-function-hyperparam', 'value', agent)

        # Value layers
        sweep_config = format_wandb_model_layers(sweep_config, all_values, all_ids, all_indexed_values, all_indexed_ids, 'value', agent)

        # Value Loss Coefficient
        sweep_config = format_wandb_config_param(sweep_config, "loss_coeff", all_values, all_ids, 'loss-coeff-hyperparam', 'value', agent)

        # Value Surrogate Clip
        sweep_config = format_wandb_config_param(sweep_config, "clip_range", all_values, all_ids, 'surrogate-clip-hyperparam', 'value', agent)

        # Value Grad Clip
        sweep_config = format_wandb_config_param(sweep_config, "grad_clip", all_values, all_ids, 'grad-clip-hyperparam', 'value', agent)

        # value output layer kernel initializers
        sweep_config = format_wandb_kernel(sweep_config, all_indexed_values, all_indexed_ids, 'value', agent, 'output')

        # Value optimizer
        sweep_config = format_wandb_config_param(sweep_config, "optimizer", all_values, all_ids, 'optimizer-hyperparam', 'value', agent)

        # Value optimizer options
        sweep_config = format_wandb_optimizer_options(sweep_config, "optimizer", all_values, all_ids, "value", agent)

        # Value learning rate scheduler
        sweep_config = format_wandb_lr_scheduler(sweep_config, all_values, all_ids, "value", agent)

        # Layer units
        # Policy layer units
        # sweep_config = format_wandb_layer_units(sweep_config, 'policy_units_layer', all_values, all_ids, 'hidden-layers-slider', 'policy', agent)

        # Value layer units
        # sweep_config = format_wandb_layer_units(sweep_config, 'value_units_layer', all_values, all_ids, 'hidden-layers-slider', 'value', agent)

        # Add save dir
        sweep_config = format_wandb_config_param(sweep_config, 'save_dir', all_values, all_ids, 'save-dir', 'none', agent)
        
        # Add training parameters
        # Timesteps
        sweep_config = format_wandb_config_param(sweep_config, 'num_timesteps', all_values, all_ids, 'num-timesteps', 'none', agent)
        # Trajectory length
        sweep_config = format_wandb_config_param(sweep_config, 'trajectory_length', all_values, all_ids, 'trajectory-length', 'none', agent)
        # Batch size
        sweep_config = format_wandb_config_param(sweep_config, "batch_size", all_values, all_ids, 'batch-size', 'none', agent)
        # Learning epochs
        sweep_config = format_wandb_config_param(sweep_config, "learning_epochs", all_values, all_ids, 'learning-epochs', 'none', agent)
        # Num evns
        sweep_config = format_wandb_config_param(sweep_config, "num_envs", all_values, all_ids, 'num-envs', 'none', agent)
        # Seed
        sweep_config = format_wandb_config_param(sweep_config, 'seed', all_values, all_ids, 'seed', 'none', agent)

                                
    # elif agent == "Reinforce" or agent == "ActorCritic":
    #     sweep_config["parameters"][agent]["parameters"] = {
    #         f"{agent}_learning_rate": {
    #             "max": 10**(get_specific_value(all_values, all_ids, 'learning-rate-slider', 'none', agent)[1]),
    #             "min": 10**(get_specific_value(all_values, all_ids, 'learning-rate-slider', 'none', agent)[0]),
    #             },
    #         f"{agent}_discount": {
    #             "max": get_specific_value(all_values, all_ids, 'discount-slider', 'none', agent)[1],
    #             "min": get_specific_value(all_values, all_ids, 'discount-slider', 'none', agent)[0],
    #             },
    #         f"{agent}_policy_num_layers": {
    #             "max": get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'policy', agent)[1],
    #             "min": get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'policy', agent)[0],
    #             },
    #         f"{agent}_policy_activation": {
    #             "values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'policy', agent)
    #             },
    #         f"{agent}_policy_optimizer": {
    #             "values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'policy', agent)
    #             },
    #         f"{agent}_value_num_layers": {
    #             "max": get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'value', agent)[1],
    #             "min": get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'value', agent)[0],
    #             },
    #         f"{agent}_value_activation": {
    #             "values": get_specific_value(all_values, all_ids, 'activation-function-hyperparam', 'value', agent)
    #             },
    #         f"{agent}_value_optimizer": {
    #             "values": get_specific_value(all_values, all_ids, 'optimizer-hyperparam', 'value', agent)
    #             },
    #         },

    #     if agent == "ActorCritic":
    #         sweep_config["parameters"][agent]["parameters"]["policy_trace_decay"] = {
    #             "max": get_specific_value(all_values, all_ids, 'trace-decay-hyperparam', 'policy', agent)[1],
    #             "min": get_specific_value(all_values, all_ids, 'trace-decay-hyperparam', 'policy', agent)[0],
    #         }
    #         sweep_config["parameters"][agent]["parameters"]["value_trace_decay"] = {
    #             "max": get_specific_value(all_values, all_ids, 'trace-decay-hyperparam', 'value', agent)[1],
    #             "min": get_specific_value(all_values, all_ids, 'trace-decay-hyperparam', 'value', agent)[0],
    #         }

    #     # add kernel options to sweep config
    #     # policy kernel options
    #     for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'policy', agent):
    #         if f"{agent}_policy_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"] = {}
    #         if kernel == "constant":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"]["parameters"] = {
    #                 "value": {
    #                     "max": get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'policy', agent)[0],
    #                 },
    #             }
    #         elif kernel == "variance_scaling":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"]["parameters"] = {
    #                 "scale": {
    #                     "max": get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'policy', agent)[0],
    #                 },
    #                 "mode": {
    #                     "values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'policy', agent),
    #                 },
    #                 "distribution": {
    #                     "values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'policy', agent),
    #                 },
    #             }
    #         elif kernel == "random_uniform":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"]["parameters"] = {
    #                 "maxval": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'policy', agent)[0],
    #                 },
    #                 "minval": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'policy', agent)[0],
    #                 },
    #             }
    #         elif kernel == "random_normal":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"]["parameters"] = {
    #                 "mean": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'policy', agent)[0],
    #                 },
    #                 "stddev": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'policy', agent)[0],
    #                 },
    #             }
    #         elif kernel == "truncated_normal":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_policy_kernel_{kernel}"]["parameters"] = {
    #                 "mean": {
    #                     "max": get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'policy', agent)[0],
    #                 },
    #                 "stddev": {
    #                     "max": get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'policy', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'policy', agent)[0],
    #                 },
    #             }
    #         else:
    #             if kernel not in ["constant", "glorot_uniform", "glorot_normal", "he_uniform", "he_normal", "zeros", "ones", \
    #                 "random_uniform", "random_normal", "truncated_normal", "variance_scaling"]:
    #                 raise ValueError(f"Unknown kernel: {kernel}")
            
    #     # value kernel options
    #     for kernel in get_specific_value(all_values, all_ids, 'kernel-function-hyperparam', 'value', agent):
    #         if f"{agent}_value_kernel_{kernel}" not in sweep_config["parameters"][agent]["parameters"]:
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"] = {}
    #         if kernel == "constant":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"]["parameters"] = {
    #                 "value": {
    #                     "max": get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'constant-value-hyperparam', 'value', agent)[0],
    #                 },
    #             }
    #         elif kernel == "variance_scaling":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"]["parameters"] = {
    #                 "scale": {
    #                     "max": get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'variance-scaling-scale-hyperparam', 'value', agent)[0],
    #                 },
    #                 "mode": {
    #                     "values": get_specific_value(all_values, all_ids, 'variance-scaling-mode-hyperparam', 'value', agent),
    #                 },
    #                 "distribution": {
    #                     "values": get_specific_value(all_values, all_ids, 'variance-scaling-distribution-hyperparam', 'value', agent),
    #                 },
    #             }
    #         elif kernel == "random_uniform":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"]["parameters"] = {
    #                 "maxval": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-uniform-maxval-hyperparam', 'value', agent)[0],
    #                 },
    #                 "minval": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-uniform-minval-hyperparam', 'value', agent)[0],
    #                 },
    #             }
    #         elif kernel == "random_normal":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"]["parameters"] = {
    #                 "mean": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-normal-mean-hyperparam', 'value', agent)[0],
    #                 },
    #                 "stddev": {
    #                     "max": get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'random-normal-stddev-hyperparam', 'value', agent)[0],
    #                 },
    #             }
    #         elif kernel == "truncated-normal":
    #             sweep_config["parameters"][agent]["parameters"][f"{agent}_value_kernel_{kernel}"]["parameters"] = {
    #                 "mean": {
    #                     "max": get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'truncated-normal-mean-hyperparam', 'value', agent)[0],
    #                 },
    #                 "stddev": {
    #                     "max": get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'value', agent)[1],
    #                     "min": get_specific_value(all_values, all_ids, 'truncated-normal-stddev-hyperparam', 'value', agent)[0],
    #                 },
    #             }
    #         else:
    #             raise ValueError(f"Unknown kernel: {kernel}")
        
    #     # add units per layer to sweep config
    #     for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'policy', agent)[1] + 1):
    #         sweep_config["parameters"][agent]["parameters"][f"policy_units_layer_{i}_{agent}"] = {
    #             "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'policy', agent) 
    #         }
    #     for i in range(1, get_specific_value(all_values, all_ids, 'hidden-layers-slider', 'value', agent)[1] + 1):
    #         sweep_config["parameters"][agent]["parameters"][f"value_units_layer_{i}_{agent}"] = {
    #             "values": get_specific_value(all_values, all_ids, f'layer-{i}-units-slider', 'value', agent)
    #         }
        
    ##DEBUG
    print(f'Sweep Config: {sweep_config}')
        
    return sweep_config

def update_heatmap(data):
    if data is not None:

        #DEBUG
        print(f'update heatmap data: {data}')
        # Convert the data to a numpy array
        data_np = np.array(data['matrix_data']['data'])

        # Format hyperparameter strings (remove model type if appears twice)
        columns = [wandb_support.parse_parameter_name(col) for col in data['matrix_data']['columns']]
        index = [wandb_support.parse_parameter_name(col) for col in data['matrix_data']['index']]

        # Create a mask to set the upper triangular part to NaN
        mask = np.triu(np.ones_like(data_np, dtype=bool), k=1)
        matrix_data_masked = np.where(mask, np.nan, data_np)

        # create stacked bar charts for legend of show hyperparameter bins
        stacked_bar_graph = go.Figure()

        for hp, hp_bin_ranges in data['bin_ranges'].items():
            bin_labels = [f"Bin {i}" for i in range(len(hp_bin_ranges)-1)]
            bin_ranges = [hp_bin_ranges[i+1] - hp_bin_ranges[i] for i in range(len(hp_bin_ranges)-1)]
        
            for i in range(len(bin_labels)):
                stacked_bar_graph.add_trace(go.Bar(
                    y=[hp],
                    x=[hp_bin_ranges[i]],
                    name=f"{hp} - {bin_labels[i]}",
                    orientation="h",
                    text=[bin_labels[i]],
                    textposition="inside",
                    insidetextanchor="middle",
                    hoverinfo="text",
                    hovertext=[f"{hp_bin_ranges[i]} - {hp_bin_ranges[i+1]}"]
                ))
        # create the layout of the stacked bar graph
        stacked_bar_graph.update_layout(
            title="Hyperparameter Bin Ranges",
            barmode="stack",
            yaxis=dict(title="Hyperparameter"),
            xaxis_title="Range",
            showlegend=False,
        )

        # Create the heatmap figure using Plotly
        heatmap = px.imshow(
            img=matrix_data_masked,
            labels={'x':'Hyperparameters','y':'Hyperparameters'},
            title='Co-occurrence Matrix',
            x=columns,
            y=index,
            color_continuous_scale='RdBu_r',
            text_auto=True,
            aspect="auto",
        )

        return heatmap, stacked_bar_graph
    else:
        return None
    
def render_heatmap(page):
    return html.Div([
        html.Label('Bins', style={'text-decoration': 'underline'}),
        dcc.Slider(
            id={'type':'bin-slider', 'page':page},
            min=1,
            max=10,
            value=5,
            marks={i: str(i) for i in range(1, 11)},
            step=1,
        ),
        html.Div(id={'type':'legend-container', 'page':page}),
        html.Label('Reward Threshold', style={'text-decoration': 'underline'}),
        dcc.Input(
            id={'type':'reward-threshold', 'page':page},
            type='number',
            value=0,
            style={'display':'inline-block'}
        ),
        dcc.Checklist(
            id={'type':'z-score-checkbox', 'page':page},
            options=[
                {'label': ' Display Z-Scores', 'value': 'zscore'},
            ],
            value=[],
            style={'display':'inline-block'}
        ),
        html.Div(id={'type':'heatmap-container', 'page':page}),
        html.Div(
            id={'type':'heatmap-placeholder', 'page':page},
            children=[
                html.P('The co-occurrence graph will load once enough data has been retrieved.'),
                html.Img(src='path/to/placeholder-image.png', alt='Placeholder Image')
            ],
            style={'display': 'none'}
        )
    ])

def create_sweep_options():
    return html.Div(
            [
                html.H4('Search Method'),
                dcc.RadioItems(
                    id='search-type',
                    options=[
                        {'label': 'Random Search', 'value': 'random'},
                        {'label': 'Grid Search', 'value': 'grid'},
                        {'label': 'Bayesian Search', 'value': 'bayes'},
                    ],
                    value='bayes',
                ),
                html.Div(
                    [
                        html.H6('Set Goal', style={'textAlign': 'left'}),
                        html.Div(
                            [
                                html.Div(
                                    [
                                        dcc.RadioItems(
                                            id='goal-type',
                                            options=[
                                                {'label': 'Maximize', 'value': 'maximize'},
                                                {'label': 'Minimize', 'value': 'minimize'},
                                            ],
                                        ),
                                    ],
                                    style={'display': 'flex'}
                                ),
                                html.Div(
                                    [
                                        dcc.RadioItems(
                                            id='goal-metric',
                                            options=[
                                                {'label': 'Episode Reward', 'value': 'episode_reward'},
                                                {'label': 'Value Loss', 'value': 'value_loss'},
                                                {'label': 'Policy Loss', 'value': 'policy_loss'},
                                            ],
                                        ),
                                    ],
                                    style={'display': 'flex'}
                                ),
                            ],
                            style={'display': 'flex'}
                        ),
                    ]
                ),
                dcc.Input(
                    id='sweep-name',
                    type='text',
                    placeholder='Sweep Name',
                ),
                dcc.Input(
                    id='num-sweeps',
                    type='number',
                    placeholder='Number of Sweeps',
                ),
            ]
        )

    
def create_agent_sweep_options(agent_type):
    """Returns Div of agent sweep options dependent on agent type

    Args:
        agent_type (str): agent selected in 'Agent Configuration' dropdown

    Returns:
        Div: Div object containing sweep options for agent_type selected
    """
    if agent_type == 'PPO':
        return create_ppo_sweep_options(agent_type)
    else:
        pass

def create_ppo_sweep_options(agent_type):
    """Returns Div of ppo agent sweep options

    Returns:
        Div: Div object of ppo agent sweep options
    """
    return html.Div(
                id='ppo-sweep-options',
                children=[
                    dcc.Input(
                        id={
                            'type':'num-timesteps',
                            'model':'none',
                            'agent': agent_type,
                        },
                        type='number',
                        placeholder='Total Timesteps',
                    ),
                    dcc.Dropdown(
                        id={
                            'type':'trajectory-length',
                            'model':'none',
                            'agent': agent_type,
                        },
                        options=[{'label':str(i), 'value':i} for i in [100,200,300,400,500,1000,2000,5000,10000]],
                        placeholder='Trajectory Length',
                    ),
                    dcc.Dropdown(
                        id={
                            'type':'batch-size',
                            'model':'none',
                            'agent': agent_type,
                        },
                        options=[{'label':str(i), 'value':i} for i in [32,64,128,256,512,1024,2048,5096,10192]],
                        placeholder='Learning Batch Sizes',
                    ),
                    dcc.Dropdown(
                        id={
                            'type':'learning-epochs',
                            'model':'none',
                            'agent': agent_type,
                        },
                        options=[{'label':str(i), 'value':i} for i in [1,2,4,8,12,16,20]],
                        placeholder='Learning Updates per Epoch',
                    ),
                    dcc.Dropdown(
                        id={
                            'type':'num-envs',
                            'model':'none',
                            'agent': agent_type,
                        },
                        options=[{'label':str(i), 'value':i} for i in [1,2,4,8,12,16,20]],
                        placeholder='Number of Envs',
                    ),
                    dcc.Input(
                        id={
                            'type':'seed',
                            'model':'none',
                            'agent': agent_type,
                        },
                        type='number',
                        placeholder='Random Seed',
                    ),
                ]
            ),

## TUNE FUNCTIONS ##
def create_tune_config(method, project, sweep_name, metric_name, metric_goal,
                       env_library, env, env_params, env_wrappers, agent,
                       all_values, all_ids, all_indexed_values, all_indexed_ids):
    # For Ray Tune, the "config" is usually just the hyperparameter search space.
    tune_config = {
        "env_library": env_library,
        "env_id": env,
    }
    # Add each environment parameter as a fixed value.
    for param, value in env_params.items():
        tune_config[f"env_{param}"] = value
    # For lists (like env_wrappers), you can either use the raw list or a search space (e.g., tune.choice)
    tune_config["env_wrappers"] = env_wrappers  # or: tune.choice(env_wrappers)
    tune_config["model_type"] = agent

    # Add agent-specific hyperparameters. Here we assume agent == "PPO".
    if agent == "PPO":
        # For each parameter, we use a helper that returns a Tune search space object.
        tune_config["policy_learning_rate_constant"] = format_tune_config_param(
            "learning_rate_constant", all_values, all_ids, "learning-rate-const-hyperparam", "policy", agent)
        tune_config["policy_learning_rate_exponent"] = format_tune_config_param(
            "learning_rate_exponent", all_values, all_ids, "learning-rate-exp-hyperparam", "policy", agent)
        tune_config["value_learning_rate_constant"] = format_tune_config_param(
            "learning_rate_constant", all_values, all_ids, "learning-rate-const-hyperparam", "value", agent)
        tune_config["value_learning_rate_exponent"] = format_tune_config_param(
            "learning_rate_exponent", all_values, all_ids, "learning-rate-exp-hyperparam", "value", agent)
        tune_config["distribution"] = format_tune_config_param(
            "distribution", all_values, all_ids, "distribution-hyperparam", "policy", agent)
        tune_config["discount"] = format_tune_config_param(
            "discount", all_values, all_ids, "discount-slider", "none", agent)
        tune_config["reward_clip"] = format_tune_config_param(
            "reward_clip", all_values, all_ids, "reward-clip-hyperparam", "none", agent)
        tune_config["advantage"] = format_tune_config_param(
            "advantage", all_values, all_ids, "advantage-coeff-hyperparam", "none", agent)
        tune_config["model_type_param"] = format_tune_config_param(
            "model_type", all_values, all_ids, "model-type-hyperparam", "policy", agent)
        tune_config["policy_clip_range"] = format_tune_config_param(
            "clip_range", all_values, all_ids, "surrogate-clip-hyperparam", "policy", agent)
        tune_config["policy_grad_clip"] = format_tune_config_param(
            "grad_clip", all_values, all_ids, "grad-clip-hyperparam", "policy", agent)
        tune_config["entropy"] = format_tune_config_param(
            "entropy", all_values, all_ids, "entropy-coeff-hyperparam", "none", agent)
        tune_config["kl"] = format_tune_config_param(
            "kl", all_values, all_ids, "kl-coeff-hyperparam", "none", agent)
        tune_config["normalize_advantage"] = format_tune_config_param(
            "normalize_advantage", all_values, all_ids, "normalize-advantage-hyperparam", "none", agent)
        tune_config["normalize_values"] = format_tune_config_param(
            "normalize_values", all_values, all_ids, "normalize-values-hyperparam", "value", agent)
        tune_config["normalize_values_clip"] = format_tune_config_param(
            "normalize_values_clip", all_values, all_ids, "norm-values-clip-hyperparam", "value", agent)
        tune_config["device"] = format_tune_config_param(
            "device", all_values, all_ids, "device", "none", agent)

        # Model architecture parameters.
        tune_config["policy_layers"] = format_tune_model_layers(
            all_values, all_ids, all_indexed_values, all_indexed_ids, "policy", agent)
        tune_config["policy_kernel_output"] = format_tune_kernel(
            all_indexed_values, all_indexed_ids, "policy", agent, "output")
        tune_config["policy_optimizer"] = format_tune_config_param(
            "optimizer", all_values, all_ids, "optimizer-hyperparam", "policy", agent)
        tune_config["policy_optimizer_options"] = format_tune_optimizer_options(
            "optimizer", all_values, all_ids, "policy", agent)
        tune_config["policy_lr_scheduler"] = format_tune_lr_scheduler(
            all_values, all_ids, "policy", agent)

        tune_config["value_layers"] = format_tune_model_layers(
            all_values, all_ids, all_indexed_values, all_indexed_ids, "value", agent)
        tune_config["loss_coeff"] = format_tune_config_param(
            "loss_coeff", all_values, all_ids, "loss-coeff-hyperparam", "value", agent)
        tune_config["value_clip_range"] = format_tune_config_param(
            "clip_range", all_values, all_ids, "surrogate-clip-hyperparam", "value", agent)
        tune_config["value_grad_clip"] = format_tune_config_param(
            "grad_clip", all_values, all_ids, "grad-clip-hyperparam", "value", agent)
        tune_config["value_kernel_output"] = format_tune_kernel(
            all_indexed_values, all_indexed_ids, "value", agent, "output")
        tune_config["value_optimizer"] = format_tune_config_param(
            "optimizer", all_values, all_ids, "optimizer-hyperparam", "value", agent)
        tune_config["value_optimizer_options"] = format_tune_optimizer_options(
            "optimizer", all_values, all_ids, "value", agent)
        tune_config["value_lr_scheduler"] = format_tune_lr_scheduler(
            all_values, all_ids, "value", agent)

        # Training parameters.
        tune_config["save_dir"] = format_tune_config_param(
            "save_dir", all_values, all_ids, "save-dir", "none", agent)
        tune_config["num_timesteps"] = format_tune_config_param(
            "num_timesteps", all_values, all_ids, "num-timesteps", "none", agent)
        tune_config["trajectory_length"] = format_tune_config_param(
            "trajectory_length", all_values, all_ids, "trajectory-length", "none", agent)
        tune_config["batch_size"] = format_tune_config_param(
            "batch_size", all_values, all_ids, "batch-size", "none", agent)
        tune_config["learning_epochs"] = format_tune_config_param(
            "learning_epochs", all_values, all_ids, "learning-epochs", "none", agent)
        tune_config["num_envs"] = format_tune_config_param(
            "num_envs", all_values, all_ids, "num-envs", "none", agent)
        tune_config["seed"] = format_tune_config_param(
            "seed", all_values, all_ids, "seed", "none", agent)
    return tune_config

def format_tune_config_param(config, param_name, all_values, all_ids, dash_id, model, agent, index=None, is_range=False):
    """
    For Ray Tune, returns a fixed value or search space object (e.g., tune.uniform or tune.choice)
    for a given parameter.
    """
    if index:
        value = get_specific_value_id(all_values, all_ids, dash_id, model, agent, index)
    else:
        value = get_specific_value(all_values, all_ids, dash_id, model, agent)
    
    if is_range:
        # If the two endpoints are equal, use the fixed value;
        # otherwise, use a uniform search space between min and max.
        if value[0] == value[1]:
            param = value[0]
        else:
            param = tune.uniform(value[0], value[1])
    else:
        # If value is a list, assume we want a choice over them.
        if isinstance(value, list):
            param = tune.choice(value)
        else:
            param = value

    if index:
        config[f"{agent}_{model}_{index}_{param_name}"] = param
    else:
        config[f"{agent}_{model}_{param_name}"] = param

    return config

def format_tune_optimizer_options(config, param_name, all_values, all_ids, model, agent):
    # Assume that config[f"{agent}_{model}_{param_name}"] is a list of optimizer names.
    for value in config[f"{agent}_{model}_{param_name}"]:
        if value == 'Adam':
            config[f"{agent}_{model}_{param_name}_{value}_weight_decay"] = tune.choice(
                get_specific_value(all_values, all_ids, 'adam-weight-decay-hyperparam', model, agent)
            )
        elif value == 'Adagrad':
            config[f"{agent}_{model}_{param_name}_{value}_weight_decay"] = tune.choice(
                get_specific_value(all_values, all_ids, 'adagrad-weight-decay-hyperparam', model, agent)
            )
            config[f"{agent}_{model}_{param_name}_{value}_lr_decay"] = tune.choice(
                get_specific_value(all_values, all_ids, 'adagrad-lr-decay-hyperparam', model, agent)
            )
        elif value == 'RMSprop':
            config[f"{agent}_{model}_{param_name}_{value}_weight_decay"] = tune.choice(
                get_specific_value(all_values, all_ids, 'rmsprop-weight-decay-hyperparam', model, agent)
            )
            config[f"{agent}_{model}_{param_name}_{value}_momentum"] = tune.choice(
                get_specific_value(all_values, all_ids, 'rmsprop-momentum-hyperparam', model, agent)
            )
        elif value == 'SGD':
            config[f"{agent}_{model}_{param_name}_{value}_weight_decay"] = tune.choice(
                get_specific_value(all_values, all_ids, 'sgd-weight-decay-hyperparam', model, agent)
            )
            config[f"{agent}_{model}_{param_name}_{value}_momentum"] = tune.choice(
                get_specific_value(all_values, all_ids, 'sgd-momentum-hyperparam', model, agent)
            )
    return config

def format_tune_kernel(config, all_indexed_values, all_indexed_ids, model, agent, layer):
    if isinstance(layer, int):
        layer_types = get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-type-hyperparam', model, agent, layer)
        for layer_type in layer_types:
            if layer_type in ['dense', 'conv2d']:
                config = format_tune_config_param(config, "kernel", all_indexed_values, all_indexed_ids,
                                                    'kernel-function-hyperparam', model, agent, layer)
                config = format_tune_kernel_options(config, all_indexed_values, all_indexed_ids, model, agent, layer)
    else:
        # For output layers, use index 'output'
        config = format_tune_config_param(config, "kernel", all_indexed_values, all_indexed_ids,
                                            "kernel-function-hyperparam", model, agent, 'output')
        config = format_tune_kernel_options(config, all_indexed_values, all_indexed_ids, model, agent, 'output')
    return config

def format_tune_kernel_options(config, all_values, all_ids, model, agent, layer_num):
    # For each kernel option, update config with additional parameters.
    for kernel in get_specific_value_id(all_values, all_ids, 'kernel-function-hyperparam', model, agent, layer_num):
        if kernel != 'default':
            if kernel == "constant":
                config = format_tune_config_param(config, "kernel", all_values, all_ids,
                                                    'constant-value-hyperparam', model, agent, layer_num)
            elif kernel == "variance_scaling":
                config[f"{agent}_{model}_{layer_num}_{kernel}_scale"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'variance-scaling-scale-hyperparam', model, agent, layer_num)
                )
                config[f"{agent}_{model}_{layer_num}_{kernel}_mode"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'variance-scaling-mode-hyperparam', model, agent, layer_num)
                )
                config[f"{agent}_{model}_{layer_num}_{kernel}_distribution"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'variance-scaling-distribution-hyperparam', model, agent, layer_num)
                )
            elif kernel == "uniform":
                config[f"{agent}_{model}_{layer_num}_{kernel}_maxval"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'random-uniform-maxval-hyperparam', model, agent, layer_num)
                )
                config[f"{agent}_{model}_{layer_num}_{kernel}_minval"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'random-uniform-minval-hyperparam', model, agent, layer_num)
                )
            elif kernel == "normal":
                config[f"{agent}_{model}_{layer_num}_{kernel}_mean"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'random-normal-mean-hyperparam', model, agent, layer_num)
                )
                config[f"{agent}_{model}_{layer_num}_{kernel}_stddev"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'random-normal-stddev-hyperparam', model, agent, layer_num)
                )
            elif kernel == "truncated_normal":
                config[f"{agent}_{model}_{layer_num}_{kernel}_mean"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'truncated-normal-mean-hyperparam', model, agent, layer_num)
                )
                config[f"{agent}_{model}_{layer_num}_{kernel}_stddev"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'truncated-normal-stddev-hyperparam', model, agent, layer_num)
                )
            elif kernel == "xavier_uniform":
                config[f"{agent}_{kernel}_{layer_num}_{model}_gain"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'xavier-uniform-gain-hyperparam', model, agent, layer_num)
                )
            elif kernel == "xavier_normal":
                config[f"{agent}_{model}_{layer_num}_{kernel}_gain"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'xavier-normal-gain-hyperparam', model, agent, layer_num)
                )
            elif kernel == "kaiming_uniform":
                config[f"{agent}_{model}_{layer_num}_{kernel}_mode"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'kaiming-uniform-mode-hyperparam', model, agent, layer_num)
                )
            elif kernel == "kaiming_normal":
                config[f"{agent}_{model}_{layer_num}_{kernel}_mode"] = tune.choice(
                    get_specific_value_id(all_values, all_ids, 'kaiming-normal-mode-hyperparam', model, agent, layer_num)
                )
            else:
                if kernel not in ["default", "constant", "xavier_uniform", "xavier_normal", "kaiming_uniform",
                                  "kaiming_normal", "zeros", "ones", "uniform", "normal", "truncated_normal", "variance_scaling"]:
                    raise ValueError(f"Unknown kernel: {kernel}")
    return config

def format_tune_lr_scheduler(config, all_values, all_ids, model, agent):
    schedulers = get_specific_value(all_values, all_ids, 'lr-scheduler-hyperparam', model, agent)
    for scheduler in schedulers:
        config = format_tune_config_param(config, "scheduler", all_values, all_ids,
                                          'lr-scheduler-hyperparam', model, agent)
        config = format_tune_lr_scheduler_options(config, all_values, all_ids, model, agent, scheduler)
    return config

def format_tune_lr_scheduler_options(config, all_values, all_ids, model, agent, scheduler):
    if scheduler == 'step':
        config = format_tune_steplr_options(config, all_values, all_ids, model, agent)
    elif scheduler == 'exponential':
        config = format_tune_exponentiallr_options(config, all_values, all_ids, model, agent)
    elif scheduler == 'cosineannealing':
        config = format_tune_cosineannealinglr_options(config, all_values, all_ids, model, agent)
    return config

def format_tune_steplr_options(config, all_values, all_ids, model, agent):
    config = format_tune_config_param(config, "step_size", all_values, all_ids,
                                      'lr-step-size-hyperparam', model, agent)
    config = format_tune_config_param(config, "gamma", all_values, all_ids,
                                      'lr-gamma-hyperparam', model, agent)
    return config

def format_tune_exponentiallr_options(config, all_values, all_ids, model, agent):
    config = format_tune_config_param(config, "step_size", all_values, all_ids,
                                      'lr-gamma-hyperparam', model, agent)
    return config

def format_tune_cosineannealinglr_options(config, all_values, all_ids, model, agent):
    config = format_tune_config_param(config, "step_size", all_values, all_ids,
                                      'lr-t-max-hyperparam', model, agent)
    config = format_tune_config_param(config, "step_size", all_values, all_ids,
                                      'lr-eta-min-hyperparam', model, agent)
    return config

def format_tune_model_layers(config, all_values, all_ids, all_indexed_values, all_indexed_ids, model, agent):
    num_layers = get_specific_value(all_values, all_ids, 'hidden-layers-slider', model, agent)[1]
    config = format_tune_config_param(config, 'num_layers', all_values, all_ids,
                                      'hidden-layers-slider', model, agent, index=None, is_range=True)
    for layer in range(1, num_layers + 1):
        layer_types = get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-type-hyperparam', model, agent, layer)
        config = format_tune_config_param(config, 'layer_types', all_indexed_values, all_indexed_ids,
                                          'layer-type-hyperparam', model, agent, layer)
        for layer_type in layer_types:
            if layer_type == 'dense':
                config = format_tune_config_param(config, 'num_units', all_indexed_values, all_indexed_ids,
                                                  'layer-units-slider', model, agent, layer)
                config = format_tune_config_param(config, 'bias', all_indexed_values, all_indexed_ids,
                                                  'dense-bias-hyperparam', model, agent, layer)
                config = format_tune_kernel(config, all_indexed_values, all_indexed_ids, model, agent, layer)
            elif layer_type == 'cnn':
                config = format_tune_config_param(config, 'out_channels', all_indexed_values, all_indexed_ids,
                                                  'layer-units-slider', model, agent, layer)
    return config

def format_tune_layer_units(config, param_name, all_values, all_ids, all_indexed_values, all_indexed_ids, id, model, agent):
    num_layers = get_specific_value(all_values, all_ids, id, model, agent)
    for i in range(1, num_layers[1] + 1):
        config[f"{agent}_{model}_layer_{i}_{param_name}"] = tune.choice(
            get_specific_value_id(all_indexed_values, all_indexed_ids, 'layer-units-slider', model, agent, i)
        )
    return config



## GYMNASIUM FUNCTIONS
def get_extra_gym_params(env_spec_id):
    extra_params = {}

    if env_spec_id == 'CarRacing-v2':
        extra_params = {
            'lap_complete_percent': {
                'type': 'float',
                'default': 0.95,
                'description': 'Percentage of tiles that must be visited by the agent before a lap is considered complete'
            },
            'domain_randomize': {
                'type': 'boolean',
                'default': False,
                'description': 'Enable domain randomization (background and track colors are different on every reset)'
            },
            'continuous': {
                'type': 'boolean',
                'default': True,
                'description': 'Use continuous action space (False for discrete action space)'
            }
        }

    elif env_spec_id == 'LunarLander-v2':
        extra_params = {
            'continuous': {
                'type': 'boolean',
                'default': False,
                'description': 'Use continuous actions'
            },
            'gravity': {
                'type': 'float',
                'default': -10.0,
                'description': 'Gravitational acceleration (negative number)'
            },
            'enable_wind': {
                'type': 'boolean',
                'default': False,
                'description': 'Add wind force to the lunar lander'
            },
            'wind_power': {
                'type': 'float',
                'default': 15.0,
                'description': 'Strength of wind force if enable_wind is True'
            },
            'turbulence_power': {
                'type': 'float',
                'default': 1.5,
                'description': 'Strength of turbulence if enable_wind is True'
            }
        }

    elif env_spec_id == 'BipedalWalker-v3':
        extra_params = {
            'hardcore': {
                'type': 'boolean',
                'default': False,
                'description': 'Use the hardcore version with obstacles'
            }
        }

    elif env_spec_id.startswith('FetchReach') or env_spec_id.startswith('FetchPush') or \
            env_spec_id.startswith('FetchSlide') or env_spec_id.startswith('FetchPickAndPlace'):
        extra_params = {
            'max_episode_steps': {
                'type': 'int',
                'default': 50,
                'description': "Number of steps per episode"
            }
        }

    elif env_spec_id.startswith('HandReach') or env_spec_id.startswith('HandManipulate'):
        extra_params = {
            'max_episode_steps': {
                'type': 'int',
                'default': 50,
                'description': "Number of steps per episode"
            }
        }

    elif env_spec_id == 'FrozenLake-v1':
        extra_params = {
            'map_name': {
                'type': 'string',
                'default': '4x4',
                'description': 'Name of the map layout'
            },
            'is_slippery': {
                'type': 'boolean',
                'default': True,
                'description': 'Whether the environment is slippery'
            }
        }

    return extra_params

def generate_gym_extra_params_container(env_spec_id):
    extra_params = get_extra_gym_params(env_spec_id)

    if not extra_params:
        return None

    params_container = html.Div([
        html.H4('Extra Parameters'),
        html.Div([
            generate_gym_param_input(param_name, param_info)
            for param_name, param_info in extra_params.items()
        ])
    ])

    return params_container


def generate_gym_param_input(param_name, param_info):
    param_type = param_info['type']
    param_default = param_info['default']
    param_description = param_info['description']

    if param_type == 'boolean':
        input_component = dcc.Dropdown(
            id=f'env-param-{param_name}',
            options=[
                {'label': 'True', 'value': True},
                {'label': 'False', 'value': False}
            ],
            value=param_default
        )
    elif param_type == 'float':
        input_component = dcc.Input(
            id=f'env-param-{param_name}',
            type='number',
            value=param_default
        )
    else:  # Assuming 'string' type
        input_component = dcc.Input(
            id=f'env-param-{param_name}',
            type='text',
            value=param_default
        )

    return html.Div([
        html.Label(param_name, style={'text-decoration': 'underline'}),
        html.Div(param_description, style={'fontSize': '12px', 'color': 'gray'}),
        input_component
    ], style={'marginBottom': '10px'})

def extract_gym_params(component):
    print('extract gym params called..')
    gym_params = {}

    if isinstance(component, dict):
        if component['type'] == 'Label' and 'children' in component['props']:
            param_name = component['props']['children']
            gym_params[param_name] = None  # Initialize parameter value as None

        if 'children' in component['props']:
            children = component['props']['children']
            if isinstance(children, list):
                for child in children:
                    child_params = extract_gym_params(child)
                    gym_params.update(child_params)
            elif isinstance(children, dict):
                child_params = extract_gym_params(children)
                gym_params.update(child_params)

        if component['type'] in ['Input', 'Dropdown', 'Slider'] and 'id' in component['props']:
            param_id = component['props']['id']
            if param_id.startswith('env-param-'):
                param_name = param_id.replace('env-param-', '')
                param_value = component['props'].get('value')
                gym_params[param_name] = param_value

    return gym_params
```

---

## dashboard.py

```python
import json
import logging
import multiprocessing
import dash
from dash import dcc, html, Input, Output, callback
import dash_bootstrap_components as dbc
import ale_py


import layouts
import dash_callbacks
import dash_utils
import gymnasium as gym
import gymnasium_robotics as gym_robo



app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY], suppress_callback_exceptions=True)

log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)  # Suppress INFO logs; show only errors

navbar = html.Div(
    [
        dbc.NavLink("Home", href="/", className="nav-link"),
        dbc.NavLink("Build Agent", href="/build-agent", className="nav-link"),
        dbc.NavLink("Train Agent", href="/train-agent", className="nav-link"),
        dbc.NavLink("Test Agent", href="/test-agent", className="nav-link"),
        dbc.NavLink("Hyperparameter Search", href="/hyperparameter-search", className="nav-link"),
        dbc.NavLink("Co-Occurrence Analysis", href="/co-occurrence-analysis", className="nav-link"),
        dbc.NavLink("WandB Utils", href="/wandb-utils", className="nav-link"),
    ],
    className="navbar",
)

banner = html.Div([
    html.Div([
        # html.Img(src='/assets/banner_edit.png', className='banner_img'),
        # html.Div("Phoenix AI", className="header-title"),
    ], className="banner"),
])

app.layout = dbc.Container(
    [
        dcc.Location(id='url', refresh=False),
        banner,
        navbar,
        html.Div(id="page-content"),
        # dcc.Store(id='task-store')
    ],
    fluid=True,
)
if __name__ == "__main__":
    # Register Gym Robotics envs
    gym_robo.register_robotics_envs()
    # Register Ale Atari envs
    gym.register_envs(ale_py)
    # Create a multiprocessing manager
    manager = multiprocessing.Manager()
    # Create a shared dictionary to store data between processes
    shared_data = manager.dict()

    # Pass the shared data to the register_callbacks function
    dash_callbacks.register_callbacks(app, shared_data)
    app.run(debug=False, dev_tools_ui=False, dev_tools_props_check=False)
```

---

## ddpg_train_mpi.py

```python
import sys
import json
import logging

from rl_agents import DDPG

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def train_agent(config):
    try:
        agent_type = config['agent_type']
        load_weights = config['load_weights']
        num_episodes = config['num_episodes']
        render = config['render']
        render_freq = config['render_freq']
        save_dir = config['save_dir']

        assert agent_type == 'DDPG', f"Unsupported agent type: {agent_type}"

        if agent_type:
            agent = DDPG.load(config, load_weights)
            agent.train(num_episodes, render, render_freq, save_dir)

    except KeyError as e:
        logging.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logging.error(str(e))
        raise

    except Exception as e:
        logging.exception("An unexpected error occurred during training")
        raise

if __name__ == '__main__':
    if len(sys.argv) > 1:
        config_path = sys.argv[1]

        try:
            with open(config_path, 'r', encoding="utf-8") as f:
                config = json.load(f)

            train_agent(config)

        except FileNotFoundError:
            logging.error(f"Configuration file not found: {config_path}")

        except json.JSONDecodeError:
            logging.error(f"Invalid JSON format in configuration file: {config_path}")

    else:
        logging.error("Configuration file path not provided.")
```

---

## encoder.py

```python
import json
import gymnasium as gym
from gymnasium.envs.registration import EnvSpec, WrapperSpec

class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        #DEBUG
        print(f'custom encoder obj: {obj}')
        if isinstance(obj, EnvSpec):
            return serialize_env_spec(obj)
        if isinstance(obj, WrapperSpec):
            return wrapper_to_dict(obj)
        if callable(obj):
            return str(obj)  # Convert functions, including lambdas, to strings

        # Let the base class default method raise the TypeError for unknown types
        return json.JSONEncoder.default(self, obj)

def wrapper_to_dict(wrapper_spec):
    if isinstance(wrapper_spec, WrapperSpec):
        # Convert WrapperSpec to a dictionary dynamically
        wrapper_dict = {}
        for attr in dir(wrapper_spec):
            if not attr.startswith('__') and not callable(getattr(wrapper_spec, attr)):
                wrapper_dict[attr] = getattr(wrapper_spec, attr)
            elif callable(getattr(wrapper_spec, attr)):
                wrapper_dict[attr] = str(getattr(wrapper_spec, attr))  # Convert callable to string
        return wrapper_dict
    return str(wrapper_spec)

def serialize_env_spec(env_spec):
    """Extracts and serializes the relevant parts of the environment specification."""
    env_spec_dict = {
        "id": env_spec.id,
        "entry_point": env_spec.entry_point,
        "reward_threshold": env_spec.reward_threshold,
        "nondeterministic": env_spec.nondeterministic,
        "max_episode_steps": env_spec.max_episode_steps,
        "order_enforce": env_spec.order_enforce,
        "disable_env_checker": env_spec.disable_env_checker,
        "kwargs": env_spec.kwargs,
        # "additional_wrappers": [wrapper_to_dict(wrapper) for wrapper in env_spec.additional_wrappers],
        "additional_wrappers": [],
        "vector_entry_point": env_spec.vector_entry_point,
    }
    return env_spec_dict

```

---

## env_wrapper.py

```python
import json
from abc import ABC, abstractmethod
import numpy as np
import gymnasium as gym
from gymnasium.envs.registration import EnvSpec
from gymnasium.wrappers import *
from gymnasium.vector import VectorEnv, SyncVectorEnv

WRAPPER_REGISTRY = {
    "AtariPreprocessing": {
        "cls": AtariPreprocessing,
        "default_params": {
            "frame_skip": 1,
            "grayscale_obs": True,
            "scale_obs": True
        }
    },
    "TimeLimit": {
        "cls": TimeLimit,
        "default_params": {
            "max_episode_steps": 1000
        }
    },
    "TimeAwareObservation": {
        "cls": TimeAwareObservation,
        "default_params": {
            "flatten": False,
            "normalize_time": False
        }
    },
    "FrameStackObservation": {
        "cls": FrameStackObservation,
        "default_params": {
            "stack_size": 4
        }
    },
    "ResizeObservation": {
        "cls": ResizeObservation,
        "default_params": {
            "shape": 84
        }
    }
}

# def atari_wrappers(env):
#     """
#     Wrap an Atari environment with preprocessing and frame stacking.

#     This function applies standard Atari preprocessing, including converting to grayscale,
#     resizing, scaling, and stacking multiple consecutive frames for better temporal
#     context.

#     Args:
#         env (gym.Env): The original Atari environment.

#     Returns:
#         gym.Env: The wrapped environment with preprocessing and frame stacking applied.
#     """
#     env = AtariPreprocessing(
#         env,
#         frame_skip=1,
#         grayscale_obs=True,
#         scale_obs=True,
#         screen_size=84
#     )
#     env = FrameStackObservation(env, stack_size=4)
#     return env

def wrap_env(vec_env, wrappers):
    wrapper_list = []
    for wrapper in wrappers:
        if wrapper['type'] in WRAPPER_REGISTRY:
            # print(f'wrapper type:{wrapper["type"]}')
            # Use a copy of default_params to avoid modifying the registry
            default_params = WRAPPER_REGISTRY[wrapper['type']]["default_params"].copy()
            
            if wrapper['type'] == "ResizeObservation":
                # Ensure shape is a tuple for ResizeObservation
                default_params['shape'] = (default_params['shape'], default_params['shape']) if isinstance(default_params['shape'], int) else default_params['shape']
            
            # print(f'default params:{default_params}')
            override_params = wrapper.get("params", {})
            
            if wrapper['type'] == "ResizeObservation":
                # Ensure override_params shape is a tuple
                if 'shape' in override_params:
                    override_params['shape'] = (override_params['shape'], override_params['shape']) if isinstance(override_params['shape'], int) else override_params['shape']
            
            # print(f'override params:{override_params}')
            final_params = {**default_params, **override_params}
            # print(f'final params:{final_params}')
            
            def wrapper_factory(env, cls=WRAPPER_REGISTRY[wrapper['type']]["cls"], params=final_params):
                return cls(env, **params)
            
            wrapper_list.append(wrapper_factory)
    
    # Define apply_wrappers outside the loop
    def apply_wrappers(env):
        for wrapper in wrapper_list:
            env = wrapper(env)
            # print(f'length of obs space:{len(env.observation_space.shape)}')
            # print(f'env obs space shape:{env.observation_space.shape}')
        return env
    
    # print(f'wrapper list:{wrapper_list}')
    envs = [lambda: apply_wrappers(gym.make(vec_env.spec.id, render_mode="rgb_array")) for _ in range(vec_env.num_envs)]    
    return SyncVectorEnv(envs)

class EnvWrapper(ABC):
    """
    Abstract base class for environment wrappers.

    This class defines the required interface for custom environment wrappers.
    """

    @abstractmethod
    def reset(self):
        """
        Reset the environment to an initial state.

        Returns:
            Any: Initial observation of the environment.
        """
        pass
    
    @abstractmethod
    def step(self, action):
        """
        Take an action in the environment.

        Args:
            action: The action to be taken.

        Returns:
            Tuple: Observation, reward, done flag, and additional info.
        """
        pass
    
    @abstractmethod
    def render(self, mode="rgb_array"):
        """
        Render the environment.

        Args:
            mode (str): The render mode (default: "rgb_array").

        Returns:
            Any: Rendered frame or visualization.
        """
        pass

    @abstractmethod
    def _initialize_env(self, render_freq: int = 0, num_envs: int = 1, seed: int = None):
        """
        Initialize the environment with optional rendering and seeding.

        Args:
            render_freq (int): Frequency of rendering (default: 0).
            num_envs (int): Number of parallel environments (default: 1).
            seed (int): Random seed for the environment (default: None).

        Returns:
            Any: The initialized environment.
        """
        pass
    
    @property
    @abstractmethod
    def observation_space(self):
        """
        Get the observation space of the environment.

        Returns:
            gym.Space: The observation space.
        """
        pass
    
    @property
    @abstractmethod
    def action_space(self):
        """
        Get the action space of the environment.

        Returns:
            gym.Space: The action space.
        """
        pass

    @abstractmethod
    def to_json(self) -> str:
        """
        Serialize the environment wrapper configuration to JSON.

        Returns:
            str: JSON string representing the environment configuration.
        """
        pass

    @classmethod
    def from_json(cls, json_string: str):
        """
        Create an environment wrapper instance from a JSON string.

        This method will delegate to the appropriate subclass's `from_json` method
        based on the type specified in the JSON.

        Args:
            json_string (str): JSON string representing the environment configuration.

        Returns:
            EnvWrapper: A new environment wrapper instance.

        Raises:
            ValueError: If the type in the JSON is not recognized or if instantiation fails.
        """
        config = json.loads(json_string)
        try:
            if config['type'] == 'GymnasiumWrapper':
                return GymnasiumWrapper.from_json(json_string)
            # Add more conditions here for other subclasses if they exist
            else:
                raise ValueError(f"Unknown environment wrapper type: {config['type']}")
        except KeyError as e:
            raise ValueError(f"Missing 'type' key in JSON configuration: {e}")
        except Exception as e:
            raise ValueError(f"Failed to instantiate environment from JSON: {e}")


class GymnasiumWrapper(EnvWrapper):
    """
    Wrapper for Gymnasium environments with additional utilities.

    This wrapper supports initialization, resetting, stepping, rendering,
    and JSON-based serialization of Gymnasium environments.
    """
    def __init__(self, env_spec: EnvSpec, wrappers: list[dict] = None):
        self.env_spec = env_spec
        self.wrappers = wrappers
        self.env = self._initialize_env()

    def _initialize_env(self, render_freq: int = 0, num_envs: int = 1, seed: int = None):
        """
        Initialize the Gymnasium environment with unique seeds for each environment.

        Args:
            render_freq (int): Frequency of rendering (default: 0).
            num_envs (int): Number of parallel environments (default: 1).
            seed (int): Base random seed for the environment (default: None).

        Returns:
            gym.Env: The initialized Gymnasium environment.
        """
        self.seed = seed
        if self.seed is None:
            seeds = [None] * num_envs
        else:
            seeds = [self.seed + i for i in range(num_envs)]  # Create different seeds for each environment
        
        # Create a list of environment factories, each with its unique seed
        env_fns = []
        for i in range(num_envs):
            def make_env(i=i):  # Use default argument to capture i
                env = gym.make(self.env_spec.id, render_mode="rgb_array" if render_freq > 0 else None)
                if seeds[i] is not None:
                    env.reset(seed=seeds[i])  # Set seed for each environment
                    env.action_space.seed(seeds[i])  # Also seed the action space
                if self.wrappers:
                    for wrapper in self.wrappers:
                        if wrapper['type'] in WRAPPER_REGISTRY:
                            default_params = WRAPPER_REGISTRY[wrapper['type']]["default_params"].copy()
                            override_params = wrapper.get("params", {})
                            final_params = {**default_params, **override_params}
                            env = WRAPPER_REGISTRY[wrapper['type']]["cls"](env, **final_params)
                return env
            
            env_fns.append(make_env)

        vec_env = SyncVectorEnv(env_fns)

        return vec_env

    def reset(self):
        """
        Reset the environment.

        Returns:
            Any: Initial observation of the environment.
        """
        if self.seed is not None:
            return self.env.reset(seed=self.seed)
        return self.env.reset()
    
    def step(self, action):
        """
        Take an action in the environment.

        Args:
            action: The action to be taken.

        Returns:
            Tuple: Observation, reward, done flag, and additional info.
        """
        return self.env.step(action)
    
    def render(self, mode="rgb_array"):
        """
        Render the environment.

        Args:
            mode (str): The render mode (default: "rgb_array").

        Returns:
            Any: Rendered frame or visualization.
        """
        return self.env.render(mode=mode)
    
    def format_actions(self, actions: np.ndarray, testing=False):
        if isinstance(self.action_space, gym.spaces.Box):
            if testing:
                num_envs = 1
            else:
                num_envs = self.env.num_envs
            num_actions = self.action_space.shape[-1]
            return actions.reshape(num_envs, num_actions)
        if isinstance(self.action_space, gym.spaces.Discrete) or isinstance(self.action_space, gym.spaces.MultiDiscrete):
            return actions.ravel()
        
    def get_base_env(self, env_idx:int=0):
        """Recursively unwrap an environment to get the base environment."""
        env = self.env.envs[env_idx]
        while hasattr(env, 'env'):
            env = env.env
        return env
    
    def close(self):
        """
        Close the environment.
        """
        self.env.close()
    
    @property
    def observation_space(self):
        """
        Get the observation space of the environment.

        Returns:
            gym.Space: The observation space.
        """
        return self.env.observation_space
    
    @property
    def action_space(self):
        """
        Get the action space of the environment.

        Returns:
            gym.Space: The action space.
        """
        return self.env.action_space
    
    @property
    def single_action_space(self):
        """
        Get the single action space for vectorized environments.

        Returns:
            gym.Space: The single action space.
        """
        return self.env.single_action_space

    @property
    def single_observation_space(self):
        """
        Get the single observation space for vectorized environments.

        Returns:
            gym.Space: The single observation space.
        """
        return self.env.single_observation_space
    
    @property
    def config(self):
        """
        Get the configuration of the wrapper.

        Returns:
            dict: Configuration dictionary.
        """
        return {
            "type": self.__class__.__name__,
            "env": self.env_spec.to_json(),
            "wrappers": self.wrappers,
        }
    
    def to_json(self):
        """
        Serialize the wrapper configuration to JSON.

        Returns:
            str: JSON string representing the configuration.
        """
        return json.dumps(self.config)

    @classmethod
    def from_json(cls, json_env_spec):
        """
        Create a Gymnasium wrapper instance from a JSON string.

        Args:
            json_env_spec (str): JSON string representing the configuration.

        Returns:
            GymnasiumWrapper: A new Gymnasium wrapper instance.
        """
        #DEBUG
        # print('GymnasiumWrapper from_json called')
        # print(f'from json env spec:{json_env_spec}, type:{type(json_env_spec)}')
        config = json.loads(json_env_spec)
        #DEBUG
        # print(f'from json config:{config}, type:{type(config)}')
        env_spec = EnvSpec.from_json(config['env'])
        #DEBUG
        # print(f'wrappers in gym from json:{config["wrappers"]}')
        try:
            return cls(env_spec, config["wrappers"])
        except Exception as e:
            raise ValueError(f"Environment wrapper error: {config}, {e}")
    
class IsaacSimWrapper(EnvWrapper):
    def __init__(self, env_spec):
        """
        Placeholder wrapper for Isaac Sim environments.

        This class is a template and needs implementation based on Isaac Sim's API.
        """
        pass
```

---

## gym_helper.py

```python
import gymnasium as gym
import numpy as np

## HER/PER FUNCTIONALITY ##

def get_goal_envs():
    """Returns a list of envs that use goal states"""

    envs = []

def get_her_goal_functions(env:gym.Env):
    """Returns a list of goal functions for the HER algorithm."""

    spec_id = env.spec.id
    funcs = {
        'CarRacing-v2':{
            'desired_goal': car_racing_desired_goal,
            'achieved_goal': car_racing_achieved_goal,
            'reward': car_racing_reward
        },

        'Reacher-v4':{
            'desired_goal': reacher_desired_goal,
            'achieved_goal': reacher_achieved_goal,
            'reward': reacher_reward
        },

        # 'Pusher-v5':{ 
        #     'desired_goal': pusher_desired_goal,
        #     'achieved_goal': pusher_achieved_goal,
        #     'reward': pusher_reward
        # }

        'FetchReach-v2':{
            'desired_goal': fetch_reach_desired_goal,
            'achieved_goal': fetch_reach_achieved_goal,
            'reward': fetch_reach_reward
        },

        'FetchPickAndPlace-v2':{
            'desired_goal': fetch_pick_place_desired_goal,
            'achieved_goal': fetch_pick_place_achieved_goal,
            'reward': fetch_pick_place_reward
        },

        'FetchPush-v2':{
            'desired_goal': fetch_push_desired_goal,
            'achieved_goal': fetch_push_achieved_goal,
            'reward': fetch_push_reward
        },

        'FetchSlide-v2':{
            'desired_goal': fetch_slide_desired_goal,
            'achieved_goal': fetch_slide_achieved_goal,
            'reward': fetch_slide_reward
        },
    }

    return funcs[spec_id].values()

 
def car_racing_desired_goal(env):
    """Returns the desired goal for the CarRacing environment."""
    return np.array([len(env.get_wrapper_attr('track'))])

def car_racing_achieved_goal(env):
    """Returns the achieved goal for the CarRacing environment."""
    return np.array([env.get_wrapper_attr('tile_visited_count')])

def car_racing_reward(env, action, state_achieved_goal, next_state_achieved_goal, desired_goal, tolerance):
    diff = desired_goal - next_state_achieved_goal
    # if diff <= tolerance:
        # print('within tolerance')
    if diff <= tolerance:
        return 0,1 
    else:
        return -1,0

def reacher_desired_goal(env):
    """Returns the desired goal for the Reacher Mujoco environment."""
    return np.array([0.0, 0.0, 0.0])

def reacher_achieved_goal(env):
    return env.get_wrapper_attr("_get_obs")()[8::]

def reacher_reward(env, action, state_achieved_goal, next_state_achieved_goal, desired_goal, tolerance):
    distance = np.linalg.norm(desired_goal - next_state_achieved_goal)
    if distance <= tolerance:
        return 0,1
    else:
        return -1,0

# def pusher_desired_goal(env):
#     return env.get_wrapper_attr("get_body_com")("goal")

# def pusher_achieved_goal(env):
#     return env.get_wrapper_attr("get_body_com")("object")

# def pusher_reward(env, action, state_achieved_goal, next_state_achieved_goal, desired_goal, tolerance):
#     goal_distance = np.linalg.norm(next_state_achieved_goal - desired_goal)
#     arm_distance = np.linalg.norm(next_state_achieved_goal - env.get_wrapper_attr("get_body_com")("tips_arm"))
#     # add distances
#     total_distance = goal_distance + arm_distance
#     # print(f'distance = {total_distance}')
#     if total_distance <= tolerance:
#         # print(f'distance within in tolerance')
#         return 0,1
#     else:
#         return -1,0

def fetch_reach_desired_goal(env):
    return env.get_wrapper_attr("_get_obs")()['desired_goal']

def fetch_reach_achieved_goal(env):
    return env.get_wrapper_attr("_get_obs")()['achieved_goal']

def fetch_reach_reward(env, action=None, state_achieved_goal=None, next_state_achieved_goal=None, desired_goal=None, tolerance=None):
    
    distance = np.linalg.norm(next_state_achieved_goal - desired_goal, axis=-1)
    reward = env.get_wrapper_attr("compute_reward")(next_state_achieved_goal, desired_goal, None)

    if distance <= tolerance:
        return reward, 1
    else:
        return reward, 0
    
def fetch_pick_place_desired_goal(env):
    return env.get_wrapper_attr("_get_obs")()['desired_goal']

def fetch_pick_place_achieved_goal(env):
    return env.get_wrapper_attr("_get_obs")()['achieved_goal']

def fetch_pick_place_reward(env, action=None, state_achieved_goal=None, next_state_achieved_goal=None, desired_goal=None, tolerance=None):
    
    distance = np.linalg.norm(next_state_achieved_goal - desired_goal, axis=-1)
    reward = env.get_wrapper_attr("compute_reward")(next_state_achieved_goal, desired_goal, None)

    if distance <= tolerance:
        return reward, 1
    else:
        return reward, 0

def fetch_push_desired_goal(env):
    return env.get_wrapper_attr("_get_obs")()['desired_goal']

def fetch_push_achieved_goal(env):
    return env.get_wrapper_attr("_get_obs")()['achieved_goal']

def fetch_push_reward(env, action=None, state_achieved_goal=None, next_state_achieved_goal=None, desired_goal=None, tolerance=None):
    
    distance = np.linalg.norm(next_state_achieved_goal - desired_goal, axis=-1)
    # print(f'distance: {distance}')
    reward = env.get_wrapper_attr("compute_reward")(next_state_achieved_goal, desired_goal, None)

    if distance <= tolerance:
        return reward, 1
    else:
        return reward, 0

def fetch_slide_desired_goal(env):
    return env.get_wrapper_attr("_get_obs")()['desired_goal']

def fetch_slide_achieved_goal(env):
    return env.get_wrapper_attr("_get_obs")()['achieved_goal']

def fetch_slide_reward(env, action=None, state_achieved_goal=None, next_state_achieved_goal=None, desired_goal=None, tolerance=None):
    
    #DEBUG
    # print(f'next state achieved goal: {next_state_achieved_goal}')
    # print(f'desired goal: {desired_goal}')
    distance = np.linalg.norm(next_state_achieved_goal - desired_goal, axis=-1)
    reward = env.get_wrapper_attr("compute_reward")(next_state_achieved_goal, desired_goal, None)
    #DEBUG
    # print(f'distance: {distance}')
    # print(f'reward: {reward}')

    if distance <= tolerance:
        return reward, 1
    else:
        return reward, 0
```

---

## her_train_1.py

```python
import os
import random
import numpy as np
import torch as T
# from mpi4py import MPI


import gymnasium as gym
import gymnasium_robotics as gym_robo
from rl_agents import HER, DDPG
from models import ActorModel, CriticModel
from helper import NormalNoise, MPIHelper
from gym_helper import get_her_goal_functions
from rl_callbacks import WandbCallback
import wandb

def main():

    mpi_helper = MPIHelper()

    if mpi_helper.is_main_process():
        # Login to wandb
        wandb.login(key=)
    
    # Set device
    device = 'cpu'

    # Create gym environment
    env_id = 'FetchReach-v2'
    env = gym.make(env_id)

    # Get reward functions for environment
    desired_goal_func, achieved_goal_func, reward_func = get_her_goal_functions(env)

    # Get goal shape (requires gym.reset() to initialize env state)
    _,_ = env.reset()
    goal_shape = desired_goal_func(env).shape

    # Build actor model
    dense_layers = [
        (
            64,
            "relu",
            {
                "default": {
                }
            },
        ),
        (
            64,
            "relu",
            {
                "default": {\
                }
            },
        ),
        (
            64,
            "relu",
            {
                "default": {
                }
            },
        )
    ]

    actor = ActorModel(env, cnn_model=None, dense_layers=dense_layers, output_layer_kernel='default', goal_shape=goal_shape,
                       optimizer='Adam', optimizer_params={'weight_decay':0.0},
                       learning_rate=0.001, normalize_layers=False, clamp_output=None, device=device)

    # build critic
    state_layers = [
        (
            64,
            "relu",
            {
                "default": {
                }
            },
        ),
    ]

    merged_layers = [
        (
            64,
            "relu",
            {
                "default": {
                }
            },
        ),
        (
            64,
            "relu",
            {
                "default": {
                }
            },
        ),
    ]

    critic = CriticModel(env=env, cnn_model=None, state_layers=state_layers,
                         merged_layers=merged_layers, output_layer_kernel='default', goal_shape=goal_shape,
                         optimizer="Adam", optimizer_params={'weight_decay':0.0},
                         learning_rate=0.001, normalize_layers=False, device=device)

    # Instantiate replay buffer
    # replay_buffer = ReplayBuffer(env, 100000, goal_shape)
    # Instantiate noise object
    noise = NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05, device=device)

    # Create DDPG agent
    ddpg_agent = DDPG(env=env,
                      actor_model=actor,
                      critic_model=critic,
                      discount=0.98,
                      tau=0.05,
                      action_epsilon=0.2,
                      batch_size=128,
                      noise=noise,
                      callbacks=[WandbCallback("FetchReach-v2")],
                    #   save_dir="fetch_reach_v2_a/models/ddpg/"
                      )

    # Instantiate HER object
    her = HER(agent=ddpg_agent,
              strategy='future',
              tolerance=0.05,
              num_goals=4,
              desired_goal=desired_goal_func,
              achieved_goal=achieved_goal_func,
              reward_fn=reward_func,
              normalizer_clip=5.0,
              device=device, # param not used
              save_dir="fetch_reach_v2_a/models/her/"
            )
    
    # if hasattr(her.agent.env, "distance_threshold"):
    print(f'distance threshold: {her.agent.env.get_wrapper_attr("distance_threshold")}')

    # Run the training process
    her.train(num_epochs=10,
              num_cycles=50,
              num_episodes=10,
              num_updates=40,
              render=True,
              render_freq=500)

if __name__ == '__main__':
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['IN_MPI'] = '1'
    main()
```

---

## init_sweep.py

```python
import argparse
# import logging
from logging_config import logger
import json
import wandb
from mpi4py import MPI
import mpi_helper

from rl_agents import init_sweep

parser = argparse.ArgumentParser(description='Sweep MPI')
parser.add_argument('--sweep_config', type=str, required=True, help='Path to sweep_config.json to load agent')
parser.add_argument('--train_config', type=str, required=True, help='Path to train_config.json to set training params')

args = parser.parse_args()

sweep_config_path = args.sweep_config
train_config_path = args.train_config

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def main(sweep_config, train_config):
    logger.debug("init_sweep main fired")

    if train_config["use_mpi"]:
        size = MPI.COMM_WORLD.Get_size()
        rank = MPI.COMM_WORLD.Get_rank()

        num_agents = train_config['num_agents']
        assert size % num_agents == 0, "Number of workers must be divisible by number of agents."

        # group_size = size // num_agents
        group_size = mpi_helper.set_group_size(MPI.COMM_WORLD, num_agents)
        # group = rank // group_size
        group = mpi_helper.set_group(MPI.COMM_WORLD, group_size)

        if num_agents > 1:
            comm = MPI.COMM_WORLD.Split(color=group, key=rank)
            comm.Set_name(f"Group_{group}")
            new_rank = comm.Get_rank()

            logger.debug(f"Global rank {rank} assigned to {comm.Get_name()} with new rank {new_rank}")
        
        else:
            comm = MPI.COMM_WORLD
            new_rank = rank
            comm.Set_name(f"Group_{group}")
            
            logger.debug(f"Global rank {rank} assigned to {comm.Get_name()}")

        if rank == 0:
            try:
                sweep_id = wandb.sweep(sweep_config, project=sweep_config["project"])
                logger.debug(f"Sweep ID: {sweep_id}")
            except Exception as e:
                logger.error(f"error creating sweep ID: {e}", exc_info=True)
                sweep_id = None
        else:
            sweep_id = None

        # Broadcast the sweep ID from rank 0 of COMM_WORLD to all other ranks
        sweep_id = MPI.COMM_WORLD.bcast(sweep_id, root=0)
        logger.debug(f"Rank {rank} received Sweep ID: {sweep_id}")

        if new_rank == 0:
            try:
                wandb.agent(
                    sweep_id,
                    function=lambda: init_sweep(sweep_config, comm),
                    count=train_config['num_sweeps'],
                    project=sweep_config["project"],
                )
            except Exception as e:
                logger.error(f"error in init_sweep.py main process: {e}", exc_info=True)
        else:
            try:
                for _ in range(train_config['num_sweeps']):
                    init_sweep(sweep_config, train_config, comm)
            except Exception as e:
                logger.error(f"error in init_sweep.py main process: {e}", exc_info=True)
    else:
        try:
            sweep_id = wandb.sweep(sweep_config, project=sweep_config["project"])
            logger.debug(f"Sweep ID: {sweep_id}")
            wandb.agent(
                sweep_id,
                function=lambda: init_sweep(sweep_config, train_config),
                count=train_config['num_sweeps'],
                project=sweep_config["project"],
            )
        except Exception as e:
            logger.error(f"error in init_sweep.py main process: {e}", exc_info=True)


if __name__ == "__main__":
        
        try:
            logger.debug("init_sweep.py fired")
            sweep_config = load_config(sweep_config_path)
            logger.debug("sweep config loaded")
            train_config = load_config(train_config_path)
            logger.debug("train config loaded")
            main(sweep_config, train_config)
            MPI.Finalize()
        except Exception as e:
              logger.error(f"error in init_sweep.py __main__ process: {e}", exc_info=True)

```

---

## layouts.py

```python
from dash import html, dcc
import dash_bootstrap_components as dbc
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as pyo
import plotly.tools as tls

import dash_utils

# Placeholder for page content functions
def home(page):
    return html.Div("Home Page Content")

def build_agent(page):
    return html.Div([
        dbc.Container([
            dcc.Store(id="agent-params-store", data={}),
            html.H1("Build Agent", style={'textAlign': 'center'}),
            # Define tabs
            dbc.Tabs([
                # Tab 1: Environment
                dbc.Tab(label="Environment", children=[
                    dash_utils.env_dropdown_component(page),
                    html.Div(id={'type': 'gym-params', 'page': page}),  # empty for building agent
                    html.Div(id={'type': 'env-description', 'page': page}),
                    html.Img(id={'type': 'env-gif', 'page': page}, style={'width': '300px'})
                ]),

                # Tab 2: Agent
                dbc.Tab(label="Agent", children=[
                    dcc.Dropdown(
                        id={
                            'type': 'agent-type-dropdown',
                            'page': page
                        },
                        options=[
                            {'label': 'Reinforce', 'value': 'Reinforce'},
                            {'label': 'Actor Critic', 'value': 'ActorCritic'},
                            {'label': 'Deep Deterministic Policy Gradient', 'value': 'DDPG'},
                            {'label': 'TD3', 'value': 'TD3'},
                            {'label': 'Hindsight Experience Replay (DDPG)', 'value': 'HER_DDPG'},
                            {'label': 'Hindsight Experience Replay (TD3)', 'value': 'HER_TD3'},
                            {'label': 'Proximal Policy Optimization', 'value': 'PPO'},
                        ],
                        placeholder="Select Agent Type",
                    ),
                    html.Div(id='agent-parameters-inputs'),
                ]),

                # Tab 3: WANDB
                dbc.Tab(label="WANDB", children=[
                    html.Div(id='callback-selection'),
                    html.Div(id={
                        'type': 'wandb-login-container',
                        'page': page
                    }),
                    html.Div(id={'type': 'project-selection-container', 'page': page}),
                ]),

                # Tab 4: Build
                dbc.Tab(label="Build", children=[
                    html.Div(id='save-directory-selection'),
                    dbc.Button("Build Model", id='build-agent-button', n_clicks=0),
                    html.Div(id='build-agent-status')
                ]),
            ])
        ])
    ])

def train_agent(page):
   
    return dbc.Container([
        dcc.Store(id={"type":"run-params-store", "page":page}, data={}),
        html.H1("Train Agent", style={'textAlign': 'center'}),
        dcc.Store(id={'type':'agent-store', 'page':page}),
        dash_utils.upload_component(page),
        html.Div(id={'type':'output-agent-load', 'page':page}),
        # dash_utils.env_dropdown_component(page),
        # html.Div(id={'type':'env-description', 'page':page}),
        # html.Img(id={'type':'env-gif', 'page':page}, style={'width': '300px'}),
        # html.Div(id={'type':'gym-params', 'page':page}),
        # dash_utils.run_agent_settings_component(page),
        html.Div(id={'type': 'run-options', 'page': page}),
        dbc.Button("Start",
            id={
                'type': 'start',
                'page': page,
            },
            n_clicks=0
        ),
        # dcc.Loading(
        #     id={
        #         'type':'loading',
        #         'page':page,
        #         },
        #     type="default", 
        #     children=html.Div(
        #         id={
        #             'type':'loading-output',
        #             'page':page,
        #         },
        #     ),
        # ),
        dcc.Store(
            id={
                'type':'storage',
                'page':page,
            },
            data={
                'status': " Training Initiated...",
                'progress': 0,
                'data': {},
            },
        ),
        html.Div(id={'type':'status', 'page':page}),
        dash_utils.video_carousel_component(page),  # Initially empty list of video paths
        dcc.Interval(
            id={
                'type':'interval-component',
                'page':page
            },
            interval=1*1000,  # in milliseconds
            n_intervals=0
        ),
    ], fluid=True)

def test_agent(page):
    
    return dbc.Container([
        dcc.Store(id={"type":"run-params-store", "page":page}, data={}),
        html.H1("Test Agent", style={'textAlign': 'center'}),
        dcc.Store(id={'type':'agent-store', 'page':page}),
        dash_utils.upload_component(page),
        html.Div(id={'type':'output-agent-load', 'page':page}),
        # dash_utils.env_dropdown_component(page),
        # html.Div(id={'type':'env-description', 'page':page}),
        # html.Img(id={'type':'env-gif', 'page':page}, style={'width': '300px'}),
        # html.Div(id={'type':'gym-params', 'page':page}),
        html.Div(id={'type': 'run-options', 'page': page}),
        dbc.Button("Start",
            id={
                'type': 'start',
                'page': page,
            },
            n_clicks=0
        ),
        dcc.Store(
            id={
                'type':'storage',
                'page':page,
            },
            data={
                'status': "Testing Initiated...",
                'progress': 0,
                'data': {},
            },
        ),
        html.Div(id={'type':'status', 'page':page}),
        dash_utils.video_carousel_component(page),  # Initially empty list of video paths
        dcc.Interval(
            id={
                'type':'interval-component',
                'page':page
            },
            interval=1*1000,  # in milliseconds
            n_intervals=0
        ),
    ], fluid=True)


def hyperparameter_search(page):
    right_column = dbc.Col(
        [
            html.H3('Wandb Project'),
            dash_utils.create_wandb_project_dropdown(page),
            html.Hr(),
            html.H3("Search Configuration"),
            dash_utils.env_dropdown_component(page),
            html.Div(id={'type':'gym-params', 'page':page}),
            html.Div(id={'type': 'env-description', 'page': page}),
            html.Img(id={'type': 'env-gif', 'page': page}, style={'width': '300px'}),
            dash_utils.create_sweep_options(),
            # dcc.Input(
            #     id='num-episodes',
            #     type='number',
            #     placeholder='Episodes per Sweep',
            # ),
            html.Div(
                id='agent-sweep-options',
            ),
            html.Div(
                id='her-options-hyperparam',
                hidden=True,
                children=[
                    dcc.Input(
                        id='num-epochs',
                        type='number',
                        placeholder='Epochs per Sweep',
                    ),
                    dcc.Input(
                        id='num-cycles',
                        type='number',
                        placeholder='Cycles per Episode',
                    ),
                    dcc.Input(
                        id='num-updates',
                        type='number',
                        placeholder='Updates per Episode',
                    ),
                ]
            ),
            html.Div(
                id={
                    'type': 'mpi-options',
                    'page': page,
                },
                style={'display': 'none'},
                children=[
                    html.Label('Use MPI', style={'text-decoration': 'underline'}),
                    dcc.RadioItems(
                        id={
                            'type': 'mpi',
                            'page': page,
                        },
                        options=[
                        {'label': 'Yes', 'value': True},
                        {'label': 'No', 'value': False},
                        ],
                    ),
                    dcc.Input(
                        id={
                            'type': 'workers',
                            'page': page,
                        },
                        type='number',
                        placeholder="Number of Workers",
                        min=1,
                        style={'display': 'none'},
                    ),
                ]
            ),
            html.Div(
                id={
                    'type': 'sweep-options',
                    'page': page,
                },
                style={'display': 'none'},
                children=[
                    dcc.Input(
                        id={
                            'type': 'num-sweep-agents',
                            'page': page,
                        },
                        type='number',
                        placeholder="Number of Sweep Agents",
                        min=1,
                    ),
                ],
            ),
            dash_utils.create_seed_component(page),
            html.Hr(),
            dbc.Button("Download WandB Config", id="download-wandb-config-button", color="primary", className="mr-2"),
            dcc.Download(id="download-wandb-config"),
            dbc.Button("Download Sweep Config", id="download-sweep-config-button", color="primary", className="mr-2"),
            dcc.Download(id="download-sweep-config"),
        ],
        md=6,
    )

    left_column = dbc.Col(
        [
            html.H3('Agent Configuration'),
            dcc.Dropdown(
                id={
                    'type':'agent-type-selector',
                    'page':page,
                    },
                options=[
                    {'label': 'Reinforce', 'value': 'Reinforce'},
                    {'label': 'Actor Critic', 'value': 'ActorCritic'},
                    {'label': 'Deep Deterministic Policy Gradient', 'value': 'DDPG'},
                    {'label': 'TD3', 'value': 'TD3'},
                    {'label': 'Hindsight Experience Replay (DDPG)', 'value': 'HER_DDPG'},
                    {'label': 'Proximal Policy Optimization', 'value': 'PPO'},
                ],
                placeholder="Select Agent",
            ),
            html.Div(
                id='agent-hyperparameters-inputs',
                children=[
                    dcc.Tabs(
                        id='agent-options-tabs',
                    ),
                ]
            ),
            html.Hr(),
        ]
    )

    row = dbc.Row([left_column, right_column])

    inputs = html.Div(
        id={
            'type': 'hyperparameter-inputs',
            'page': page,
        },
        hidden=True,
        children=[
            dcc.Store(id={'type':'agent-store', 'page':page}),
            dash_utils.upload_component(page),
            html.Div(id={'type':'output-agent-load', 'page':page}),
            row,
            dbc.Button("Start Sweep",
                id={
                    'type': 'start',
                    'page': page,
                },
                n_clicks=0
            ),
            dcc.Store(
                id={
                    'type': 'storage',
                    'page': page,
                },
                data={
                    'status': "Sweep Initiated...",
                    'progress': 0,
                    'data': {},
                },
            ),
            html.Div(id={'type': 'status', 'page': page}),
            html.Div(
                id={
                    'type': 'hidden-div-hyperparam',
                    'page': page,
                },
                style={'display': 'none'}
            ),
            dcc.Dropdown(
                id={'type':'hyperparameter-selector', 'page':page},
                options=[],
                multi=True,
                placeholder="Select Hyperparameters",
            ),
            dcc.Interval(
                id='update-hyperparam-selector',
                interval=1*1000,
                n_intervals=0
            ),
            
            dash_utils.render_heatmap(page),
            dcc.Store(id={'type':'heatmap-data-store', 'page':page},
                  data={'formatted_data':None, 'matrix_data':None, 'bin_ranges':None}),
            dcc.Interval(
                id='heatmap-store-data-interval',
                interval=10*1000,
                n_intervals=0
            ),
            dcc.Interval(
                id='start-fetch-process-interval',
                interval=10*1000,
                n_intervals=0,
                max_intervals=1,
            ),
            dcc.Interval(
                id='start-matrix-process-interval',
                interval=10*1000,
                n_intervals=0,
            ),
            dcc.Interval(
                id={
                    'type':'interval-component',
                    'page':page
                },
                interval=1*1000,  # in milliseconds
                n_intervals=0
            ),
            html.Div(id='hidden-div-fetch-process', style={'display': 'none'}),
            html.Div(id='hidden-div-matrix-process', style={'display': 'none'}),
        ]
    )

    return dbc.Container(
        [
            html.Div(
                [
                    html.H1("Hyperparameter Search", style={'textAlign': 'center'}),
                    html.Div(
                        [  
                            dash_utils.create_wandb_login(page),
                        ],
                        style={'textAlign': 'center'},
                    ),
                ],
            ),
            inputs
        ]
    )


def co_occurrence_analysis(page):
    return html.Div([
        html.H1("Co-Occurrence Analysis", style={'textAlign': 'center'}),
        dash_utils.create_wandb_project_dropdown(page),
        dash_utils.create_sweeps_dropdown(page),
        html.Button('Get Data', id={'type':'sweep-data-button', 'page':page}, n_clicks=0),
        html.Div(id={'type':'output-data-upload', 'page':page}),
        dcc.Dropdown(
            id={'type':'hyperparameter-selector', 'page':page},
            options=[],
            multi=True,
            placeholder="Select Hyperparameters",
        ),
        dash_utils.render_heatmap(page),
        dcc.Store(id={'type':'heatmap-data-store', 'page':page},
                  data={'formatted_data':None, 'matrix_data':None, 'bin_ranges':None}),
    ])


def wandb_utils(page):
    return html.Div([
        dbc.Container([
            html.H1("wandb-utils"),
            html.Div(id='wandb-utils'),
        ])
    ])
```

---

## logging_config.py

```python
import logging
import os

# Clear the debug.log file if it exists
log_file = 'debug.log'
if os.path.exists(log_file):
    with open(log_file, 'w'):
        pass

# Create a custom logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)  # Set the lowest level to capture all types of log messages

# Create handlers
console_handler = logging.StreamHandler()
file_handler = logging.FileHandler(log_file)

# Set levels for handlers
console_handler.setLevel(logging.ERROR)
file_handler.setLevel(logging.ERROR)

# Create formatters and add them to the handlers
console_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')
file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

console_handler.setFormatter(console_format)
file_handler.setFormatter(file_format)

# Clear any existing handlers
if logger.hasHandlers():
    logger.handlers.clear()

# Add handlers to the logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)
```

---

## models.py

```python
"""Holds Model classes used for Reinforcement learning."""

# imports
from abc import abstractmethod
import json
import os
from typing import List, Tuple, Dict
from pathlib import Path
# import time

import torch as T
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torch.distributions import Categorical, Beta, Normal
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ExponentialLR

import gymnasium as gym
from gymnasium.envs.registration import EnvSpec
import numpy as np
import cnn_models
from torch_utils import get_device, VarianceScaling_
from logging_config import logger
from env_wrapper import EnvWrapper, GymnasiumWrapper, IsaacSimWrapper
from utils import check_for_inf_or_NaN

class Model(nn.Module):
    """
    Base class for all reinforcement learning models.

    This class dynamically constructs a neural network based on the provided layer configuration
    and supports various optimizers and learning rate schedulers.

    Attributes:
        env (EnvWrapper): The environment wrapper for the model.
        layer_config (list): List of dictionaries specifying the layers and their parameters.
        optimizer_params (dict): Dictionary specifying optimizer type and parameters.
        scheduler_params (dict): Dictionary specifying scheduler type and parameters (optional).
        device (str): The device ('cpu' or 'cuda') to run the model on.
    """
    def __init__(self, env: EnvWrapper, layer_config, optimizer_params: dict = None,
                 scheduler_params: dict = None, device=None):
        """
        Sets up the module dictionary of layers (most of which
        will be lazy).

        Args:
            env (EnvWrapper): Environment wrapper.
            layer_config (list): List of dictionaries specifying the layers and params.
            optimizer_params (dict): Optimizer configuration.
            scheduler_params (dict): LR scheduler configuration.
            device (str): Device to run on.
        """
        super().__init__()
        self.env = env
        self.layer_config = layer_config
        self.layers = nn.ModuleDict()
        self.optimizer_params = optimizer_params or {'type': 'Adam', 'params': {'lr': 0.001}}
        self.scheduler_params = scheduler_params
        self.device = get_device(device)

        # Build the layers dynamically based on config
        for i, layer_info in enumerate(self.layer_config):
            layer_type = layer_info['type']
            layer_params = layer_info.get('params', {})
            self.layers[f'{layer_type}_{i}'] = self._build_layer(layer_type, layer_params)
            
        # add module to model
        # self.add_module('layers', self.layers)
        # Set optimizer and scheduler to None (set in init_parameters function after dry run)
        self.optimizer = None
        self.scheduler = None

        # Move the model to device
        self.to(self.device)
        
    def _init_model(self, module_dict: nn.ModuleDict, layer_config: list):
        """
        Performs a "dry run" forward pass with dummy_input to initialize
        all lazy modules. Then, initializes weights and optimizer/scheduler.

        Args:
            dummy_input (Tensor, optional): If None, automatically creates
                a dummy input based on env.observation_space.shape. If your
                environment is a 3D image (C, H, W), use (1, C, H, W).
        """
        obs_space = (self.env.single_observation_space if hasattr(self.env, "single_observation_space") 
                        else self.env.observation_space)
        # Dry run forward pass to initialize lazy modules
        # Check if the observation space is a dictionary for goal-aware environments
        if isinstance(obs_space, gym.spaces.Dict):
            obs_shape = obs_space['observation'].shape
            goal_shape = obs_space['desired_goal'].shape
            state_input = T.ones((1, *obs_shape), device=self.device, dtype=T.float)
            goal_input = T.ones((1, *goal_shape), device=self.device, dtype=T.float)
            # Check if CriticModel instance to pass action dummy values
            if isinstance(self, CriticModel):
                action_shape = self.env.single_action_space.shape
                action_input = T.ones((1, *action_shape), device=self.device, dtype=T.float)
                with T.no_grad():
                    _ = self.forward(state_input, action_input, goal_input)
            else:
                with T.no_grad():
                    _ = self.forward(state_input, goal_input)
        else:
            obs_shape = obs_space.shape
            #DEBUG
            # print(f'init model obs_shape:{obs_shape}')
            state_input = T.ones((1, *obs_shape), device=self.device, dtype=T.float)
            #DEBUG
            # print(f'state input shape:{state_input.shape}')
            if isinstance(self, CriticModel):
                action_shape = self.env.single_action_space.shape
                action_input = T.ones((1, *action_shape), device=self.device, dtype=T.float)
                with T.no_grad():
                    _ = self.forward(state_input, action_input)
            else:
                with T.no_grad():
                    _ = self.forward(state_input)

        # Initialize weights after lazy modules are materialized
        self._init_weights(layer_config, module_dict)

        # Now that parameters exist, create the optimizer & scheduler
        self.optimizer = self._init_optimizer()
        if self.scheduler_params:
            self.scheduler = self._init_scheduler()

    def _build_layer(self, layer_type, params):
        """
        Build a specific layer based on its type and parameters.

        Args:
            layer_type (str): Type of the layer (e.g., 'dense', 'conv2d', etc.).
            params (dict): Parameters for the layer.

        Returns:
            nn.Module: Constructed layer.
        """
        if layer_type == 'dense':
            return nn.LazyLinear(params["units"])

        elif layer_type == 'conv2d':
            return nn.LazyConv2d(
                out_channels=params.get('out_channels', 64),
                kernel_size=params.get('kernel_size', 3),
                stride=params.get('stride', 1),
                padding=params.get('padding', 0),
                bias=True
            )

        elif layer_type == 'pool':
            return nn.MaxPool2d(**params)

        elif layer_type == 'dropout':
            return nn.Dropout(**params)

        elif layer_type == 'batchnorm2d':
            return nn.LazyBatchNorm2d()

        elif layer_type == 'layernorm':
            return nn.LayerNorm(**params)

        elif layer_type == 'flatten':
            return nn.Flatten()

        elif layer_type == 'relu':
            return nn.ReLU()
        
        elif layer_type == 'leakyrelu':
            return nn.LeakyReLU()

        elif layer_type == 'tanh':
            return nn.Tanh()

        else:
            raise ValueError(f"Unsupported layer type: {layer_type}")

    def _init_weights(self, layer_config, layers):
        """
        Initialize the weights for the model.

        Args:
            layer_config (dict): configuration of layer.
            layers (torch layers): torch.nn.Module.layers.
        """
        # Loop through each layer config and corresponding layer
        for config, (layer_name, layer) in zip(layer_config, layers.items()):
            if not hasattr(layer, 'weight'):
                continue
            
            # If the params of the layer config dict contains a kernel, apply it to layer
            # if config['type'] in ['dense', 'transformer']:
            kernel = config.get('params', {}).get('kernel', 'default')  # Get kernel or 'default'
            kernel_params = config.get('params', {}).get('kernel params', {}) # Get kernel params or empty dict
            # Apply the specified initialization scheme
            if kernel == 'kaiming_uniform':
                nn.init.kaiming_uniform_(layer.weight, **kernel_params)
            elif kernel == 'kaiming_normal':
                nn.init.kaiming_normal_(layer.weight)
            elif kernel == 'xavier_uniform':
                nn.init.xavier_uniform_(layer.weight)
            elif kernel == 'xavier_normal':
                nn.init.xavier_normal_(layer.weight)
            elif kernel == 'truncated_normal':
                nn.init.trunc_normal_(layer.weight, **kernel_params)
                # nn.init.trunc_normal_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'uniform':
                nn.init.uniform_(layer.weight, **kernel_params)
                # nn.init.uniform_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'normal':
                nn.init.normal_(layer.weight, **kernel_params)
                # nn.init.normal_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'orthogonal':
                nn.init.orthogonal_(layer.weight, **kernel_params)
            elif kernel == 'constant':
                nn.init.constant_(layer.weight, **kernel_params)
                # nn.init.constant_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'ones':
                nn.init.ones_(layer.weight, **kernel_params)
                # nn.init.ones_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'zeros':
                nn.init.zeros_(layer.weight, **kernel_params)
                # nn.init.zeros_(layer.bias, **config['params']['kernel params'])
            elif kernel == 'variance_scaling':
                VarianceScaling_(layer.weight, **kernel_params)
            elif kernel == 'default':
                # Use PyTorch's default initialization (skip)
                pass
            else:
                raise ValueError(f"Unsupported initialization: {kernel}")
            # Initialize bias
            if hasattr(layer, 'bias'):
                nn.init.zeros_(layer.bias)

    def _init_optimizer(self):
        """
        Initialize the optimizer for the model.

        Returns:
            torch.optim.Optimizer: Configured optimizer.
        """
        optimizer_type = self.optimizer_params['type']
        optimizer_params = self.optimizer_params['params']
        if optimizer_type == 'Adam':
            return optim.Adam(self.parameters(), **optimizer_params)
        elif optimizer_type == 'SGD':
            return optim.SGD(self.parameters(), **optimizer_params)
        elif optimizer_type == 'RMSprop':
            return optim.RMSprop(self.parameters(), **optimizer_params)
        elif optimizer_type == 'Adagrad':
            return optim.Adagrad(self.parameters(), **optimizer_params)
        else:
            raise NotImplementedError(f"Unsupported optimizer type: {optimizer_type}")
    
    def _init_scheduler(self):
        """
        Initialize the learning rate scheduler for the model.

        Args:
            scheduler (dict): Scheduler configuration.

        Returns:
            torch.optim.lr_scheduler: Configured scheduler.
        """
        scheduler_type = self.scheduler_params.get('type', '').lower()
        scheduler_params = self.scheduler_params.get('params', {})
        if scheduler_type == 'cosineannealing':
            return optim.lr_scheduler.CosineAnnealingLR(self.optimizer, **scheduler_params)
        elif scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(self.optimizer, **scheduler_params)
        elif scheduler_type == 'exponential':
            return optim.lr_scheduler.ExponentialLR(self.optimizer, **scheduler_params)
        elif scheduler_type == 'linear':
            return optim.lr_scheduler.LinearLR(self.optimizer, **scheduler_params)
        else:
            raise ValueError(f"Unsupported scheduler type: {scheduler_type}")
    
    @abstractmethod
    def forward(self, x):
        """
        Forward pass through the model.

        Args:
            x (Tensor): Input tensor.

        Returns:
            Tensor: Output tensor after passing through all layers.
        """
        pass

    @abstractmethod
    def get_config(self):
        """
        Retrieve the model configuration.

        Returns:
            dict: Configuration dictionary.
        """
        pass

    @abstractmethod
    def save(self, folder):
        """
        Save the model and its configuration.

        Args:
            folder (str): Directory to save the model.
        """
        pass

    @classmethod
    @abstractmethod
    def load(cls, folder):
        """
        Load a model from a saved configuration.

        Args:
            folder (str): Directory containing the saved model.

        Returns:
            Model: Loaded model instance.
        """
        pass


class StochasticDiscretePolicy(Model):
    """
    Policy model for predicting a probability distribution over a discrete action space.

    This class builds on the `Model` base class and adds functionality specific to
    policies with a discrete action space, such as using a Categorical distribution
    for action selection.

    Attributes:
        env (EnvWrapper): The environment wrapper.
        layer_config (list): Configuration of hidden layers.
        output_layer_kernel (dict): Configuration of the output layer weights.
        optimizer_params (dict): Parameters for the optimizer. (default: Adam with lr=0.001)
        scheduler_params (dict): Parameters for the learning rate scheduler (optional).
        distribution (str): Type of distribution for action selection (default: 'categorical').
        device (str): Device to run the model on (default: 'cuda').
    """

    def __init__(
        self,
        env: EnvWrapper,
        layer_config: list,
        output_layer_kernel: dict = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}],
        optimizer_params:dict = {'type':'Adam', 'params':{'lr':0.001}},
        scheduler_params:dict = None,
        distribution: str = 'categorical',
        device: str = None
    ):
        """
        Initialize the policy model.

        Args:
            env (EnvWrapper): The environment wrapper.
            layer_config (list): List of dictionaries specifying hidden layer configurations.
            output_layer_kernel (dict): Configuration for output layer initialization (default: {}).
            optimizer_params (dict, optional): Optimizer parameters (default: Adam with lr=0.001).
            scheduler_params (dict, optional): Scheduler parameters (default: None).
            distribution (str): Type of distribution for actions (default: 'categorical').
            device (str): Device for computation (default: 'cuda').
        """
        
        super().__init__(env, layer_config, optimizer_params, scheduler_params, device)
        self.output_config = output_layer_kernel
        self.distribution = distribution

        # Get the action space of the environment
        # self.act_space = self.env.single_action_space if isinstance(self.env, GymnasiumWrapper) else self.env.action_space
        self.act_space = (self.env.single_action_space 
                          if hasattr(self.env, "single_action_space") 
                          else self.env.action_space)
        num_actions = self.act_space.n

        # Create the output layer
        self.output_layer = nn.ModuleDict({
            'policy_dense_output': nn.LazyLinear(num_actions)
        })
        # self.add_module('output_layer', self.output_layer)

        # Initialize weights
        # self._init_weights(self.layer_config, self.layers)
        # self._init_weights(self.output_config, self.output_layer)

        # Initialize optimizer
        # self.optimizer = self._init_optimizer()

        # Move to device
        self.to(self.device)

        # initialize params
        self._init_model(self.layers, self.layer_config)
        self._init_model(self.output_layer, self.output_config)

    def forward(self, x):
        """
        Perform a forward pass through the model.

        Args:
            x (Tensor): Input tensor (e.g., observation from the environment).

        Returns:
            Tuple[Categorical, Tensor]: Action distribution and logits for the action space.
        """
        #DEBUG
        # print(f'discrete policy shape of x: {x.shape}')
        # print(f'discrete policy x:{x}')
        if x.dim() == 1: # Check if tensor is flat
            x = x.unsqueeze(-1)  # Reshape to (batch, 1)
        if x.dim() == 3:
            x = x.unsqueeze(1)
        # Check if observation is image-like (HWC)
        if isinstance(self.env, GymnasiumWrapper):
            # obs_shape = self.env.single_observation_space.shape
            #DEBUG
            # print(f'observation space shape:{obs_shape}')
            if x.dim() == 4 and x.shape[-1] in [3,4]:
                # DEBUG
                # print(f'permutation fired')
                x = x.permute(0, 3, 1, 2)  #  (B, C, H, W)
        #DEBUG
        # print(f'discrete policy new x shape:{x.shape}')
        x = x.to(self.device)
        for layer in self.layers.values():
            x = layer(x)
        x = self.output_layer['policy_dense_output'](x)
        if self.distribution == 'categorical':
            dist = Categorical(logits=x)
            return dist, x
        else:
            raise ValueError(f'Distribution {self.distribution} not supported.')

    def get_config(self):
        """
        Retrieve the configuration of the policy model.

        Returns:
            dict: Configuration dictionary with details about the model.
        """
        config = {
            "env": self.env.to_json(),
            'num_layers': len(self.layers),
            'layer_config': self.layer_config,
            'output_layer_kernel': self.output_config,
            'optimizer_params': self.optimizer_params,
            'scheduler_params': self.scheduler_params,
            'distribution': self.distribution,
            'device': self.device.type,
        }
        return config

    def save(self, folder):
        """
        Save the model to the specified folder.

        Args:
            folder (str): Path to the directory where the model should be saved.
        """
        # Ensure the model directory exists
        model_dir = Path(folder) / "policy_model"
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save the model parameters
        T.save(self.state_dict(), model_dir / 'pytorch_model.onnx')
        T.save(self.state_dict(), model_dir / 'pytorch_model.pt')

        # Save the model configuration
        config = self.get_config()
        with open(model_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

    @classmethod
    def load(cls, config_path, load_weights=True):
        """
        Load a policy model from a saved configuration.

        Args:
            config_path (str): Path to the configuration file.
            load_weights (bool): Whether to load the model weights (default: True).

        Returns:
            StochasticDiscretePolicy: Loaded policy model instance.
        """
        model_dir = Path(config_path) / "policy_model"
        config_path = model_dir / "config.json"
        model_path = model_dir / 'pytorch_model.onnx'

        if config_path.is_file():
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            raise FileNotFoundError(f"No configuration file found in {config_path}")
        
        # # Determine which wrapper to use
        # wrapper_dict = json.loads(config['env'])
        # wrapper_type = wrapper_dict.get("type")
        # if wrapper_type == 'GymnasiumWrapper':
        #     env = GymnasiumWrapper(wrapper_dict.get("env"))
        # else:
        #     raise ValueError(f"Unsupported wrapper type: {wrapper_type}")

        env = EnvWrapper.from_json(config.get("env"))

        model = cls(env = env,
                    layer_config = config.get("layer_config"),
                    output_layer_kernel = config.get("output_layer_kernel", {"default":{}}),
                    optimizer_params = config.get("optimizer_params", {}),
                    scheduler_params = config.get("scheduler_params", None),
                    distribution = config.get("distribution", "categorical"),
                    device = config.get("device", "cpu")
                    )

        # Load weights if True
        if load_weights:
            model.load_state_dict(T.load(model_path))

        return model

class StochasticContinuousPolicy(Model):
    """
    Policy model for predicting a probability distribution over a continuous action space.

    This class extends the `Model` base class to implement policies for continuous action spaces,
    supporting Beta and Normal distributions.

    Attributes:
        env (EnvWrapper): The environment wrapper.
        layer_config (list): Configuration of hidden layers.
        output_layer_kernel (dict): Configuration of the output layer weights.
        optimizer_params (dict): Parameters for the optimizer.
        scheduler_params (dict): Parameters for the learning rate scheduler (optional).
        distribution (str): Type of distribution for action selection ('beta' or 'normal').
        device (str): Device to run the model on (default: 'cuda').
    """

    def __init__(
        self,
        env:EnvWrapper,
        layer_config: List[Dict],
        output_layer_kernel: dict = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}],
        optimizer_params:dict = {'type':'Adam', 'params':{'lr':0.001}},
        scheduler_params:dict = None,
        distribution: str = 'beta',
        device: str = None
    ):
        """
        Initialize the policy model.

        Args:
            env (EnvWrapper): The environment wrapper.
            layer_config (list): List of dictionaries specifying hidden layer configurations.
            output_layer_kernel (dict): Configuration for output layer initialization (default: {}).
            optimizer_params (dict, optional): Optimizer parameters (default: Adam with lr=0.001).
            scheduler_params (dict, optional): Scheduler parameters (default: None).
            distribution (str): Type of distribution for actions (default: 'beta').
            device (str): Device for computation (default: 'cuda').
        """
        super().__init__(env, layer_config, optimizer_params, scheduler_params, device)
        self.output_config = output_layer_kernel
        self.distribution = distribution
        # Get the action space of the environment
        self.act_space = (self.env.single_action_space 
                          if hasattr(self.env, "single_action_space") 
                          else self.env.action_space)
        
        num_actions = self.act_space.shape[-1]
        # Create the output layer
        self.output_layer = nn.ModuleDict({
            'policy_output_param_1': nn.LazyLinear(num_actions),
            'policy_output_param_2': nn.LazyLinear(num_actions),
        })

        # Move model to device
        self.to(self.device)

        # initialize params
        self._init_model(self.layers, self.layer_config)
        self._init_model(self.output_layer, self.output_config)

    def forward(self, x):
        """
        Perform a forward pass through the model.

        Args:
            x (Tensor): Input tensor (e.g., observation from the environment).

        Returns:
            Tuple[Distribution, Tensor, Tensor]: Action distribution and its parameters.
        """
        #DEBUG
        # print(f'state shape sent to policy forward:{x.shape}')
        if x.dim() == 1: # Check if tensor is flat
            x = x.unsqueeze(-1)  # Reshape to (batch, 1)
        if x.dim() == 3:
            x = x.unsqueeze(1)
        # Check if observation is image-like (HWC)
        if isinstance(self.env, GymnasiumWrapper):
            # obs_shape = self.env.single_observation_space.shape
            #DEBUG
            # print(f'observation space shape:{obs_shape}')
            if x.dim() == 4 and x.shape[-1] in [3,4]:
                # DEBUG
                # print(f'permutation fired')
                x = x.permute(0, 3, 1, 2)  #  (B, C, H, W)
        x = x.to(self.device)
        for layer in self.layers.values():
            x = layer(x)
        param_1 = self.output_layer['policy_output_param_1'](x)
        param_2 = self.output_layer['policy_output_param_2'](x)
        #DEBUG
        # print(f'param 1 shape:{param_1.shape}')
        # print(f'param 2 shape:{param_2.shape}')
        if self.distribution == 'beta':
            alpha = F.softplus(param_1) + 1.0
            beta = F.softplus(param_2) + 1.0
            dist = Beta(alpha, beta)
            return dist, alpha, beta
        elif self.distribution == 'normal':
            mu = param_1
            sigma = F.softplus(param_2)
            dist = Normal(mu, sigma)
            return dist, mu, sigma
        else:
            raise ValueError(f"Distribution {self.distribution} not supported.")

    def get_config(self):
        """
        Retrieve the configuration of the policy model.

        Returns:
            dict: Configuration dictionary with details about the model.
        """
        config = {
            "env": self.env.to_json(),
            'num_layers': len(self.layers),
            'layer_config': self.layer_config,
            'output_layer_kernel': self.output_config,
            'optimizer_params': self.optimizer_params,
            'scheduler_params': self.scheduler_params,
            'distribution': self.distribution,
            'device': self.device.type,
        }
        return config

    def save(self, folder):
        """
        Save the model to the specified folder.

        Args:
            folder (str): Path to the directory where the model should be saved.
        """
        model_dir = Path(folder) / "policy_model"
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save the model parameters
        T.save(self.state_dict(), model_dir / 'pytorch_model.onnx')
        T.save(self.state_dict(), model_dir / 'pytorch_model.pt')

        config = self.get_config()

        with open(model_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

    @classmethod
    def load(cls, config_path, load_weights=True):
        """
        Load a policy model from a saved configuration.

        Args:
            config_path (str): Path to the configuration file.
            load_weights (bool): Whether to load the model weights (default: True).

        Returns:
            StochasticContinuousPolicy: Loaded policy model instance.
        """
        model_dir = Path(config_path) / "policy_model"
        config_path = model_dir / "config.json"
        model_path = model_dir / 'pytorch_model.onnx'

        if config_path.is_file():
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            raise FileNotFoundError(f"No configuration file found in {config_path}")
        
        # Determine which wrapper to use
        # wrapper_dict = json.loads(config['env'])
        # wrapper_type = wrapper_dict.get("type")
        # if wrapper_type == 'GymnasiumWrapper':
        #     env = GymnasiumWrapper(wrapper_dict.get("env"))
        # else:
        #     raise ValueError(f"Unsupported wrapper type: {wrapper_type}")
        #DEBUG
        # print(f'config:{config}')
        env = EnvWrapper.from_json(config.get("env"))
        #DEBUG
        # print(f'env:{env.config}')

        model = cls(env = env,
                    layer_config = config.get("layer_config"),
                    output_layer_kernel = config.get("output_layer_kernel", {"default":{}}),
                    optimizer_params = config.get("optimizer_params", {}),
                    scheduler_params = config.get("scheduler_params", None),
                    distribution = config.get("distribution", "beta"),
                    device = config.get("device", "cpu")
                    )

        # Load weights if True
        if load_weights:
            model.load_state_dict(T.load(model_path))

        return model


class ValueModel(Model):
    """
    Value model for predicting state values.

    This class extends the `Model` base class to implement a neural network for value function approximation in reinforcement learning.

    Attributes:
        env (EnvWrapper): The environment wrapper.
        layer_config (list): Configuration of hidden layers.
        output_layer_kernel (dict): Configuration of the output layer weights.
        optimizer_params (dict): Parameters for the optimizer.
        scheduler_params (dict): Parameters for the learning rate scheduler (optional).
        device (str): Device to run the model on (default: 'cuda').
    """

    def __init__(
        self,
        env: EnvWrapper,
        layer_config: List[Dict],
        output_layer_kernel: dict = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}],
        optimizer_params:dict = {'type':'Adam', 'params':{'lr':0.001}},
        scheduler_params = None,
        device = None
    ):
        """
        Initialize the value model.

        Args:
            env (EnvWrapper): The environment wrapper.
            layer_config (list): List of dictionaries specifying hidden layer configurations.
            output_layer_kernel (dict): Configuration for output layer initialization (default: {}).
            optimizer_params (dict, optional): Optimizer parameters (default: Adam with lr=0.001).
            scheduler_params (dict, optional): Scheduler parameters (default: None).
            device (str): Device for computation (default: 'cuda').
        """
        super().__init__(env, layer_config, optimizer_params, scheduler_params, device)
        self.output_config = output_layer_kernel

        # Create the output layer
        self.output_layer = nn.ModuleDict({
            'value_dense_output': nn.LazyLinear(1)
        })
        self.add_module('output_layer', self.output_layer)

        # Move model to device
        self.to(self.device)

        # initialize params
        self._init_model(self.layers, self.layer_config)
        self._init_model(self.output_layer, self.output_config)

    def forward(self, x):
        """
        Perform a forward pass through the model.

        Args:
            x (Tensor): Input tensor (e.g., observation from the environment).

        Returns:
            Tensor: Predicted state value.
        """
        #DEBUG
        # print(f'value model x shape:{x.shape}')
        # print(f'value model x:{x}')
        if x.dim() == 1: # Check if tensor is flat
            x = x.unsqueeze(-1)  # Reshape to (batch, 1)
        if x.dim() == 3:
            x = x.unsqueeze(1)
        # Check if observation is image-like (HWC)
        if isinstance(self.env, GymnasiumWrapper):
            obs_shape = self.env.single_observation_space.shape
            #DEBUG
            # print(f'observation space shape:{obs_shape}')
            if x.dim() == 4 and x.shape[-1] in [3,4]:
                # DEBUG
                # print(f'permutation fired')
                x = x.permute(0, 3, 1, 2)  #  (B, C, H, W)
        #DEBUG
        # print(f'value model new x shape:{x.shape}')
        x = x.to(self.device)
        #DEBUG
        # print(f'Value Model Input Shape: {x.size()}')
        for layer in self.layers.values():
            x = layer(x)

        x = self.output_layer['value_dense_output'](x)

        return x


    def get_config(self):
        """
        Retrieve the configuration of the value model.

        Returns:
            dict: Configuration dictionary with details about the model.
        """

        config = {
            "env": self.env.to_json(),
            'num_layers': len(self.layers),
            'layer_config': self.layer_config,
            'output_layer_kernel': self.output_config,
            'optimizer_params': self.optimizer_params,
            'scheduler_params': self.scheduler_params,
            'device': self.device.type,
        }

        return config


    def save(self, folder):
        """
        Save the model to the specified folder.

        Args:
            folder (str): Path to the directory where the model should be saved.
        """
        model_dir = Path(folder) / "value_model"
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save the model parameters
        T.save(self.state_dict(), model_dir / 'pytorch_model.onnx')
        T.save(self.state_dict(), model_dir / 'pytorch_model.pt')

        config = self.get_config()

        with open(model_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)


    @classmethod
    def load(cls, config_path, load_weights:bool=True):
        """
        Load a value model from a saved configuration.

        Args:
            config_path (str): Path to the configuration file.
            load_weights (bool): Whether to load the model weights (default: True).

        Returns:
            ValueModel: Loaded value model instance.
        """
        model_dir = Path(config_path) / "value_model"
        config_path = model_dir / "config.json"
        model_path = model_dir / 'pytorch_model.onnx'

        if config_path.is_file():
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            raise FileNotFoundError(f"No configuration file found in {config_path}")
        
        # Determine the wrapper type from the environment configuration
        # wrapper_dict = json.loads(config['env'])
        # wrapper_type = wrapper_dict.get("type")
        # if wrapper_type == 'GymnasiumWrapper':
        #     env = GymnasiumWrapper(wrapper_dict.get("env"))
        # else:
        #     raise ValueError(f"Unsupported wrapper type: {wrapper_type}")

        env = EnvWrapper.from_json(config.get("env"))

        model = cls(env = env,
                    layer_config = config.get("layer_config"),
                    output_layer_kernel = config.get("output_layer_kernel"),
                    optimizer_params = config.get("optimizer_params"),
                    scheduler_params = config.get("scheduler_params", None),
                    device = config.get("device")
                    )

        # Load weights if True
        if load_weights:
            model.load_state_dict(T.load(model_path))

        return model

class ActorModel(Model):
    
    def __init__(self,
                 env: EnvWrapper,
                 layer_config: List[Dict],
                 output_layer_kernel: dict = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}],
                 optimizer_params: dict={'type':'Adam', 'params':{'lr':0.001}},
                 scheduler_params: dict=None,
                 device: str=None
                 ):
        super().__init__(env, layer_config, optimizer_params, scheduler_params, device)
        self.output_config = output_layer_kernel

        # Create the output layer
        self.output_layer = nn.ModuleDict({
            'actor_mu': nn.LazyLinear(self.env.single_action_space.shape[-1]),
            'actor_pi': nn.Tanh()
        })

        # Move the model to the specified device
        self.to(self.device)

        # initialize params
        self._init_model(self.layers, self.layer_config)
        self._init_model(self.output_layer, self.output_config)

    def forward(self, x, goal=None):
        x = x.to(self.device)

        if goal is not None:
            goal = goal.to(self.device)
            x = T.cat([x, goal], dim=-1)

        for layer in self.layers.values():
            x = layer(x)

        mu = self.output_layer["actor_mu"](x)
        pi = self.output_layer["actor_pi"](mu)
        pi = pi * T.tensor(self.env.single_action_space.high, dtype=T.float32, device=self.device)
        return mu, pi

    def get_config(self):
        config = {
            "env": self.env.to_json(),
            'num_layers': len(self.layers),
            'layer_config': self.layer_config,
            'output_layer_kernel':self.output_config,
            'optimizer_params': self.optimizer_params,
            'scheduler_params': self.scheduler_params,
            'device': self.device.type,
        }

        return config


    def get_clone(self, weights=True):
        # Reconstruct the model from its configuration
        env = GymnasiumWrapper(self.env.env_spec, self.env.wrappers)
        cloned_model = ActorModel(
            env=env,
            layer_config=self.layer_config.copy(),
            output_layer_kernel=self.output_config.copy(),
            optimizer_params=self.optimizer_params.copy(),
            scheduler_params=self.scheduler_params.copy() if self.scheduler_params else None,
            device=self.device.type
        )
        
        if weights:
            # Copy the model weights
            cloned_model.load_state_dict(self.state_dict())

        return cloned_model


    def save(self, folder):
        """
        Save the model to the specified folder.

        Args:
            folder (str): Path to the directory where the model should be saved.
        """
        model_dir = Path(folder) / "actor_model"
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save the model parameters
        T.save(self.state_dict(), model_dir / 'pytorch_model.onnx')
        T.save(self.state_dict(), model_dir / 'pytorch_model.pt')

        config = self.get_config()

        with open(model_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)


    @classmethod
    def load(cls, config_path, load_weights=True):
        """
        Load an actor model from a saved configuration.

        Args:
            config_path (str): Path to the configuration file.
            load_weights (bool): Whether to load the model weights (default: True).

        Returns:
            ActorModel: Loaded actor model instance.
        """
        model_dir = Path(config_path) / "actor_model"
        config_path = model_dir / "config.json"
        model_path = model_dir / 'pytorch_model.onnx'

        if config_path.is_file():
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            raise FileNotFoundError(f"No configuration file found in {config_path}")
        
        env = EnvWrapper.from_json(config.get("env"))

        model = cls(env = env,
                    layer_config = config.get("layer_config"),
                    output_layer_kernel = config.get("output_layer_kernel"),
                    # goal_shape = config.get("goal_shape", None)
                    optimizer_params = config.get("optimizer_params"),
                    scheduler_params = config.get("scheduler_params", None),
                    device = config.get("device")
                    )

        # Load weights if True
        if load_weights:
            model.load_state_dict(T.load(model_path))

        return model


class CriticModel(Model):
    def __init__(self,
                 env: EnvWrapper,
                 state_layers: List[Dict],
                 merged_layers: List[Dict],
                 output_layer_kernel: [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}],
                #  goal_shape: tuple=None,
                 optimizer_params: dict={'type':'Adam', 'params':{'lr':0.001}},
                 scheduler_params: dict=None,
                 device: str=None
                 ):
        super().__init__(env, state_layers, optimizer_params, scheduler_params, device)
        self.env = env
        # self.state_config = state_layers # Stored as layer config in parent
        self.merged_config = merged_layers
        self.output_config = output_layer_kernel
        # self.goal_shape = goal_shape

        # instantiate ModuleDicts for merged and Modules
        self.merged_layers = nn.ModuleDict()
        # self.output_layer = nn.ModuleDict()

        # set internal attributes
        for i, layer_info in enumerate(self.merged_config):
            layer_type = layer_info['type']
            layer_params = layer_info.get('params', {})
            self.merged_layers[f'{layer_type}_{i}'] = self._build_layer(layer_type, layer_params)
            
        # add module to model
        # self.add_module('merged_layers', self.merged_layers)

        # Create the output layer
        self.output_layer = nn.ModuleDict({
            'state_action_value': nn.LazyLinear(1)
        })
        # self.add_module('critic_output_layer', self.output_layer)

         # Move the model to the specified device
        self.to(self.device)

        # initialize params
        self._init_model(self.layers, self.layer_config)
        self._init_model(self.merged_layers, self.merged_config)
        self._init_model(self.output_layer, self.output_config)

    def forward(self, state, action, goal=None):
        state = state.to(self.device)
        action = action.to(self.device)
        #DEBUG
        # print(f'critic state input shape:{state.size()}')
        # print(f'critic action input shape:{action.size()}')
        if goal is not None:
            goal = goal.to(self.device)
            state = T.cat([state, goal], dim=-1)

        # if self.goal_shape is not None:
            # state = T.cat([state, goal], dim=-1)

        for layer in self.layers.values():
            state = layer(state)
            #DEBUG
            # print(f'critic {layer} output shape:{state.size()}')

        merged = T.cat([state, action], dim=-1)
        #DEBUG
        # print(f'critic merged shape:{merged.size()}')
        for layer in self.merged_layers.values():
            merged = layer(merged)
            #DEBUG
            # print(f'critic {layer} output shape:{merged.size()}')

        for layer in self.output_layer.values():
            output = layer(merged)
            #DEBUG
            # print(f'critic {layer} output shape:{output.size()}')
        
        return output

    def get_config(self):
        config = {
            "env": self.env.to_json(),
            'num_layers': len(self.layers) + len(self.merged_layers),
            'state_layers': self.layer_config,
            'merged_layers': self.merged_config,
            'output_layer_kernel': self.output_config,
            # 'goal_shape': self.goal_shape,
            'optimizer_params': self.optimizer_params,
            'scheduler_params': self.scheduler_params,
            'device': self.device.type,
        }

        return config
    
    def get_clone(self, weights=True):
        # Reconstruct the model from its configuration
        env = GymnasiumWrapper(self.env.env_spec)
        cloned_model = CriticModel(
            env=env,
            state_layers=self.layer_config.copy(),
            merged_layers=self.merged_config.copy(),
            output_layer_kernel=self.output_config.copy(),
            # goal_shape=self.goal_shape.copy(),
            optimizer_params=self.optimizer_params.copy(),
            scheduler_params=self.scheduler_params.copy() if self.scheduler_params else None,
            device=self.device.type
        )
        
        if weights:
            # Copy the model weights
            cloned_model.load_state_dict(self.state_dict())
            
            # # Optionally, clone the optimizer (requires more manual work, shown below)
            # cloned_optimizer = type(self.optimizer)(cloned_model.parameters(), **self.optimizer.defaults)
            # cloned_optimizer.load_state_dict(self.optimizer.state_dict())

        return cloned_model


    def save(self, folder):
        """
        Save the model to the specified folder.

        Args:
            folder (str): Path to the directory where the model should be saved.
        """
        model_dir = Path(folder) / "critic_model"
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save the model parameters
        T.save(self.state_dict(), model_dir / 'pytorch_model.onnx')
        T.save(self.state_dict(), model_dir / 'pytorch_model.pt')

        config = self.get_config()

        with open(model_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)


    @classmethod
    def load(cls, config_path, load_weights=True):
        """
        Load a critic model from a saved configuration.

        Args:
            config_path (str): Path to the configuration file.
            load_weights (bool): Whether to load the model weights (default: True).

        Returns:
            CriticModel: Loaded critic model instance.
        """
        model_dir = Path(config_path) / "critic_model"
        config_path = model_dir / "config.json"
        model_path = model_dir / 'pytorch_model.onnx'

        if config_path.is_file():
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            raise FileNotFoundError(f"No configuration file found in {config_path}")
        
        env = EnvWrapper.from_json(config.get("env"))

        model = cls(env = env,
                    state_layers = config.get("state_layers"),
                    merged_layers = config.get("merged_layers"),
                    output_layer_kernel = config.get("output_layer_kernel"),
                    # goal_shape = config.get("goal_shape", None)
                    optimizer_params = config.get("optimizer_params"),
                    scheduler_params = config.get("scheduler_params", None),
                    device = config.get("device")
                    )

        # Load weights if True
        if load_weights:
            model.load_state_dict(T.load(model_path))

        return model


def build_layers(types: List[str], units_per_layer: List[int], initializers: List[str], kernel_params:List[dict]):
    """formats config into policy and value layers"""
    # get policy layers
    layers = []
    for type, units, kernel, k_param in zip(types, units_per_layer, initializers, kernel_params):
        layers.append({
            'type':type, 
            'params':{
                'units': units,
                'kernel': kernel,
                'kernel params': k_param
            }
        })
        
    return layers

def select_policy_model(env):
    """
    Select the appropriate policy model based on the environment's action space.

    Args:
        env (gym.Env): The environment object.

    Returns:
        Class: The class of the appropriate policy model.
    """
    #DEBUG
    # print(f'env action space type:{env.action_space}')
    # print(f'env observation space:{env.observation_space.shape}')
    # Check if the action space is discrete
    if isinstance(env.action_space, gym.spaces.Discrete) or isinstance(env.action_space, gym.spaces.MultiDiscrete):
        model_class = StochasticDiscretePolicy
    # Check if the action space is continuous
    elif isinstance(env.action_space, gym.spaces.Box):
        model_class = StochasticContinuousPolicy
    else:
        raise ValueError("Unsupported action space type. Only Discrete and Box spaces are supported.")
    return model_class

```

---

## mpi_helper.py

```python
from mpi4py import MPI
from logging_config import logger

# def set_comm_name(comm, name):
#     try:
#         keyval = MPI.Comm.Create_keyval()
#         comm.Set_attr(keyval, name)
#         return keyval
#     except Exception as e:
#         logger.error(f"Error in set_comm_name; {e}", exc_info=True)
#         return None

# def get_comm_name(comm, keyval):
#     try:
#         return comm.Get_attr(keyval)
#     except Exception as e:
#         logger.error(f"Error in get_comm_name; {e}", exc_info=True)
#         return None
    
def set_group_size(comm, num_groups):
    try:
        size = comm.Get_size()
        group_size = size // num_groups
        return group_size
    except Exception as e:
        logger.error(f"Error in set_group_size; {e}", exc_info=True)
        return None
    
def set_group(comm, group_size):
    try:
        rank = comm.Get_rank()
        group = rank // group_size
        return group
    except Exception as e:
        logger.error(f"Error in set_group_size; {e}", exc_info=True)
        return None
```

---

## noise.py

```python
import torch as T
from torch.distributions import uniform, normal
import numpy as np
from torch_utils import get_device

class Noise:
    """
    Base class for noise processes.
    """

    def __init__(self, device=None):
        self.device = get_device(device)

    def __call__(self, shape):
        """
        Generate noise based on the specific implementation.

        Args:
            shape (tuple): Shape of the noise to generate.
        """
        pass

    def reset(self):
        """
        Reset the noise process (if applicable).
        """
        pass

    def get_config(self) -> dict:
        """
        Retrieve the configuration of the noise process.

        Returns:
            dict: Configuration details.
        """
        pass

    def clone(self):
        """
        Clone the noise process.

        Returns:
            Noise: A new instance of the same noise process.
        """
        pass

    @classmethod
    def create_instance(cls, noise_class_name: str, **kwargs) -> 'Noise':
        """
        Creates an instance of the requested noise class.

        Args:
            noise_class_name (str): Name of the noise class to instantiate.
            kwargs: Parameters for the noise class.

        Returns:
            Noise: An instance of the requested noise class.

        Raises:
            ValueError: If the noise class is not recognized.
        """
        noise_classes = {
            "Ornstein-Uhlenbeck": OUNoise,
            "OUNoise": OUNoise,
            "Normal": NormalNoise,
            "NormalNoise": NormalNoise,
            "Uniform": UniformNoise,
            "UniformNoise": UniformNoise,
        }

        if noise_class_name in noise_classes:
            return noise_classes[noise_class_name](**kwargs)
        else:
            raise ValueError(f"{noise_class_name} is not a recognized noise class")

class UniformNoise(Noise):
    """
    Uniform noise generator.
    """
    def __init__(self, shape, minval=0, maxval=1, device=None):
        super().__init__(device)
        self.shape = shape
        # self.device = T.device("cuda" if device == 'cuda' and T.cuda.is_available() else "cpu")
        self.minval = T.tensor(minval, device=self.device)
        self.maxval = T.tensor(maxval, device=self.device)
        
        self.noise_gen = uniform.Uniform(low=self.minval, high=self.maxval)

    def __call__(self) -> T.Tensor:
        """
        Generate uniform noise.

        Returns:
            T.Tensor: Generated noise.
        """
        return self.noise_gen.sample(self.shape)

    def get_config(self) -> dict:
        """
        Retrieve the configuration of the UniformNoise.

        Returns:
            dict: Configuration details.
        """
        return {
            'class_name': 'UniformNoise',
            'config': {
                'shape': self.shape,
                'minval': self.minval.item(),
                'maxval': self.maxval.item(),
                'device': self.device.type,
            }
        }
    
    def clone(self) -> 'UniformNoise':
        """
        Clone the UniformNoise instance.

        Returns:
            UniformNoise: A new instance with the same configuration.
        """
        return UniformNoise(self.shape, self.minval.item(), self.maxval.item(), self.device)

class NormalNoise(Noise):
    """
    Normal (Gaussian) noise generator.
    """
    def __init__(self, shape, mean=0.0, stddev=1.0, device=None):
        super().__init__(device)
        self.shape = shape
        # self.device = T.device("cuda" if device == 'cuda' and T.cuda.is_available() else "cpu")
        self.mean = T.tensor(mean, dtype=T.float32, device=self.device)
        self.stddev = T.tensor(stddev, dtype=T.float32, device=self.device)
        self.reset_noise_gen()

    def reset_noise_gen(self) -> None:
        """
        Reset the noise generator to the original mean and standard deviation.
        """
        self.noise_gen = normal.Normal(loc=self.mean, scale=self.stddev)

    def __call__(self) -> T.Tensor:
        """
        Generate normal noise.

        Returns:
            T.Tensor: Generated noise.
        """
        return self.noise_gen.sample(self.shape)

    def __getstate__(self):
        # Only the numpy arrays are serialized
        state = self.__dict__.copy()
        # Remove the noise generator since it can't be pickled
        del state['noise_gen']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        # Recreate the noise generator after deserialization
        self.reset_noise_gen()

    def get_config(self) -> dict:
        """
        Retrieve the configuration of the NormalNoise.

        Returns:
            dict: Configuration details.
        """
        return {
            'class_name': 'NormalNoise',
            'config': {
                'shape': self.shape,
                'mean': self.mean.item(),
                'stddev': self.stddev.item(),
                'device': self.device.type,
            }
        }
    
    def clone(self) -> 'NormalNoise':
        """
        Clone the NormalNoise instance.

        Returns:
            NormalNoise: A new instance with the same configuration.
        """
        return NormalNoise(self.shape, self.mean.item(), self.stddev.item(), self.device)
    
class OUNoise(Noise):
    """
    Ornstein-Uhlenbeck noise process.

    Commonly used in reinforcement learning for exploration in continuous action spaces.
    """

    def __init__(self, shape: tuple, mean: float = 0.0, theta: float = 0.15, sigma: float = 0.2, dt: float = 1e-2, device=None):
        super().__init__(device)
        # self.device = T.device("cuda" if device == 'cuda' and T.cuda.is_available() else "cpu")
        self.shape = shape
        self.mean = T.tensor(mean, device=self.device)
        self.mu = T.ones(self.shape, device=self.device) * self.mean
        self.theta = T.tensor(theta, device=self.device)
        self.sigma = T.tensor(sigma, device=self.device)
        self.dt = T.tensor(dt, device=self.device)
        self.reset()

    def __call__(self) -> T.Tensor:
        """
        Generate Ornstein-Uhlenbeck noise.

        Returns:
            T.Tensor: Generated noise.
        """
        dx = self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * T.randn(self.shape, device=self.device)
        x = self.x_prev + dx
        self.x_prev = x
        return x

    def reset(self, mu: float = None) -> None:
        """
        Reset the noise process to its initial state.

        Args:
            mu (float, optional): New mean value. Defaults to the original mean.
        """
        # self.mu = T.ones(self.shape, device=self.device) * self.mean if mu is None else T.tensor(mu, device=self.device)
        # self.x_prev = T.ones(self.shape, device=self.device) * self.mu
        self.x_prev = T.ones(self.shape, device=self.device) * (mu if mu is not None else self.mean)

    def get_config(self) -> dict:
        """
        Retrieve the configuration of the OUNoise.

        Returns:
            dict: Configuration details.
        """
        return {
            'class_name': 'OUNoise',
            'config': {
                "shape": self.shape,
                "mean": self.mean.item(),
                "theta": self.theta.item(),
                "sigma": self.sigma.item(),
                "dt": self.dt.item(),
                'device': self.device.type,
            }
        }
        
    def clone(self) -> 'OUNoise':
        """
        Clone the OUNoise instance.

        Returns:
            OUNoise: A new instance with the same configuration.
        """
        return OUNoise(self.shape, self.mean.item(), self.theta.item(), self.sigma.item(), self.dt.item(), self.device)
```

---

## normalizer.py

```python
import torch as T
import numpy as np
from torch_utils import get_device

class Normalizer:
    """
    Normalizes data using running statistics (mean and standard deviation).

    Attributes:
        size (int): Size of the input data to normalize.
        eps (float): Small constant to prevent division by zero.
        clip_range (float): Range to clip normalized values.
        device (str): Device to run the normalizer on ('cpu' or 'cuda').
    """
    def __init__(self, size: int, eps: float = 1e-2, clip_range: float = 5.0, device: str = None):
        self.size = size
        self.device = get_device(device)
        self.eps = T.tensor(eps, device=self.device)
        self.clip_range = T.tensor(clip_range, device=self.device)

        # Local statistics
        self.local_sum = T.zeros(self.size, dtype=T.float32, device=self.device)
        self.local_sum_sq = T.zeros(self.size, dtype=T.float32, device=self.device)
        self.local_cnt = T.zeros(1, dtype=T.int32, device=self.device)

        # Running statistics
        self.running_mean = T.zeros(self.size, dtype=T.float32, device=self.device)
        self.running_std = T.ones(self.size, dtype=T.float32, device=self.device)
        self.running_sum = T.zeros(self.size, dtype=T.float32, device=self.device)
        self.running_sum_sq = T.zeros(self.size, dtype=T.float32, device=self.device)
        self.running_cnt = T.zeros(1, dtype=T.int32, device=self.device)

    def normalize(self, v: T.Tensor) -> T.Tensor:
        """
        Normalize a tensor using running statistics.

        Args:
            v (T.Tensor): Input tensor to normalize.

        Returns:
            T.Tensor: Normalized tensor.
        """
        # Ensure input tensor is on the same device as normalizer
        if v.device != self.device:
            v = v.to(self.device)
        
        return T.clamp((v - self.running_mean) / self.running_std,
                       -self.clip_range, self.clip_range).float()

    def denormalize(self, v: T.Tensor) -> T.Tensor:
        """
        Denormalize a tensor using running statistics.

        Args:
            v (T.Tensor): Input tensor to denormalize.

        Returns:
            T.Tensor: Denormalized tensor.
        """
        return (v * self.running_std) + self.running_mean
    
    def update_local_stats(self, new_data: T.Tensor) -> None:
        """
        Update local statistics with new data.

        Args:
            new_data (T.Tensor): New data to update local statistics.
        """
        try:
            self.local_sum += new_data.sum(dim=0).to(self.device)
            self.local_sum_sq += (new_data**2).sum(dim=0).to(self.device)
            self.local_cnt += new_data.size(0)
        except Exception as e:
            print(f"Error during local stats update: {e}")
    
    def update_global_stats(self) -> None:
        """
        Update running statistics based on local statistics.
        """

        self.running_cnt += self.local_cnt
        self.running_sum += self.local_sum
        self.running_sum_sq += self.local_sum_sq

        self.local_cnt.zero_()
        self.local_sum.zero_()
        self.local_sum_sq.zero_()

        # Ensure all calculations remain on the correct device
        self.running_mean = (self.running_sum / self.running_cnt).to(self.device)
        tmp = (self.running_sum_sq / self.running_cnt - (self.running_sum / self.running_cnt)**2).to(self.device)
        eps_squared = self.eps**2
        self.running_std = T.sqrt(T.maximum(eps_squared, tmp)).to(self.device)

    def get_config(self) -> dict:
        """
        Retrieve the configuration and state of the normalizer.

        Returns:
            dict: Configuration and state of the normalizer.
        """
        return {
            "params":{
                'size':self.size,
                'eps':self.eps,
                'clip_range':self.clip_range,
            },
            "state":{
                'local_sum':self.local_sum.cpu().numpy(),
                'local_sum_sq':self.local_sum_sq.cpu().numpy(),
                'local_cnt':self.local_cnt.cpu().numpy(),
                'running_mean':self.running_mean.cpu().numpy(),
                'running_std':self.running_std.cpu().numpy(),
                'running_sum':self.running_sum.cpu().numpy(),
                'running_sum_sq':self.running_sum_sq.cpu().numpy(),
                'running_cnt':self.running_cnt.cpu().numpy(),
            },
        }

    def save_state(self, file_path: str) -> None:
        """
        Save the current state of the normalizer to a file.

        Args:
            file_path (str): Path to save the state.
        """
        T.save({
            'local_sum': self.local_sum,
            'local_sum_sq': self.local_sum_sq,
            'local_cnt': self.local_cnt,
            'running_mean': self.running_mean,
            'running_std': self.running_std,
            'running_sum': self.running_sum,
            'running_sum_sq': self.running_sum_sq,
            'running_cnt': self.running_cnt,
        }, file_path)

    @classmethod
    def load_state(cls, file_path: str, device: str = 'cpu') -> 'Normalizer':
        """
        Load a normalizer's state from a file.

        Args:
            file_path (str): Path to load the state from.
            device (str): Device to load the state to ('cpu' or 'cuda').

        Returns:
            Normalizer: A Normalizer instance with the loaded state.
        """
        state = T.load(file_path)
        normalizer = cls(size=state['running_mean'].shape[0], device=device)
        target_device = normalizer.device
        
        # Ensure all loaded tensors are moved to the correct device
        normalizer.local_sum = state['local_sum'].to(target_device)
        normalizer.local_sum_sq = state['local_sum_sq'].to(target_device)
        normalizer.local_cnt = state['local_cnt'].to(target_device)
        normalizer.running_mean = state['running_mean'].to(target_device)
        normalizer.running_std = state['running_std'].to(target_device)
        normalizer.running_sum = state['running_sum'].to(target_device)
        normalizer.running_sum_sq = state['running_sum_sq'].to(target_device)
        normalizer.running_cnt = state['running_cnt'].to(target_device)
        
        return normalizer

    
class SharedNormalizer:
    def __init__(self, size, eps=1e-2, clip_range=5.0):
        self.size = size
        self.eps = eps
        self.clip_range = clip_range

        # self.lock = manager.Lock()
        self.lock = threading.Lock()

        # Create shared memory blocks
        total_byte_size = np.prod(self.size) * np.float32().itemsize
        self.shared_local_sum = shared_memory.SharedMemory(create=True, size=total_byte_size)
        self.shared_local_sum_sq = shared_memory.SharedMemory(create=True, size=total_byte_size)
        self.shared_local_cnt = shared_memory.SharedMemory(create=True, size=np.float32().itemsize)

        self.local_sum = np.ndarray(self.size, dtype=np.float32, buffer=self.shared_local_sum.buf)
        self.local_sum_sq = np.ndarray(self.size, dtype=np.float32, buffer=self.shared_local_sum_sq.buf)
        self.local_cnt = np.ndarray(1, dtype=np.int32, buffer=self.shared_local_cnt.buf)

        # Initiate shared arrays to zero
        self.local_sum.fill(0)
        self.local_sum_sq.fill(0)
        self.local_cnt.fill(0)

        self.running_mean = np.zeros(self.size, dtype=np.float32)
        self.running_std = np.ones(self.size, dtype=np.float32)
        self.running_sum = np.zeros(self.size, dtype=np.float32)
        self.running_sum_sq = np.zeros(self.size, dtype=np.float32)
        self.running_cnt = np.zeros(1, dtype=np.int32)

    def normalize(self, v):
        clip_range = self.clip_range
        return np.clip((v - self.running_mean) / self.running_std,
                       -clip_range, clip_range).astype(np.float32)
    
    def update_local_stats(self, new_data):
        # print('SharedNormalizer update_local_stats fired...')
        try:
            with self.lock:
                # print('SharedNormalizer update_local_stats lock acquired...')
                # print(f'data: {new_data}')
                # print('previous local stats')
                # print(f'local sum: {self.local_sum}')
                # print(f'local sum sq: {self.local_sum_sq}')
                # print(f'local_cnt: {self.local_cnt}')
                self.local_sum += new_data#.sum(axis=1)
                self.local_sum_sq += (np.square(new_data))#.sum(axis=1)
                self.local_cnt += 1 #new_data.shape[0]
                # print('new local values')
                # print(f'local sum: {self.local_sum}')
                # print(f'local sum sq: {self.local_sum_sq}')
                # print(f'local_cnt: {self.local_cnt}')
        except Exception as e:
            print(f"Error during update: {e}")
    
    def update_global_stats(self):
        with self.lock:
            # make copies of local stats
            local_cnt = self.local_cnt.copy()
            local_sum = self.local_sum.copy()
            local_sum_sq = self.local_sum_sq.copy()
            
            # Zero out local stats
            self.local_cnt[...] = 0
            self.local_sum[...] = 0
            self.local_sum_sq[...] = 0
            
            # Add local stats to global stats
            self.running_cnt += local_cnt
            self.running_sum += local_sum
            self.running_sum_sq += local_sum_sq

            # Calculate new mean, sum_sq, and std
            self.running_mean = self.running_sum / self.running_cnt
            tmp = self.running_sum_sq / self.running_cnt -\
                np.square(self.running_sum / self.running_cnt)
            self.running_std = np.sqrt(np.maximum(np.square(self.eps), tmp))

    def get_config(self):
        return {
            "params":{
                'size':self.size,
                'eps':self.eps,
                'clip_range':self.clip_range,
            },
            "state":{
                'local_sum':self.local_sum,
                'local_sum_sq':self.local_sum_sq,
                'local_cnt':self.local_cnt,
                'running_mean':self.running_mean,
                'running_std':self.running_std,
                'running_sum':self.running_sum,
                'running_sum_sq':self.running_sum_sq,
                'running_cnt':self.running_cnt,
            },
        }


    def save_state(self, file_path):
        np.savez(
            file_path,
            local_sum=self.local_sum,
            local_sum_sq=self.local_sum_sq,
            local_cnt=self.local_cnt,
            running_mean=self.running_mean,
            running_std=self.running_std,
            running_sum=self.running_sum,
            running_sum_sq=self.running_sum_sq,
            running_cnt=self.running_cnt,
        )

    def cleanup(self):
        # Close and unlink shared memory blocks
        try:
            if self.shared_local_sum:
                self.shared_local_sum.unlink()
                self.shared_local_sum.close()
                self.shared_local_sum = None
        except FileNotFoundError as e:
            print(f"Shared local sum already cleaned up: {e}")
        try:
            if self.shared_local_sum_sq:
                self.shared_local_sum_sq.unlink()
                self.shared_local_sum_sq.close()
                self.shared_local_sum_sq = None
        except FileNotFoundError as e:
            print(f"Shared local sum sq already cleaned up: {e}")
        try:
            if self.shared_local_cnt:
                self.shared_local_cnt.unlink()
                self.shared_local_cnt.close()
                self.shared_local_cnt = None
        except FileNotFoundError as e:
            print(f"Shared local sum cnt already cleaned up: {e}")

        print("SharedNormalizer resources have been cleaned up.")

    def __del__(self):
        self.cleanup()


    @classmethod
    def load_state(cls, file_path):
        with np.load(file_path) as data:
            normalizer = cls(size=data['running_mean'].shape)
            normalizer.local_sum = data['local_sum']
            normalizer.local_sum_sq = data['local_sum_sq']
            normalizer.local_cnt = data['local_cnt']
            normalizer.running_mean = data['running_mean']
            normalizer.running_std = data['running_std']
            normalizer.running_sum = data['running_sum']
            normalizer.running_sum_sq = data['running_sum_sq']
            normalizer.running_cnt = data['running_cnt']
        return normalizer
```

---

## rl_agents.py

```python
"""This module holds the Agent base class and all RL agents as subclasses  It also 
provides helper functions for loading any subclass of type Agent."""

# imports
import json
import os
from typing import Optional
from pathlib import Path
import time
from collections import deque
from logging_config import logger
import copy
from encoder import CustomJSONEncoder, serialize_env_spec
from moviepy.editor import ImageSequenceClip
from umap import UMAP
import plotly.express as px

from rl_callbacks import WandbCallback, Callback
from rl_callbacks import load as callback_load
from models import select_policy_model, StochasticContinuousPolicy, StochasticDiscretePolicy, ValueModel, CriticModel, ActorModel
from schedulers import ScheduleWrapper
from adaptive_kl import AdaptiveKL
from buffer import Buffer, ReplayBuffer, PrioritizedReplayBuffer, Buffer
from normalizer import Normalizer, SharedNormalizer
from noise import Noise, NormalNoise, UniformNoise, OUNoise
import wandb
import wandb_support
from torch_utils import set_seed, get_device, VarianceScaling_
# import dash_callbacks
# import gym_helper
from env_wrapper import EnvWrapper, GymnasiumWrapper, IsaacSimWrapper
from utils import render_video, build_env_wrapper_obj, check_for_inf_or_NaN

import torch as T
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical, Beta, Normal, kl_divergence
from torch.multiprocessing import spawn, Manager
import gymnasium as gym
import gymnasium_robotics as gym_robo
from gymnasium.envs.registration import EnvSpec
import numpy as np
import pandas as pd
import random
import torch.profiler


# Agent class
class Agent:
    """Base class for all RL agents."""

    def _initialize_callbacks(self, callbacks):
        """
        Initialize and configure callbacks for logging and monitoring.

        Args:
            callbacks (list): List of callback objects.
        """
        try:
            self.callbacks = callbacks
            if callbacks:
                for callback in self.callbacks:
                    self._config = callback._config(self)
                    if isinstance(callback, WandbCallback):
                        self._wandb = True
            else:
                self.callback_list = None
                self._wandb = False
        except Exception as e:
            logger.error(f"Error initializing callbacks: {e}", exc_info=True)

    def get_action(self, state):
        """Returns an action given a state."""

    def train(
        self, num_episodes, render: bool = False, render_freq: int = None, save_dir=None
    ):
        """Trains the model for 'episodes' number of episodes."""

    def learn(self):
        """Updates the model."""

    def test(self, num_episodes=None, render=False, render_freq=10):
        """Runs a test over 'num_episodes'."""

    def save(self):
        """Saves the model."""

    @classmethod
    def load(cls, folder: str = "models"):
        """Loads the model."""


class ActorCritic(Agent):
    """Actor Critic Agent."""

    def __init__(
        self,
        env: EnvWrapper,
        policy_model: StochasticDiscretePolicy,
        value_model: ValueModel,
        discount: float=0.99,
        policy_trace_decay: float=0.0,
        value_trace_decay: float=0.0,
        callbacks: Optional[list[Callback]] = None,
        save_dir: str = "models/",
        device: str = None,
    ):
        
        # Set the device
        self.device = get_device(device)

        self.env = env
        self.policy_model = policy_model
        self.value_model = value_model
        self.discount = discount
        self.policy_trace_decay = policy_trace_decay
        self.value_trace_decay = value_trace_decay
        # self.callbacks = callbacks
        # self.save_dir = save_dir
        if save_dir is not None and "/actor_critic/" not in save_dir:
            self.save_dir = save_dir + "/actor_critic/"
        elif save_dir is not None:
            self.save_dir = save_dir

        # Initialize callback configurations
        self._initialize_callbacks(callbacks)
        self._train_config = {}
        self._train_episode_config = {}
        self._train_step_config = {}
        self._test_config = {}
        self._test_step_config = {}
        self._test_episode_config = {}

        self._step = None
        # set self.action to None
        self.action = None
        # instantiate and set policy and value traces
        self.policy_trace = []
        self.value_trace = []
        self._set_traces()

    # def _initialize_callbacks(self, callbacks):
    #     """
    #     Initialize and configure callbacks for logging and monitoring.

    #     Args:
    #         callbacks (list): List of callback objects.
    #     """
    #     try:
    #         self.callbacks = callbacks
    #         if callbacks:
    #             for callback in self.callbacks:
    #                 self._config = callback._config(self)
    #                 if isinstance(callback, WandbCallback):
    #                     self._wandb = True
    #         else:
    #             self.callback_list = None
    #             self._wandb = False
    #     except Exception as e:
    #         logger.error(f"Error initializing callbacks: {e}", exc_info=True)

#     @classmethod
#     def build(
#         cls,
#         env,
#         policy_layers,
#         value_layers,
#         callbacks,
#         config,#: wandb.config,
#         save_dir: str = "models/",
#     ):
#         """Builds the agent."""
#         policy_optimizer = wandb.config.policy_optimizer
#         value_optimizer = wandb.config.value_optimizer
#         policy_learning_rate = wandb.config.learning_rate
#         value_learning_rate = wandb.config.learning_rate
#         policy_model = models.StochasticDiscretePolicy(
#             env, dense_layers=policy_layers, optimizer=policy_optimizer, learning_rate=policy_learning_rate
#         )
#         value_model = models.ValueModel(
#             env, dense_layers=value_layers, optimizer=value_optimizer, learning_rate=value_learning_rate
#         )

#         agent = cls(
#             env,
#             policy_model,
#             value_model,
#             config.discount,
#             config.policy_trace_decay,
#             config.value_trace_decay,
#             callbacks,
#             save_dir=save_dir,
#         )

#         agent.save()

#         return agent

    def _set_traces(self):
        for weights in self.policy_model.parameters():
            self.policy_trace.append(T.zeros_like(weights, device=self.device))
            #DEBUG
            # print(f'policy trace shape: {weights.size()}')

        for weights in self.value_model.parameters():
            self.value_trace.append(T.zeros_like(weights, device=self.device))
            #DEBUG
            # print(f'value trace shape: {weights.size()}')


    def _update_traces(self):
        with T.no_grad():
            for i, weights in enumerate(self.policy_model.parameters()):
                self.policy_trace[i] = (self.discount * self.policy_trace_decay * self.policy_trace[i]) + weights.grad

            for i, weights in enumerate(self.value_model.parameters()):
                self.value_trace[i] = (self.discount * self.value_trace_decay * self.value_trace[i]) + weights.grad

        # log to train step
        # for i, (v_trace, p_trace) in enumerate(zip(self.value_trace, self.policy_trace)):
            # self._train_step_config[f"value trace {i}"] = T.histc(v_trace, bins=20)
            # self._train_step_config[f"policy trace {i}"] = T.histc(p_trace, bins=20)

    def get_action(self, states):
        # state =  T.from_numpy(state).to(self.device)
        states = T.tensor(states, dtype=T.float32, device=self.policy_model.device)
        dist, logits = self.policy_model(states)
        actions = dist.sample()
        # log_probs = dist.log_prob(actions)
        
        actions = actions.detach().cpu().numpy() # Detach from graph, move to CPU and convert to numpy for Gym
        return actions, dist, logits

    def train(self, num_episodes, num_envs: int, seed: int | None = None, render_freq: int = 0):
        """Trains the model for 'episodes' number of episodes."""
        # set models to train mode
        self.policy_model.train()
        self.value_model.train()

        self.num_envs = num_envs

        if seed is None:
            seed = np.random.randint(100)

        # Set seeds
        set_seed(seed)

        if self.callbacks:
            for callback in self.callbacks:
                if isinstance(callback, WandbCallback):
                    self._config['num_episodes'] = num_episodes
                    self._config['seed'] = seed
                    self._config['num_envs'] = self.num_envs
                    callback.on_train_begin((self.policy_model, self.value_model,), logs=self._config)
                else:
                    callback.on_train_begin(logs=self._config)
        
        try:
            # instantiate new vec environment
            self.env.env = self.env._initialize_env(0, self.num_envs, seed)
        except Exception as e:
            logger.error(f"Error in ActorCritic.train self.env._initialize_env process: {e}", exc_info=True)

        # set step counter
        self._step = 0
        # Instantiate counter to keep track of number of episodes completed
        self.completed_episodes = 0
        # set best reward
        best_reward = -np.inf
        # Instantiate a deque to track last 10 scores for computing avg
        completed_scores = deque(maxlen=10)
        episode_scores = np.zeros(self.num_envs)
        states, _ = self.env.reset()
        
        while self.completed_episodes < num_episodes:
            # Increment step counter
            self._step += 1
            # if self.callbacks:
            #     for callback in self.callbacks:
            #         callback.on_train_epoch_begin(epoch=self._step, logs=None)
            
            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_step_begin(step=self._step, logs=None)
            
            actions, dist, logits = self.get_action(states)
            # actions, log_probs = self.get_action(states)
            actions = self.env.format_actions(actions)
            next_states, rewards, terms, truncs, _ = self.env.step(actions)
            self._train_step_config["step_reward"] = rewards.mean()
            # self._train_step_config["logits"] = wandb.Histogram(logits.detach().cpu().numpy())
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)
            self.learn(states, rewards, next_states, actions, dist, dones)
            # self.learn(states, rewards, next_states, actions, log_probs, dones)

            for i in range(self.num_envs):
                if dones[i]:
                    # Increment completed episodes
                    self.completed_episodes += 1
                    # Append episode score to completed scores deque
                    completed_scores.append(episode_scores[i])
                    self._train_episode_config["episode_reward"] = episode_scores[i]
                    self._train_episode_config["episode"] = self.completed_episodes
                    # Reset episode score
                    episode_scores[i] = 0
                    avg_reward = sum(completed_scores) / len(completed_scores)

                    # check if best reward
                    if avg_reward > best_reward:
                        best_reward = avg_reward
                        self._train_episode_config["best"] = 1
                        # save model
                        self.save()
                    else:
                        self._train_episode_config["best"] = 0

                    if self.callbacks:
                        for callback in self.callbacks:
                            callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)

                    # Check if number of completed episodes should trigger render
                    if render_freq > 0:
                        if self.completed_episodes % render_freq == 0 and not rendered:
                            print(f"Rendering episode {self.completed_episodes} during training...")
                            # Call the test function to render an episode
                            self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes}.mp4")
                            # Log the video to wandb
                            if self.callbacks:
                                for callback in self.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")}, step=self._step)
                            rendered = True
                            # Switch models back to train mode after rendering
                            self.policy_model.train()
                            self.value_model.train()
                        else:
                            rendered = False

                    print(f"episode {self.completed_episodes}, score {completed_scores[-1]}, avg_score {avg_reward}")

            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_step_end(step=self._step, logs=self._train_step_config)
            
            states = next_states

        if self.callbacks:
            for callback in self.callbacks:
                callback.on_train_end(logs=self._train_episode_config)

    def learn(self, states, rewards, next_states, actions, dist, dones):
        self.policy_model.optimizer.zero_grad()
        self.value_model.optimizer.zero_grad()

        states = T.tensor(states, dtype=T.float32, device=self.device)
        rewards = T.tensor(rewards, dtype=T.float32, device=self.device)
        next_states = T.tensor(next_states, dtype=T.float32, device=self.device)
        actions = T.tensor(actions, dtype=T.long, device=self.device)
        dones = T.tensor(dones, dtype=T.int, device=self.device)
        # Run states through policy model to get distribution and log probs of actions
        # dist, logits = self.policy_model(states)
        log_probs = dist.log_prob(actions)

        state_values = self.value_model(states)
        next_state_values = self.value_model(next_states).detach()
        temporal_difference = (
            rewards + self.discount * next_state_values.squeeze() * (1 - dones) - state_values.squeeze())
        value_loss = temporal_difference.square().mean()

        value_loss.backward()

        policy_loss = -(log_probs * temporal_difference.detach()).mean()
        policy_loss.backward()

        # total_loss = value_loss + policy_loss
        # total_loss.backward()

        self._update_traces()

        #DEBUG
        # print(f'dones:{dones}')
        # print(f'states size: {states.size()}')
        # print(f'next states size: {next_states.size()}')
        # print(f'rewards size: {rewards.size()}')
        # print(f'actions size: {actions.size()}')
        # print(f'dones size: {dones.size()}')
        # print(f'log_probs size: {log_probs.size()}')
        # print(f'state values size: {state_values.size()}')
        # print(f'squeezed state values:{state_values.squeeze().size()}')
        # print(f'squeezed next state values:{next_state_values.squeeze().size()}')
        # print(f'next state values size: {next_state_values.size()}')
        # print(f'temporal_difference size: {temporal_difference.size()}')
        # print(f'value loss size: {value_loss.size()}')
        # print(f'policy loss size: {policy_loss.size()}')

        #copy traces to weight gradients
        with T.no_grad():
            for i, weights in enumerate(self.policy_model.parameters()):
                weights.grad = self.policy_trace[i]

            for i, weights in enumerate(self.value_model.parameters()):
                weights.grad = self.value_trace[i]

        self.value_model.optimizer.step()
        self.policy_model.optimizer.step()

        
        self._train_step_config["policy_loss"] = policy_loss.item()
        self._train_step_config["value_loss"] = value_loss.item()
        self._train_step_config["temporal_difference"] = temporal_difference.mean()
        # self._train_step_config[f"logits"] = wandb.Histogram(logits.detach().cpu().numpy())
        self._train_step_config[f"actions"] = wandb.Histogram(actions.detach().cpu().numpy())
        self._train_step_config[f"log_probabilities"] = wandb.Histogram(log_probs.detach().cpu().numpy())
        self._train_step_config[f"action_probabilities"] = wandb.Histogram(dist.probs.detach().cpu().numpy())
        self._train_step_config["entropy"] = dist.entropy().mean().item()

    def test(self, num_episodes:int, num_envs: int=1, seed: int=None, render_freq: int=0, training: bool=False):
        """Runs a test over 'num_episodes'."""
        # Set models to eval mode
        self.policy_model.eval()
        self.value_model.eval()

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        if render_freq == None:
            render_freq = 0

        # Set seeds
        set_seed(seed)

        try:
            # instantiate new vec environment
            env = self.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error(f"Error in ActorCritic.test agent._initialize_env process: {e}", exc_info=True)

        if self.callbacks and not training:
            print('test begin callback if statement fired')
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    # Add to config to send to wandb for logging
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)
        completed_episodes = 0
        # Instantiate array to keep track of current episode scores
        episode_scores = np.zeros(num_envs)
        # Instantiate a deque to track last 'episodes_per_update' scores for computing avg
        completed_scores = deque(maxlen=num_episodes)
        # Instantiate list to keep track of frames for rendering
        frames = []
        states, _ = env.reset()
        _step = 0
            
        while completed_episodes < num_episodes:
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_begin(epoch=_step, logs=None)
            actions, _, _ = self.get_action(states)
            actions = self.env.format_actions(actions)
            next_states, rewards, terms, truncs, _ = env.step(actions)
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)

            for i in range(self.num_envs):
                if dones[i]:
                    completed_scores.append(episode_scores[i])
                    # Add the episode reward to the episode log for callbacks
                    self._test_episode_config["episode_reward"] = episode_scores[i]
                    # Reset the episode score of the env back to 0
                    episode_scores[i] = 0
                    # check if best reward
                    avg_reward = sum(completed_scores) / len(completed_scores)
                    # Increment completed_episodes counter
                    completed_episodes += 1
                    # Log completed episodes to callback episode config
                    self._test_episode_config["episode"] = completed_episodes
                    # Save the video if the episode number is divisible by render_freq
                    if (render_freq > 0) and ((completed_episodes) % render_freq == 0):
                        if training:
                            render_video(frames, self.completed_episodes, self.save_dir, 'train')
                        else:
                            render_video(frames, completed_episodes, self.save_dir, 'test')
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/test/episode_{completed_episodes}.mp4")
                            # Log the video to wandb
                            if self.callbacks:
                                for callback in self.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})
                        # Empty frames array
                        frames = []
                    # Signal to all callbacks that an episode (epoch) has completed and to log data
                    if self.callbacks and not training:
                        for callback in self.callbacks:
                            callback.on_test_epoch_end(
                            epoch=_step, logs=self._test_episode_config
                        )
                    if not training:
                        print(f"episode {completed_episodes}, score {completed_scores[-1]}, avg_score {avg_reward}")

            if render_freq > 0:
                # Capture the frame
                frame = env.render()[0]
                # print(f'frame:{frame}')
                frames.append(frame)

            states = next_states

            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_step_end(step=_step, logs=self._test_step_config)

        if self.callbacks and not training:
            for callback in self.callbacks:
                callback.on_test_end(logs=self._test_episode_config)

    def get_config(self):
        return {
            "agent_type": self.__class__.__name__,
            "env": self.env.to_json(),
            "policy_model": self.policy_model.get_config(),
            "value_model": self.value_model.get_config(),
            "discount": self.discount,
            "policy_trace_decay": self.policy_trace_decay,
            "value_trace_decay": self.value_trace_decay,
            "callbacks": [callback.get_config() for callback in self.callbacks] if self.callbacks else None,
            "save_dir": self.save_dir
        }


    def save(self):
        """Saves the model."""
        config = self.get_config()

        # makes directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)

        # writes and saves JSON file of actor critic agent config
        with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

        # saves policy and value model
        self.policy_model.save(self.save_dir)
        self.value_model.save(self.save_dir)

        # # if wandb callback, save wandb config
        # if self._wandb:
        #     for callback in self.callbacks:
        #         if isinstance(callback, rl_callbacks.WandbCallback):
        #             callback.save(self.save_dir + "/wandb_config.json")

    @classmethod
    def load(cls, config, load_weights=True):
        """Loads the model."""
        # Load EnvWrapper
        env_wrapper = EnvWrapper.from_json(config["env"])
        # load policy model
        policy_model = StochasticDiscretePolicy.load(config['save_dir'], load_weights)
        # load value model
        value_model = ValueModel.load(config['save_dir'], load_weights)
        # load callbacks
        callbacks = [callback_load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]\
                    if config['callbacks'] else None

        # return Actor-Critic agent
        agent = cls(
            env=env_wrapper,
            policy_model=policy_model,
            value_model=value_model,
            discount=config["discount"],
            policy_trace_decay=config["policy_trace_decay"],
            value_trace_decay=config["value_trace_decay"],
            callbacks=callbacks,
            save_dir=config["save_dir"],
        )

        return agent


class Reinforce(Agent):
    def __init__(
        self,
        env: EnvWrapper,
        policy_model: StochasticDiscretePolicy,
        value_model: Optional[ValueModel] = None,
        discount: float = 0.99,
        callbacks: Optional[list[Callback]] = None,
        save_dir: str = "models",
        device: str = None,
    ):
        # Set the device
        self.device = get_device(device)

        self.env = env
        self.policy_model = policy_model
        self.value_model = value_model
        self.discount = discount
        # self.callbacks = callbacks
        # self.save_dir = save_dir
        if save_dir is not None and "/reinforce/" not in save_dir:
            self.save_dir = save_dir + "/reinforce/"
        elif save_dir is not None:
            self.save_dir = save_dir

        self._step = None
        self._cur_learning_steps = None
            
        # Initialize callback configurations
        self._initialize_callbacks(callbacks)
        self._train_config = {}
        self._train_episode_config = {}
        self._train_step_config = {}
        self._test_config = {}
        self._test_step_config = {}
        self._test_episode_config = {}

    def _initialize_callbacks(self, callbacks):
        """
        Initialize and configure callbacks for logging and monitoring.

        Args:
            callbacks (list): List of callback objects.
        """
        try:
            self.callbacks = callbacks
            if callbacks:
                for callback in self.callbacks:
                    self._config = callback._config(self)
                    if isinstance(callback, WandbCallback):
                        self._wandb = True
            else:
                self.callback_list = None
                self._wandb = False
        except Exception as e:
            logger.error(f"Error initializing callbacks: {e}", exc_info=True)

    # @classmethod
    # def build(
    #     cls,
    #     env,
    #     policy_layers,
    #     value_layers,
    #     callbacks,
    #     config,#: wandb.config,
    #     save_dir: str = "models/",
    # ):
    #     """Builds the agent."""
    #     policy_optimizer = config.policy_optimizer
    #     value_optimizer = config.value_optimizer
    #     policy_model = StochasticDiscretePolicy(
    #         env, dense_layers=policy_layers, optimizer=policy_optimizer, learning_rate=config.learning_rate
    #     )
    #     value_model = ValueModel(
    #         env, dense_layers=value_layers, optimizer=value_optimizer, learning_rate=config.learning_rate
    #     )

    #     agent = cls(
    #         env,
    #         policy_model,
    #         value_model,
    #         config.discount,
    #         callbacks,
    #         save_dir=save_dir,
    #     )

    #     agent.save()

    #     return agent

    def get_return(self, trajectories):
        """Compute expected returns per timestep for each trajectory."""

        for trajectory in trajectories:
            _return = 0.0
            for step in reversed(trajectory):
                _return = step["reward"] + self.discount * _return
                step["return"] = _return

        return trajectories
    
    def get_action(self, state):
        # with T.no_grad():
        state = T.tensor(state, dtype=T.float32, device=self.policy_model.device)
        dist, logits = self.policy_model(state)
        actions = dist.sample()
        log_probs = dist.log_prob(actions)

        # Reshape if flat
        # if actions.dim() == 1:  # Adjust for scalar action
        #     actions = actions.unsqueeze(0)

        actions = actions.detach().cpu().numpy()
        # log_probs = log_probs.detach().cpu().numpy() # Keep as tensor for gradient
        
        
        
        return actions, log_probs

    def learn(self, trajectories):
        # Clear gradients
        self.policy_model.optimizer.zero_grad()
        self.value_model.optimizer.zero_grad()

        # Get returns for each step in all trajectories
        trajectories = self.get_return(trajectories)
            
        # Flatten trajectories across all envs
        all_steps = [step for trajectory in trajectories for step in trajectory]
        # DEBUG
        # print(f'all_steps:{all_steps}')
        # print(f'length of all_steps:{len(all_steps)}')
        # Extract states, returns, and log probs from trajectories
        all_states = [step["state"] for step in all_steps]
        all_returns = [step["return"] for step in all_steps]
        all_actions = [step["action"] for step in all_steps]
        # all_log_probs = [step["log_prob"] for step in all_steps]
        # Convert to tensors and store on correct device
        states = T.tensor(all_states, dtype=T.float32, device=self.device)
        returns = T.tensor(all_returns, dtype=T.float32, device=self.device)
        actions = T.tensor(all_actions, dtype=T.long, device=self.device)
        # log_probs = torch.stack(all_log_probs, dim=0)
        # Compute log probs by passing states through policy model, re-computing the
        # computational graph only using the log_probs for the batch update (detaches log_probs from
        # all envs in the env vector)
        dist, logits = self.policy_model(states)
        log_probs = dist.log_prob(actions)
        #DEBUG
        print(f'states shape:{states.size()}')
        # print(f'returns shape:{returns.size()}')
        # print(f'log probs shape:{log_probs.size()}')
        # print(f'log probs mean:{log_probs.mean()}')
        

        # Insert dimension at end of states if flat
        if states.dim() == 1:
            states = states.unsqueeze(-1)
            #DEBUG
            print(f'new state shape:{states.size()}')

        # Get state values and calculate value loss
        if self.value_model:
            state_values = self.value_model(states)
            advantages = returns - state_values.squeeze(-1)
            value_loss = (advantages ** 2).mean()
        else:
            advantages = returns
            value_loss = 0
        #DEBUG
        # print(f'advantages shape:{advantages.size()}')
        
        # Get policy loss
        policy_loss = -(log_probs * advantages.detach()).mean()

        # Calculate gradients
        total_loss = policy_loss + value_loss
        total_loss.backward()
        # if self.value_model:
        #     value_loss.backward()

        # Update weights
        self.policy_model.optimizer.step()
        if self.value_model:
            self.value_model.optimizer.step()

        # log the metrics for callbacks
        # if self._wandb:
        self._train_step_config["advantages"] = advantages.mean()
        self._train_step_config["policy_loss"] = policy_loss.item()
        self._train_step_config["value_loss"] = value_loss.item()
        self._train_step_config[f"logits"] = wandb.Histogram(logits.detach().cpu().numpy())
        self._train_step_config[f"actions"] = wandb.Histogram(actions.detach().cpu().numpy())
        self._train_step_config[f"log_probabilities"] = wandb.Histogram(log_probs.detach().cpu().numpy())
        self._train_step_config[f"action_probabilities"] = wandb.Histogram(dist.probs.detach().cpu().numpy())
        self._train_step_config["entropy"] = dist.entropy().mean().item()

    def train(self, num_episodes: int, num_envs: int, trajectories_per_update: int=10, seed: int | None = None, render_freq: int = 0):
        """Trains the model for 'episodes' number of episodes."""

        # set models to train mode
        self.policy_model.train()
        self.value_model.train()

        # set num_envs as attribute
        self.num_envs = num_envs

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        # if render_freq == None:
        #     render_freq = 0

        # Set seeds
        set_seed(seed)

        if self.callbacks:
            for callback in self.callbacks:
                if isinstance(callback, WandbCallback):
                    self._config['num_episodes'] = num_episodes
                    self._config['seed'] = seed
                    self._config['num_envs'] = self.num_envs
                    callback.on_train_begin((self.policy_model, self.value_model,), logs=self._config)
        
        try:
            # instantiate new vec environment
            self.env.env = self.env._initialize_env(0, self.num_envs, seed)
        except Exception as e:
            logger.error(f"Error in Reinforce.train self.env._initialize_env process: {e}", exc_info=True)

        # set step counter
        self._step = 0
        # set current learning steps
        # self._cur_learning_steps = []
        # Instantiate counter to keep track of number of episodes completed
        self.completed_episodes = 0
        # Instantiate array to keep track of current episode scores
        episode_scores = np.zeros(self.num_envs)
        # instantiate 
        # set best reward
        best_reward = -np.inf
        # Instantiate a deque to track last 10 scores for computing avg
        completed_scores = deque(maxlen=10)
        # Instantiate a list of num_envs lists to store trajectories
        episode_trajectories = [[] for _ in range(self.num_envs)]
        # Instantiate a list to store completed trajectories
        completed_trajectories = []
        # Reset all envs to get initial states
        states, _ = self.env.reset()
        # Set rendered flag to trigger episode rendering during training
        rendered = False
        while self.completed_episodes < num_episodes:
            # Increment step counter
            self._step += 1
            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_epoch_begin(epoch=self._step, logs=None)
            
            # if self.callbacks:
            #     for callback in self.callbacks:
            #         callback.on_train_step_begin(step=self._step, logs=None)
            
            # self._cur_learning_steps.append(self._step)
            actions, _ = self.get_action(states)
            actions = self.env.format_actions(actions)
            #DEBUG
            # print(f'actions pre convert:{actions}')
            # Convert actions to list of ints
            # actions = [int(a) for a in actions]
            #DEBUG
            # print(f'actions post convert:{actions}')
            next_states, rewards, terms, truncs, _ = self.env.step(actions)
            # Add data to train step log
            # self._train_step_config["actions"] = actions
            self._train_step_config["step_rewards"] = rewards.mean()
            # self._train_step_config["log_probabilities"] = log_probs
            # Add rewards to episode scores
            episode_scores += rewards
            # store trajectories
            for i in range(self.num_envs):
                episode_trajectories[i].append(
                    {
                        "state": states[i],
                        "action": actions[i],
                        # "log_prob": log_probs[i],
                        "reward": rewards[i],
                        "done": terms[i] or truncs[i]
                    }
                )
                # Perform updates if term or trunc
                if terms[i] or truncs[i]:
                    # Append episode trajectory to completed trajectories
                    completed_trajectories.append(episode_trajectories[i])
                    # Append environment score to completed scores
                    completed_scores.append(episode_scores[i])
                    # Add the episode reward to the episode log for callbacks
                    self._train_episode_config["episode_reward"] = episode_scores[i]
                    # Reset the episode score of the env back to 0
                    episode_scores[i] = 0
                    # check if best reward
                    avg_reward = sum(completed_scores) / len(completed_scores)
                    if avg_reward > best_reward:
                        best_reward = avg_reward
                        self._train_episode_config["best"] = 1
                        # save model
                        self.save()
                    else:
                        self._train_episode_config["best"] = 0
                    
                    # Reset env trajectory
                    episode_trajectories[i] = []
                    # Increment completed_episodes counter
                    self.completed_episodes += 1
                    # Log completed episodes to callback episode config
                    self._train_episode_config["episode"] = self.completed_episodes
                    # Signal to all callbacks that an episode (epoch) has completed and to log data
                    if self.callbacks:
                        for callback in self.callbacks:
                            callback.on_train_epoch_end(
                            epoch=self._step, logs=self._train_episode_config
                        )
                    # Check if number of completed episodes should trigger render
                    if self.completed_episodes % render_freq == 0 and not rendered:
                        print(f"Rendering episode {self.completed_episodes} during training...")
                        # Call the test function to render an episode
                        self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
                        # Add render to wandb log
                        video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes}.mp4")
                        # Log the video to wandb
                        if self.callbacks:
                            for callback in self.callbacks:
                                if isinstance(callback, WandbCallback):
                                    wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")})
                        rendered = True
                        # Switch models back to train mode after rendering
                        self.policy_model.train()
                        self.value_model.train()
                    else:
                        rendered = False
                    # Print episode update to console
                    print(f"episode {self.completed_episodes}/{num_episodes} score: {completed_scores[-1]} avg score: {avg_reward}")

            states = next_states

            # Perform an update if the number of completed trajectories is greater than or
            # equal to the number of trajectories per update
            if len(completed_trajectories) >= trajectories_per_update:
                self.learn(completed_trajectories)
                # Clear completed_trajectories
                completed_trajectories = []

            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_step_end(step=self._step, logs=self._train_step_config)

        if self.callbacks:
            for callback in self.callbacks:
                callback.on_train_end(logs=self._train_episode_config)
        # close the environment
        # self.env.close()

    def test(self, num_episodes: int, num_envs: int=1, seed: int=None, render_freq: int=0, training: bool=False):
        """Runs a test over 'num_episodes'."""

        # Set models to eval mode
        self.policy_model.eval()
        self.value_model.eval()

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        if render_freq == None:
            render_freq = 0

        # Set seeds
        set_seed(seed)

        try:
            # instantiate new vec environment
            env = self.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error(f"Error in Reinforce.test agent._initialize_env process: {e}", exc_info=True)

        if self.callbacks and not training:
            print('test begin callback if statement fired')
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    # Add to config to send to wandb for logging
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)

        _step = 0
        completed_episodes = 0
        # Instantiate array to keep track of current episode scores
        episode_scores = np.zeros(num_envs)
        # Instantiate a deque to track last 'episodes_per_update' scores for computing avg
        completed_scores = deque(maxlen=num_episodes)
        # Instantiate list to keep track of frames for rendering
        frames = []
        # Reset environment to get starting state
        states, _ = env.reset()
        while completed_episodes < num_episodes:
            # Increment step counter
            _step += 1
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_begin(epoch=_step, logs=None)
            
            # if self.callbacks:
            #     for callback in self.callbacks:
            #         callback.on_train_step_begin(step=self._step, logs=None)
            
            # self._cur_learning_steps.append(self._step)
            actions, _ = self.get_action(states)
            actions = self.env.format_actions(actions)
            #DEBUG
            # print(f'actions pre convert:{actions}')
            # Convert actions to list of ints
            # actions = [int(a) for a in actions]
            #DEBUG
            # print(f'actions post convert:{actions}')
            next_states, rewards, terms, truncs, _ = env.step(actions)
            # Add data to train step log
            # self._train_step_config["actions"] = actions
            self._train_step_config["step_rewards"] = rewards
            # self._train_step_config["log_probabilities"] = log_probs
            # Add rewards to episode scores
            episode_scores += rewards
            # store trajectories
            for i in range(num_envs):
                # episode_trajectories[i].append(
                #     {
                #         "state": states[i],
                #         "action": actions[i],
                #         # "log_prob": log_probs[i],
                #         "reward": rewards[i],
                #         "done": terms[i] or truncs[i]
                #     }
                # )
                # Perform updates if term or trunc
                if terms[i] or truncs[i]:
                    # Append episode trajectory to completed trajectories
                    # completed_trajectories.append(episode_trajectories[i])
                    # Append environment score to completed scores
                    completed_scores.append(episode_scores[i])
                    # Add the episode reward to the episode log for callbacks
                    self._test_episode_config["episode_reward"] = episode_scores[i]
                    # Reset the episode score of the env back to 0
                    episode_scores[i] = 0
                    # check if best reward
                    avg_reward = sum(completed_scores) / len(completed_scores)
                    # if avg_reward > best_reward:
                    #     best_reward = avg_reward
                    #     self._test_episode_config["best"] = 1
                    #     # save model
                    #     self.save()
                    # else:
                    #     self._test_episode_config["best"] = 0
                    
                    # Reset env trajectory
                    # episode_trajectories[i] = []
                    # Increment completed_episodes counter
                    completed_episodes += 1
                    # Log completed episodes to callback episode config
                    self._test_episode_config["episode"] = completed_episodes
                    # Save the video if the episode number is divisible by render_freq
                    if (render_freq > 0) and ((completed_episodes) % render_freq == 0):
                        if training:
                            render_video(frames, self.completed_episodes, self.save_dir, 'train')
                        else:
                            render_video(frames, completed_episodes, self.save_dir, 'test')
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/test/episode_{completed_episodes}.mp4")
                            # Log the video to wandb
                            if self.callbacks:
                                for callback in self.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})
                        # Empty frames array
                        frames = []
                    # Signal to all callbacks that an episode (epoch) has completed and to log data
                    if self.callbacks and not training:
                        for callback in self.callbacks:
                            callback.on_test_epoch_end(
                            epoch=_step, logs=self._test_episode_config
                        )
                    if not training:
                        # Print episode update to console
                        print(f"episode {completed_episodes}/{num_episodes} score: {completed_scores[-1]} avg score: {avg_reward}")
                
            if render_freq > 0:
                # Capture the frame
                frame = env.render()[0]
                # print(f'frame:{frame}')
                frames.append(frame)

            states = next_states

            # Perform an update if the number of completed trajectories is greater than or
            # equal to the number of trajectories per update
            # if len(completed_trajectories) >= trajectories_per_update:
            #     self.learn(completed_trajectories)
            #     # Clear completed_trajectories
            #     completed_trajectories = []

            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_step_end(step=_step, logs=self._test_step_config)

        if self.callbacks and not training:
            for callback in self.callbacks:
                callback.on_test_end(logs=self._test_episode_config)
        # close the environment
        # self.env.close()

    def get_config(self):
        return {
            "agent_type": self.__class__.__name__,
            "env": self.env.to_json(),
            "policy_model": self.policy_model.get_config(),
            "value_model": self.value_model.get_config(),
            "discount": self.discount,
            "callbacks": [callback.get_config() for callback in self.callbacks] if self.callbacks else None,
            "save_dir": self.save_dir
        }

    def save(self):
        """Saves the model."""
        config = self.get_config()

        # makes directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)

        # writes and saves JSON file of reinforce agent config
        with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

        # saves policy and value model
        self.policy_model.save(self.save_dir)
        if self.value_model:
            self.value_model.save(self.save_dir)

        # if wandb callback, save wandb config
        # if self._wandb:
        #     for callback in self.callbacks:
        #         if isinstance(callback, rl_callbacks.WandbCallback):
        #             callback.save(self.save_dir + "/wandb_config.json")

    @classmethod
    def load(cls, config, load_weights):
        """Loads the model."""
        # # load reinforce agent config
        # with open(
        #     Path(folder).joinpath(Path("obj_config.json")), "r", encoding="utf-8"
        # ) as f:
        #     obj_config = json.load(f)

        env_wrapper = EnvWrapper.from_json(config["env"])

        # load policy model
        policy_model = StochasticDiscretePolicy.load(config['save_dir'], load_weights)
        if config["value_model"]:
            # load value model
            value_model = ValueModel.load(config['save_dir'], load_weights)
        # load callbacks
        callbacks = [callback_load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]\
                    if config['callbacks'] else None

        # return reinforce agent
        agent = cls(
            env=env_wrapper,
            policy_model=policy_model,
            value_model=value_model,
            discount=config["discount"],
            callbacks=callbacks,
            save_dir=config["save_dir"],
        )

        return agent
    

class DDPG(Agent):
    """Deep Deterministic Policy Gradient Agent."""

    def __init__(
        self, env: EnvWrapper,
        actor_model: ActorModel,
        critic_model: CriticModel,
        replay_buffer: Buffer = None,
        discount: float=0.99,
        tau: float=0.001,
        action_epsilon: float = 0.0,
        batch_size: int = 64,
        noise: Noise=None,
        noise_schedule: ScheduleWrapper=None,
        normalize_inputs: bool=False,
        normalizer_clip: float=5.0,
        normalizer_eps: float=0.01,
        warmup: int=1000,
        callbacks: Optional[list[Callback]] = None,
        save_dir: str = "models",
        device: str = None
    ):
        try:
            self.device = get_device(device)
            self.env = env
            self.actor_model = actor_model
            self.critic_model = critic_model
            # set target actor and critic models
            self.target_actor_model = self.clone_model(actor_model)
            self.target_critic_model = self.clone_model(critic_model)
            self.discount = discount
            self.tau = tau
            self.action_epsilon = action_epsilon
            self.replay_buffer = replay_buffer
            self.batch_size = batch_size
            self.noise = noise
            self.noise_schedule = noise_schedule
            self.normalize_inputs = normalize_inputs
            # self.normalize_kwargs = normalize_kwargs
            self.normalizer_clip = normalizer_clip
            self.normalizer_eps = normalizer_eps
            self.warmup = warmup
            # logger.debug(f"rank {self.rank} DDPG init attributes set")
        except Exception as e:
            logger.error(f"Error in DDPG init: {e}", exc_info=True)
        
        # set internal attributes
        try:
            obs_space = (self.env.single_observation_space if hasattr(self.env, "single_observation_space") 
                        else self.env.observation_space)
            # Check if the observation space is a dictionary for goal-aware environments
            if isinstance(obs_space, gym.spaces.Dict):
                shape = obs_space['observation'].shape
                # goal_shape = obs_space['desired_goal'].shape
                # shape = (observation_shape[0] + goal_shape[0],)
            else:
                shape = obs_space.shape

            if self.normalize_inputs:
                self.state_normalizer = Normalizer(shape, self.normalizer_eps, self.normalizer_clip, self.device)
            
            if save_dir is not None and "/ddpg/" not in save_dir:
                self.save_dir = save_dir + "/ddpg/"
            elif save_dir is not None:
                self.save_dir = save_dir

            # instantiate internal attribute use_her to be switched by HER class if using DDPG
            self._use_her = False
            # logger.debug(f"rank {self.rank} DDPG init: internal attributes set")
        except Exception as e:
            logger.error(f"Error in DDPG init internal attributes: {e}", exc_info=True)

        # Set callbacks
        try:
            self.callbacks = callbacks
            if callbacks:
                for callback in self.callbacks:
                    self._config = callback._config(self)
                    if isinstance(callback, WandbCallback):  
                        self._wandb = True

            else:
                self.callback_list = None
                self._wandb = False
            # logger.debug(f"rank {self.rank} DDPG init: callbacks set")
        except Exception as e:
            logger.error(f"Error in DDPG init set callbacks: {e}", exc_info=True)
        self._train_config = {}
        self._train_episode_config = {}
        self._train_step_config = {}
        self._test_config = {}
        self._test_episode_config = {}
        self._test_step_config = {}

        self._step = None

    def clone(self):
        
        env = GymnasiumWrapper(self.env.env_spec)
        actor = self.clone_model(self.actor_model)
        critic = self.clone_model(self.critic_model)
        replay_buffer = self.replay_buffer.clone()
        noise = self.noise.clone()

        return DDPG(
            env,
            actor,
            critic,
            replay_buffer,
            self.discount,
            self.tau,
            self.action_epsilon,
            self.batch_size,
            noise,
            self.normalize_inputs,
            self.normalizer_clip,
            self.normalizer_eps,
            self.warmup,
            None,
            self.save_dir,
            device = self.device
        )
        
    
    def clone_model(self, model):
        """Clones a model."""
        return model.get_clone()

    def _init_her(self):
            self._use_her = True

    def get_action(self, state, goal=None, test=False,
                   state_normalizer:Normalizer=None,
                   goal_normalizer:Normalizer=None):

        # make sure state is a tensor and on correct device
        state = T.tensor(state, dtype=T.float32, device=self.actor_model.device)

        if test:
            if self._use_her:
                state = state_normalizer.normalize(state)
                # make sure goal is a tensor and on correct device
                goal = T.tensor(goal, dtype=T.float32, device=self.actor_model.device)
                goal = goal_normalizer.normalize(goal)
            # use self.state_normalizer if self.normalize_inputs
            elif self.normalize_inputs and not self._use_her:
                state = self.state_normalizer.normalize(state)
            
            _, pi = self.target_actor_model(state, goal)
            return pi.cpu().detach().numpy()
                
        # if random number is less than epsilon or in warmup, sample random action
        elif np.random.random() < self.action_epsilon or self._step <= self.warmup:
            action_np = self.env.action_space.sample()
            noise_np = np.zeros((1,action_np.shape[-1]))
        
        else:
            # (HER) use passed state normalizer if using HER
            if self._use_her:
                state = state_normalizer.normalize(state)
                # make sure goal is a tensor and on correct device
                goal = T.tensor(goal, dtype=T.float32, device=self.actor_model.device)
                goal = goal_normalizer.normalize(goal)
            # use self.state_normalizer if self.normalize_inputs
            elif self.normalize_inputs and not self._use_her:
                state = self.state_normalizer.normalize(state)
            
            noise = self.noise()
            if self.noise_schedule:
                noise *= self.noise_schedule.get_factor()

            _, pi = self.actor_model(state, goal)

            # Convert the action space bounds to a tensor on the same device
            action_space_high = T.tensor(self.env.action_space.high, dtype=T.float32, device=self.actor_model.device)
            action_space_low = T.tensor(self.env.action_space.low, dtype=T.float32, device=self.actor_model.device)
            action = (pi + noise).clip(action_space_low, action_space_high)

            noise_np = noise.cpu().detach().numpy()
            action_np = action.cpu().detach().numpy()

        # if test:
            # loop over all actions to log to wandb
            # for i, a in enumerate(action_np):
            #     # Log the values to wandb
            #     self._train_step_config[f'action_{i}'] = a

        # Loop over the noise and action values and log them to wandb
        for i in range(action_np.shape[-1]):
            # Log the values to wandb
            # self._train_step_config[f'action_{i}'] = a
            self._train_step_config[f'action_{i}'] = action_np[:,i].mean()
            self._train_step_config[f'action_{i}_noise'] = noise_np[:,i].mean()

        return action_np


    def learn(self, state_normalizer: Normalizer=None, goal_normalizer: Normalizer=None):
        # Sample a batch of experiences from the replay buffer
        if hasattr(self.replay_buffer, 'update_priorities'):  # Check if using prioritized replay
            if self._use_her:  # HER with prioritized replay
                states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals, weights, indices = self.replay_buffer.sample(self.batch_size)
            else:  # Just prioritized replay
                states, actions, rewards, next_states, dones, weights, indices = self.replay_buffer.sample(self.batch_size)
                
            # Log PER-specific metrics
            if self._wandb:
                # Get priority info for logging
                if hasattr(self.replay_buffer, 'sum_tree'):
                    indices_tensor = T.tensor(indices, device=self.device)
                    # Get tree indices for sampled transitions
                    tree_indices = indices_tensor + self.replay_buffer.sum_tree.capacity - 1
                    # Get priorities for sampled transitions
                    sampled_priorities = self.replay_buffer.sum_tree.tree[tree_indices].cpu().numpy()
                    
                    # IMPORTANT: Only consider valid entries in the buffer
                    # Get the actual size of used buffer (not the full capacity)
                    actual_size = min(self.replay_buffer.counter, self.replay_buffer.buffer_size)
                    # Only get priorities for actual entries in the buffer
                    valid_indices = T.arange(actual_size, device=self.device)
                    valid_tree_indices = valid_indices + self.replay_buffer.sum_tree.capacity - 1
                    buffer_priorities = self.replay_buffer.sum_tree.tree[valid_tree_indices].cpu().numpy()
                    
                    # Debug prints
                    # print(f"Max recorded priority: {self.replay_buffer.sum_tree.max_recorded_priority.item():.2f}")
                    # print(f"Raw buffer priorities - max: {np.max(buffer_priorities):.2e}, mean: {np.mean(buffer_priorities):.2e}")
                    # print(f"Raw sampled priorities - max: {np.max(sampled_priorities):.2e}, mean: {np.mean(sampled_priorities):.2e}")
                    
                    # Log metrics
                    wandb.log({
                        'PER/beta': self.replay_buffer.beta,
                        'PER/sampled_priorities': sampled_priorities,
                        'PER/buffer_priorities': buffer_priorities,
                        'PER/weights': weights,
                        'PER/mean_sampled_priority': np.mean(sampled_priorities),
                        'PER/mean_buffer_priority': np.mean(buffer_priorities),
                        'PER/max_sampled_priority': np.max(sampled_priorities),
                        'PER/max_buffer_priority': np.max(buffer_priorities),
                        'PER/weight_mean': np.mean(weights.cpu().numpy()) if weights is not None else 0.0,
                        'PER/weight_std': np.std(weights.cpu().numpy()) if weights is not None else 0.0
                    }, step=self._step)
        else:  # Standard replay buffer
            if self._use_her:
                states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = self.replay_buffer.sample(self.batch_size)
            else:
                states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
            
            weights = None
            indices = None

        # Normalize states if self.normalize_inputs
        if self.normalize_inputs and not self._use_her:
            states = self.state_normalizer.normalize(states)
            next_states = self.state_normalizer.normalize(next_states)
            desired_goals = None
        elif self._use_her:
            states = state_normalizer.normalize(states)
            next_states = state_normalizer.normalize(next_states)
            desired_goals = goal_normalizer.normalize(desired_goals)
        else:
            desired_goals = None

        # Convert rewards and dones to 2D tensors
        rewards = rewards.unsqueeze(1)
        dones = dones.unsqueeze(1)

        # Get target values
        with T.no_grad():
            _, target_actions = self.target_actor_model(next_states, desired_goals)
            # noise = self.target_noise()
            
            # Apply noise clipping if needed
            # if hasattr(self, 'target_noise_clip') and self.target_noise_clip > 0:
            #     noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)
                
            # Apply noise scaling if scheduled
            # if hasattr(self, 'target_noise_schedule') and self.target_noise_schedule is not None:
            #     noise *= self.target_noise_schedule.get_factor()
                
            # Add noise to target actions and clamp to action space
            # target_actions = (target_actions + noise).clamp(float(self.env.action_space.low.min()), float(self.env.action_space.high.max()))
            
            # Get target critic values from both critic networks
            target_critic_values = self.target_critic_model(next_states, target_actions, desired_goals)
            # target_critic_values_b = self.target_critic_model_b(next_states, target_actions, desired_goals)
            
            # Take minimum of both critic values for stability
            # target_critic_values = T.min(target_critic_values_a, target_critic_values_b)
            
            # Calculate target Q-values
            targets = rewards + (1 - dones) * self.discount * target_critic_values
            
            # Apply HER-specific clamping if needed
            if self._use_her:
                targets = T.clamp(targets, min=-1/(1-self.discount), max=0)

        # Get current critic predictions
        predictions = self.critic_model(states, actions, desired_goals)
        # predictions_b = self.critic_model_b(states, actions, desired_goals)

        # Calculate TD errors
        error = targets - predictions
        # error_b = targets - predictions_b
        # error = (error_a.abs() + error_b.abs()) / 2  # Average of absolute errors for priorities

        # Apply importance sampling weights if using prioritized replay
        if weights is not None:
            critic_loss = (weights * error.pow(2)).mean()
            # critic_loss_b = (weights * error_b.pow(2)).mean()
            # critic_loss = critic_loss_a + critic_loss_b
        else:
            # critic_loss = F.mse_loss(predictions, targets)
            critic_loss = error.pow(2).mean()

        # Update critic
        self.critic_model.optimizer.zero_grad()
        critic_loss.backward()
        self.critic_model.optimizer.step()

        # Only update actor every actor_update_delay steps
        # if self._step % self.actor_update_delay == 0:
        # Get actor's action predictions
        pre_act_values, action_values = self.actor_model(states, desired_goals)
        
        # Calculate actor loss based on critic
        critic_values = self.critic_model(states, action_values, desired_goals)
        actor_loss = -T.mean(critic_values)
        
        # Add HER-specific regularization if needed
        if self._use_her:
            actor_loss += pre_act_values.pow(2).mean()

        # Update actor
        self.actor_model.optimizer.zero_grad()
        actor_loss.backward()
        self.actor_model.optimizer.step()

        # Perform soft update on target networks
        if not self._use_her:
            self.soft_update(self.actor_model, self.target_actor_model)
            self.soft_update(self.critic_model, self.target_critic_model)

        # Update priorities if using prioritized replay - only on update_freq steps
        if hasattr(self.replay_buffer, 'update_priorities') and indices is not None and hasattr(self.replay_buffer, 'update_freq'):
            if self._step % self.replay_buffer.update_freq == 0:
                # Get the absolute values of the TD errors
                abs_error = error.abs().detach().flatten()
                # Handle NaN values if they occur
                if T.isnan(abs_error).any():
                    abs_error = T.nan_to_num(abs_error, nan=1.0)
                # if self._use_her:
                #     # Add goal distance regularization (Euclidean distance between achieved and desired)
                #     goal_distance = T.norm(achieved_goals - desired_goals, dim=-1)
                #     abs_error = abs_error + 0.1 * goal_distance
                # Update priorities
                self.replay_buffer.update_priorities(indices, abs_error)

        # Add metrics to step_logs
        self._train_step_config['actor_predictions'] = action_values.mean()
        self._train_step_config['critic_predictions'] = critic_values.mean() if 'critic_values' in locals() else predictions.mean()
        self._train_step_config['target_actor_predictions'] = target_actions.mean()
        self._train_step_config['target_critic_predictions'] = target_critic_values.mean()

        return actor_loss.item() if actor_loss is not None else 0.0, critic_loss.item()
        
    
    def soft_update(self, current, target):
        with T.no_grad():
            for current_params, target_params in zip(current.parameters(), target.parameters()):
                target_params.data.copy_(self.tau * current_params.data + (1 - self.tau) * target_params.data)

    # @classmethod
    # def sweep_train(
    #     cls,
    #     config, # wandb.config,
    #     train_config,
    #     env_spec,
    #     callbacks,
    #     run_number,
    #     comm=None,
    # ):
    #     """Builds and trains agents from sweep configs. Works with MPI"""
    #     rank = MPI.COMM_WORLD.rank

    #     if comm is not None:
    #         logger.debug(f"Rank {rank} comm detected")
    #         rank = comm.Get_rank()
    #         logger.debug(f"Global rank {MPI.COMM_WORLD.Get_rank()} in {comm.Get_name()} set to comm rank {rank}")
    #         logger.debug(f"init_sweep fired: global rank {MPI.COMM_WORLD.rank}, group rank {rank}, {comm.Get_name()}")
    #     else:
    #         logger.debug(f"init_sweep fired")
    #     try:
    #         # rank = MPI.COMM_WORLD.rank
    #         # Instantiate env from env_spec
    #         env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))
    #         # agent_config_path = f'sweep/agent_config_{run_number}.json'
    #         # logger.debug(f"rank {rank} agent config path: {agent_config_path}")
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train config: {train_config}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} env spec id: {env.spec.id}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} callbacks: {callbacks}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} run number: {run_number}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} config set: {config}")
    #         else:
    #             logger.debug(f"train config: {train_config}")
    #             logger.debug(f"env spec id: {env.spec.id}")
    #             logger.debug(f"callbacks: {callbacks}")
    #             logger.debug(f"run number: {run_number}")
    #             logger.debug(f"config set: {config}")
    #         model_type = list(config.keys())[0]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} model type: {model_type}")
    #         else:
    #             logger.debug(f"model type: {model_type}")

    #         actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels = wandb_support.format_layers(config)
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} layers built")
    #         else:
    #             logger.debug(f"layers built")
    #         # Actor
    #         actor_learning_rate=config[model_type][f"{model_type}_actor_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor learning rate set")
    #         else:
    #             logger.debug(f"actor learning rate set")
    #         actor_optimizer = config[model_type][f"{model_type}_actor_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer set")
    #         else:
    #             logger.debug(f"actor optimizer set")
    #         # get optimizer params
    #         actor_optimizer_params = {}
    #         if actor_optimizer == "Adam":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
            
    #         elif actor_optimizer == "Adagrad":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_lr_decay']
            
    #         elif actor_optimizer == "RMSprop" or actor_optimizer == "SGD":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_momentum']

    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer params set")
    #         else:
    #             logger.debug(f"actor optimizer params set")
    #         actor_normalize_layers = config[model_type][f"{model_type}_actor_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor normalize layers set")
    #         else:
    #             logger.debug(f"actor normalize layers set")
    #         # Critic
    #         critic_learning_rate=config[model_type][f"{model_type}_critic_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic learning rate set")
    #         else:
    #             logger.debug(f"critic learning rate set")
    #         critic_optimizer = config[model_type][f"{model_type}_critic_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer set")
    #         else:
    #             logger.debug(f"critic optimizer set")
    #         critic_optimizer_params = {}
    #         if critic_optimizer == "Adam":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
            
    #         elif critic_optimizer == "Adagrad":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_lr_decay']
            
    #         elif critic_optimizer == "RMSprop" or critic_optimizer == "SGD":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_momentum']
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer params set")
    #         else:
    #             logger.debug(f"critic optimizer params set")

    #         critic_normalize_layers = config[model_type][f"{model_type}_critic_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic normalize layers set")
    #         else:
    #             logger.debug(f"critic normalize layers set")
    #         # Set device
    #         device = config[model_type][f"{model_type}_device"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} device set")
    #         else:
    #             logger.debug(f"device set")
    #         # Check if CNN layers and if so, build CNN model
    #         if actor_cnn_layers:
    #             actor_cnn_model = cnn_models.CNN(actor_cnn_layers, env)
    #         else:
    #             actor_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor cnn layers set: {actor_cnn_layers}")
    #         else:
    #             logger.debug(f"actor cnn layers set: {actor_cnn_layers}")

    #         if critic_cnn_layers:
    #             critic_cnn_model = cnn_models.CNN(critic_cnn_layers, env)
    #         else:
    #             critic_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic cnn layers set: {critic_cnn_layers}")
    #         else:
    #             logger.debug(f"critic cnn layers set: {critic_cnn_layers}")
    #         # Get actor clamp value
    #         # clamp_output = config[model_type][f"{model_type}_actor_clamp_output"]
    #         # if comm is not None:
    #         #     logger.debug(f"{comm.Get_name()}; Rank {rank} clamp output set: {clamp_output}")
    #         # else:
    #         #     logger.debug(f"clamp output set: {clamp_output}")
    #         actor_model = models.ActorModel(env = env,
    #                                         cnn_model = actor_cnn_model,
    #                                         dense_layers = actor_layers,
    #                                         output_layer_kernel=kernels[f'actor_output_kernel'],
    #                                         optimizer = actor_optimizer,
    #                                         optimizer_params = actor_optimizer_params,
    #                                         learning_rate = actor_learning_rate,
    #                                         normalize_layers = actor_normalize_layers,
    #                                         # clamp_output=clamp_output,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor model built: {actor_model.get_config()}")
    #         else:
    #             logger.debug(f"actor model built: {actor_model.get_config()}")
    #         critic_model = models.CriticModel(env = env,
    #                                         cnn_model = critic_cnn_model,
    #                                         state_layers = critic_state_layers,
    #                                         merged_layers = critic_merged_layers,
    #                                         output_layer_kernel=kernels[f'critic_output_kernel'],
    #                                         optimizer = critic_optimizer,
    #                                         optimizer_params = critic_optimizer_params,
    #                                         learning_rate = critic_learning_rate,
    #                                         normalize_layers = critic_normalize_layers,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic model built: {critic_model.get_config()}")
    #         else:
    #             logger.debug(f"critic model built: {critic_model.get_config()}")
    #         # get normalizer clip value
    #         normalizer_clip = config[model_type][f"{model_type}_normalizer_clip"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} normalizer clip set: {normalizer_clip}")
    #         else:
    #             logger.debug(f"normalizer clip set: {normalizer_clip}")
    #         # get action epsilon
    #         action_epsilon = config[model_type][f"{model_type}_epsilon_greedy"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} action epsilon set: {action_epsilon}")
    #         else:
    #             logger.debug(f"action epsilon set: {action_epsilon}")
    #         # Replay buffer size
    #         replay_buffer_size = config[model_type][f"{model_type}_replay_buffer_size"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} replay buffer size set: {replay_buffer_size}")
    #         else:
    #             logger.debug(f"replay buffer size set: {replay_buffer_size}")
    #         # Save dir
    #         save_dir = config[model_type][f"{model_type}_save_dir"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} save dir set: {save_dir}")
    #         else:
    #             logger.debug(f"save dir set: {save_dir}")

    #         # create replay buffer
    #         replay_buffer = ReplayBuffer(env, replay_buffer_size, device=device)
    #         # create DDPG agent
    #         ddpg_agent= cls(
    #             env = env,
    #             actor_model = actor_model,
    #             critic_model = critic_model,
    #             discount = config[model_type][f"{model_type}_discount"],
    #             tau = config[model_type][f"{model_type}_tau"],
    #             action_epsilon = action_epsilon,
    #             replay_buffer = replay_buffer,
    #             batch_size = config[model_type][f"{model_type}_batch_size"],
    #             noise = Noise.create_instance(config[model_type][f"{model_type}_noise"], shape=env.action_space.shape, **config[model_type][f"{model_type}_noise_{config[model_type][f'{model_type}_noise']}"], device=device),
    #             warmup = config[model_type][f"{model_type}_warmup"],
    #             callbacks = callbacks,
    #             comm = comm,
    #             device = device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} ddpg agent built: {ddpg_agent.get_config()}")
    #         else:
    #             logger.debug(f"ddpg agent built: {ddpg_agent.get_config()}")

    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier called")
    #         else:
    #             logger.debug(f"train barrier called")

    #         if comm is not None:
    #             comm.Barrier()
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier passed")

    #         ddpg_agent.train(
    #                 num_episodes=train_config['num_episodes'],
    #                 render=False,
    #                 render_freq=0,
    #                 )

    #     except Exception as e:
    #         logger.error(f"An error occurred: {e}", exc_info=True)

    def train(self, num_episodes: int, num_envs: int, seed: int | None = None, render_freq: int = 0):
        """Trains the model for 'episodes' number of episodes."""

        # set models to train mode
        self.actor_model.train()
        self.critic_model.train()

         # set num_envs as attribute
        self.num_envs = num_envs

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        # if render_freq == None:
        #     render_freq = 0

        # Set seeds
        set_seed(seed)

        if self.callbacks:
            for callback in self.callbacks:
                if isinstance(callback, WandbCallback):
                    self._config['num_episodes'] = num_episodes
                    self._config['seed'] = seed
                    self._config['num_envs'] = self.num_envs
                    callback.on_train_begin((self.actor_model, self.critic_model,), logs=self._config)
        
        try:
            # instantiate new vec environment
            self.env.env = self.env._initialize_env(0, self.num_envs, seed)
        except Exception as e:
            logger.error(f"Error in DDPG.train self.env")
        
        # initialize step counter (for logging)
        self._step = 0
        best_reward = -np.inf
        score_history = deque(maxlen=100) # Keeps track of last 100 episode scores
        trajectories = [[] for _ in range(self.num_envs)] # Keeps track of trajectories per env
        episode_scores = np.zeros(self.num_envs) # Keeps track of current scores per env
        self.completed_episodes = np.zeros(self.num_envs) # Keeps track of completed episodes per env
        # Initialize environments
        states, _ = self.env.reset()
        while self.completed_episodes.sum() < num_episodes:
            self._step += 1

            rendered = False # Flag to keep track of render status to avoid rendering multiple times per step
            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_epoch_begin(epoch=self._step, logs=None)
            # reset noise
            if type(self.noise) == OUNoise:
                self.noise.reset()
            actions = self.get_action(states)
            # Format actions
            actions = self.env.format_actions(actions)
            next_states, rewards, terms, truncs, _ = self.env.step(actions)
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)
            
            # Store transitions in the env trajectory
            for i in range(self.num_envs):
                trajectories[i].append((states[i], actions[i], rewards[i], next_states[i], dones[i]))

            completed_episodes = np.flatnonzero(dones) # Get indices of completed episodes
            for i in completed_episodes:
                self.replay_buffer.add(*zip(*trajectories[i]))
                trajectories[i] = []

                # Increment completed episodes for env by 1
                self.completed_episodes[i] += 1
                score_history.append(episode_scores[i]) 
                avg_reward = sum(score_history) / len(score_history)
                self._train_episode_config['episode'] = self.completed_episodes.sum()
                self._train_episode_config["episode_reward"] = episode_scores[i]

                # check if best reward
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    self._train_episode_config["best"] = 1
                    # save model
                    self.save()
                else:
                    self._train_episode_config["best"] = 0

                if self.callbacks:
                    for callback in self.callbacks:
                        callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)

                # Check if number of completed episodes should trigger render
                if self.completed_episodes.sum() % render_freq == 0 and not rendered:
                    print(f"Rendering episode {self.completed_episodes.sum()} during training...")
                    # Call the test function to render an episode
                    self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
                    # Add render to wandb log
                    video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes.sum()}.mp4")
                    # Log the video to wandb
                    if self.callbacks:
                        for callback in self.callbacks:
                            if isinstance(callback, WandbCallback):
                                wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")}, step=self._step)
                    rendered = True
                    # Switch models back to train mode after rendering
                    self.actor_model.train()
                    self.critic_model.train()
                # else:
                #     rendered = False

                print(f"Environment {i}: Episode {int(self.completed_episodes.sum())}, Score {episode_scores[i]}, Avg_Score {avg_reward}")

                # Reset score of episode to 0
                episode_scores[i] = 0
                    
            states = next_states
            
            # Check if past warmup
            if self._step > self.warmup:
                # check if enough samples in replay buffer and if so, learn from experiences
                if self.replay_buffer.counter > self.batch_size:
                    actor_loss, critic_loss = self.learn()
                    self._train_step_config["actor_loss"] = actor_loss
                    self._train_step_config["critic_loss"] = critic_loss
                    # Step scheduler if not None
                    if self.noise_schedule:
                        self.noise_schedule.step()
                        self._train_step_config["noise_anneal"] = self.noise_schedule.get_factor()


            self._train_step_config["step_reward"] = rewards.mean()
            
            # log to wandb if using wandb callback
            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_step_end(step=self._step, logs=self._train_step_config)

            

        if self.callbacks:
            for callback in self.callbacks:
                callback.on_train_end(logs=self._train_episode_config)
       
    def test(self, num_episodes: int, num_envs: int=1, seed: int=None, render_freq: int=0, training: bool=False):
        """Runs a test over 'num_episodes'."""

        # set model in eval mode
        self.actor_model.eval()
        self.critic_model.eval()

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        if render_freq == None:
            render_freq = 0

        # Set seeds
        set_seed(seed)

        try:
            # instantiate new vec environment
            env = self.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error(f"Error in ddpg.test agent._initialize_env process: {e}", exc_info=True)

        if self.callbacks and not training:
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    # Add to config to send to wandb for logging
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)

        _step = 0
        # Instantiate array to keep track of number of completed episodes per env
        completed_episodes = np.zeros(num_envs)
        # Instantiate array to keep track of current episode scores
        episode_scores = np.zeros(num_envs)
        # Instantiate a deque to track last 'episodes_per_update' scores for computing avg
        completed_scores = deque(maxlen=num_episodes)
        # Instantiate list to keep track of frames for rendering
        frames = []
        # Reset environment to get starting state
        states, _ = env.reset()
        while completed_episodes.sum() < num_episodes:
            # Increment step counter
            _step += 1
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_begin(epoch=_step, logs=None)
            
            # if self.callbacks:
            #     for callback in self.callbacks:
            #         callback.on_train_step_begin(step=self._step, logs=None)
            actions = self.get_action(states, test=True)
            # Format actions
            actions = self.env.format_actions(actions, testing=True)
            next_states, rewards, terms, truncs, _ = env.step(actions)
            self._test_step_config["step_reward"] = rewards
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)

            for i in range(num_envs):
                if dones[i]:
                    # Increment completed episodes for env by 1
                    completed_episodes[i] += 1
                    # Append environment score to completed scores
                    completed_scores.append(episode_scores[i])
                    # Add the episode reward to the episode log for callbacks
                    self._test_episode_config["episode_reward"] = episode_scores[i]
                    # Reset the episode score of the env back to 0
                    episode_scores[i] = 0
                    # check if best reward
                    avg_reward = sum(completed_scores) / len(completed_scores)
                    # Log completed episodes to callback episode config
                    self._test_episode_config["episode"] = completed_episodes.sum()
                    # Save the video if the episode number is divisible by render_freq
                    if (render_freq > 0) and ((completed_episodes.sum()) % render_freq == 0):
                        if training:
                            render_video(frames, self.completed_episodes.sum(), self.save_dir, 'train')
                        else:
                            render_video(frames, completed_episodes.sum(), self.save_dir, 'test')
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/test/episode_{completed_episodes.sum()}.mp4")
                            # Log the video to wandb
                            if self.callbacks:
                                for callback in self.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})
                        # Empty frames array
                        frames = []
                    # Signal to all callbacks that an episode (epoch) has completed and to log data
                    if self.callbacks and not training:
                        for callback in self.callbacks:
                            callback.on_test_epoch_end(
                            epoch=_step, logs=self._test_episode_config
                        )
                    if not training:
                        # Print episode update to console
                        print(f"Environment {i}: Episode {int(completed_episodes.sum())}/{num_episodes} Score: {completed_scores[-1]} Avg Score: {avg_reward}")
                
            if render_freq > 0:
                # Capture the frame
                frame = env.render()[0]
                # print(f'frame:{frame}')
                frames.append(frame)

            states = next_states

            # Perform an update if the number of completed trajectories is greater than or
            # equal to the number of trajectories per update
            # if len(completed_trajectories) >= trajectories_per_update:
            #     self.learn(completed_trajectories)
            #     # Clear completed_trajectories
            #     completed_trajectories = []

            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_step_end(step=_step, logs=self._test_step_config)

        if self.callbacks and not training:
            for callback in self.callbacks:
                callback.on_test_end(logs=self._test_episode_config)


    def get_config(self):
        return {
                "agent_type": self.__class__.__name__,
                "env": self.env.to_json(),
                "actor_model": self.actor_model.get_config(),
                "critic_model": self.critic_model.get_config(),
                "replay_buffer": self.replay_buffer.get_config() if self.replay_buffer is not None else None,
                "discount": self.discount,
                "tau": self.tau,
                "action_epsilon": self.action_epsilon,
                "batch_size": self.batch_size,
                "noise": self.noise.get_config(),
                "noise_schedule": self.noise_schedule.get_config() if self.noise_schedule is not None else None,
                'normalize_inputs': self.normalize_inputs,
                'normalizer_clip': self.normalizer_clip,
                'normalizer_eps': self.normalizer_eps,
                'warmup': self.warmup,
                "callbacks": [callback.get_config() for callback in self.callbacks] if self.callbacks else None,
                "save_dir": self.save_dir,
                "device": self.device.type,
            }


    def save(self):
        """Saves the model."""

        # Change self.save_dir if save_dir
        # if save_dir is not None:
        #     self.save_dir = save_dir + "/ddpg/"

        config = self.get_config()

        # makes directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)

        # writes and saves JSON file of DDPG agent config
        with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

        # saves policy and value model
        self.actor_model.save(self.save_dir)
        self.critic_model.save(self.save_dir)

        if self.normalize_inputs:
            self.state_normalizer.save_state(self.save_dir + "state_normalizer.npz")

    @classmethod
    def load(cls, config, load_weights=True):
        """Loads the model."""

        # Load EnvWrapper
        env_wrapper = EnvWrapper.from_json(config["env"])
            
        # load policy model
        actor_model = ActorModel.load(config['save_dir'], load_weights)
        # load value model
        critic_model = CriticModel.load(config['save_dir'], load_weights)
        # load replay buffer if not None
        if config['replay_buffer'] is not None:
            config['replay_buffer']['config']['env'] = env_wrapper
            if config['replay_buffer']['class_name'] == 'PrioritizedReplayBuffer':
                replay_buffer = PrioritizedReplayBuffer(**config["replay_buffer"]["config"])
            else:
                replay_buffer = ReplayBuffer(**config["replay_buffer"]["config"])
        else:
            replay_buffer = None
        # load noise
        noise = Noise.create_instance(config["noise"]["class_name"], **config["noise"]["config"])
        # if normalizer, load
        normalize_inputs = config['normalize_inputs']
        # normalize_kwargs = obj_config['normalize_kwargs']
        normalizer_clip = config['normalizer_clip']
        # load callbacks
        callbacks = [callback_load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]\
                    if config['callbacks'] else None

        # return DDPG agent
        agent = cls(
            env = env_wrapper,
            actor_model = actor_model,
            critic_model = critic_model,
            discount=config["discount"],
            tau=config["tau"],
            action_epsilon=config["action_epsilon"],
            replay_buffer=replay_buffer,
            batch_size=config["batch_size"],
            noise=noise,
            noise_schedule=ScheduleWrapper(config["noise_schedule"]),
            normalize_inputs = normalize_inputs,
            normalizer_clip = normalizer_clip,
            warmup = config['warmup'],
            callbacks=callbacks,
            save_dir=config["save_dir"],
            device=config["device"],
        )

        if agent.normalize_inputs:
            agent.state_normalizer = Normalizer.load_state(config['save_dir'] + "state_normalizer.npz")

        return agent
    

class TD3(Agent):
    """Twin Delayed Deep Deterministic Policy Gradient Agent."""
    
    def __init__(
        self,
        env: EnvWrapper,
        actor_model: ActorModel,
        critic_model: CriticModel,
        discount: float = 0.99,
        tau: float = 0.005,
        action_epsilon: float = 0.0,
        replay_buffer: Buffer = None,
        batch_size: int = 256,
        noise: Noise = None,
        noise_schedule: ScheduleWrapper=None,
        target_noise: Noise = None,
        target_noise_schedule: ScheduleWrapper=None,
        target_noise_clip: float = 0.5,
        actor_update_delay: int = 2,
        normalize_inputs: bool = False,
        normalizer_clip: float = 5.0,
        normalizer_eps: float = 0.01,
        warmup: int = 1000,
        callbacks: list = None,
        save_dir: str = "models",
        device: str = None
    ):
        try:
            self.device = get_device(device)
            self.env = env
            self.actor_model = actor_model
            self.critic_model_a = critic_model
            # clone second critic (do not copy weights)
            self.critic_model_b = self.clone_model(critic_model, weights=False)
            # set target networks as clones of the main networks
            self.target_actor_model = self.clone_model(actor_model)
            self.target_critic_model_a = self.clone_model(critic_model)
            self.target_critic_model_b = self.clone_model(self.critic_model_b)
            self.discount = discount
            self.tau = tau
            self.action_epsilon = action_epsilon
            self.replay_buffer = replay_buffer
            self.batch_size = batch_size
            self.noise = noise
            self.noise_schedule = noise_schedule
            if target_noise == None:
                target_noise = NormalNoise(self.env.single_action_space.shape, stddev=0.2, device=device)
            self.target_noise = target_noise
            self.target_noise_schedule = target_noise_schedule
            self.target_noise_clip = target_noise_clip
            self.actor_update_delay = actor_update_delay
            self.normalize_inputs = normalize_inputs
            self.normalizer_clip = normalizer_clip
            self.normalizer_eps = normalizer_eps
            self.warmup = warmup

        except Exception as e:
            logger.error(f"Error in TD3 init: {e}", exc_info=True)
        
        try:
            # Determine the observation shape
            obs_space = (self.env.single_observation_space 
                         if hasattr(self.env, "single_observation_space") 
                         else self.env.observation_space)
            if isinstance(obs_space, gym.spaces.Dict):
                self._obs_space_shape = obs_space['observation'].shape
            else:
                self._obs_space_shape = obs_space.shape
            if self.normalize_inputs:
                self.state_normalizer = Normalizer(self._obs_space_shape, self.normalizer_eps, self.normalizer_clip, device=device)
            # Update the save directory: append '/td3/' if not already there
            if save_dir is not None:
                if "/td3/" not in save_dir:
                    self.save_dir = os.path.join(save_dir, "td3")
                else:
                    self.save_dir = save_dir
            else:
                self.save_dir = "models/td3"
        except Exception as e:
            logger.error(f"Error in TD3 init internal attributes: {e}", exc_info=True)

        # instantiate internal attribute use_her to be switched by HER class if using TD3
        self._use_her = False
        
        try:
            self.callbacks = callbacks
            if callbacks:
                for callback in self.callbacks:
                    self._config = callback._config(self)
                    if isinstance(callback, WandbCallback):
                        self._wandb = True
            else:
                self.callback_list = None
                self._wandb = False
        except Exception as e:
            logger.error(f"Error in TD3 init set callbacks: {e}", exc_info=True)
        
        self._train_config = {}
        self._train_episode_config = {}
        self._train_step_config = {}
        self._test_config = {}
        self._test_episode_config = {}
        self._test_step_config = {}
        self._step = 0
        self._opt_step = 0 # number of times optimizer (learn()) has run

    def clone(self, weights=True):
        env = GymnasiumWrapper(self.env.env_spec)
        actor = self.clone_model(self.actor_model, weights)
        critic = self.clone_model(self.critic_model_a, weights)
        replay_buffer = self.replay_buffer.clone() if self.replay_buffer is not None else None
        noise = self.noise.clone() if self.noise is not None else None
        target_noise = self.target_noise.clone()
        return TD3(
            env,
            actor,
            critic,
            self.discount,
            self.tau,
            self.action_epsilon,
            replay_buffer,
            self.batch_size,
            noise,
            target_noise,
            self.target_noise_clip,
            self.actor_update_delay,
            self.normalize_inputs,
            self.normalizer_clip,
            self.normalizer_eps,
            self.warmup,
            None,
            self.save_dir,
            device=self.device
        )
        
    
    def clone_model(self, model, weights=True):
        """Clones a model."""
        return model.get_clone(weights)
    
    # @classmethod
    # def build(
    #     cls,
    #     env,
    #     actor_cnn_layers,
    #     critic_cnn_layers,
    #     actor_layers,
    #     critic_state_layers,
    #     critic_merged_layers,
    #     kernels,
    #     callbacks,
    #     config,#: wandb.config,
    #     save_dir: str = "models/",
    # ):
    #     """Builds the agent."""
    #     # Actor
    #     actor_learning_rate=config[config.model_type][f"{config.model_type}_actor_learning_rate"]
    #     actor_optimizer = config[config.model_type][f"{config.model_type}_actor_optimizer"]
    #     # get optimizer params
    #     actor_optimizer_params = {}
    #     if actor_optimizer == "Adam":
    #         actor_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
        
    #     elif actor_optimizer == "Adagrad":
    #         actor_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #         actor_optimizer_params['lr_decay'] = \
    #             config[config.model_type][f"{config.model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_lr_decay']
        
    #     elif actor_optimizer == "RMSprop" or actor_optimizer == "SGD":
    #         actor_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #         actor_optimizer_params['momentum'] = \
    #             config[config.model_type][f"{config.model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_momentum']

    #     actor_normalize_layers = config[config.model_type][f"{config.model_type}_actor_normalize_layers"]

    #     # Critic
    #     critic_learning_rate=config[config.model_type][f"{config.model_type}_critic_learning_rate"]
    #     critic_optimizer = config[config.model_type][f"{config.model_type}_critic_optimizer"]
    #     critic_optimizer_params = {}
    #     if critic_optimizer == "Adam":
    #         critic_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
        
    #     elif critic_optimizer == "Adagrad":
    #         critic_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #         critic_optimizer_params['lr_decay'] = \
    #             config[config.model_type][f"{config.model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_lr_decay']
        
    #     elif critic_optimizer == "RMSprop" or critic_optimizer == "SGD":
    #         critic_optimizer_params['weight_decay'] = \
    #             config[config.model_type][f"{config.model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #         critic_optimizer_params['momentum'] = \
    #             config[config.model_type][f"{config.model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_momentum']
        
    #     critic_normalize_layers = config[config.model_type][f"{config.model_type}_critic_normalize_layers"]

    #     # Check if CNN layers and if so, build CNN model
    #     if actor_cnn_layers:
    #         actor_cnn_model = cnn_models.CNN(actor_cnn_layers, env)
    #     else:
    #         actor_cnn_model = None

    #     if critic_cnn_layers:
    #         critic_cnn_model = cnn_models.CNN(critic_cnn_layers, env)
    #     else:
    #         critic_cnn_model = None

    #     # Set device
    #     device = config[config.model_type][f"{config.model_type}_device"]

    #     # get desired, achieved, reward func for env
    #     desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)
    #     goal_shape = desired_goal_func(env).shape

    #     # Get actor clamp value
    #     # clamp_output = config[config.model_type][f"{config.model_type}_actor_clamp_output"]
        
    #     actor_model = models.ActorModel(env = env,
    #                                     cnn_model = actor_cnn_model,
    #                                     dense_layers = actor_layers,
    #                                     output_layer_kernel=kernels[f'actor_output_kernel'],
    #                                     goal_shape=goal_shape,
    #                                     optimizer = actor_optimizer,
    #                                     optimizer_params = actor_optimizer_params,
    #                                     learning_rate = actor_learning_rate,
    #                                     normalize_layers = actor_normalize_layers,
    #                                     # clamp_output=clamp_output,
    #                                     device=device,
    #     )
    #     critic_model = models.CriticModel(env = env,
    #                                       cnn_model = critic_cnn_model,
    #                                       state_layers = critic_state_layers,
    #                                       merged_layers = critic_merged_layers,
    #                                       output_layer_kernel=kernels[f'critic_output_kernel'],
    #                                       goal_shape=goal_shape,
    #                                       optimizer = critic_optimizer,
    #                                       optimizer_params = critic_optimizer_params,
    #                                       learning_rate = critic_learning_rate,
    #                                       normalize_layers = critic_normalize_layers,
    #                                       device=device,
    #     )

    #     # action epsilon
    #     action_epsilon = config[config.model_type][f"{config.model_type}_epsilon_greedy"]

    #     # normalize inputs
    #     normalize_inputs = config[config.model_type][f"{config.model_type}_normalize_input"]
    #     # normalize_kwargs = {}
    #     if "True" in normalize_inputs:
    #         # normalize_kwargs = config[config.model_type][f"{config.model_type}_normalize_clip"]
    #         normalizer_clip = config[config.model_type][f"{config.model_type}_normalize_clip"]

    #     agent = cls(
    #         env = env,
    #         actor_model = actor_model,
    #         critic_model = critic_model,
    #         discount = config[config.model_type][f"{config.model_type}_discount"],
    #         tau = config[config.model_type][f"{config.model_type}_tau"],
    #         action_epsilon = action_epsilon,
    #         replay_buffer = ReplayBuffer(env=env),
    #         batch_size = config[config.model_type][f"{config.model_type}_batch_size"],
    #         noise = Noise.create_instance(config[config.model_type][f"{config.model_type}_noise"], shape=env.action_space.shape, **config[config.model_type][f"{config.model_type}_noise_{config[config.model_type][f'{config.model_type}_noise']}"]),
    #         normalize_inputs = normalize_inputs,
    #         # normalize_kwargs = normalize_kwargs,
    #         normalizer_clip = normalizer_clip,
    #         callbacks = callbacks,
    #         save_dir = save_dir,
    #     )

    #     agent.save()

    #     return agent
    
    def _init_her(self):
            self._use_her = True

    def get_action(self, state, goal=None, test=False,
                   state_normalizer:Normalizer=None,
                   goal_normalizer:Normalizer=None):

        # make sure state is a tensor and on correct device
        state = T.tensor(state, dtype=T.float32, device=self.actor_model.device)
        if goal is not None:
            goal = T.tensor(goal, dtype=T.float32, device=self.actor_model.device)
        
        # check if get action is for testing
        if test:
            with T.no_grad():
                # normalize state if self.normalize_inputs
                if self.normalize_inputs and not self._use_her:
                    state = self.state_normalizer.normalize(state)
                # (HER) else if using HER, normalize using passed normalizer
                elif self._use_her:
                    state = state_normalizer.normalize(state)
                    goal = goal_normalizer.normalize(goal)

                # get action
                _, action = self.target_actor_model(state, goal) # use target network for testing
                # transfer action to cpu, detach from any graphs, tranform to numpy, and flatten
                action_np = action.cpu().detach().numpy()#.flatten()
        
        else:
            # check if using epsilon greedy
            if np.random.random() < self.action_epsilon or self._step <= self.warmup:
                action_np = self.env.action_space.sample()
                noise_np = np.zeros((1,action_np.shape[-1]))
            
            else:
                if self._use_her:
                    state = state_normalizer.normalize(state)
                    # make sure goal is a tensor and on correct device
                    goal = T.tensor(goal, dtype=T.float32, device=self.actor_model.device)
                    goal = goal_normalizer.normalize(goal)
                # use self.state_normalizer if self.normalize_inputs
                elif self.normalize_inputs and not self._use_her:
                    state = self.state_normalizer.normalize(state)
                
                # Create noise
                noise = self.noise()
                if self.noise_schedule:
                    noise *= self.noise_schedule.get_factor()

                _, pi = self.actor_model(state, goal)
                # print(f'pi: {pi}')

                # Convert the action space bounds to a tensor on the same device
                action_space_high = T.tensor(self.env.single_action_space.high, dtype=T.float32, device=self.actor_model.device)
                action_space_low = T.tensor(self.env.single_action_space.low, dtype=T.float32, device=self.actor_model.device)
                action = (pi + noise).clip(action_space_low, action_space_high)
                # print(f'action + noise: {action}')

                noise_np = noise.cpu().detach().numpy()#.flatten()
                action_np = action.cpu().detach().numpy()#.flatten()
                # print(f'action np: {action_np}')

        if test:
            # loop over all actions to log to wandb
            # for i, a in enumerate(action_np):
            #     # Log the values to wandb
            #     self._train_step_config[f'action_{i}'] = a
            for i in range(action_np.shape[-1]):
                # Log the values to wandb
                self._test_step_config[f'action_{i}'] = action_np[:,i].mean()
                # self._train_step_config[f'action_{i}_noise'] = noise_np[i]

        else:
            # Loop over the noise and action values and log them to wandb
            # for i, (a,n) in enumerate(zip(action_np, noise_np)):
            #     # Log the values to wandb
            #     self._train_step_config[f'action_{i}'] = a
            #     self._train_step_config[f'noise_{i}'] = n

            # Loop over the noise and action values and log them to wandb
            for i in range(action_np.shape[-1]):
                # Log the values to wandb
                # self._train_step_config[f'action_{i}'] = a
                self._train_step_config[f'action_{i}'] = action_np[:,i].mean()
                self._train_step_config[f'action_{i}_noise'] = noise_np[:,i].mean()
        
        # print(f'pi: {pi}; noise: {noise}; action_np: {action_np}')

        return action_np


    def learn(self, state_normalizer: Normalizer = None, goal_normalizer: Normalizer = None):
        self._opt_step += 1

        # Sample a batch of experiences from the replay buffer
        if hasattr(self.replay_buffer, 'update_priorities'):  # Check if using prioritized replay
            if self._use_her:  # HER with prioritized replay
                states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals, weights, indices = self.replay_buffer.sample(self.batch_size)
            else:  # Just prioritized replay
                states, actions, rewards, next_states, dones, weights, indices = self.replay_buffer.sample(self.batch_size)
        else:  # Standard replay buffer
            if self._use_her:
                states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = self.replay_buffer.sample(self.batch_size)
            else:
                states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
                weights = None
                indices = None

        # Normalize states if self.normalize_inputs
        if self.normalize_inputs and not self._use_her:
            states = self.state_normalizer.normalize(states)
            next_states = self.state_normalizer.normalize(next_states)
            desired_goals = None
        elif self._use_her:
            states = state_normalizer.normalize(states)
            next_states = state_normalizer.normalize(next_states)
            desired_goals = goal_normalizer.normalize(desired_goals)
        else:
            desired_goals = None

        # Convert rewards and dones to 2D tensors
        rewards = rewards.unsqueeze(1)
        dones = dones.unsqueeze(1)

        # Get target values
        with T.no_grad():
            _, target_actions = self.target_actor_model(next_states, desired_goals)
            noise = self.target_noise()
            
            # Apply noise clipping if needed
            if self.target_noise_clip > 0:
                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)
                
            # Apply noise scaling if scheduled
            if self.target_noise_schedule is not None:
                noise *= self.target_noise_schedule.get_factor()
                
            # Add noise to target actions and clamp to action space
            target_actions = (target_actions + noise).clamp(float(self.env.action_space.low.min()), float(self.env.action_space.high.max()))
            
            # Get target critic values from both critic networks
            target_critic_values_a = self.target_critic_model_a(next_states, target_actions, desired_goals)
            target_critic_values_b = self.target_critic_model_b(next_states, target_actions, desired_goals)
            
            # Take minimum of both critic values for stability
            target_critic_values = T.min(target_critic_values_a, target_critic_values_b)
            
            # Calculate target Q-values
            targets = rewards + (1 - dones) * self.discount * target_critic_values
            
            # Apply HER-specific clamping if needed
            if self._use_her:
                targets = T.clamp(targets, min=-1/(1-self.discount), max=0)

        # Get current critic predictions
        predictions_a = self.critic_model_a(states, actions, desired_goals)
        predictions_b = self.critic_model_b(states, actions, desired_goals)

        # Calculate TD errors (use average of both critic networks for PER)
        error_a = targets - predictions_a
        error_b = targets - predictions_b
        error = (error_a.abs() + error_b.abs()) / 2  # Average of absolute errors for priorities

        # Apply importance sampling weights if using prioritized replay
        if weights is not None:
            critic_loss_a = (weights * error_a.pow(2)).mean()
            critic_loss_b = (weights * error_b.pow(2)).mean()
            critic_loss = critic_loss_a + critic_loss_b
        else:
            critic_loss = F.mse_loss(predictions_a, targets) + F.mse_loss(predictions_b, targets)

        # Update critics
        self.critic_model_a.optimizer.zero_grad()
        self.critic_model_b.optimizer.zero_grad()
        critic_loss.backward()
        self.critic_model_a.optimizer.step()
        self.critic_model_b.optimizer.step()
        
        # Get actor's action predictions
        pre_act_values, action_values = self.actor_model(states, desired_goals)
        
        # Calculate actor loss based on critic A
        critic_values = self.critic_model_a(states, action_values, desired_goals)
        actor_loss = -T.mean(critic_values)
        
        # Add HER-specific regularization if needed
        if self._use_her:
            actor_loss += pre_act_values.pow(2).mean()

        
        # Update actor
        # Only update actor every actor_update_delay steps
        if self._opt_step % self.actor_update_delay == 0:
            self.actor_model.optimizer.zero_grad()
            actor_loss.backward()
            self.actor_model.optimizer.step()

            if not self._use_her:
                # Perform soft update on target networks
                self.soft_update(self.actor_model, self.target_actor_model)
                self.soft_update(self.critic_model_a, self.target_critic_model_a)
                self.soft_update(self.critic_model_b, self.target_critic_model_b)
        # else:
        #     actor_loss = None
        #     action_values = actions  # Use original actions for metrics

        # Update priorities if using prioritized replay - only on update_freq steps
        if hasattr(self.replay_buffer, 'update_priorities') and indices is not None and hasattr(self.replay_buffer, 'update_freq'):
            if self._step % self.replay_buffer.update_freq == 0:
                # Use the combined error for priority updates
                abs_error = error.detach().flatten()
                # Handle NaN values if they occur
                if T.isnan(abs_error).any():
                    abs_error = T.nan_to_num(abs_error, nan=1.0)
                # Update priorities directly
                self.replay_buffer.update_priorities(indices, abs_error)

        # Add metrics to step_logs
        self._train_step_config['actor_predictions'] = action_values.mean()
        self._train_step_config['critic_predictions'] = critic_values.mean() if 'critic_values' in locals() else predictions_a.mean()
        self._train_step_config['target_actor_predictions'] = target_actions.mean()
        self._train_step_config['target_critic_predictions'] = target_critic_values.mean()
        # if 'noise' in locals():
        self._train_step_config['target_noise'] = noise.mean()

        return actor_loss.item(), critic_loss.item()
        
    
    def soft_update(self, current, target):
        with T.no_grad():
            for current_params, target_params in zip(current.parameters(), target.parameters()):
                target_params.data.copy_(self.tau * current_params.data + (1 - self.tau) * target_params.data)

    # @classmethod
    # def sweep_train(
    #     cls,
    #     config, # wandb.config,
    #     train_config,
    #     env_spec,
    #     callbacks,
    #     run_number,
    #     comm=None,
    # ):
    #     """Builds and trains agents from sweep configs. Works with MPI"""
    #     rank = MPI.COMM_WORLD.rank

    #     if comm is not None:
    #         logger.debug(f"Rank {rank} comm detected")
    #         rank = comm.Get_rank()
    #         logger.debug(f"Global rank {MPI.COMM_WORLD.Get_rank()} in {comm.Get_name()} set to comm rank {rank}")
    #         logger.debug(f"init_sweep fired: global rank {MPI.COMM_WORLD.rank}, group rank {rank}, {comm.Get_name()}")
    #     else:
    #         logger.debug(f"init_sweep fired: global rank")
    #     try:
    #         # rank = MPI.COMM_WORLD.rank
    #         # Instantiate env from env_spec
    #         env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))
    #         # agent_config_path = f'sweep/agent_config_{run_number}.json'
    #         # logger.debug(f"rank {rank} agent config path: {agent_config_path}")
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train config: {train_config}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} env spec id: {env.spec.id}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} callbacks: {callbacks}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} run number: {run_number}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} config set: {config}")
    #         else:
    #             logger.debug(f"train config: {train_config}")
    #             logger.debug(f"env spec id: {env.spec.id}")
    #             logger.debug(f"callbacks: {callbacks}")
    #             logger.debug(f"run number: {run_number}")
    #             logger.debug(f"config set: {config}")
    #         model_type = list(config.keys())[0]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} model type: {model_type}")
    #         else:
    #             logger.debug(f"model type: {model_type}")
    #         # Only primary process (rank 0) calls wandb.init() to build agent and log data

    #         actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels = wandb_support.format_layers(config)
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} layers built")
    #         else:
    #             logger.debug(f"layers built")
    #         # Actor
    #         actor_learning_rate=config[model_type][f"{model_type}_actor_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor learning rate set")
    #         else:
    #             logger.debug(f"actor learning rate set")
    #         actor_optimizer = config[model_type][f"{model_type}_actor_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer set")
    #         else:
    #             logger.debug(f"actor optimizer set")
    #         # get optimizer params
    #         actor_optimizer_params = {}
    #         if actor_optimizer == "Adam":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
            
    #         elif actor_optimizer == "Adagrad":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_lr_decay']
            
    #         elif actor_optimizer == "RMSprop" or actor_optimizer == "SGD":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_momentum']
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer params set")
    #         else:
    #             logger.debug(f"actor optimizer params set")
    #         actor_normalize_layers = config[model_type][f"{model_type}_actor_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor normalize layers set")
    #         else:
    #             logger.debug(f"actor normalize layers set")
    #         # Critic
    #         critic_learning_rate=config[model_type][f"{model_type}_critic_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic learning rate set")
    #         else:
    #             logger.debug(f"critic learning rate set")
    #         critic_optimizer = config[model_type][f"{model_type}_critic_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer set")
    #         else:
    #             logger.debug(f"critic optimizer set")
    #         critic_optimizer_params = {}
    #         if critic_optimizer == "Adam":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
            
    #         elif critic_optimizer == "Adagrad":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_lr_decay']
            
    #         elif critic_optimizer == "RMSprop" or critic_optimizer == "SGD":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_momentum']
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer params set")
    #         else:
    #             logger.debug(f"critic optimizer params set")

    #         critic_normalize_layers = config[model_type][f"{model_type}_critic_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic normalize layers set")
    #         else:
    #             logger.debug(f"critic normalize layers set")
    #         # Set device
    #         device = config[model_type][f"{model_type}_device"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} device set")
    #         else:
    #             logger.debug(f"device set")
    #         # Check if CNN layers and if so, build CNN model
    #         if actor_cnn_layers:
    #             actor_cnn_model = cnn_models.CNN(actor_cnn_layers, env)
    #         else:
    #             actor_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor cnn layers set: {actor_cnn_layers}")
    #         else:
    #             logger.debug(f"actor cnn layers set: {actor_cnn_layers}")

    #         if critic_cnn_layers:
    #             critic_cnn_model = cnn_models.CNN(critic_cnn_layers, env)
    #         else:
    #             critic_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic cnn layers set: {critic_cnn_layers}")
    #         else:
    #             logger.debug(f"critic cnn layers set: {critic_cnn_layers}")
    #         # # Get actor clamp value
    #         # clamp_output = config[model_type][f"{model_type}_actor_clamp_output"]
    #         # if comm is not None:
    #         #     logger.debug(f"{comm.Get_name()}; Rank {rank} clamp output set: {clamp_output}")
    #         # else:
    #         #     logger.debug(f"clamp output set: {clamp_output}")
    #         actor_model = models.ActorModel(env = env,
    #                                         cnn_model = actor_cnn_model,
    #                                         dense_layers = actor_layers,
    #                                         output_layer_kernel=kernels[f'actor_output_kernel'],
    #                                         optimizer = actor_optimizer,
    #                                         optimizer_params = actor_optimizer_params,
    #                                         learning_rate = actor_learning_rate,
    #                                         normalize_layers = actor_normalize_layers,
    #                                         # clamp_output=clamp_output,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor model built: {actor_model.get_config()}")
    #         else:
    #             logger.debug(f"actor model built: {actor_model.get_config()}")
    #         critic_model = models.CriticModel(env = env,
    #                                         cnn_model = critic_cnn_model,
    #                                         state_layers = critic_state_layers,
    #                                         merged_layers = critic_merged_layers,
    #                                         output_layer_kernel=kernels[f'critic_output_kernel'],
    #                                         optimizer = critic_optimizer,
    #                                         optimizer_params = critic_optimizer_params,
    #                                         learning_rate = critic_learning_rate,
    #                                         normalize_layers = critic_normalize_layers,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic model built: {critic_model.get_config()}")
    #         else:
    #             logger.debug(f"critic model built: {critic_model.get_config()}")
    #         # get normalizer clip value
    #         normalizer_clip = config[model_type][f"{model_type}_normalizer_clip"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} normalizer clip set: {normalizer_clip}")
    #         else:
    #             logger.debug(f"normalizer clip set: {normalizer_clip}")
    #         # get action epsilon
    #         action_epsilon = config[model_type][f"{model_type}_epsilon_greedy"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} action epsilon set: {action_epsilon}")
    #         else:
    #             logger.debug(f"action epsilon set: {action_epsilon}")
    #         # Replay buffer size
    #         replay_buffer_size = config[model_type][f"{model_type}_replay_buffer_size"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} replay buffer size set: {replay_buffer_size}")
    #         else:
    #             logger.debug(f"replay buffer size set: {replay_buffer_size}")
    #         # Save dir
    #         save_dir = config[model_type][f"{model_type}_save_dir"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} save dir set: {save_dir}")
    #         else:
    #             logger.debug(f"save dir set: {save_dir}")

    #         # create replay buffer
    #         replay_buffer = ReplayBuffer(env, replay_buffer_size, device=device)

    #         # create TD3 agent
    #         td3_agent= cls(
    #             env = env,
    #             actor_model = actor_model,
    #             critic_model = critic_model,
    #             discount = config[model_type][f"{model_type}_discount"],
    #             tau = config[model_type][f"{model_type}_tau"],
    #             action_epsilon = action_epsilon,
    #             replay_buffer = replay_buffer,
    #             batch_size = config[model_type][f"{model_type}_batch_size"],
    #             noise = Noise.create_instance(config[model_type][f"{model_type}_noise"], shape=env.action_space.shape, **config[model_type][f"{model_type}_noise_{config[model_type][f'{model_type}_noise']}"], device=device),
    #             target_noise_stddev = config[model_type][f"{model_type}_target_action_stddev"],
    #             target_noise_clip = config[model_type][f"{model_type}_target_action_clip"],
    #             actor_update_delay = config[model_type][f"{model_type}_actor_update_delay"],
    #             warmup = config[model_type][f"{model_type}_warmup"],
    #             callbacks = callbacks,
    #             comm = comm,
    #             device = device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} TD3 agent built: {td3_agent.get_config()}")
    #         else:
    #             logger.debug(f"TD3 agent built: {td3_agent.get_config()}")
            
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier called")
    #         else:
    #             logger.debug(f"train barrier called")

    #         if comm is not None:
    #             comm.Barrier()
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier passed")

    #         td3_agent.train(
    #                 num_episodes=train_config['num_episodes'],
    #                 render=False,
    #                 render_freq=0,
    #                 )

    #     except Exception as e:
    #         logger.error(f"An error occurred: {e}", exc_info=True)

    # def train(
    #     self, num_episodes, render_freq: int = None, save_dir=None, run_number=None):
    #     """Trains the model for 'episodes' number of episodes."""

    #     # set models to train mode
    #     self.actor_model.train()
    #     self.critic_model_a.train()
    #     self.critic_model_b.train()

    #     # Update save_dir if passed
    #     if save_dir is not None and save_dir.split("/")[-2] != "td3":
    #         self.save_dir = save_dir + "/td3/"
    #         print(f'new save dir: {self.save_dir}')
    #     elif save_dir is not None and save_dir.split("/")[-2] == "td3":
    #         self.save_dir = save_dir
    #         print(f'new save dir: {self.save_dir}')
        
    #     if self.callbacks:
    #         for callback in self.callbacks:
    #                 self._config = callback._config(self)
    #         if self.use_mpi:
    #             if self.rank == 0:
    #                 for callback in self.callbacks:
    #                     if isinstance(callback, rl_callbacks.WandbCallback):
    #                         callback.on_train_begin((self.critic_model_a, self.critic_model_b, self.actor_model,), logs=self._config)
    #                         # logger.debug(f'{self.group}; Rank {self.rank} TD3.train on train begin callback complete')
    #                     else:
    #                         callback.on_train_begin(logs=self._config)
    #         else:
    #             for callback in self.callbacks:
    #                 if isinstance(callback, rl_callbacks.WandbCallback):
    #                     callback.on_train_begin((self.critic_model_a, self.critic_model_b, self.actor_model,), logs=self._config)
    #                     # logger.debug(f'TD3.train on train begin callback complete')
    #                 else:
    #                     callback.on_train_begin(logs=self._config)

        
    #     if self.use_mpi:
    #         try:
    #             # instantiate new environment. Only rank 0 env will render episodes if render==True
    #             if self.rank == 0:
    #                 self.env = self._initialize_env(render, render_freq, context='train')
    #                 # logger.debug(f'{self.group}; Rank {self.rank} initiating environment with render {render}')
    #             else:
    #                 self.env = self._initialize_env(False, 0, context='train')
    #                 # logger.debug(f'{self.group}; Rank {self.rank} initializing environment')
    #         except Exception as e:
    #             logger.error(f"{self.group}; Rank {self.rank} Error in TD3.train agent._initialize_env process: {e}", exc_info=True)
        
    #     else:
    #         try:
    #             # instantiate new environment. Only rank 0 env will render episodes if render==True
    #             self.env = self._initialize_env(render, render_freq, context='train')
    #             # logger.debug(f'initiating environment with render {render}')
    #         except Exception as e:
    #             logger.error(f"Error in TD3.train agent._initialize_env process: {e}", exc_info=True)

    #     # initialize step counter (for logging)
    #     self._step = 1
    #     # set best reward
    #     try:
    #         best_reward = self.env.reward_range[0]
    #     except:
    #         best_reward = -np.inf
    #     # instantiate list to store reward history
    #     reward_history = []
    #     # instantiate lists to store time history
    #     episode_time_history = []
    #     step_time_history = []
    #     learning_time_history = []
    #     steps_per_episode_history = []  # List to store steps per episode

    #     # Calculate total_steps and wait_steps
    #     # max_episode_steps = self.env.spec.max_episode_steps
    #     # total_steps = num_episodes * max_episode_steps
    #     # profiling_steps = (self.profiler_active_steps + self.profiler_warmup_steps) * self.profiler_repeat
    #     # wait_steps = (total_steps - profiling_steps) // self.profiler_repeat

    #     # Profile setup
    #     # with torch.profiler.profile(
    #     #     activities=[
    #     #         torch.profiler.ProfilerActivity.CPU,
    #     #         torch.profiler.ProfilerActivity.CUDA,
    #     #     ],
    #     #     schedule=torch.profiler.schedule(
    #     #         wait=wait_steps,
    #     #         warmup=self.profiler_warmup_steps,
    #     #         active=self.profiler_active_steps,
    #     #         repeat=self.profiler_repeat
    #     #     ),
    #     #     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/td3'),
    #     #     record_shapes=True,
    #     #     profile_memory=True,
    #     #     with_stack=True
    #     # ) as prof:
    #     for i in range(num_episodes):
    #         episode_start_time = time.time()
    #         if self.callbacks:
    #             if self.use_mpi:
    #                 if self.rank == 0:
    #                     for callback in self.callbacks:
    #                         callback.on_train_epoch_begin(epoch=self._step, logs=None)
    #                         # logger.debug(f'{self.group}; Rank {self.rank} TD3.train on train epoch begin callback completed')
    #             else:
    #                 for callback in self.callbacks:
    #                     callback.on_train_epoch_begin(epoch=self._step, logs=None)
    #                     # logger.debug(f'TD3.train on train epoch begin callback completed')
    #         # reset noise
    #         if type(self.noise) == OUNoise:
    #             self.noise.reset()
    #         # reset environment
    #         state, _ = self.env.reset()
    #         done = False
    #         episode_reward = 0
    #         episode_steps = 0  # Initialize steps counter for the episode
    #         while not done:
    #             # run callbacks on train batch begin
    #             # if self.callbacks:
    #             #     for callback in self.callbacks:
    #             #         callback.on_train_step_begin(step=self._step, logs=None)
    #             step_start_time = time.time()
    #             action = self.get_action(state)
    #             next_state, reward, term, trunc, _ = self.env.step(action)
    #             # extract observation from next state if next_state is dict (robotics)
    #             if isinstance(next_state, dict):
    #                 next_state = next_state['observation']

    #             # store trajectory in replay buffer
    #             self.replay_buffer.add(state, action, reward, next_state, done)
    #             if term or trunc:
    #                 done = True
    #             episode_reward += reward
    #             state = next_state
    #             episode_steps += 1
                
    #             # check if enough samples in replay buffer and if so, learn from experiences
    #             if self.replay_buffer.counter > self.batch_size and self.replay_buffer.counter > self.warmup:
    #                 learn_time = time.time()
    #                 actor_loss, critic_loss = self.learn()
    #                 self._train_step_config["actor_loss"] = actor_loss
    #                 self._train_step_config["critic_loss"] = critic_loss

    #                 learning_time_history.append(time.time() - learn_time)
                
    #             step_time = time.time() - step_start_time
    #             step_time_history.append(step_time)

    #             self._train_step_config["step_reward"] = reward
    #             self._train_step_config["step_time"] = step_time
                
    #             # log to wandb if using wandb callback
    #             if self.callbacks:
    #                 if self.use_mpi:
    #                     # only have the main process log callback values to avoid multiple callback calls
    #                     if self.rank == 0:
    #                         for callback in self.callbacks:
    #                             callback.on_train_step_end(step=self._step, logs=self._train_step_config)
    #                             # logger.debug(f'{self.group}; Rank {self.rank} TD3.train on train step end callback completed')
    #                 else:
    #                     for callback in self.callbacks:
    #                         callback.on_train_step_end(step=self._step, logs=self._train_step_config)
    #                         # logger.debug(f'TD3.train on train step end callback completed')
                
    #             # prof.step()

    #             if not done:
    #                 self._step += 1
            
    #         episode_time = time.time() - episode_start_time
    #         episode_time_history.append(episode_time)
    #         reward_history.append(episode_reward)
    #         steps_per_episode_history.append(episode_steps) 
    #         avg_reward = np.mean(reward_history[-100:])
    #         avg_episode_time = np.mean(episode_time_history[-100:])
    #         avg_step_time = np.mean(step_time_history[-100:])
    #         avg_learn_time = np.mean(learning_time_history[-100:])
    #         avg_steps_per_episode = np.mean(steps_per_episode_history[-100:])  # Calculate average steps per episode

    #         self._train_episode_config['episode'] = i
    #         self._train_episode_config["episode_reward"] = episode_reward
    #         self._train_episode_config["avg_reward"] = avg_reward
    #         self._train_episode_config["episode_time"] = episode_time

    #         # check if best reward
    #         if avg_reward > best_reward:
    #             best_reward = avg_reward
    #             self._train_episode_config["best"] = True
    #             # save model
    #             self.save()
    #         else:
    #             self._train_episode_config["best"] = False

    #         if self.callbacks:
    #             if self.use_mpi:
    #                 if self.rank == 0:
    #                     for callback in self.callbacks:
    #                         callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)
    #                         # logger.debug(f'{self.group}; Rank {self.rank} TD3.train on train epoch callback completed')
    #             else:
    #                 for callback in self.callbacks:
    #                     callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)
    #                     # logger.debug(f'TD3.train on train epoch callback completed')

    #         print(f"episode {i+1}, score {episode_reward}, avg_score {avg_reward}, episode_time {episode_time:.2f}s, avg_episode_time {avg_episode_time:.2f}s, avg_step_time {avg_step_time:.6f}s, avg_learn_time {avg_learn_time:.6f}s, avg_steps_per_episode {avg_steps_per_episode:.2f}")

    #     if self.callbacks:
    #         if self.use_mpi:
    #             if self.rank == 0:
    #                 for callback in self.callbacks:
    #                     callback.on_train_end(logs=self._train_episode_config)
    #                     # logger.debug(f'{self.group}; Rank {self.rank} TD3.train on train end callback complete')
    #         else:
    #             for callback in self.callbacks:
    #                 callback.on_train_end(logs=self._train_episode_config)
    #                 # logger.debug(f'TD3.train on train end callback complete')
    #     # close the environment
    #     self.env.close()

    def train(self, num_episodes: int, num_envs: int, seed: int = None, render_freq: int = 0):
        """Trains the TD3 agent for a given number of episodes."""
        self.actor_model.train()
        self.critic_model_a.train()
        self.critic_model_b.train()
        self.num_envs = num_envs
        if seed is None:
            seed = np.random.randint(1000)
        set_seed(seed)
        if self.callbacks:
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    callback.on_train_begin((self.critic_model_a, self.critic_model_b, self.actor_model,), logs=self._config)
                else:
                    callback.on_train_begin(logs=self._config)
        try:
            # Use the EnvWrapper's _initialize_env method
            self.env.env = self.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error("Error in TD3.train during env initialization", exc_info=True)
        self._step = 0
        best_reward = -np.inf
        score_history = deque(maxlen=100)
        episode_scores = np.zeros(self.num_envs)
        self.completed_episodes = np.zeros(self.num_envs)
        states, _ = self.env.reset()
        while self.completed_episodes.sum() < num_episodes:
            self._step += 1
            rendered = False # Flag to keep track of render status to avoid rendering multiple times per step
            if self.callbacks:
                for callback in self.callbacks:
                    callback.on_train_epoch_begin(epoch=self._step, logs=None)
            actions = self.get_action(states)
            actions = self.env.format_actions(actions)
            next_states, rewards, terms, truncs, _ = self.env.step(actions)
            self._train_step_config["step_reward"] = rewards
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)
            for i in range(self.num_envs):
                self.replay_buffer.add(states[i], actions[i], rewards[i], next_states[i], dones[i])
                if dones[i]:
                    # increment completed episodes for env by 1
                    self.completed_episodes[i] += 1
                    score_history.append(episode_scores[i])
                    avg_reward = sum(score_history) / len(score_history)
                    self._train_episode_config['episode'] = int(self.completed_episodes.sum())
                    self._train_episode_config['episode_reward'] = episode_scores[i]
                    if avg_reward > best_reward:
                        best_reward = avg_reward
                        self._train_episode_config["best"] = 1
                        self.save()
                    else:
                        self._train_episode_config["best"] = 0
                    
                    # Check if number of completed episodes should trigger render
                    if self.completed_episodes.sum() % render_freq == 0 and not rendered:
                        print(f"Rendering episode {self.completed_episodes.sum()} during training...")
                        # Call the test function to render an episode
                        self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
                        # Add render to wandb log
                        video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes.sum()}.mp4")
                        # Log the video to wandb
                        if self.callbacks:
                            for callback in self.callbacks:
                                if isinstance(callback, WandbCallback):
                                    wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")}, step=self._step)
                                    rendered = True
                                    # Switch models back to train mode after rendering
                                    self.actor_model.train()
                                    self.critic_model_a.train()
                                    self.critic_model_b.train()
                                # else:
                                #     rendered = False


                    if self.callbacks:
                        for callback in self.callbacks:
                            callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)
                    print(f"Environment {i}: Episode {int(self.completed_episodes.sum())}, Score {episode_scores[i]}, Avg Score {avg_reward}")
                    episode_scores[i] = 0
            states = next_states
            if self.replay_buffer.counter > self.batch_size:
                actor_loss, critic_loss = self.learn()
                self._train_step_config["critic_loss"] = critic_loss
                if actor_loss is not None:
                    self._train_step_config["actor_loss"] = actor_loss
                # Step schedulers if not None
                if self.noise_schedule:
                    self.noise_schedule.step()
                    self._train_step_config["noise_anneal"] = self.noise_schedule.get_factor()
                if self.target_noise_schedule:
                    self.target_noise_schedule.step()
                    self._train_step_config["target_noise_anneal"] = self.target_noise_schedule.get_factor()
                if self.callbacks:
                    for callback in self.callbacks:
                        callback.on_train_step_end(step=self._step, logs=self._train_step_config)
        if self.callbacks:
            for callback in self.callbacks:
                callback.on_train_end(logs=self._train_episode_config)

       
    # def test(self, num_episodes, render, render_freq):
    #     """Runs a test over 'num_episodes'."""

    #     # set model in eval mode
    #     self.actor_model.eval()
    #     self.critic_model_a.eval()
    #     self.critic_model_b.eval()

    #     # instantiate list to store reward history
    #     reward_history = []
    #     # instantiate new environment
    #     self.env = self._initialize_env(render, render_freq, context='test')
    #     if self.callbacks:
    #         for callback in self.callbacks:
    #             callback.on_test_begin(logs=self._config)

    #     self._step = 1
    #     # set the model to calculate no gradients during evaluation
    #     with T.no_grad():
    #         for i in range(num_episodes):
    #             if self.callbacks:
    #                 for callback in self.callbacks:
    #                     callback.on_test_epoch_begin(epoch=self._step, logs=None) # update to pass any logs if needed
    #             states = []
    #             next_states = []
    #             actions = []
    #             rewards = []
    #             state, _ = self.env.reset()
    #             done = False
    #             episode_reward = 0
    #             while not done:
    #                 action = self.get_action(state, test=True)
    #                 next_state, reward, term, trunc, _ = self.env.step(action)
    #                 # extract observation from next state if next_state is dict (robotics)
    #                 if isinstance(next_state, dict):
    #                     next_state = next_state['observation']
    #                 # store trajectories
    #                 states.append(state)
    #                 actions.append(action)
    #                 next_states.append(next_state)
    #                 rewards.append(reward)
    #                 if term or trunc:
    #                     done = True
    #                 episode_reward += reward
    #                 state = next_state
    #                 self._step += 1
    #             reward_history.append(episode_reward)
    #             avg_reward = np.mean(reward_history[-100:])
    #             self._test_episode_config["episode_reward"] = episode_reward
    #             self._test_episode_config["avg_reward"] = avg_reward
    #             if self.callbacks:
    #                 for callback in self.callbacks:
    #                     callback.on_test_epoch_end(epoch=self._step, logs=self._test_episode_config)

    #             print(f"episode {i+1}, score {episode_reward}, avg_score {avg_reward}")

    #         if self.callbacks:
    #             for callback in self.callbacks:
    #                 callback.on_test_end(logs=self._test_episode_config)
    #         # close the environment
    #         self.env.close()

    def test(self, num_episodes: int, num_envs: int = 1, seed: int = None, render_freq: int = 0, training: bool = False):
        """Tests the TD3 agent for a given number of episodes."""
        self.actor_model.eval()
        self.critic_model_a.eval()
        self.critic_model_b.eval()
        if seed is None:
            seed = np.random.randint(100)
        if render_freq is None:
            render_freq = 0
        set_seed(seed)
        try:
            env = self.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error("Error in TD3.test during env initialization", exc_info=True)
        if self.callbacks and not training:
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)
        _step = 0
        completed_episodes = np.zeros(num_envs)
        episode_scores = np.zeros(num_envs)
        completed_scores = deque(maxlen=num_episodes)
        frames = []
        states, _ = env.reset()
        while completed_episodes.sum() < num_episodes:
            _step += 1
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_begin(epoch=_step, logs=None)
            actions = self.get_action(states, test=True)
            actions = self.env.format_actions(actions, testing=True)
            next_states, rewards, terms, truncs, _ = env.step(actions)
            self._test_step_config["step_reward"] = rewards
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)
            completed_episodes += dones
            for i in range(num_envs):
                if dones[i]:
                    completed_scores.append(episode_scores[i])
                    self._test_episode_config["episode_reward"] = episode_scores[i]
                    # Save the video if the episode number is divisible by render_freq
                    if (render_freq > 0) and ((completed_episodes.sum()) % render_freq == 0):
                        if training:
                            render_video(frames, self.completed_episodes.sum(), self.save_dir, 'train')
                        else:
                            render_video(frames, completed_episodes.sum(), self.save_dir, 'test')
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/test/episode_{completed_episodes.sum()}.mp4")
                            # Log the video to wandb
                            if self.callbacks:
                                for callback in self.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})
                        # Empty frames array
                        frames = []
                    if self.callbacks and not training:
                        for callback in self.callbacks:
                            callback.on_test_epoch_end(epoch=_step, logs=self._test_episode_config)
                    print(f"Environment {i}: Episode {int(completed_episodes.sum())}/{num_episodes} Score: {completed_scores[-1]} Avg Score: {sum(completed_scores)/len(completed_scores)}")
                    episode_scores[i] = 0
            if render_freq > 0:
                frame = env.render()[0]
                frames.append(frame)
                states = next_states
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_step_end(step=_step, logs=self._test_step_config)
        if self.callbacks and not training:
            for callback in self.callbacks:
                callback.on_test_end(logs=self._test_episode_config)


    def get_config(self):
        return {
            "agent_type": "TD3",
            "env": self.env.to_json(),
            "actor_model": self.actor_model.get_config(),
            "critic_model_a": self.critic_model_a.get_config(),
            "critic_model_b": self.critic_model_b.get_config(),
            "discount": self.discount,
            "tau": self.tau,
            "action_epsilon": self.action_epsilon,
            "replay_buffer": self.replay_buffer.get_config() if self.replay_buffer is not None else None,
            "batch_size": self.batch_size,
            "noise": self.noise.get_config() if self.noise is not None else None,
            "noise_schedule": self.noise_schedule.get_config() if self.noise_schedule is not None else None,
            "target_noise": self.target_noise.get_config() if self.target_noise is not None else None,
            "target_noise_schedule": self.target_noise_schedule.get_config() if self.target_noise_schedule is not None else None,
            "target_noise_clip": self.target_noise_clip,
            "actor_update_delay": self.actor_update_delay,
            "normalize_inputs": self.normalize_inputs,
            "normalizer_clip": self.normalizer_clip,
            "normalizer_eps": self.normalizer_eps,
            "warmup": self.warmup,
            "callbacks": [callback.get_config() for callback in self.callbacks] if self.callbacks else None,
            "save_dir": self.save_dir,
            "device": self.device.type,
            }

    def save(self):

        config = self.get_config()
        os.makedirs(self.save_dir, exist_ok=True)
        with open(os.path.join(self.save_dir, "config.json"), "w", encoding="utf-8") as f:
            json.dump(config, f)
        self.actor_model.save(self.save_dir)
        self.critic_model_a.save(self.save_dir)
        self.critic_model_b.save(self.save_dir)
        if self.normalize_inputs:
            self.state_normalizer.save_state(os.path.join(self.save_dir, "state_normalizer.npz"))


    # def save(self, save_dir=None):
    #     """Saves the model."""

    #     # Change self.save_dir if save_dir
    #     # if save_dir is not None:
    #     #     self.save_dir = save_dir + "/ddpg/"

    #     config = self.get_config()

    #     # makes directory if it doesn't exist
    #     os.makedirs(self.save_dir, exist_ok=True)

    #     # writes and saves JSON file of DDPG agent config
    #     with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
    #         json.dump(config, f, cls=CustomJSONEncoder)

    #     # saves policy and value model
    #     self.actor_model.save(self.save_dir)
    #     self.critic_model_a.save(self.save_dir)
    #     self.critic_model_b.save(self.save_dir)

    #     if self.normalize_inputs:
    #         self.state_normalizer.save_state(self.save_dir + "state_normalizer.npz")

        # if wandb callback, save wandb config
        # if self._wandb:
        #     for callback in self.callbacks:
        #         if isinstance(callback, rl_callbacks.WandbCallback):
        #             callback.save(self.save_dir + "/wandb_config.json")


    # @classmethod
    # def load(cls, config, load_weights=True):
    #     """Loads the model."""
    #     # # load reinforce agent config
    #     # with open(
    #     #     Path(folder).joinpath(Path("obj_config.json")), "r", encoding="utf-8"
    #     # ) as f:
    #     #     config = json.load(f)

    #     # create EnvSpec from config
    #     env_spec_json = json.dumps(config["env"])
    #     env_spec = gym.envs.registration.EnvSpec.from_json(env_spec_json)

    #     # load policy model
    #     actor_model = models.ActorModel.load(config['save_dir'], load_weights)
    #     # load value model
    #     critic_model = models.CriticModel.load(config['save_dir'], load_weights)
    #     # load replay buffer if not None
    #     if config['replay_buffer'] is not None:
    #         config['replay_buffer']['config']['env'] = gym.make(env_spec)
    #         replay_buffer = ReplayBuffer(**config["replay_buffer"]["config"])
    #     else:
    #         replay_buffer = None
    #     # load noise
    #     noise = Noise.create_instance(config["noise"]["class_name"], **config["noise"]["config"])
    #     # load callbacks
    #     callbacks = [rl_callbacks.load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]

    #     # return TD3 agent
    #     agent = cls(
    #         gym.make(env_spec),
    #         actor_model = actor_model,
    #         critic_model = critic_model,
    #         discount=config["discount"],
    #         tau=config["tau"],
    #         action_epsilon=config["action_epsilon"],
    #         replay_buffer=replay_buffer,
    #         batch_size=config["batch_size"],
    #         noise=noise,
    #         target_noise_stddev = config['target_noise_stddev'],
    #         target_noise_clip = config['target_noise_clip'],
    #         actor_update_delay = config['actor_update_delay'],
    #         normalize_inputs = config['normalize_inputs'],
    #         normalizer_clip = config['normalizer_clip'],
    #         warmup = config['warmup'],
    #         callbacks=callbacks,
    #         save_dir=config["save_dir"],
    #         device=config["device"],
    #     )

    #     if agent.normalize_inputs:
    #         agent.state_normalizer = Normalizer.load_state(config['save_dir'] + "state_normalizer.npz")

    #     return agent

    @classmethod
    def load(cls, config, load_weights=True):
        """Loads the model."""
        # Load EnvWrapper
        env_wrapper = EnvWrapper.from_json(config["env"])
            
        # load policy model
        actor_model = ActorModel.load(config['save_dir'], load_weights)
        # load value model
        critic_model = CriticModel.load(config['save_dir'], load_weights)
        # load replay buffer if not None
        if config['replay_buffer'] is not None:
            config['replay_buffer']['config']['env'] = env_wrapper
            replay_buffer = ReplayBuffer(**config["replay_buffer"]["config"])
        else:
            replay_buffer = None
        noise = Noise.create_instance(config["noise"]["class_name"], **config["noise"]["config"])
        target_noise = Noise.create_instance(config["target_noise"]["class_name"], **config["target_noise"]["config"])

        normalize_inputs = config['normalize_inputs']
        normalizer_clip = config['normalizer_clip']
        callbacks = [callback_load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]\
                    if config['callbacks'] else None

        agent = cls(
            env=env_wrapper,
            actor_model=actor_model,
            critic_model=critic_model,
            discount=config["discount"],
            tau=config["tau"],
            action_epsilon=config["action_epsilon"],
            replay_buffer=replay_buffer,
            batch_size=config["batch_size"],
            noise=noise,
            noise_schedule=ScheduleWrapper(config["noise_schedule"]),
            target_noise=target_noise,
            target_noise_schedule=ScheduleWrapper(config["target_noise_schedule"]),
            target_noise_clip=config["target_noise_clip"],
            actor_update_delay=config["actor_update_delay"],
            normalize_inputs=config["normalize_inputs"],
            normalizer_clip=config["normalizer_clip"],
            normalizer_eps=config["normalizer_eps"],
            warmup=config["warmup"],
            callbacks=callbacks,
            save_dir=config["save_dir"],
            device=config["device"],
        )
        if agent.normalize_inputs:
            agent.state_normalizer = Normalizer.load_state(os.path.join(config["save_dir"], "state_normalizer.npz"))
        return agent

class HER(Agent):
    """Hindsight Experience Replay Agent wrapper."""

    def __init__(
        self,
        agent: Agent,
        strategy: str = 'final',
        tolerance: float = 0.5,
        num_goals: int = 4,
        normalizer_clip: float = 5.0,
        normalizer_eps: float = 0.01,
        device: str = None,
        save_dir: str = "models",
        # callbacks: Optional[list[Callback]] = None
    ):
        """
        Initializes the HER agent wrapper.

        Args:
            agent (Agent): The underlying agent (e.g., DDPG, TD3) to wrap with HER.
            strategy (str): HER strategy for goal sampling ('final', 'future', etc.).
            tolerance (float): Distance threshold for success determination.
            num_goals (int): Number of goals to sample for hindsight replay.
            normalizer_clip (float): Clipping value for state and goal normalizers.
            normalizer_eps (float): Epsilon for numerical stability in normalizers.
            replay_buffer_size (int): Size of the replay buffer.
            device (str): Device for computation ('cuda' or 'cpu').
            save_dir (str): Directory to save models and logs.
            # callbacks (Optional[list[Callback]]): List of callbacks for training.
        """
        try:
            # Set device
            self.device = T.device("cuda" if device == 'cuda' and T.cuda.is_available() else "cpu")
            self.agent = agent
            self.strategy = strategy
            self.tolerance = tolerance
            self.num_goals = num_goals
            self.normalizer_clip = normalizer_clip
            self.normalizer_eps = normalizer_eps
            # self.replay_buffer_size = replay_buffer_size
            
            # Set save directory
            # if save_dir is not None and "/her/" not in save_dir:
            #     self.save_dir = os.path.join(save_dir, "her")
            #     agent_name = os.path.basename(os.path.dirname(self.agent.save_dir))
            #     self.agent.save_dir = os.path.join(self.save_dir, agent_name)
            # elif save_dir is not None:
            #     self.save_dir = save_dir
            #     agent_name = os.path.basename(os.path.dirname(self.agent.save_dir))
            #     self.agent.save_dir = os.path.join(self.save_dir, agent_name)
            if save_dir is not None and "/her/" not in save_dir:
                self.save_dir = save_dir + "/her/"
            elif save_dir is not None:
                self.save_dir = save_dir

        except Exception as e:
            logger.error(f"Error in HER init: {e}", exc_info=True)

        # Internal attributes
        try:
            obs_space = (self.agent.env.single_observation_space 
                        if hasattr(self.agent.env, "single_observation_space")
                        else self.agent.env.observation_space)
            if isinstance(obs_space, gym.spaces.Dict):
                self._obs_space_shape = obs_space['observation'].shape
                self._goal_shape = obs_space['desired_goal'].shape
            else:
                raise ValueError("HER requires a goal-aware observation space (gym.spaces.Dict)")

            # Initialize HER flag in agent
            self.agent._init_her()
            
            # Set distance threshold based on environment type
            if isinstance(self.agent.env.env, gym.vector.SyncVectorEnv):
                # Vectorized environment: set distance_threshold for each sub-environment
                for i in range(len(self.agent.env.env.envs)):
                    base_env = self.agent.env.get_base_env(i)
                    if hasattr(base_env, "distance_threshold"):
                        base_env.distance_threshold = self.tolerance
                    else:
                        logger.warning(f"Environment {base_env} does not have distance_threshold attribute")
            else:
                # Non-vectorized environment: set directly if attribute exists
                if hasattr(self.agent.env.env, "distance_threshold"):
                    self.agent.env.env.distance_threshold = self.tolerance
                else:
                    logger.warning("Underlying environment does not have distance_threshold attribute")

            # Initialize replay buffer and normalizers
            # self.replay_buffer = ReplayBuffer(
            #     env=self.agent.env,
            #     buffer_size=self.replay_buffer_size,
            #     goal_shape=self._goal_shape,
            #     device=self.device.type,
            # )
            self.state_normalizer = Normalizer(
                self._obs_space_shape, self.normalizer_eps, self.normalizer_clip, self.device
            )
            self.goal_normalizer = Normalizer(
                self._goal_shape, self.normalizer_eps, self.normalizer_clip, self.device
            )

        except Exception as e:
            logger.error(f"Error in HER init internal attributes: {e}", exc_info=True)

        # # Set callbacks
        # try:
        #     self.callbacks = callbacks if callbacks else self.agent.callbacks
        #     if self.callbacks:
        #         for callback in self.callbacks:
        #             self._config = callback._config(self)
        #             if isinstance(callback, WandbCallback):
        #                 self._wandb = True
        #     else:
        #         self._wandb = False
        # except Exception as e:
        #     logger.error(f"Error in HER init callbacks: {e}", exc_info=True)

        # Initialize config dictionaries
        # self._train_config = {}
        # self._train_episode_config = {}
        # self._train_step_config = {}
        # self._test_config = {}
        # self._test_episode_config = {}
        # self._test_step_config = {}
        # self._step = 0


        
    # @classmethod
    # def sweep_train(
    #     cls,
    #     config, # wandb.config,
    #     train_config,
    #     env_spec,
    #     callbacks,
    #     run_number,
    #     comm=None,
    # ):
    #     """Builds and trains agents from sweep configs. Works with MPI"""
    #     rank = MPI.COMM_WORLD.rank

    #     if comm is not None:
    #         logger.debug(f"Rank {rank} comm detected")
    #         rank = comm.Get_rank()
    #         logger.debug(f"Global rank {MPI.COMM_WORLD.Get_rank()} in {comm.Get_name()} set to comm rank {rank}")

    #         logger.debug(f"init_sweep fired: global rank {MPI.COMM_WORLD.rank}, group rank {rank}, {comm.Get_name()}")
    #     else:
    #         logger.debug(f"init_sweep fired: global rank")
    #     try:
    #         # rank = MPI.COMM_WORLD.rank
    #         # Instantiate her variable 
    #         her = None
    #         # Instantiate env from env_spec
    #         env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))
    #         # agent_config_path = f'sweep/agent_config_{run_number}.json'
    #         # logger.debug(f"rank {rank} agent config path: {agent_config_path}")
    #         model_type = list(config.keys())[0]
    #         # config = wandb.config
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train config: {train_config}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} env spec id: {env.spec.id}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} callbacks: {callbacks}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} run number: {run_number}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} config set: {config}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} model type: {model_type}")
    #             # Only primary process (rank 0) calls wandb.init() to build agent and log data
    #         else:
    #             logger.debug(f"train config: {train_config}")
    #             logger.debug(f"env spec id: {env.spec.id}")
    #             logger.debug(f"callbacks: {callbacks}")
    #             logger.debug(f"run number: {run_number}")
    #             logger.debug(f"config set: {config}")
    #             logger.debug(f"model type: {model_type}")

    #         actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels = wandb_support.format_layers(config)
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} layers built")
    #         else:
    #             logger.debug(f"layers built")
    #         # Actor
    #         actor_learning_rate=config[model_type][f"{model_type}_actor_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor learning rate set")
    #         else:
    #             logger.debug(f"actor learning rate set")
    #         actor_optimizer = config[model_type][f"{model_type}_actor_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer set")
    #         else:
    #             logger.debug(f"actor optimizer set")
    #         # get optimizer params
    #         actor_optimizer_params = {}
    #         if actor_optimizer == "Adam":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
            
    #         elif actor_optimizer == "Adagrad":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_lr_decay']
            
    #         elif actor_optimizer == "RMSprop" or actor_optimizer == "SGD":
    #             actor_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_weight_decay']
    #             actor_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_actor_optimizer_{actor_optimizer}_options"][f'{actor_optimizer}_momentum']

    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor optimizer params set")
    #         else:
    #             logger.debug(f"actor optimizer params set")
    #         actor_normalize_layers = config[model_type][f"{model_type}_actor_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor normalize layers set")
    #         else:
    #             logger.debug(f"actor normalize layers set")
    #         # Critic
    #         critic_learning_rate=config[model_type][f"{model_type}_critic_learning_rate"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic learning rate set")
    #         else:
    #             logger.debug(f"critic learning rate set")
    #         critic_optimizer = config[model_type][f"{model_type}_critic_optimizer"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer set")
    #         else:
    #             logger.debug(f"critic optimizer set")
    #         critic_optimizer_params = {}
    #         if critic_optimizer == "Adam":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
            
    #         elif critic_optimizer == "Adagrad":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['lr_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_lr_decay']
            
    #         elif critic_optimizer == "RMSprop" or critic_optimizer == "SGD":
    #             critic_optimizer_params['weight_decay'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_weight_decay']
    #             critic_optimizer_params['momentum'] = \
    #                 config[model_type][f"{model_type}_critic_optimizer_{critic_optimizer}_options"][f'{critic_optimizer}_momentum']
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic optimizer params set")
    #         else:
    #             logger.debug(f"critic optimizer params set")

    #         critic_normalize_layers = config[model_type][f"{model_type}_critic_normalize_layers"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic normalize layers set")
    #         else:
    #             logger.debug(f"critic normalize layers set")
    #         # Set device
    #         device = config[model_type][f"{model_type}_device"]
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} device set")
    #         else:
    #             logger.debug(f"device set")
    #         # Check if CNN layers and if so, build CNN model
    #         if actor_cnn_layers:
    #             actor_cnn_model = cnn_models.CNN(actor_cnn_layers, env)
    #         else:
    #             actor_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor cnn layers set: {actor_cnn_layers}")
    #         else:
    #             logger.debug(f"actor cnn layers set: {actor_cnn_layers}")

    #         if critic_cnn_layers:
    #             critic_cnn_model = cnn_models.CNN(critic_cnn_layers, env)
    #         else:
    #             critic_cnn_model = None
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic cnn layers set: {critic_cnn_layers}")
    #         else:
    #             logger.debug(f"critic cnn layers set: {critic_cnn_layers}")
    #         # get desired, achieved, reward func for env
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} second call env.spec: {env.spec.id}")
    #         else:
    #             logger.debug(f"second call env.spec: {env.spec.id}")
    #         desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} goal function set")
    #         else:
    #             logger.debug(f"goal function set")
    #         # Reset env state to initiate state to detect correct goal shape
    #         _,_ = env.reset()
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} env reset")
    #         else:
    #             logger.debug(f"env reset")
    #         goal_shape = desired_goal_func(env).shape
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} goal shape set: {goal_shape}")
    #         else:
    #             logger.debug(f"goal shape set: {goal_shape}")
    #         # Get actor clamp value
    #         # clamp_output = config[model_type][f"{model_type}_actor_clamp_output"]
    #         # logger.debug(f"{comm.Get_name()}; Rank {rank} clamp output set: {clamp_output}")
    #         actor_model = models.ActorModel(env = env,
    #                                         cnn_model = actor_cnn_model,
    #                                         dense_layers = actor_layers,
    #                                         output_layer_kernel=kernels[f'actor_output_kernel'],
    #                                         goal_shape=goal_shape,
    #                                         optimizer = actor_optimizer,
    #                                         optimizer_params = actor_optimizer_params,
    #                                         learning_rate = actor_learning_rate,
    #                                         normalize_layers = actor_normalize_layers,
    #                                         # clamp_output=clamp_output,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} actor model built: {actor_model.get_config()}")
    #         else:
    #             logger.debug(f"actor model built: {actor_model.get_config()}")
    #         critic_model = models.CriticModel(env = env,
    #                                         cnn_model = critic_cnn_model,
    #                                         state_layers = critic_state_layers,
    #                                         merged_layers = critic_merged_layers,
    #                                         output_layer_kernel=kernels[f'critic_output_kernel'],
    #                                         goal_shape=goal_shape,
    #                                         optimizer = critic_optimizer,
    #                                         optimizer_params = critic_optimizer_params,
    #                                         learning_rate = critic_learning_rate,
    #                                         normalize_layers = critic_normalize_layers,
    #                                         device=device,
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} critic model built: {critic_model.get_config()}")
    #         else:
    #             logger.debug(f"critic model built: {critic_model.get_config()}")
    #         # get goal metrics
    #         strategy = config[model_type][f"{model_type}_goal_strategy"]
            
    #         tolerance = config[model_type][f"{model_type}_goal_tolerance"]
            
    #         num_goals = config[model_type][f"{model_type}_num_goals"]
            
    #         # get normalizer clip value
    #         normalizer_clip = config[model_type][f"{model_type}_normalizer_clip"]
            
    #         # get action epsilon
    #         action_epsilon = config[model_type][f"{model_type}_epsilon_greedy"]
            
    #         # Replay buffer size
    #         replay_buffer_size = config[model_type][f"{model_type}_replay_buffer_size"]
            
    #         # Save dir
    #         save_dir = config[model_type][f"{model_type}_save_dir"]
            
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} strategy set: {strategy}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} tolerance set: {tolerance}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} num goals set: {num_goals}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} normalizer clip set: {normalizer_clip}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} action epsilon set: {action_epsilon}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} replay buffer size set: {replay_buffer_size}")
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} save dir set: {save_dir}")
    #         else:
    #             logger.debug(f"strategy set: {strategy}")
    #             logger.debug(f"tolerance set: {tolerance}")
    #             logger.debug(f"num goals set: {num_goals}")
    #             logger.debug(f"normalizer clip set: {normalizer_clip}")
    #             logger.debug(f"action epsilon set: {action_epsilon}")
    #             logger.debug(f"replay buffer size set: {replay_buffer_size}")
    #             logger.debug(f"save dir set: {save_dir}")
            
            
    #         if model_type == "HER_DDPG":
    #             ddpg_agent= DDPG(
    #                 env = env,
    #                 actor_model = actor_model,
    #                 critic_model = critic_model,
    #                 discount = config[model_type][f"{model_type}_discount"],
    #                 tau = config[model_type][f"{model_type}_tau"],
    #                 action_epsilon = action_epsilon,
    #                 replay_buffer = None,
    #                 batch_size = config[model_type][f"{model_type}_batch_size"],
    #                 noise = Noise.create_instance(config[model_type][f"{model_type}_noise"], shape=env.action_space.shape, **config[model_type][f"{model_type}_noise_{config[model_type][f'{model_type}_noise']}"], device=device),
    #                 callbacks = callbacks,
    #                 comm = comm
    #             )
    #             if comm is not None:
    #                 logger.debug(f"{comm.Get_name()}; Rank {rank} ddpg agent built: {ddpg_agent.get_config()}")
    #             else:
    #                 logger.debug(f"ddpg agent built: {ddpg_agent.get_config()}")

    #         elif model_type == "HER_TD3":
    #             ddpg_agent= TD3(
    #                 env = env,
    #                 actor_model = actor_model,
    #                 critic_model = critic_model,
    #                 discount = config[model_type][f"{model_type}_discount"],
    #                 tau = config[model_type][f"{model_type}_tau"],
    #                 action_epsilon = action_epsilon,
    #                 replay_buffer = None,
    #                 batch_size = config[model_type][f"{model_type}_batch_size"],
    #                 noise = Noise.create_instance(config[model_type][f"{model_type}_noise"], shape=env.action_space.shape, **config[model_type][f"{model_type}_noise_{config[model_type][f'{model_type}_noise']}"], device=device),
    #                 target_noise_stddev= config[model_type][f"{model_type}_target_action_stddev"],
    #                 target_noise_clip= config[model_type][f"{model_type}_target_action_clip"],
    #                 actor_update_delay= config[model_type][f"{model_type}_actor_update_delay"],
    #                 callbacks = callbacks,
    #                 comm = comm
    #             )
    #             if comm is not None:
    #                 logger.debug(f"{comm.Get_name()}; Rank {rank} ddpg agent built: {ddpg_agent.get_config()}")
    #             else:
    #                 logger.debug(f"ddpg agent built: {ddpg_agent.get_config()}")

    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} build barrier called")
    #             comm.Barrier()
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} build barrier passed")

    #         her = cls(
    #             agent = ddpg_agent,
    #             strategy = strategy,
    #             tolerance = tolerance,
    #             num_goals = num_goals,
    #             desired_goal = desired_goal_func,
    #             achieved_goal = achieved_goal_func,
    #             reward_fn = reward_func,
    #             normalizer_clip = normalizer_clip,
    #             replay_buffer_size = replay_buffer_size,
    #             device = device,
    #             save_dir = save_dir,
    #             comm = comm
    #         )
    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} her agent built: {her.get_config()}")
    #         else:
    #             logger.debug(f"her agent built: {her.get_config()}")

    #         if comm is not None:
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier called")
    #             comm.Barrier()
    #             logger.debug(f"{comm.Get_name()}; Rank {rank} train barrier passed")

    #         her.train(
    #                 num_epochs=train_config['num_epochs'],
    #                 num_cycles=train_config['num_cycles'],
    #                 num_episodes=train_config['num_episodes'],
    #                 num_updates=train_config['num_updates'],
    #                 render=False,
    #                 render_freq=0,
    #                 )

    #     except Exception as e:
    #         logger.error(f"An error occurred: {e}", exc_info=True)

    # def train(self, num_episodes: int, num_updates: int, render_freq: int, num_envs: int = 1, seed: int = None):
    #     """
    #     Train the HER agent with a vectorized environment setup.

    #     Args:
    #         num_epochs (int): Number of training epochs.
    #         num_cycles (int): Number of cycles per epoch.
    #         num_episodes (int): Number of episodes per cycle.
    #         num_updates (int): Number of learning updates per step when buffer is sufficiently large.
    #         render (bool): Whether to render the environment.
    #         render_freq (int): Frequency of rendering.
    #         save_dir (str, optional): Directory to save models.
    #         num_envs (int): Number of parallel environments (default: 1).
    #         seed (int, optional): Random seed for reproducibility.
    #     """
    #     try:
    #         logger.debug("HER train fired")

    #         # Set models to train mode
    #         self.agent.actor_model.train()
    #         if hasattr(self.agent, 'critic_model'):
    #             self.agent.critic_model.train()  # For DDPG
    #         if hasattr(self.agent, 'critic_model_a'):
    #             self.agent.critic_model_a.train()  # For TD3
    #         if hasattr(self.agent, 'critic_model_b'):
    #             self.agent.critic_model_b.train()  # For TD3

    #         # Update agent config
    #         if self.agent.callbacks:
    #             self.agent._config.update({
    #                 # 'num_epochs': num_epochs,
    #                 # 'num_cycles': num_cycles,
    #                 'num_episodes': num_episodes,
    #                 'num_updates': num_updates,
    #                 'tolerance': self.tolerance,
    #                 'num_envs': num_envs,
    #                 'seed': seed
    #             })
    #             logger.debug("HER.train: train config added to agent config")

    #         # Initialize callbacks
    #         if self.agent.callbacks:
    #             for callback in self.agent.callbacks:
    #                 if isinstance(callback, WandbCallback):
    #                     models = (self.agent.critic_model, self.agent.actor_model)
    #                     if isinstance(self.agent, TD3):
    #                         models = (self.agent.critic_model_a, self.agent.critic_model_b, self.agent.actor_model)
    #                     callback.on_train_begin(models, logs=self.agent._config)
    #                 else:
    #                     callback.on_train_begin(logs=self.agent._config)

    #         # Initialize environment
    #         try:
    #             self.agent.env.env = self.agent.env._initialize_env(render_freq, num_envs, seed)
    #             logger.debug(f"Initializing environment with render_freq={render_freq}, num_envs={num_envs}, seed={seed}")
    #         except Exception as e:
    #             logger.error(f"Error in HER.train environment initialization: {e}", exc_info=True)

    #         # Initialize counters and histories
    #         self.agent._step = 0  # Use agent's step counter for consistency with TD3/DDPG
    #         self.completed_episodes = np.zeros(num_envs)
    #         episode_scores = np.zeros(num_envs)
    #         score_history = deque(maxlen=100)
    #         trajectory_buffers = [[] for _ in range(num_envs)]
    #         best_reward = -np.inf
    #         success_counter = 0.0

    #         states, _ = self.agent.env.reset()

    #         # Total episodes across all environments
    #         # total_episodes = num_epochs * num_cycles * num_episodes
    #         while self.completed_episodes.sum() < num_episodes:
    #             self.agent._step += 1
    #             rendered = False # Flag to keep track of render status to avoid rendering multiple times per step

    #             if self.agent.callbacks:
    #                 for callback in self.agent.callbacks:
    #                     callback.on_train_epoch_begin(epoch=self.agent._step, logs=None)

    #             # Get actions for all environments
    #             actions = self.agent.get_action(
    #                 states['observation'],
    #                 states['desired_goal'],
    #                 test=False,  # Training mode
    #                 state_normalizer=self.state_normalizer,
    #                 goal_normalizer=self.goal_normalizer
    #             )
    #             # Format actions for vectorized environment
    #             formatted_actions = self.agent.env.format_actions(actions)
    #             # Step the environment
    #             next_states, rewards, terms, truncs, _ = self.agent.env.step(formatted_actions)
    #             #DEBUG
    #             # if np.any(rewards == 0.0):
    #             #     print(f"0 reward found at step {self.agent._step}: {rewards}")
    #             dones = np.logical_or(terms, truncs)
    #             episode_scores += rewards

    #             for i in range(num_envs):
    #                 # Add original transition to replay buffer
    #                 self.agent.replay_buffer.add(
    #                     states['observation'][i],
    #                     actions[i],
    #                     rewards[i],
    #                     next_states['observation'][i],
    #                     dones[i],
    #                     states['achieved_goal'][i],
    #                     next_states['achieved_goal'][i],
    #                     states['desired_goal'][i]
    #                 )

    #                 # Append transition to trajectory buffer
    #                 trajectory_buffers[i].append({
    #                     'state': states['observation'][i],
    #                     'action': actions[i],
    #                     'reward': rewards[i],
    #                     'next_state': next_states['observation'][i],
    #                     'done': dones[i],
    #                     'achieved_goal': states['achieved_goal'][i],
    #                     'next_achieved_goal': next_states['achieved_goal'][i],
    #                     'desired_goal': states['desired_goal'][i]
    #                 })

    #                 # Update normalizer local stats with obs and goal info
    #                 self.state_normalizer.update_local_stats(
    #                     T.tensor(states['observation'][i], dtype=T.float32,
    #                              device=self.state_normalizer.device.type)
    #                 )

    #                 self.goal_normalizer.update_local_stats(
    #                     T.tensor(states['achieved_goal'][i], dtype=T.float32,
    #                              device=self.state_normalizer.device.type)
    #                 )

    #                 # calculate success rate
    #                 goal_distance = np.linalg.norm(states['achieved_goal'][i] - states['desired_goal'][i], axis=-1)
    #                 # goal_distance = self.agent.env.get_base_env().goal_distance(states['achieved_goal'][i], states['desired_goal'][i])
    #                 success = (goal_distance <= self.tolerance).astype(np.float32)
    #                 # success = self.agent.env.get_base_env(i)._is_success(states['achieved_goal'][i], states['desired_goal'][i])
    #                 #DEBUG
    #                 # print(f'goal distance:{goal_distance}')
    #                 # print(f'success:{success}')
    #                 success_counter += success
    #                 # print(f'success counter:{success_counter}')
    #                 # To correctly calculate success percentage, must divide success counter by
    #                 # num envs to put on same scale as self.agent._step
    #                 success_perc = (success_counter / num_envs) / self.agent._step
    #                 # store success metrics to train step config
    #                 self.agent._train_step_config["success rate"] = success_perc
    #                 self.agent._train_step_config["goal distance"] = goal_distance

    #                 if dones[i]:
    #                     # Apply hindsight to the completed trajectory
    #                     self.store_hindsight_trajectory(trajectory_buffers[i])
    #                     # Reset trajectory buffer
    #                     trajectory_buffers[i] = []
    #                     # Increment completed episodes
    #                     self.completed_episodes[i] += 1
    #                     # Update score history
    #                     score_history.append(episode_scores[i])
    #                     avg_reward = np.mean(score_history) if score_history else 0
    #                     self.agent._train_episode_config.update({
    #                         'episode': int(self.completed_episodes.sum()),
    #                         'episode_reward': episode_scores[i],
    #                         # 'avg_reward': avg_reward
    #                     })
    #                     # Check for best reward and save
    #                     if avg_reward > best_reward:
    #                         best_reward = avg_reward
    #                         self.agent._train_episode_config["best"] = 1
    #                         self.save()
    #                     else:
    #                         self.agent._train_episode_config["best"] = 0

    #                     if self.agent.callbacks:
    #                         for callback in self.agent.callbacks:
    #                             callback.on_train_epoch_end(epoch=self.agent._step, logs=self.agent._train_episode_config)

    #                     # Check if number of completed episodes should trigger render
    #                     if self.completed_episodes.sum() % render_freq == 0 and not rendered:
    #                         print(f"Rendering episode {self.completed_episodes.sum()} during training...")
    #                         # Call the test function to render an episode
    #                         self.test(num_episodes=1, seed=None, render_freq=1, training=True)
    #                         # Add render to wandb log
    #                         video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes.sum()}.mp4")
    #                         # Log the video to wandb
    #                         if self.agent.callbacks:
    #                             for callback in self.agent.callbacks:
    #                                 if isinstance(callback, WandbCallback):
    #                                     wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")}, step=self.agent._step)
    #                         rendered = True
    #                         # Set models to train mode
    #                         self.agent.actor_model.train()
    #                         if hasattr(self.agent, 'critic_model'):
    #                             self.agent.critic_model.train()  # For DDPG
    #                         if hasattr(self.agent, 'critic_model_a'):
    #                             self.agent.critic_model_a.train()  # For TD3
    #                         if hasattr(self.agent, 'critic_model_b'):
    #                             self.agent.critic_model_b.train()  # For TD3

    #                     print(f"Environment {i}: episode {int(self.completed_episodes[i])}, score {episode_scores[i]}, avg_score {avg_reward}")
    #                     episode_scores[i] = 0

    #             # Update normalizer global values
    #             self.state_normalizer.update_global_stats()
    #             self.goal_normalizer.update_global_stats()

    #             # Perform learning updates
    #             if self.agent._step > self.agent.warmup:
    #                 if self.agent.replay_buffer.counter > self.agent.batch_size:
    #                     for _ in range(num_updates):
    #                         actor_loss, critic_loss = self.agent.learn(
    #                             # replay_buffer=self.replay_buffer,
    #                             state_normalizer=self.state_normalizer,
    #                             goal_normalizer=self.goal_normalizer
    #                         )
    #                         self.agent._train_step_config.update({
    #                             "actor_loss": actor_loss,
    #                             "critic_loss": critic_loss
    #                         })
    #                         # if isinstance(self.agent, DDPG):
    #                         #     self.agent.soft_update(self.agent.actor_model, self.agent.target_actor_model)
    #                         #     self.agent.soft_update(self.agent.critic_model, self.agent.target_critic_model)

    #                     # Step scheduler if not None
    #                     if self.agent.noise_schedule:
    #                         self.agent.noise_schedule.step()
    #                         self.agent._train_step_config["noise_anneal"] = self.agent.noise_schedule.get_factor()

    #             # Update states
    #             states = next_states

    #             # Log step metrics
    #             self.agent._train_step_config["step_reward"] = rewards.mean()
    #             if self.agent.callbacks:
    #                 for callback in self.agent.callbacks:
    #                     callback.on_train_step_end(step=self.agent._step, logs=self.agent._train_step_config)

    #         if self.agent.callbacks:
    #             for callback in self.agent.callbacks:
    #                 callback.on_train_end(logs=self.agent._train_episode_config)

    #         self.agent.env.close()

    #     except Exception as e:
    #         logger.error(f"Error during HER train process: {e}", exc_info=True)

    def train(self, num_epochs: int, num_cycles: int, num_episodes_per_cycle: int, num_updates: int, render_freq: int, num_envs: int = 1, seed: int = None):
        """
        Train the HER agent with a vectorized environment setup, following the HER paper's experiment structure.

        Args:
            num_epochs (int): Number of training epochs.
            num_cycles (int): Number of cycles per epoch.
            num_episodes_per_cycle (int): Number of episodes to collect per cycle across all environments.
            num_updates (int): Number of optimization steps per cycle after collecting episodes.
            render_freq (int): Frequency of rendering (in total completed episodes).
            num_envs (int): Number of parallel environments (default: 1).
            seed (int, optional): Random seed for reproducibility.
        """
        try:
            logger.debug("HER train fired")

            # Set models to train mode
            self.agent.actor_model.train()
            if hasattr(self.agent, 'critic_model'):
                self.agent.critic_model.train()  # For DDPG
            if hasattr(self.agent, 'critic_model_a'):
                self.agent.critic_model_a.train()  # For TD3
            if hasattr(self.agent, 'critic_model_b'):
                self.agent.critic_model_b.train()  # For TD3

            # Update agent config
            if self.agent.callbacks:
                self.agent._config.update({
                    'strategy': self.strategy,
                    'num_goals': self.num_goals if self.strategy == 'future' else None,
                    'num_epochs': num_epochs,
                    'num_cycles': num_cycles,
                    'num_episodes_per_cycle': num_episodes_per_cycle,
                    'num_updates': num_updates,
                    'tolerance': self.tolerance,
                    'num_envs': num_envs,
                    'seed': seed
                })
                logger.debug("HER.train: train config added to agent config")

            # Initialize callbacks
            if self.agent.callbacks:
                for callback in self.agent.callbacks:
                    if isinstance(callback, WandbCallback):
                        if isinstance(self.agent, DDPG):
                            models = (self.agent.critic_model, self.agent.actor_model)
                        elif isinstance(self.agent, TD3):
                            models = (self.agent.critic_model_a, self.agent.critic_model_b, self.agent.actor_model)
                        callback.on_train_begin(models, logs=self.agent._config)
                    else:
                        callback.on_train_begin(logs=self.agent._config)

            # Initialize environment
            try:
                self.agent.env.env = self.agent.env._initialize_env(render_freq, num_envs, seed)
                logger.debug(f"Initializing environment with render_freq={render_freq}, num_envs={num_envs}, seed={seed}")
            except Exception as e:
                logger.error(f"Error in HER.train environment initialization: {e}", exc_info=True)

            # Initialize counters and histories
            self.agent._step = 0
            self.completed_episodes = np.zeros(num_envs)
            episode_scores = np.zeros(num_envs)
            score_history = deque(maxlen=100)
            trajectories = [[] for _ in range(num_envs)]
            best_reward = -np.inf
            success_counter = 0.0

            states, _ = self.agent.env.reset()

            # Training loop
            for epoch in range(num_epochs):
                if self.agent.callbacks:
                    for callback in self.agent.callbacks:
                        callback.on_train_epoch_begin(epoch=epoch, logs=None)

                for cycle in range(num_cycles):
                    completed_before_cycle = self.completed_episodes.sum()
                    rendered = False  # Reset render flag per cycle

                    # Collect episodes until num_episodes_per_cycle are completed
                    while self.completed_episodes.sum() < completed_before_cycle + num_episodes_per_cycle:
                        self.agent._step += 1

                        # Get actions for all environments
                        actions = self.agent.get_action(
                            states['observation'],
                            states['desired_goal'],
                            test=False,
                            state_normalizer=self.state_normalizer,
                            goal_normalizer=self.goal_normalizer
                        )
                        formatted_actions = self.agent.env.format_actions(actions)
                        next_states, rewards, terms, truncs, _ = self.agent.env.step(formatted_actions)
                        dones = np.logical_or(terms, truncs)
                        episode_scores += rewards
                        # Store transitions in the env trajectory
                        for i in range(num_envs):
                            trajectories[i].append(
                                (
                                    states['observation'][i],
                                    actions[i],
                                    rewards[i],
                                    next_states['observation'][i],
                                    dones[i],
                                    states['achieved_goal'][i],
                                    next_states['achieved_goal'][i],
                                    states['desired_goal'][i]
                                )
                            )

                            # Update normalizers
                            self.state_normalizer.update_local_stats(
                                T.tensor(states['observation'][i], dtype=T.float32, device=self.state_normalizer.device.type)
                            )
                            self.goal_normalizer.update_local_stats(
                                T.tensor(states['achieved_goal'][i], dtype=T.float32, device=self.goal_normalizer.device.type)
                            )

                        completed_episodes = np.flatnonzero(dones) # Get indices of completed episodes
                        for i in completed_episodes:
                        # for i in range(num_envs):
                            #DEBUG
                            # print(f'trajectories[{i}]: {trajectories[i]}')
                            self.store_hindsight_trajectory(trajectories[i])
                            # Calculate success rate
                            goal_distance = np.linalg.norm(states['achieved_goal'][i] - states['desired_goal'][i], axis=-1)
                            success = (goal_distance <= self.tolerance).astype(np.float32)
                            success_counter += success
                            success_perc = (success_counter / self.completed_episodes.sum())
                            self.agent._train_step_config.update({
                                "success rate": success_perc,
                                "goal distance": goal_distance,
                                "step_reward": rewards.mean()
                            })
                            trajectories[i] = []
                            self.completed_episodes[i] += 1
                            # Add original transition to replay buffer
                            # self.agent.replay_buffer.add(
                            #     states['observation'][i],
                            #     actions[i],
                            #     rewards[i],
                            #     next_states['observation'][i],
                            #     dones[i],
                            #     states['achieved_goal'][i],
                            #     next_states['achieved_goal'][i],
                            #     states['desired_goal'][i]
                            # )

                            # Append transition to trajectory buffer
                            score_history.append(episode_scores[i])
                            avg_reward = np.mean(score_history) if score_history else 0
                            self.agent._train_episode_config.update({
                                'episode': int(self.completed_episodes.sum()),
                                'episode_reward': episode_scores[i],
                            })

                            if avg_reward > best_reward:
                                best_reward = avg_reward
                                self.agent._train_episode_config["best"] = 1
                                self.save()
                            else:
                                self.agent._train_episode_config["best"] = 0

                            if self.agent.callbacks:
                                for callback in self.agent.callbacks:
                                    callback.on_train_epoch_end(epoch=self.agent._step, logs=self.agent._train_episode_config)

                            if self.completed_episodes.sum() % render_freq == 0 and not rendered:
                                print(f"Rendering episode {self.completed_episodes.sum()} during training...")
                                self.test(num_episodes=1, seed=None, render_freq=1, training=True)
                                video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.completed_episodes.sum()}.mp4")
                                if self.agent.callbacks:
                                    for callback in self.agent.callbacks:
                                        if isinstance(callback, WandbCallback):
                                            wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")}, step=self.agent._step)
                                rendered = True
                                # Reset models to train mode
                                self.agent.actor_model.train()
                                if hasattr(self.agent, 'critic_model'):
                                    self.agent.critic_model.train()
                                if hasattr(self.agent, 'critic_model_a'):
                                    self.agent.critic_model_a.train()
                                if hasattr(self.agent, 'critic_model_b'):
                                    self.agent.critic_model_b.train()

                            print(f"Environment {i}: episode {int(self.completed_episodes[i])}, score {episode_scores[i]}, avg_score {avg_reward}")
                            episode_scores[i] = 0


                        

                            # if dones[i]:
                            #     # Apply hindsight to the completed trajectory
                            #     self.store_hindsight_trajectory(trajectory_buffers[i])
                            #     trajectory_buffers[i] = []
                            #     self.completed_episodes[i] += 1

                        states = next_states

                        if self.agent.callbacks:
                            for callback in self.agent.callbacks:
                                callback.on_train_step_end(step=self.agent._step, logs=self.agent._train_step_config)
                                
                    # Update normalizers and states
                    self.state_normalizer.update_global_stats()
                    self.goal_normalizer.update_global_stats()

                    # Perform optimization after collecting episodes
                    if self.agent._step > self.agent.warmup:
                        if self.agent.replay_buffer.counter > self.agent.batch_size:
                            for _ in range(num_updates):
                                actor_loss, critic_loss = self.agent.learn(
                                    state_normalizer=self.state_normalizer,
                                    goal_normalizer=self.goal_normalizer
                                )
                                self.agent._train_step_config.update({
                                    "actor_loss": actor_loss,
                                    "critic_loss": critic_loss
                                })
                            # Update target networks
                            if isinstance(self.agent, DDPG):
                                self.agent.soft_update(self.agent.actor_model, self.agent.target_actor_model)
                                self.agent.soft_update(self.agent.critic_model, self.agent.target_critic_model)
                            elif isinstance(self.agent, TD3):
                                self.agent.soft_update(self.agent.actor_model, self.agent.target_actor_model)
                                self.agent.soft_update(self.agent.critic_model_a, self.agent.target_critic_model_a)
                                self.agent.soft_update(self.agent.critic_model_b, self.agent.target_critic_model_b)

                            # Step noise scheduler if not None
                            if self.agent.noise_schedule:
                                self.agent.noise_schedule.step()
                                self.agent._train_step_config["noise_anneal"] = self.agent.noise_schedule.get_factor()
                            # Step target noise scheduler if is attr and not None
                            if hasattr(self.agent, 'target_noise_schedule') and self.agent.target_noise_schedule:
                                self.agent.target_noise_schedule.step()
                                self.agent._train_step_config["target_noise_anneal"] = self.agent.target_noise_schedule.get_factor()

                if self.agent.callbacks:
                    for callback in self.agent.callbacks:
                        callback.on_train_epoch_end(epoch=epoch, logs=self.agent._train_episode_config)

            if self.agent.callbacks:
                for callback in self.agent.callbacks:
                    callback.on_train_end(logs=self.agent._train_episode_config)

            self.agent.env.close()

        except Exception as e:
            logger.error(f"Error during HER train process: {e}", exc_info=True)
    
    def test(self, num_episodes: int, num_envs: int = 1, seed: int = None, render_freq: int = 0, training: bool = False):
        """Runs a test over 'num_episodes'."""

        # Set models to eval mode
        self.agent.actor_model.eval()
        if hasattr(self.agent, 'critic_model'):
            self.agent.critic_model.eval()  # For DDPG
        if hasattr(self.agent, 'critic_model_a'):
            self.agent.critic_model_a.eval()  # For TD3
        if hasattr(self.agent, 'critic_model_b'):
            self.agent.critic_model_b.eval()  # For TD3
        
        if seed is None:
            seed = np.random.randint(10000)
        if render_freq is None:
            render_freq = 0
        set_seed(seed)

        try:
            env = self.agent.env._initialize_env(render_freq, num_envs, seed)
        except Exception as e:
            logger.error("Error in HER.test during env initialization", exc_info=True)

        if self.agent.callbacks and not training:
            for callback in self.agent.callbacks:
                self.agent._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    self.agent._config['seed'] = seed
                    self.agent._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)
        _step = 0
        completed_episodes = np.zeros(num_envs)
        episode_scores = np.zeros(num_envs)
        completed_scores = deque(maxlen=num_episodes)
        frames = []
        states, _ = env.reset()
        while completed_episodes.sum() < num_episodes:
            _step += 1
            if self.agent.callbacks and not training:
                for callback in self.agent.callbacks:
                    callback.on_test_epoch_begin(epoch=_step, logs=None)
            # Get actions
            actions = self.agent.get_action(
                    states['observation'],
                    states['desired_goal'],
                    test=True,  # Test mode
                    state_normalizer=self.state_normalizer,
                    goal_normalizer=self.goal_normalizer
                )
            actions = self.agent.env.format_actions(actions, testing=True)
            next_states, rewards, terms, truncs, _ = env.step(actions)
            self.agent._test_step_config["step_reward"] = rewards
            episode_scores += rewards
            dones = np.logical_or(terms, truncs)
            completed_episodes += dones
            for i in range(num_envs):
                if dones[i]:
                    completed_scores.append(episode_scores[i])
                    self.agent._test_episode_config["episode_reward"] = episode_scores[i]
                    # Save the video if the episode number is divisible by render_freq
                    if (render_freq > 0) and ((completed_episodes.sum()) % render_freq == 0):
                        if training:
                            render_video(frames, self.completed_episodes.sum(), self.save_dir, 'train')
                        else:
                            render_video(frames, completed_episodes.sum(), self.save_dir, 'test')
                            # Add render to wandb log
                            video_path = os.path.join(self.save_dir, f"renders/test/episode_{completed_episodes.sum()}.mp4")
                            # Log the video to wandb
                            if self.agent.callbacks:
                                for callback in self.agent.callbacks:
                                    if isinstance(callback, WandbCallback):
                                        wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})
                        # Empty frames array
                        frames = []
                    if self.agent.callbacks and not training:
                        for callback in self.agent.callbacks:
                            callback.agent.on_test_epoch_end(epoch=_step, logs=self.agent._test_episode_config)
                    
                    print(f"Environment {i}: Episode {int(completed_episodes.sum())}/{num_episodes} Score: {completed_scores[-1]} Avg Score: {sum(completed_scores)/len(completed_scores)}")
                    episode_scores[i] = 0

            if render_freq > 0:
                frame = env.render()[0]
                frames.append(frame)
            states = next_states
            if self.agent.callbacks and not training:
                for callback in self.agent.callbacks:
                    callback.on_test_step_end(step=_step, logs=self.agent._test_step_config)
        if self.agent.callbacks and not training:
            for callback in self.agent.callbacks:
                callback.on_test_end(logs=self.agent._test_episode_config)

    def store_hindsight_trajectory(self, trajectory):
        """
        Store hindsight-augmented transitions from a completed trajectory into the replay buffer.
        
        Args:
            trajectory (list): List of dictionaries, each containing transition data with keys:
                'state', 'action', 'reward', 'next_state', 'done', 'achieved_goal',
                'next_achieved_goal', 'desired_goal'
        """
        states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = zip(*trajectory)

        # # Extract values from the list of dictionaries into separate lists
        # states = [t['state'] for t in trajectory]
        # actions = [t['action'] for t in trajectory]
        # rewards = [t['reward'] for t in trajectory]
        # next_states = [t['next_state'] for t in trajectory]
        # dones = [t['done'] for t in trajectory]
        # achieved_goals = [t['achieved_goal'] for t in trajectory]
        # next_achieved_goals = [t['next_achieved_goal'] for t in trajectory]
        # desired_goals = [t['desired_goal'] for t in trajectory]

        # # Convert lists to NumPy arrays for efficiency
        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.array(next_states)
        dones = np.array(dones)
        achieved_goals = np.array(achieved_goals)
        next_achieved_goals = np.array(next_achieved_goals)
        desired_goals = np.array(desired_goals)

        

        #DEBUG
        # print(f'states shape: {states.shape}')
        # print(f'unique states: {len(np.unique(states))}')
        # print(f'states: {states}')
        # print(f'actions shape: {actions.shape}')
        # print(f'unique actions: {len(np.unique(actions))}')
        # print(f'rewards shape: {rewards.shape}')
        # print(f'unique rewards: {len(np.unique(rewards))}')
        # print(f'next_states shape: {next_states.shape}')
        # print(f'unique next_states: {len(np.unique(next_states))}')
        # print(f'dones shape: {dones.shape}')
        # print(f'unique dones: {len(np.unique(dones))}')
        # print(f'achieved_goals shape: {achieved_goals.shape}')
        # print(f'unique achieved_goals: {len(np.unique(achieved_goals))}')
        # print(f'next_achieved_goals shape: {next_achieved_goals.shape}')
        # print(f'unique next_achieved_goals: {len(np.unique(next_achieved_goals))}')
        # print(f'desired_goals shape: {desired_goals.shape}')
        # print(f'unique desired_goals: {len(np.unique(desired_goals))}')

        # Add actual experiences to the replay buffer
        self.agent.replay_buffer.add(*zip(*trajectory))

        tol_count = 0
        experiences = [] # Store experiences for hindsight

        # loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer
        for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, achieved_goals, next_achieved_goals, desired_goals)):

            if self.strategy == "final":
                new_desired_goal = next_achieved_goals[-1]
                # new_reward, within_tol = self.reward_fn(self.agent.env, action, state_achieved_goal, next_state_achieved_goal, new_desired_goal, self.tolerance)
                new_reward = self.agent.env.get_base_env().compute_reward(state_achieved_goal, new_desired_goal, {})
                within_tol = self.agent.env.get_base_env()._is_success(state_achieved_goal, new_desired_goal)
                # increment tol_count
                tol_count += within_tol

                # store non normalized trajectory
                experiences.append((state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal))

            elif self.strategy == 'future':
                for i in range(self.num_goals):
                    if idx + i >= len(states) -1:
                        break
                    goal_idx = np.random.randint(idx + 1, len(states))
                    new_desired_goal = next_achieved_goals[goal_idx]
                    # new_reward, within_tol = self.reward_fn(self.agent.env, action, state_achieved_goal, next_state_achieved_goal, new_desired_goal, self.tolerance)
                    new_reward = self.agent.env.get_base_env().compute_reward(state_achieved_goal, new_desired_goal, {})
                    within_tol = self.agent.env.get_base_env()._is_success(state_achieved_goal, new_desired_goal)
                    tol_count += within_tol
                    # store non normalized trajectory
                    experiences.append((state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal))

            elif self.strategy == 'none':
                break

        #DEBUG
        # trajectory = zip(*experiences)
        # print(f'trajectory: {list(trajectory)}')

        self.agent.replay_buffer.add(*zip(*experiences))

        # add tol count to train step config for callbacks
        if self.agent.callbacks:
            self.agent._train_episode_config["tolerance count"] = tol_count
                
        

    def set_normalizer_state(self, config):
        self.agent.state_normalizer.set_state(config)

    # def cleanup(self):
    #     self.replay_buffer.cleanup()
    #     self.state_normalizer.cleanup()
    #     self.goal_normalizer.cleanup()
    #     T.cuda.empty_cache()
    #     if dist.is_initialized():
    #         dist.destroy_process_group()
    #         print("Process group destroyed")
    #     print("Cleanup complete")


    def get_config(self):
        config = {
            "agent_type": self.__class__.__name__,
            "agent": self.agent.get_config(),
            "strategy": self.strategy,
            "tolerance":self.tolerance,
            "num_goals": self.num_goals,
            "normalizer_clip": self.normalizer_clip,
            "normalizer_eps": self.normalizer_eps,
            # "replay_buffer_size": self.replay_buffer_size,
            "device": self.device.type,
            "save_dir": self.save_dir,
        }

        return config
    
    def save(self):
        """Saves the model."""

        # Change self.save_dir if save_dir 
        # if save_dir is not None:
        #     self.save_dir = save_dir + "/her/"
        #     print(f'new save dir: {self.save_dir}')

        config = self.get_config()

        # makes directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)

        # writes and saves JSON file of HER agent config
        with open(self.save_dir + "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f)

        # save agent
        # if save_dir is not None:
        #     self.agent.save(self.save_dir)
        #     print(f'new agent save dir: {self.agent.save_dir}')
        # else:
        self.agent.save()

        self.state_normalizer.save_state(self.save_dir + "state_normalizer.npz")
        self.goal_normalizer.save_state(self.save_dir + "goal_normalizer.npz")

    @classmethod
    def load(cls, config, load_weights=True):
        """Loads the model."""
        # logger.debug(f'rank {MPI.COMM_WORLD.rank} HER.load called')
        # Resolve function names to actual functions
        # try:
        #     config["desired_goal"] = getattr(gym_helper, config["desired_goal"])
        #     config["achieved_goal"] = getattr(gym_helper, config["achieved_goal"])
        #     config["reward_fn"] = getattr(gym_helper, config["reward_fn"])
        #     logger.debug(f"rank {MPI.COMM_WORLD.rank} HER.load successfully loaded gym goal functions")
        # except Exception as e:
        #     logger.error(f"rank {MPI.COMM_WORLD.rank} HER.load failed to load gym goal functions: {e}", exc_info=True)

        # load agent
        try:
            agent = load_agent_from_config(config["agent"], load_weights)
        except Exception as e:
            logger.error(f"HER.load failed to load Agent: {e}", exc_info=True)

        # instantiate HER model
        try:
            her = cls(agent, config["strategy"], config["tolerance"], config["num_goals"],
                    config['normalizer_clip'], config['normalizer_eps'],
                    config["device"], config["save_dir"])
            logger.debug(f"HER.load successfully loaded HER")
        except Exception as e:
            logger.error(f"HER.load failed to load HER: {e}", exc_info=True)

        # load agent normalizers
        try:
            agent.state_normalizer = Normalizer.load_state(config['save_dir'] + "state_normalizer.npz")
            agent.goal_normalizer = Normalizer.load_state(config['save_dir'] + "goal_normalizer.npz")
            logger.debug(f"HER.load successfully loaded normalizers")
        except Exception as e:
            logger.error(f"HER.load failed to load normalizers: {e}", exc_info=True)
        
        return her
    
class PPO(Agent):
    """
    Proximal Policy Optimization (PPO) agent implementation.

    This agent uses policy and value networks to learn an optimal policy for a given environment
    using the PPO algorithm. It supports features such as Generalized Advantage Estimation (GAE),
    reward clipping, and gradient clipping for stable learning.

    Attributes:
        env (EnvWrapper): The environment wrapper for the agent.
        policy_model: The policy model used for action selection.
        value_model: The value model used for state-value prediction.
        discount (float): Discount factor for future rewards.
        gae_coefficient (float): GAE smoothing coefficient.
        policy_clip (float): Clipping value for policy ratio updates.
        policy_clip_schedule (ScheduleWrapper): Rate at which to decay policy clip per learn epoch.
        value_clip (float): Clipping value for value model updates.
        value_clip_schedule (ScheduleWrapper): Rate at which to decay value clip per learn epoch.
        value_loss_coefficient (float): value to weight the value loss by.
        entropy_coefficient (float): Coefficient for entropy regularization.
        entropy_schedule (ScheduleWrapper): Rate at which to decay entropy coefficient per learn epoch.
        kl_coefficient (float): Coefficient for KL divergence penalty.
        kl_adapter (AdaptiveKL): Adjusts kl_coefficient to keep KL Divergence near target.
        normalize_advantages (bool): Whether to normalize advantages.
        normalize_values (bool): Whether to normalize value outputs.
        value_norm_clip (float): Clipping range for value normalization.
        policy_grad_clip (float): Maximum norm for policy gradients.
        value_grad_clip (float): Maximum norm for value model gradients
        reward_clip (float): Maximum absolute value for reward clipping.
        callbacks (List): List of callback objects for logging and monitoring.
        save_dir (str): Directory to save models and configurations.
        device (str): Device for computations ('cpu' or 'cuda').
    """

    def __init__(self,
                 env: EnvWrapper,
                 policy_model: StochasticContinuousPolicy | StochasticDiscretePolicy,
                 value_model: ValueModel,
                 discount: float = 0.99,
                 gae_coefficient: float = 0.95,
                 policy_clip: float = 0.2,
                 policy_clip_schedule: Optional[ScheduleWrapper] = None,
                 value_clip: float = 0.2,
                 value_clip_schedule: Optional[ScheduleWrapper] = None,
                 value_loss_coefficient: float = 1.0,
                 entropy_coefficient: float = 0.01,
                 entropy_schedule: Optional[ScheduleWrapper] = None,
                 kl_coefficient: float = 0.0,
                 kl_adapter: Optional[AdaptiveKL] = None,
                 normalize_advantages: bool = True,
                 normalize_values: bool = False,
                 value_normalizer_clip: float = float('inf'),
                 policy_grad_clip: float = float('inf'),
                 value_grad_clip: float = float('inf'),
                 reward_clip: float = float('inf'),
                 callbacks: Optional[list[Callback]] = None,
                 save_dir: str = 'models',
                 device: str = None
                 ):
        """
        Initialize the PPO agent.

        Args:
            env (EnvWrapper): The environment wrapper for the agent.
            policy_model: The policy model used for action selection.
            value_model: The value model used for state-value prediction.
            discount (float): Discount factor for future rewards (default: 0.99).
            gae_coefficient (float): GAE smoothing coefficient (default: 0.95).
            policy_clip (float): Clipping value for policy ratio updates (default: 0.2).
            policy_clip_schedule (ScheduleWrapper): Rate at which to decay policy clip per learn epoch (default: None).
            value_clip (float): Clipping value for value model updates (default: 0.2).
            value_clip_schedule (ScheduleWrapper): Rate at which to decay value clip per learn epoch (default: None).
            value_loss_coefficient (float): value to weight the value loss by (default: 1.0).
            entropy_coefficient (float): Coefficient for entropy regularization (default: 0.01).
            entropy_schedule (ScheduleWrapper): Rate at which to decay entropy coefficient per learn epoch (default: None).
            kl_coefficient (float): Coefficient for KL divergence penalty (default: 0.01).
            kl_adapter (AdaptiveKL): Adjusts kl_coefficient to keep KL Divergence near target (default: None).
            normalize_advantages (bool): Whether to normalize advantages (default: True).
            normalize_values (bool): Whether to normalize value outputs (default: False).
            value_normalizer_clip (float): Clipping range for value normalization (default: inf).
            policy_grad_clip (float): Maximum norm for policy gradients (default: inf).
            reward_clip (float): Maximum absolute value for reward clipping (default: inf).
            callbacks (list): List of callback objects for logging and monitoring (default: []).
            save_dir (str): Directory to save models and configurations (default: 'models').
            device (str): Device for computations ('cpu' or 'cuda', default: 'cuda').
        """
        self.env = env
        self.policy_model = policy_model
        self.value_model = value_model
        self.discount = discount
        self.gae_coefficient = gae_coefficient
        self.policy_clip = policy_clip
        self.policy_clip_schedule = policy_clip_schedule
        self.value_clip = value_clip
        self.value_clip_schedule = value_clip_schedule
        self.value_loss_coefficient = value_loss_coefficient
        self.entropy_coefficient = entropy_coefficient
        self.entropy_schedule = entropy_schedule
        self.kl_coefficient = kl_coefficient
        self.kl_adapter = kl_adapter
        self.normalize_advantages = normalize_advantages
        self.normalize_values = normalize_values
        self.value_norm_clip = value_normalizer_clip
        self.policy_grad_clip = policy_grad_clip
        self.value_grad_clip = value_grad_clip
        self.reward_clip = reward_clip
        # self.callbacks = callbacks
        self.device = get_device(device)

        if self.normalize_values:
            self.normalizer = Normalizer((1), clip_range=self.value_norm_clip, device=device)

        if save_dir is not None and "/ppo/" not in save_dir:
            self.save_dir = save_dir + "/ppo/"
        elif save_dir is not None:
            self.save_dir = save_dir

        # Initialize callback configurations
        self._initialize_callbacks(callbacks)
        self._train_config = {}
        self._train_episode_config = {}
        self._train_step_config = {}
        self._test_config = {}
        self._test_step_config = {}
        self._test_episode_config = {}
        self._step = None

    def _initialize_callbacks(self, callbacks):
        """
        Initialize and configure callbacks for logging and monitoring.

        Args:
            callbacks (list): List of callback objects.
        """
        try:
            self.callbacks = callbacks
            if callbacks:
                for callback in self.callbacks:
                    self._config = callback._config(self)
                    if isinstance(callback, WandbCallback):
                        self._wandb = True
            else:
                self.callback_list = None
                self._wandb = False
        except Exception as e:
            logger.error(f"Error initializing callbacks: {e}", exc_info=True)

    def calculate_advantages_and_returns(self, rewards, states, next_states, dones):
        """
        Compute advantages and returns using GAE, correctly handling episode terminations.
        """
        #DEBUG
        # print(f'states shape:{states.shape}')
        # print(f'next states shape:{next_states.shape}')
        # print(f'rewards shape:{rewards.shape}')
        # print(f'dones shape:{dones.shape}')
        num_steps, num_envs = rewards.shape
        all_advantages = []
        all_returns = []
        all_values = []

        for env_idx in range(num_envs):
            with T.no_grad():
                rewards_env = rewards[:, env_idx]
                states_env = states[:, env_idx, ...]
                next_states_env = next_states[:, env_idx, ...]
                dones_env = dones[:, env_idx]
                values = self.value_model(states_env).squeeze(-1)
                next_values = self.value_model(next_states_env).squeeze(-1)
                advantages = T.zeros_like(rewards_env)
                returns = T.zeros_like(rewards_env)
                gae = 0.0

                #DEBUG
                # print(f'reward_env shape:{rewards_env.shape}')
                
                # Calculate deltas across the trajectory
                deltas = rewards_env + self.discount * next_values * (1.0 - dones_env) - values
                #DEBUG
                # print(f'deltas shape:{deltas.shape}')

                for t in reversed(range(num_steps)):
                    # delta = rewards_env[t] + self.discount * next_values[t] * (1-) 
                    gae = deltas[t] + self.discount * self.gae_coefficient * gae * (1.0 - dones_env[t])
                    #DEBUG
                    # print(f'gae:{gae}')
                    advantages[t] = gae
                    returns[t] = gae + values[t]

                all_advantages.append(advantages)
                all_returns.append(returns)
                all_values.append(values)

        # Stack results across environments
        advantages = T.stack(all_advantages, dim=1)
        returns = T.stack(all_returns, dim=1)
        values = T.stack(all_values, dim=1)

        self._train_episode_config["values"] = values.mean().item()
        self._train_episode_config["advantages"] = advantages.mean().item()
        self._train_episode_config["returns"] = returns.mean().item()

        # Normalize advantages if required
        if self.normalize_advantages:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-4)

        return advantages, returns, values

    def get_action(self, states):
        """
        Select an action based on the current policy.

        Args:
            states (array): Input states.

        Returns:
            Tuple[array, array]: Selected actions and their log probabilities.
        """
        with T.no_grad():
            states = T.tensor(states, dtype=T.float32, device=self.policy_model.device)
            #DEBUG
            # print(f'get action states:{states.shape}')
            if self.policy_model.distribution == 'categorical':
                dist, logits = self.policy_model(states)
            else:
                dist, _, _ = self.policy_model(states)
            actions = dist.sample()
            log_probs = dist.log_prob(actions)
            actions = actions.detach().cpu().numpy()
            log_probs = log_probs.detach().cpu().numpy()
        return actions, log_probs

    def action_adapter(self, actions):
        """
        Adapt actions to match the environment's action space.

        Args:
            actions (array): Actions to adapt.

        Returns:
            array: Adapted actions.
        """
        if isinstance(self.env, GymnasiumWrapper):
            if isinstance(self.env.single_action_space, gym.spaces.Box):
                action_space_low = self.env.single_action_space.low
                action_space_high = self.env.single_action_space.high
                # Map action values to be between 0-1 if using normal distribution
                if self.policy_model.distribution == 'normal':
                    actions = 1/(1 + np.exp(-actions))
                # Map from [0, 1] to [action_space_low, action_space_high]
                adapted_actions = action_space_low + (action_space_high - action_space_low) * actions
                return adapted_actions
            elif isinstance(self.env.single_action_space, gym.spaces.Discrete):
                n = self.env.single_action_space.n
                # Map actions from [0, 1] to [0, n-1]
                adapted_actions = (actions * n).astype(int)
                adapted_actions = np.clip(adapted_actions, 0, n - 1)
                return adapted_actions
        elif isinstance(self.env, IsaacSimWrapper):
            pass
        else:
            raise NotImplementedError(f"Action adaptation not implemented for environment type: {type(self.env)}")

        raise NotImplementedError("Unsupported action space type for the current environment")
    

    def clip_reward(self, reward):
        """
        Clip rewards to the specified range.

        Args:
            reward (float): Reward to clip.

        Returns:
            float: Clipped reward.
        """
        if reward > self.reward_clip:
            return self.reward_clip
        elif reward < -self.reward_clip:
            return -self.reward_clip
        else:
            return reward

    def train(self, timesteps, trajectory_length, batch_size, learning_epochs, num_envs, seed=None, render_freq:int=0):
        """
        Train the PPO agent.

        Args:
            timesteps (int): Total number of timesteps to train.
            trajectory_length (int): Number of timesteps per update.
            batch_size (int): Batch size for training.
            learning_epochs (int): Number of epochs per update.
            num_envs (int): Number of parallel environments.
            seed (int, optional): Random seed for reproducibility.
            render_freq (int): Frequency of rendering episodes.
            save_dir (str, optional): directory to save the model. Defaults to self.save_dir
        """
        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        if render_freq == None:
            render_freq = 0

        # Set seeds
        set_seed(seed)
        # gym.utils.seeding.np_random.seed = seed # Seeds of envs now set in _initialize_env

        if self.callbacks:
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    self._config['timesteps'] = timesteps
                    self._config['trajectory_length'] = trajectory_length
                    self._config['batch_size'] = batch_size
                    self._config['learning_epochs'] = learning_epochs
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                    callback.on_train_begin((self.value_model, self.policy_model,), logs=self._config)
                    # logger.debug(f'TD3.train on train begin callback complete')
                else:
                    callback.on_train_begin(logs=self._config)

        try:
            # instantiate new vec environment
            self.env.env = self.env._initialize_env(0, num_envs, seed)
        except Exception as e:
            logger.error(f"Error in PPO.train agent._initialize_env process: {e}", exc_info=True)

        # set best reward
        best_reward = -np.inf

        self.trajectory_length = trajectory_length
        self.num_envs = num_envs
        self.policy_model.train()
        self.value_model.train()
        self._step = 0
        all_states = []
        all_actions = []
        all_log_probs = []
        all_rewards = []
        all_next_states = []
        all_dones = []
        # policy_loss_history = []
        # value_loss_history = []
        # entropy_history = []
        # kl_history = []
        # time_history = []
        # param_history = []
        frames = []  # List to store frames for the video
        self.episodes = np.zeros(self.num_envs) # Tracks current episode for each env
        episode_lengths = np.zeros(self.num_envs) # Tracks step count for each env
        scores = np.zeros(self.num_envs) # Tracks current score for each env
        env_scores = np.zeros(self.num_envs)  # Tracks last episode score for each env
        episode_scores = deque(maxlen=100) # Tracks the last 100 episode scores
        states, _ = self.env.reset()

        # set an episode rendered flag to track if an episode has yet to be rendered
        episode_rendered = False
        # track the previous episode number of the first env for rendering
        prev_episode = self.episodes[0]

        while self._step < timesteps:
            self._step += 1
            episode_lengths += 1 # increment the step count of each episode of each env by 1
            dones = []
            actions, log_probs = self.get_action(states)
            #DEBUG
            # print(f'train actions shape:{actions.shape}')
            # print(f'train actions:{actions}')
            if self.policy_model.distribution == 'beta':
                acts = self.action_adapter(actions)
            else:
                acts = actions
            # if self.policy_model.distribution != 'categorical':
            #     acts = acts.astype(np.float32)
            #     acts = acts.tolist()
            #     acts = [[float(a) for a in act] for act in acts]
            acts = self.env.format_actions(acts)
            #DEBUG
            # print(f'acts shape:{acts.shape}')
            # print(f'acts:{acts}')

            # If using WANDB log action values of first environment
            if self.callbacks:
                for callback in self.callbacks:
                    if isinstance(callback, WandbCallback):
                        if self.policy_model.distribution != 'categorical':
                            for i, a in enumerate(acts[0]):
                                self._train_step_config[f'action_{i}'] = a
                        else:
                            self._train_step_config['action'] = acts

            next_states, rewards, terms, truncs, _ = self.env.step(acts)
            # Update scores of each episode
            scores += rewards

            self._train_step_config["step_reward"] = rewards.max()

            for i, (term, trunc) in enumerate(zip(terms, truncs)):
                if term or trunc:
                    dones.append(True)
                    env_scores[i] = scores[i]  # Store score at end of episode
                    episode_scores.append(scores[i]) # Store score in deque to compute avg
                    self._train_step_config["episode_reward"] = scores[i]
                    scores[i] = 0  # Reset score for this environment
                    self._train_step_config["episode_length"] = episode_lengths[i]
                    episode_lengths[i]  = 0 # Resets the step count of the env that returned term/trunc to 0
                else:
                    dones.append(False)

            self.episodes += dones
            # set episode rendered to false if episode number has changed
            if prev_episode != self.episodes[0]:
                episode_rendered = False
            self._train_episode_config['episode'] = self.episodes[0]
            all_states.append(states)
            all_actions.append(actions)
            all_log_probs.append(log_probs)
            clipped_rewards = [self.clip_reward(reward) for reward in rewards]
            all_rewards.append(clipped_rewards)
            all_next_states.append(next_states)
            all_dones.append(dones)

            # render episode if first env shows done and first env episode num % render_freq == 0
            if render_freq > 0 and self.episodes[0] % render_freq == 0 and episode_rendered == False:
                print(f"Rendering episode {self.episodes[0]} during training...")
                # Call the test function to render an episode
                _ = self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
                # Add render to wandb log
                video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.episodes[0]}.mp4")
                # Log the video to wandb
            if self.callbacks:
                for callback in self.callbacks:
                        if isinstance(callback, WandbCallback):
                            wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")})
                episode_rendered = True
                # Switch models back to train mode after rendering
                self.policy_model.train()
                self.value_model.train()

            prev_episode = self.episodes[0]

            # env_scores = np.array([
            #     env_score[-1] if len(env_score) > 0 else np.nan
            #     for env_score in episode_scores
            # ])

            if self._step % self.trajectory_length == 0:
                # print(f'learning timestep: {self._step}')
                trajectory = (all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones)
                # Get policy clip
                policy_clip = self.policy_clip
                if self.policy_clip_schedule:
                    policy_clip *= self.policy_clip_schedule.get_factor()                    
                self._train_episode_config["policy_clip"] = policy_clip
                # Get value clip
                value_clip = self.value_clip
                if self.value_clip_schedule:
                    value_clip *= self.value_clip_schedule.get_factor()                    
                self._train_episode_config["value_clip"] = value_clip
                # Get entropy coefficient
                entropy_coefficient = self.entropy_coefficient
                if self.entropy_schedule:
                    entropy_coefficient *= self.entropy_schedule.get_factor() 
                self._train_episode_config["entropy_coefficient"] = entropy_coefficient
                # get kl coefficient
                kl_coefficient = self.kl_coefficient
                if self.kl_adapter:
                    kl_coefficient *= self.kl_adapter.get_beta()
                self._train_episode_config["kl_coefficient"] = kl_coefficient
                
                if self.policy_model.distribution == 'categorical':
                    policy_loss, value_loss, entropy, kl, logits = self.learn(trajectory, batch_size, learning_epochs)
                else:
                    policy_loss, value_loss, entropy, kl, param1, param2 = self.learn(trajectory, batch_size, learning_epochs)
                # self._train_episode_config[f"avg_env_scores"] = np.nanmean(env_scores)
                self._train_episode_config["actor_loss"] = policy_loss
                self._train_episode_config["critic_loss"] = value_loss
                self._train_episode_config["entropy"] = entropy
                self._train_episode_config["kl_divergence"] = kl
                if self.policy_model.scheduler:
                    self._train_episode_config['policy learning rate'] = self.policy_model.scheduler.get_last_lr()[0]
                else:
                    self._train_episode_config['policy learning rate'] = self.policy_model.optimizer.param_groups[0]['lr']
                if self.value_model.scheduler:
                    self._train_episode_config['value learning rate'] = self.value_model.scheduler.get_last_lr()[0]
                else:
                    self._train_episode_config['value learning rate'] = self.value_model.optimizer.param_groups[0]['lr']
                # if self.policy_model.distribution == 'categorical':
                #     # Convert logits to probabilities
                #     probabilities = F.softmax(logits, dim=0)
                #     self._train_episode_config["probabilities"] = probabilities
                # else:
                #     self._train_episode_config["param1"] = param1.mean()
                #     self._train_episode_config["param2"] = param2.mean()

                # policy_loss_history.append(policy_loss)
                # value_loss_history.append(value_loss)
                # entropy_history.append(entropy)
                # kl_history.append(kl)
                # if self.policy_model.distribution == 'categorical':
                #     param_history.append(logits)
                # else:
                #     param_history.append((param1, param2))
                
                # Clear trajectory data
                all_states = []
                all_actions = []
                all_log_probs = []
                all_rewards = []
                all_next_states = []
                all_dones = []
                
        if self.callbacks:
            for callback in self.callbacks:
                        callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)

                # # Clear CUDA cache
                # T.cuda.empty_cache()

            # Set avg score if 1 or more episodes scores are logged, else set avg to -inf
            if len(episode_scores) > 0:
                avg_score = sum(episode_scores) / len(episode_scores) # compute avg scores
            else:
                avg_score = -np.inf
            # check if best reward
            if avg_score > best_reward:
                best_reward = avg_score
                self._train_episode_config["best"] = True
                # save model
                self.save()
            else:
                self._train_episode_config["best"] = False

            states = next_states

            if self._step % 1000 == 0:
                print(f'episode: {self.episodes}; total steps: {self._step}; episodes scores: {env_scores}; avg score: {avg_score}')

        if self.callbacks:
            for callback in self.callbacks:
                    callback.on_train_step_end(step=self._step, logs=self._train_step_config)

        if self.callbacks:
            for callback in self.callbacks:
                callback.on_train_end(logs=self._train_episode_config)

        # return {
        #         'scores': episode_scores,
        #         'policy loss': policy_loss_history,
        #         'value loss': value_loss_history,
        #         'entropy': entropy_history,
        #         'kl': kl_history,
        #         'params': param_history,
        #         }

    def learn(self, trajectory, batch_size, learning_epochs):
        """
        Perform learning updates using the collected trajectory.

        Args:
            trajectory (Tuple): Collected trajectory containing states, actions, etc.
            batch_size (int): Batch size for training.
            learning_epochs (int): Number of epochs per update.

        Returns:
            Tuple: policy loss, value loss, entropy, and KL divergence.
        """
        # Unpack trajectory
        all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones = trajectory

        # Convert lists to tensors without flattening
        # This results in tensors of shape (num_steps, num_envs, ...)
        states = T.stack([T.tensor(s, dtype=T.float32, device=self.policy_model.device) for s in all_states])
        #DEBUG
        # print(f'learn states shape:{states.shape}')
        # Convert actions to T.long values if categorical, else floats
        if self.policy_model.distribution == 'categorical':
            actions = T.stack([T.tensor(a, dtype=T.long, device=self.policy_model.device) for a in all_actions])
            #DEBUG
            # print(f'actions:{actions}')
        else:
            actions = T.stack([T.tensor(a, dtype=T.float32, device=self.policy_model.device) for a in all_actions])
        log_probs = T.stack([T.tensor(lp, dtype=T.float32, device=self.policy_model.device) for lp in all_log_probs])
        rewards = T.stack([T.tensor(r, dtype=T.float32, device=self.value_model.device) for r in all_rewards])
        next_states = T.stack([T.tensor(ns, dtype=T.float32, device=self.policy_model.device) for ns in all_next_states])
        dones = T.stack([T.tensor(d, dtype=T.int, device=self.policy_model.device) for d in all_dones])

        # Calculate advantages and returns
        advantages, returns, all_values = self.calculate_advantages_and_returns(rewards, states, next_states, dones)

        # Flatten the tensors along the time and environment dimensions for batching
        num_steps, num_envs = rewards.shape
        total_samples = num_steps * num_envs

        # Reshape observations
        # obs_shape = states.shape[2:]  # Get observation shape
        states = states.reshape(total_samples, *self.env.single_observation_space.shape)
        next_states = next_states.reshape(total_samples, *self.env.single_observation_space.shape)
        #DEBUG
        # print(f'learn reshaped states shape:{states.shape}')

        # Reshape tensors for batching
        all_values = all_values.reshape(total_samples, -1) # Shape: (total_samples, 1)
        actions = actions.reshape(total_samples, -1)     # Shape: (total_samples, action_space)
        log_probs = log_probs.reshape(total_samples, -1) # Shape: (total_samples, action_dim)
        advantages = advantages.reshape(total_samples, 1) # Shape: (total_samples, 1)
        returns = returns.reshape(total_samples, 1)      # Shape: (total_samples, 1)
        #DEBUG
        # print(f'resized values:{all_values.shape}')
        # print(f'resized actions:{actions.shape}')
        # print(f'resized log probs:{log_probs.shape}')
        # print(f'resized advantages:{advantages.shape}')
        # print(f'resized returns:{returns.shape}')

        # Set previous distribution to none (used for KL divergence calculation)
        # prev_dist = None

        # Create random indices for shuffling
        indices = T.randperm(total_samples)
        num_batches = total_samples // batch_size

        # Create instance of policy to serve as old policy
        if isinstance(self.policy_model, StochasticDiscretePolicy):
            policy = StochasticDiscretePolicy
        else:
            policy = StochasticContinuousPolicy
        
        old_policy = policy(
            env = self.env, 
            layer_config = self.policy_model.layer_config,
            output_layer_kernel = self.policy_model.output_config,
            optimizer_params = self.policy_model.optimizer_params,
            scheduler_params = self.policy_model.scheduler_params,
            distribution = self.policy_model.distribution,
            device = self.policy_model.device
        )
        old_policy.load_state_dict(self.policy_model.state_dict())
        old_policy.eval()

        # Create instance of value model to serve as old value func
        old_value_model = ValueModel(
            env = self.env,
            layer_config = self.value_model.layer_config,
            output_layer_kernel = self.value_model.output_config,
            optimizer_params = self.value_model.optimizer_params,
            scheduler_params = self.value_model.scheduler_params,
            device = self.value_model.device
        )
        old_value_model.load_state_dict(self.value_model.state_dict())
        old_value_model.eval()

        # Get current values of policy clip and entropy/kl coefficients
        policy_clip = self.policy_clip
        if self.policy_clip_schedule:
            policy_clip *= self.policy_clip_schedule.get_factor()

        value_clip = self.value_clip
        if self.value_clip_schedule:
            value_clip *= self.value_clip_schedule.get_factor()

        entropy_coefficient = self.entropy_coefficient
        if self.entropy_schedule:
            entropy_coefficient *= self.entropy_schedule.get_factor()

        kl_coefficient = self.kl_coefficient
        if self.kl_adapter:
            kl_coefficient *= self.kl_adapter.get_beta()

        # Training loop
        for epoch in range(learning_epochs):

            for batch_num in range(num_batches):
                batch_indices = indices[batch_num * batch_size : (batch_num + 1) * batch_size]
                states_batch = states[batch_indices]
                actions_batch = actions[batch_indices]
                log_probs_batch = log_probs[batch_indices]
                advantages_batch = advantages[batch_indices]
                returns_batch = returns[batch_indices]
                

                # Create new distribution
                if self.policy_model.distribution == 'categorical':
                    #DEBUG
                    # New distribution
                    new_dist, logits = self.policy_model(states_batch)
                    new_log_probs = new_dist.log_prob(actions_batch.view(-1))
                    # Old distribution
                    old_dist, old_logits = old_policy(states_batch)
                    old_log_probs = old_dist.log_prob(actions_batch.view(-1))
                    #DEBUG
                    # print(f'new logits: {logits}')
                else: # Continuous Distributions
                    # New distribution
                    new_dist, param1, param2 = self.policy_model(states_batch)
                    new_log_probs = new_dist.log_prob(actions_batch).sum(dim=-1)
                    # Old distribution
                    old_dist, old_param1, old_param2 = old_policy(states_batch)
                    old_log_probs = old_dist.log_prob(actions_batch).sum(dim=-1)


                # Calculate the ratios of new to old probabilities of actions
                if new_log_probs.dim() == 1:
                    new_log_probs = new_log_probs.unsqueeze(-1)
                    old_log_probs = old_log_probs.unsqueeze(-1)
                    advantages_batch = advantages_batch.view(-1,1)
                prob_ratio = T.exp(new_log_probs - old_log_probs)

                # Calculate Surrogate Loss
                surr1 = prob_ratio * advantages_batch
                surr2 = T.clamp(prob_ratio, 1 - policy_clip, 1 + policy_clip) * advantages_batch
                surrogate_loss = -T.min(surr1, surr2).mean()

                # Calculate Entropy penalty
                entropy = new_dist.entropy().mean()
                entropy_penalty = entropy * -entropy_coefficient 

                # Calculate the KL penalty
                kl = kl_divergence(old_dist, new_dist).mean()
                kl_penalty = kl * kl_coefficient
                
                policy_loss = surrogate_loss + entropy_penalty + kl_penalty
                
                # Update the policy
                self.policy_model.optimizer.zero_grad()
                policy_loss.backward()
                T.nn.utils.clip_grad_norm_(self.policy_model.parameters(), max_norm=self.policy_grad_clip)
                self.policy_model.optimizer.step()
                
                    
                # Update the value function
                values = self.value_model(states_batch)
                loss = (values - returns_batch).pow(2)
                old_values = old_value_model(states_batch)
                clipped_values = old_values + (values - old_values).clamp(-value_clip, value_clip)
                clipped_value_loss = (clipped_values - returns_batch).pow(2)
                value_loss = self.value_loss_coefficient * (0.5 * T.max(loss, clipped_value_loss).mean()).mean()
                self.value_model.optimizer.zero_grad()
                value_loss.backward()
                T.nn.utils.clip_grad_norm_(self.value_model.parameters(), max_norm=self.value_grad_clip)
                self.value_model.optimizer.step()

                
        # Step schedulers
        if self.policy_model.scheduler:
            self.policy_model.scheduler.step()
        if self.value_model.scheduler:
            self.value_model.scheduler.step()
        if self.policy_clip_schedule:
            self.policy_clip_schedule.step()
            policy_clip = self.policy_clip * self.policy_clip_schedule.get_factor()
        if self.value_clip_schedule:
            self.value_clip_schedule.step()
            value_clip = self.value_clip * self.value_clip_schedule.get_factor()
        if self.entropy_schedule:
            self.entropy_schedule.step()
            entropy_coefficient = self.entropy_coefficient * self.entropy_schedule.get_factor()
        if self.kl_adapter:
            self.kl_adapter.step(kl)
            kl_coefficient = self.kl_coefficient * self.kl_adapter.get_beta()

        # Create 3d scatter plot of visited states colored by state value and action magnitude
        # if self.callbacks:
        #     for callback in self.callbacks:
        #         if isinstance(callback, WandbCallback):
        #             # Reduce states to 3D embeddings
        #             reducer = UMAP(n_components=3, random_state=42)
        #             embeddings = reducer.fit_transform(states.cpu().numpy())  # Shape: (num_samples, 3)
        #             # Compute the magnitude of the actions
        #             action_magnitude = np.linalg.norm(actions.cpu().numpy(), axis=1)
        #             df = pd.DataFrame({
        #                 'embedding_x': embeddings[:, 0],
        #                 'embedding_y': embeddings[:, 1],
        #                 'embedding_z': embeddings[:, 2],
        #                 'value': all_values.cpu().numpy().flatten(),
        #                 'action_magnitude': action_magnitude,
        #                 # If you want to include specific action components:
        #                 # 'action_component_0': actions[:, 0],
        #                 # 'action_component_1': actions[:, 1],
        #                 # ...
        #             })

        #             # Create a 3D scatter plot colored by value estimates
        #             fig_value = px.scatter_3d(
        #                 df,
        #                 x='embedding_x',
        #                 y='embedding_y',
        #                 z='embedding_z',
        #                 color='value',
        #                 title='State Embeddings Colored by Value Function',
        #                 labels={'embedding_x': 'Embedding X', 'embedding_y': 'Embedding Y', 'embedding_z': 'Embedding Z', 'value': 'Value Estimate'},
        #                 opacity=0.7
        #             )
                    
        #             # Create a 3D scatter plot colored by action magnitude
        #             fig_action = px.scatter_3d(
        #                 df,
        #                 x='embedding_x',
        #                 y='embedding_y',
        #                 z='embedding_z',
        #                 color='action_magnitude',
        #                 title='State Embeddings Colored by Action Magnitude',
        #                 labels={'embedding_x': 'Embedding X', 'embedding_y': 'Embedding Y', 'embedding_z': 'Embedding Z', 'action_magnitude': 'Action Magnitude'},
        #                 opacity=0.7
        #             )

        #             # Log the 3D plots
        #             wandb.log({
        #                 "Value Function Embeddings 3D": fig_value,
        #                 "Policy Embeddings 3D": fig_action
        #             })

        # Decay Policy Clip
        # self.policy_clip *= self.clip_decay
        # Decay Entropy Coefficient
        # self.entropy_coefficient *= self.entropy_decay

        # print(f'Policy Loss: {policy_loss.sum()}')
        # print(f'Value Loss: {value_loss}')
        # print(f'Entropy: {entropy}')
        # print(f'KL Divergence: {kl}')

        if self.policy_model.distribution == 'categorical':
            return policy_loss, value_loss, entropy, kl, logits.detach().cpu().flatten()
        else:
            return policy_loss, value_loss, entropy, kl, param1.detach().cpu().flatten(), param2.detach().cpu().flatten()

    def test(self, num_episodes, num_envs:int=1, seed=None, render_freq:int=0, training: bool=False):
        """
        Test the PPO agent in the environment.

        Args:
            num_episodes (int): Number of episodes to test.
            num_envs (int): Number of parallel environments.
            seed (int, optional): Random seed for reproducibility.
            render_freq (int): Frequency of rendering episodes.
            training (bool): Whether testing is during training.

        Returns:
            dict: Test metrics including scores and log probabilities.
        """
        # Set the policy and value function models to evaluation mode
        self.policy_model.eval()
        self.value_model.eval()

        if seed is None:
            seed = np.random.randint(100)

        # Set render freq to 0 if None is passed
        if render_freq == None:
            render_freq = 0

        # Set seeds
        set_seed(seed)

        env = self.env._initialize_env(render_freq, num_envs, seed)
        if self.callbacks and not training:
            print('test begin callback if statement fired')
            for callback in self.callbacks:
                self._config = callback._config(self)
                if isinstance(callback, WandbCallback):
                    # Add to config to send to wandb for logging
                    self._config['seed'] = seed
                    self._config['num_envs'] = num_envs
                callback.on_test_begin(logs=self._config)

        # episode_scores = [[] for _ in range(num_envs)]  # Track scores for each env
        # reset step counter
        step = 0
        all_scores = []
        all_log_probs = []

        for episode in range(num_episodes):
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_begin(epoch=step, logs=None)
            done = False
            states, _ = env.reset()
            scores = 0
            log_probs = []
            frames = []  # List to store frames for the video

            while not done:

                # Get action and log probability from the current policy
                actions, log_prob = self.get_action(states)
                # print(f'actions:{actions}')
                if self.policy_model.distribution == 'beta':
                    acts = self.action_adapter(actions)
                    # print(f'formatted actions from beta:{acts}')
                else:
                    acts = actions
                # if self.policy_model.distribution != 'categorical':
                #     acts = acts.astype(np.float32)
                #     acts = np.clip(acts, env.single_action_space.low, env.single_action_space.high)
                #     acts = acts.tolist()
                #     acts = [[float(a) for a in act] for act in acts]
                acts = self.env.format_actions(acts, testing=True)

                #  log prob to log probs list
                log_probs.append(log_prob)

                # Step the environment
                next_states, rewards, terms, truncs, _ = env.step(acts)
                # Update scores of each episode
                scores += rewards

                for i, (term, trunc) in enumerate(zip(terms, truncs)):
                    if term or trunc:
                        done = True
                        # print(f'append true')
                    # else:
                    #     dones.append(False)

                if render_freq > 0:
                    # Capture the frame
                    frame = env.render()[0]
                    # print(f'frame:{frame}')
                    frames.append(frame)

                # Move to the next state
                states = next_states

                # Add metrics to test step config to log
                self._test_step_config['step_reward'] = rewards[0]
                if self.callbacks and not training:
                    for callback in self.callbacks:
                        callback.on_test_step_end(step=step, logs=self._test_step_config)

                # Increment step count
                step += 1

            # Save the video if the episode number is divisible by render_freq
            if (render_freq > 0) and ((episode + 1) % render_freq == 0):
                if training:
                    render_video(frames, self.episodes[0], self.save_dir, 'train')
                else:
                    render_video(frames, episode+1, self.save_dir, 'test')
                    # Add render to wandb log
                    video_path = os.path.join(self.save_dir, f"renders/test/episode_{episode + 1}.mp4")
                    # Log the video to wandb
                if self.callbacks:
                    for callback in self.callbacks:
                            if isinstance(callback, WandbCallback):
                                wandb.log({"training_video": wandb.Video(video_path, caption="Testing process", format="mp4")})

            # Append the results for the episode
            all_scores.append(scores)  # Store score at end of episode
            self._test_episode_config["episode_reward"] = scores[0]

            # Append log probs for the episode to all_log_probs list
            all_log_probs.append(log_probs)

            # Log to callbacks
            if self.callbacks and not training:
                for callback in self.callbacks:
                    callback.on_test_epoch_end(epoch=step, logs=self._test_episode_config)

            print(f'Episode {episode+1}/{num_episodes} - Score: {all_scores[-1]}')

            # Reset score for this environment
            scores = 0
        
        if self.callbacks and not training:
            for callback in self.callbacks:
                callback.on_test_end(logs=self._test_episode_config)

        # close the environment
        env.close()

        return {
            'scores': all_scores,
            'log probs': all_log_probs,
            # 'entropy': entropy_list,
            # 'kl_divergence': kl_list
        }

    def save(self, save_dir=None):
        """
        Save the model and its configuration.

        Args:
            save_dir (str, optional): Directory to save the model. Defaults to self.save_dir.
        """
        config = self.get_config()

        # makes directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)

        # writes and saves JSON file of DDPG agent config
        with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, cls=CustomJSONEncoder)

        # saves policy and value model
        self.policy_model.save(self.save_dir)
        self.value_model.save(self.save_dir)


    @classmethod
    def load(cls, config, load_weights=True):
        """
        Load a PPO agent from a saved configuration.

        Args:
            config (dict): Configuration dictionary.
            load_weights (bool): Whether to load model weights.

        Returns:
            PPO: Loaded PPO agent.
        """

        ## create EnvSpec from config
        # env_spec = EnvSpec.from_json(config['env'])
        
        # env_wrapper = build_env_wrapper_obj(env_spec)
        env_wrapper = EnvWrapper.from_json(config["env"])


        # load policy model
        model = select_policy_model(env_wrapper)
        policy_model = model.load(config['save_dir'], load_weights)
        # load value model
        value_model = ValueModel.load(config['save_dir'], load_weights)
        # load callbacks
        callbacks = [callback_load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]\
                    if config['callbacks'] else None

        # return PPO agent
        agent = cls(
            env_wrapper,
            policy_model = policy_model,
            value_model = value_model,
            discount=config["discount"],
            gae_coefficient = config["gae_coefficient"],
            policy_clip = config["policy_clip"],
            policy_clip_schedule = ScheduleWrapper(config["policy_clip_schedule"]),
            value_clip = config["value_clip"],
            value_clip_schedule = ScheduleWrapper(config["value_clip_schedule"]),
            value_loss_coefficient = config["value_loss_coefficient"],
            entropy_coefficient = config["entropy_coefficient"],
            entropy_schedule = ScheduleWrapper(config["entropy_schedule"]),
            kl_coefficient = config["kl_coefficient"],
            kl_adapter = AdaptiveKL(**config["kl_adapter"]) if config["kl_adapter"] else None,
            normalize_advantages = config["normalize_advantages"],
            normalize_values = config["normalize_values"],
            value_normalizer_clip = config["normalizer_clip"],
            policy_grad_clip = config["policy_grad_clip"],
            value_grad_clip = config["value_grad_clip"],
            reward_clip = config['reward_clip'],
            callbacks=callbacks,
            save_dir=config["save_dir"],
            device=config["device"],
        )

        # if agent.normalize_inputs:
        #     agent.state_normalizer = helper.Normalizer.load_state(config['save_dir'] + "state_normalizer.npz")

        return agent
    
    @classmethod
    def sweep_train(cls, config, env_spec, callbacks, run_number):
        """
        Train agents based on a sweep configuration.

        Args:
            config (dict): Configuration for the sweep.
            env_spec: Environment specification.
            callbacks (list): List of callbacks.
            run_number (int): Run number for logging.

        Returns:
            None
        """
        # Import necessary functions directly from wandb_support
        from wandb_support import get_wandb_config_value, get_wandb_config_optimizer_params

        logger.debug(f"init_sweep fired")
        try:
            # Instantiate env from env_spec
            env_spec = gym.envs.registration.EnvSpec.from_json(env_spec)
            env_library = config["parameters"]["env_library"]
            env_wrappers = config["parameters"]["env_wrappers"]
            if env_library == 'Gymnasium':
                env = GymnasiumWrapper(env_spec, env_wrappers)
            # env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))

            # logger.debug(f"train config: {train_config}")
            print(f"env library: {env_library}")
            print(f"env wrappers: {env_wrappers}")
            print(f"env spec id: {env.spec.id}")
            print(f"callbacks: {callbacks}")
            print(f"run number: {run_number}")
            print(f"config set: {config}")
            agent_type = config['model_type']
            print(f"agent type: {agent_type}")

            # Get devicez
            device = get_wandb_config_value(config, agent_type, 'none', 'device')

            # Format policy and value layers, and kernels
            model_config = wandb_support.format_layers(config)

            # Policy
            # Learning Rate
            policy_learning_rate_const = get_wandb_config_value(config, agent_type, 'policy', 'learning_rate_constant')
            policy_learning_rate_exp = get_wandb_config_value(config, agent_type, 'policy', 'learning_rate_exponent')
            policy_learning_rate = policy_learning_rate_const * (10 ** policy_learning_rate_exp)
            logger.debug(f"policy learning rate set to {policy_learning_rate}")
            # Distribution
            distribution = get_wandb_config_value(config, agent_type, 'policy', 'distribution')
            # Optimizer
            policy_optimizer = get_wandb_config_value(config, agent_type, 'policy', 'optimizer')
            logger.debug(f"policy optimizer set to {policy_optimizer}")
            # Get optimizer params
            optimizer_params = get_wandb_config_optimizer_params(config, agent_type, 'policy', 'optimizer')
            policy_optimizer_params = {'type':policy_optimizer, 'params':optimizer_params}
            logger.debug(f"policy optimizer params set to {policy_optimizer_params}")
            # Get correct policy model for env action space
            if isinstance(env.action_space, gym.spaces.Discrete):
                policy_model = StochasticDiscretePolicy(
                    env = env,
                    layer_config = model_config['policy']['hidden'],
                    output_layer_kernel = model_config['policy']['output'],
                    optimizer_params = policy_optimizer_params,
                    learning_rate = policy_learning_rate,
                    distribution = distribution,
                    device = device,
                )
            # Check if the action space is continuous
            elif isinstance(env.action_space, gym.spaces.Box):
                policy_model = StochasticContinuousPolicy(
                    env = env,
                    layer_config = model_config['policy']['hidden'],
                    output_layer_kernel = model_config['policy']['output'],
                    optimizer_params = policy_optimizer_params,
                    distribution = distribution,
                    device = device,
                )
            logger.debug(f"policy model built: {policy_model.get_config()}")

            # Value Func
            # Learning Rate
            value_learning_rate_const = get_wandb_config_value(config, agent_type, 'value', "learning_rate_constant")
            value_learning_rate_exp = get_wandb_config_value(config, agent_type, 'value', "learning_rate_exponent")
            critic_learning_rate = value_learning_rate_const * (10 ** value_learning_rate_exp)
            logger.debug(f"value learning rate set to {critic_learning_rate}")
            # Optimizer
            value_optimizer = get_wandb_config_value(config, agent_type, 'value', 'optimizer')
            logger.debug(f"value optimizer set to {value_optimizer}")
            optimizer_params = get_wandb_config_optimizer_params(config, agent_type, 'value', 'optimizer')
            value_optimizer_params = {'type':value_optimizer, 'params':optimizer_params}
            logger.debug(f"value optimizer params set to {value_optimizer_params}")

            value_model = ValueModel(
                env = env,
                layer_config = model_config['value']['hidden'],
                output_layer_kernel=model_config['value']['output'],
                optimizer_params = value_optimizer_params,
                device=device,
            )
            logger.debug(f"value model built: {value_model.get_config()}")

            # Discount
            discount = get_wandb_config_value(config, agent_type, 'none', 'discount')
            # GAE coefficient
            gae_coeff = get_wandb_config_value(config, agent_type, 'none', 'advantage')
            logger.debug(f"gae coeff set to {gae_coeff}")
            # Policy clip
            policy_clip = get_wandb_config_value(config, agent_type, 'policy', 'clip_range')
            logger.debug(f"policy clip set to {policy_clip}")
            # Value clip
            value_clip = get_wandb_config_value(config, agent_type, 'value', 'clip_range')
            logger.debug(f"value clip set to {value_clip}")
            # Entropy coefficient
            entropy_coeff = get_wandb_config_value(config, agent_type, 'none', 'entropy')
            logger.debug(f"entropy coeff set to {entropy_coeff}")
            # Normalize advantages
            normalize_advantages = get_wandb_config_value(config, agent_type, 'none', 'normalize_advantage')
            logger.debug(f"normalize advantage set to {normalize_advantages}")
            # Normalize values
            normalize_values = get_wandb_config_value(config, agent_type, 'value', 'normalize_values')
            logger.debug(f"normalize values set to {normalize_values}")
            # Normalize values clip value
            normalize_val_clip = get_wandb_config_value(config, agent_type, 'value', 'normalize_values_clip')
            if normalize_val_clip == 'infinity':
                normalize_val_clip = np.inf
            logger.debug(f"normalize values clip set to {normalize_val_clip}")
            # Policy gradient clip
            policy_grad_clip = get_wandb_config_value(config, agent_type, 'policy', 'grad_clip')
            # Change value of policy_grad_clip to np.inf if == 'infinity'
            if policy_grad_clip == "infinity":
                policy_grad_clip = np.inf
            logger.debug(f"policy grad clip set to {policy_grad_clip}")
            # Value gradient clip
            value_grad_clip = get_wandb_config_value(config, agent_type, 'value', 'grad_clip')
            # Change value of policy_grad_clip to np.inf if == 'infinity'
            if value_grad_clip == "infinity":
                value_grad_clip = np.inf
            logger.debug(f"value grad clip set to {value_grad_clip}")

            # Value Loss coefficient
            value_coeff = get_wandb_config_value(config, agent_type, 'value', 'loss_coeff')
            logger.debug(f"gae coeff set to {value_coeff}")

            # Reward clip
            reward_clip = get_wandb_config_value(config, agent_type, 'none', 'reward_clip')

            # Save dir
            save_dir = get_wandb_config_value(config, agent_type, 'none', 'save_dir')
            logger.debug(f"save dir set: {save_dir}")


            # create PPO agent
            ppo_agent= PPO(
                env = env,
                policy_model = policy_model,
                value_model = value_model,
                discount = discount,
                gae_coefficient = gae_coeff,
                policy_clip = policy_clip,
                value_clip = value_clip,
                value_loss_coefficient = value_coeff,
                entropy_coefficient = entropy_coeff,
                normalize_advantages = normalize_advantages,
                normalize_values = normalize_values,
                value_normalizer_clip = normalize_val_clip,
                policy_grad_clip = policy_grad_clip,
                value_grad_clip = value_grad_clip,
                reward_clip = reward_clip,
                callbacks = callbacks,
                device = device,
            )
            logger.debug(f"PPO agent built: {ppo_agent.get_config()}")

            timesteps = get_wandb_config_value(config, agent_type, 'none', 'num_timesteps')
            traj_length = get_wandb_config_value(config, agent_type, 'none', 'trajectory_length')
            batch_size = get_wandb_config_value(config, agent_type, 'none', 'batch_size')
            learning_epochs = get_wandb_config_value(config, agent_type, 'none', 'learning_epochs')
            num_envs = get_wandb_config_value(config, agent_type, 'none', 'num_envs')
            seed = get_wandb_config_value(config, agent_type, 'none', 'seed')

            ppo_agent.train(
                timesteps = timesteps,
                trajectory_length = traj_length,
                batch_size = batch_size,
                learning_epochs = learning_epochs,
                num_envs = num_envs,
                seed = seed,
                render_freq = 0,
            )

        except Exception as e:
            logger.error(f"An error occurred: {e}", exc_info=True)

    def get_config(self):
        """
        Get the current configuration of the PPO agent.

        Returns:
            dict: Configuration dictionary.
        """
        return {
                "agent_type": self.__class__.__name__,
                # "env": serialize_env_spec(self.env.spec),
                "env": self.env.to_json(),
                "policy": self.policy_model.get_config(),
                "value_model": self.value_model.get_config(),
                "discount": self.discount,
                "gae_coefficient": self.gae_coefficient,
                "policy_clip": self.policy_clip,
                "policy_clip_schedule": self.policy_clip_schedule.get_config() if self.policy_clip_schedule else None,
                "value_clip": self.value_clip,
                "value_clip_schedule": self.value_clip_schedule.get_config() if self.value_clip_schedule else None,
                "value_loss_coefficient": self.value_loss_coefficient,
                "entropy_coefficient": self.entropy_coefficient,
                "entropy_schedule": self.entropy_schedule.get_config() if self.entropy_schedule else None,
                "kl_coefficient": self.kl_coefficient,
                "kl_adapter": self.kl_adapter.get_config() if self.kl_adapter else None,
                "normalize_advantages":self.normalize_advantages,
                "normalize_values": self.normalize_values,
                "normalizer_clip": self.value_norm_clip,
                "policy_grad_clip": self.policy_grad_clip,
                "value_grad_clip": self.value_grad_clip,
                "reward_clip": self.reward_clip,
                "callbacks": [callback.get_config() for callback in self.callbacks] if self.callbacks else None,
                "save_dir": self.save_dir,
                "device": self.device,
                # "seed": self.seed,
            }

# class MAPPO(Agent):

#     def __init__(self,
#                  env: EnvWrapper,
#                  policy_model,
#                  value_model,
#                  distribution: str = 'beta',
#                  discount: float = 0.99,
#                  gae_coefficient: float = 0.95,
#                  policy_clip: float = 0.2,
#                  entropy_coefficient: float = 0.01,
#                  loss:str = 'clipped',
#                  kl_coefficient: float = 0.01,
#                  normalize_advantages: bool = True,
#                  normalize_values: bool = False,
#                  value_normalizer_clip: float = np.inf,
#                  policy_grad_clip:float = np.inf,
#                  reward_clip:float = np.inf,
#                  callbacks: List = [],
#                  save_dir = 'models',
#                  device = 'cuda',
#                 #  seed: float = None,
#                  ):
#         self.env = env
#         self.policy_model = policy_model
#         self.value_model = value_model
#         self.distribution = distribution
#         self.discount = discount
#         self.gae_coefficient = gae_coefficient
#         self.policy_clip = policy_clip
#         self.entropy_coefficient = entropy_coefficient
#         self.loss = loss
#         self.kl_coefficient = kl_coefficient
#         self.normalize_advantages = normalize_advantages
#         self.normalize_values = normalize_values
#         self.value_norm_clip = value_normalizer_clip
#         if self.normalize_values:
#             self.normalizer = Normalizer((1), clip_range=self.value_norm_clip, device=device)
#         self.policy_grad_clip = policy_grad_clip
#         self.reward_clip = reward_clip
#         self.callbacks = callbacks
#         self.device = device
#         # if seed is None:
#         #     seed = np.random.randint(100)
#         # self.seed = seed

#         # self.save_dir = save_dir + "/ddpg/"
#         if save_dir is not None and "/ppo/" not in save_dir:
#                 self.save_dir = save_dir + "/ppo/"
#         elif save_dir is not None and "/ppo/" in save_dir:
#                 self.save_dir = save_dir


#         # self.lambda_param = 0.5
#         if self.loss == 'hybrid':
#             # Instantiate learnable parameter to blend Clipped and KL loss objectives
#             self.lambda_param = T.nn.Parameter(T.tensor(self.lambda_))
#             # # Add lambda param to policy optimizer
#             self.policy_model.optimizer.add_param_group({'params': [self.lambda_param]})

#         # Set callbacks
#         try:
#             self.callbacks = callbacks
#             if callbacks:
#                 for callback in self.callbacks:
#                     self._config = callback._config(self)
#                     if isinstance(callback, WandbCallback):
#                         self._wandb = True

#             else:
#                 self.callback_list = None
#                 self._wandb = False
#             # if self.use_mpi:
#             #     logger.debug(f"rank {self.rank} TD3 init: callbacks set")
#             # else:
#             #     logger.debug(f"TD3 init: callbacks set")
#         except Exception as e:
#             logger.error(f"Error in TD3 init set callbacks: {e}", exc_info=True)

#         self._train_config = {}
#         self._train_episode_config = {}
#         self._train_step_config = {}
#         self._test_config = {}
#         self._test_step_config = {}
#         self._test_episode_config = {}

#         self._step = None
        
#     def calculate_advantages_and_returns(self, rewards, states, next_states, dones):
#         num_steps, num_envs = rewards.shape
#         all_advantages = []
#         all_returns = []
#         all_values = []

#         for env_idx in range(num_envs):
#             with T.no_grad():
#                 rewards_env = rewards[:, env_idx]
#                 states_env = states[:, env_idx, :]
#                 next_states_env = next_states[:, env_idx, :]
#                 dones_env = dones[:, env_idx]

#                 values = self.value_model(states_env).squeeze(-1)
#                 next_values = self.value_model(next_states_env).squeeze(-1)

#                 advantages = T.zeros_like(rewards_env)
#                 returns = T.zeros_like(rewards_env)
#                 gae = 0
#                 for t in reversed(range(len(rewards_env))):
#                     delta = rewards_env[t] + self.discount * next_values[t] * (1 - dones_env[t]) - values[t]
#                     gae = delta + self.discount * self.gae_coefficient * (1 - dones_env[t]) * gae
#                     # gae = T.tensor(gae, dtype=T.float32, device=self.value_model.device)
#                     # print(f'rewards env shape:{rewards_env.shape}')
#                     # print(f'values shape:{values.shape}')
#                     # print(f'next values shape:{next_values.shape}')
#                     # print(f'dones env shape:{dones_env.shape}')
#                     # print(f'gae shape:{gae.shape}')
#                     # print(f'advantages shape:{advantages.shape}')
#                     advantages[t] = gae
#                     returns[t] = gae + values[t]
#                     # print(f'advantages[t]:{advantages[t]}')
#                     # print(f'returns[t]:{returns[t]}')

#                 if self.normalize_advantages:
#                     advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

#                 all_advantages.append(advantages.unsqueeze(-1))
#                 all_returns.append(returns.unsqueeze(-1))
#                 all_values.append(values.unsqueeze(-1))

#         all_advantages = T.stack(all_advantages, dim=1)
#         all_returns = T.stack(all_returns, dim=1)
#         all_values = T.stack(all_values, dim=1)

#         self._train_episode_config["values"] = values.mean().item()
#         self._train_episode_config["advantages"] = all_advantages.mean().item()
#         self._train_episode_config["returns"] = all_returns.mean().item()

#         return all_advantages, all_returns, all_values


#     # def get_action(self, states):
#     #     # Run states through each Policy to get distribution params
#     #     actions = []
#     #     log_probs = []
#     #     # print(f'states sent to get action: {states.shape}')
#     #     for state in states:
#     #         with T.no_grad():
#     #             # make sure state is a tensor and on correct device
#     #             state = T.tensor(state, dtype=T.float32, device=self.policy_model.device).unsqueeze(0)
#     #             #DEBUG
#     #             # print(f'state shape in get_action:{state.shape}')
#     #             # print(f'get action state:{state}')
#     #             if self.distribution == 'categorical':
#     #                 dist, logits = self.policy_model(state)
#     #             else:
#     #                 dist, _, _ = self.policy_model(state)
#     #             action = dist.sample()
#     #             log_prob = dist.log_prob(action)
#     #             actions.append(action.detach().cpu().numpy().flatten())
#     #             log_probs.append(log_prob.detach().cpu().numpy().flatten())

#     #     return np.array(actions), np.array(log_probs)

#     def get_action(self, states):
#         with T.no_grad():
#             states = T.tensor(states, dtype=T.float32, device=self.policy_model.device)
#             # print(f'states shape:{states.shape}')
#             # if len(states.shape) == 4:
#             #     print('states len == 4 fired...')
#             #     states = states.permute(0, 3, 1, 2)
#             # print(f'new states shape:{states.shape}')
#             if self.distribution == 'categorical':
#                 dist, logits = self.policy_model(states)
#             else:
#                 dist, _, _ = self.policy_model(states)
#             actions = dist.sample()
#             log_probs = dist.log_prob(actions)
#             actions = actions.detach().cpu().numpy()
#             log_probs = log_probs.detach().cpu().numpy()
#         return actions, log_probs

#     def action_adapter(self, actions, env):
#         if isinstance(env.single_action_space, gym.spaces.Box):
#             action_space_low = env.single_action_space.low  # Array of lows per dimension
#             action_space_high = env.single_action_space.high  # Array of highs per dimension
#             # Ensure actions are in [0, 1]
#             actions = np.clip(actions, 0, 1)
#             # Map from [0, 1] to [action_space_low, action_space_high]
#             adapted_actions = action_space_low + (action_space_high - action_space_low) * actions
#             return adapted_actions
#         elif isinstance(env.single_action_space, gym.spaces.Discrete):
#             n = env.single_action_space.n
#             # Map actions from [0, 1] to [0, n-1]
#             adapted_actions = (actions * n).astype(int)
#             adapted_actions = np.clip(adapted_actions, 0, n - 1)
#             return adapted_actions
#         else:
#             raise NotImplementedError(f"Unsupported action space type: {type(env.single_action_space)}")
    
#     # def action_adapter(self, action):
#     #     # print(f'action adpater action:{action}')
#     #     # print(f'action adpater action shape:{action.shape}')
#     #     return 2 * (action.reshape(1,-1) -0.5 * self.env.action_space.high[0])
#     #     # print(f'action adpater a:{a}')
#     #     # print(f'action adpater a shape:{a.shape}')
#     #     # return a

#     def clip_reward(self, reward):
#         if reward > self.reward_clip:
#             return self.reward_clip
#         elif reward < -self.reward_clip:
#             return -self.reward_clip
#         else:
#             return reward

#     @classmethod
#     def sweep_train(
#         cls,
#         config, # wandb.config,
#         # train_config,
#         env_spec,
#         callbacks,
#         run_number,
#         # comm=None,
#     ):
#         """Builds and trains agents from sweep configs"""
#         # Import necessary functions directly from wandb_support
#         from wandb_support import get_wandb_config_value, get_wandb_config_optimizer_params

#         logger.debug(f"init_sweep fired")
#         try:
#             # Instantiate env from env_spec
#             env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))

#             # logger.debug(f"train config: {train_config}")
#             logger.debug(f"env spec id: {env.spec.id}")
#             logger.debug(f"callbacks: {callbacks}")
#             logger.debug(f"run number: {run_number}")
#             logger.debug(f"config set: {config}")
#             model_type = list(config.keys())[0]
#             logger.debug(f"model type: {model_type}")

#             # Get device
#             device = get_wandb_config_value(config, model_type, 'device')

#             # Format policy and value layers, and kernels
#             policy_layers, value_layers, kernels = wandb_support.format_layers(config)
#             # logger.debug(f"layers built")

#             # Policy
#             # Learning Rate
#             policy_learning_rate_const = get_wandb_config_value(config, model_type, 'policy_learning_rate_constant')
#             policy_learning_rate_exp = get_wandb_config_value(config, model_type, 'policy_learning_rate_exponent')
#             policy_learning_rate = policy_learning_rate_const * (10 ** policy_learning_rate_exp)
#             logger.debug(f"policy learning rate set to {policy_learning_rate}")
#             # Distribution
#             distribution = get_wandb_config_value(config, model_type, 'distribution')
#             # Optimizer
#             policy_optimizer = get_wandb_config_value(config, model_type, 'policy_optimizer')
#             logger.debug(f"policy optimizer set to {policy_optimizer}")
#             # Get optimizer params
#             policy_optimizer_params = get_wandb_config_optimizer_params(config, model_type, 'policy_optimizer')
#             logger.debug(f"policy optimizer params set to {policy_optimizer_params}")
#             # Get correct policy model for env action space
#             if isinstance(env.action_space, gym.spaces.Discrete):
#                 policy_model = StochasticDiscretePolicy(
#                     env = env,
#                     dense_layers = policy_layers,
#                     output_layer_kernel = kernels[f'policy_output_kernel'],
#                     optimizer = policy_optimizer,
#                     optimizer_params = policy_optimizer_params,
#                     learning_rate = policy_learning_rate,
#                     device = device,
#                 )
#             # Check if the action space is continuous
#             elif isinstance(env.action_space, gym.spaces.Box):
#                 policy_model = StochasticContinuousPolicy(
#                     env = env,
#                     dense_layers = policy_layers,
#                     output_layer_kernel = kernels[f'policy_output_kernel'],
#                     optimizer = policy_optimizer,
#                     optimizer_params = policy_optimizer_params,
#                     learning_rate = policy_learning_rate,
#                     distribution = distribution,
#                     device = device,
#                 )
#             logger.debug(f"policy model built: {policy_model.get_config()}")

#             # Value Func
#             # Learning Rate
#             value_learning_rate_const = get_wandb_config_value(config, model_type, "value_learning_rate_constant")
#             value_learning_rate_exp = get_wandb_config_value(config, model_type, "value_learning_rate_exponent")
#             critic_learning_rate = value_learning_rate_const * (10 ** value_learning_rate_exp)
#             logger.debug(f"value learning rate set to {critic_learning_rate}")
#             # Optimizer
#             value_optimizer = get_wandb_config_value(config, model_type, 'value_optimizer')
#             logger.debug(f"value optimizer set to {value_optimizer}")
#             value_optimizer_params = get_wandb_config_optimizer_params(config, model_type, 'value_optimizer')
#             logger.debug(f"value optimizer params set to {value_optimizer_params}")

#             # Check if CNN layers and if so, build CNN model
#             # if actor_cnn_layers:
#             #     actor_cnn_model = cnn_models.CNN(actor_cnn_layers, env)
#             # else:
#             #     actor_cnn_model = None
#             # if comm is not None:
#             #     logger.debug(f"{comm.Get_name()}; Rank {rank} actor cnn layers set: {actor_cnn_layers}")
#             # else:
#             #     logger.debug(f"actor cnn layers set: {actor_cnn_layers}")

#             # if critic_cnn_layers:
#             #     critic_cnn_model = cnn_models.CNN(critic_cnn_layers, env)
#             # else:
#             #     critic_cnn_model = None
#             # if comm is not None:
#             #     logger.debug(f"{comm.Get_name()}; Rank {rank} critic cnn layers set: {critic_cnn_layers}")
#             # else:
#             #     logger.debug(f"critic cnn layers set: {critic_cnn_layers}")
#             value_model = ValueModel(
#                 env = env,
#                 dense_layers = value_layers,
#                 output_layer_kernel=kernels[f'value_output_kernel'],
#                 optimizer = value_optimizer,
#                 optimizer_params = value_optimizer_params,
#                 learning_rate = critic_learning_rate,
#                 device=device,
#             )
#             logger.debug(f"value model built: {value_model.get_config()}")

#             # GAE coefficient
#             gae_coeff = get_wandb_config_value(config, model_type, 'advantage')
#             logger.debug(f"gae coeff set to {gae_coeff}")
#             # Policy clip
#             policy_clip = get_wandb_config_value(config, model_type, 'policy_clip')
#             logger.debug(f"policy clip set to {policy_clip}")
#             # Entropy coefficient
#             entropy_coeff = get_wandb_config_value(config, model_type, 'entropy')
#             logger.debug(f"entropy coeff set to {entropy_coeff}")
#             # Normalize advantages
#             normalize_advantages = get_wandb_config_value(config, model_type, 'normalize_advantage')
#             logger.debug(f"normalize advantage set to {normalize_advantages}")
#             # Normalize values
#             normalize_values = get_wandb_config_value(config, model_type, 'normalize_values')
#             logger.debug(f"normalize values set to {normalize_values}")
#             # Normalize values clip value
#             normalize_val_clip = get_wandb_config_value(config, model_type, 'normalize_values_clip')
#             if normalize_val_clip == 'infinity':
#                 normalize_val_clip = np.inf
#             logger.debug(f"normalize values clip set to {normalize_val_clip}")
#             # Policy gradient clip
#             policy_grad_clip = get_wandb_config_value(config, model_type, 'policy_grad_clip')
#             # Change value of policy_grad_clip to np.inf if == 'infinity'
#             if policy_grad_clip == "infinity":
#                 policy_grad_clip = np.inf
#             logger.debug(f"policy grad clip set to {policy_grad_clip}")

#             # Save dir
#             save_dir = get_wandb_config_value(config, model_type, 'policy_grad_clip')
#             logger.debug(f"save dir set: {save_dir}")


#             # create PPO agent
#             ppo_agent= cls(
#                 env = env,
#                 policy_model = policy_model,
#                 value_model = value_model,
#                 distribution = distribution,
#                 discount = config[model_type][f"{model_type}_discount"],
#                 gae_coefficient = gae_coeff,
#                 policy_clip = policy_clip,
#                 entropy_coefficient = entropy_coeff,
#                 normalize_advantages = normalize_advantages,
#                 normalize_values = normalize_values,
#                 value_normalizer_clip = normalize_val_clip,
#                 policy_grad_clip = policy_grad_clip,
#                 callbacks = callbacks,
#                 device = device,
#             )
#             logger.debug(f"PPO agent built: {ppo_agent.get_config()}")

#             timesteps = get_wandb_config_value(config, model_type, 'num_timesteps')
#             traj_length = get_wandb_config_value(config, model_type, 'trajectory_length')
#             batch_size = get_wandb_config_value(config, model_type, 'batch_size')
#             learning_epochs = get_wandb_config_value(config, model_type, 'learning_epochs')
#             num_envs = get_wandb_config_value(config, model_type, 'num_envs')
#             seed = get_wandb_config_value(config, model_type, 'seed')

#             ppo_agent.train(
#                 timesteps = timesteps,
#                 trajectory_length = traj_length,
#                 batch_size = batch_size,
#                 learning_epochs = learning_epochs,
#                 num_envs = num_envs,
#                 seed = seed,
#                 render_freq = 0,
#             )

#         except Exception as e:
#             logger.error(f"An error occurred: {e}", exc_info=True)

#     def train(self, timesteps, trajectory_length, batch_size, learning_epochs, num_envs, seed=None, avg_num=10, render_freq:int=0, save_dir:str=None, run_number:int=None):
#         """
#         Trains the model for 'timesteps' number of 'timesteps',
#         updating the model every 'trajectory_length' number of timesteps.

#         Args:
#             timesteps: Number of timesteps to train for.
#             trajectory_length: Number of timesteps between updates.
#             batch_size: Number of samples in a batch.
#             learning_epochs: Number of epochs to train for.
#             num_envs: Number of environments.
#             avg_num: Number of episodes to average over.
#         """

#         # Update save_dir if passed
#         if save_dir is not None and save_dir.split("/")[-2] != "ppo":
#             self.save_dir = save_dir + "/ppo/"
#             print(f'new save dir: {self.save_dir}')
#         elif save_dir is not None and save_dir.split("/")[-2] == "ppo":
#             self.save_dir = save_dir
#             print(f'new save dir: {self.save_dir}')


#         if seed is None:
#             seed = np.random.randint(100)

#         # Set render freq to 0 if None is passed
#         if render_freq == None:
#             render_freq = 0

#         # Set seeds
#         T.manual_seed(seed)
#         T.cuda.manual_seed(seed)
#         np.random.seed(seed)
#         # gym.utils.seeding.np_random.seed = seed # Seeds of envs now set in _initialize_env

#         if self.callbacks:
#             for callback in self.callbacks:
#                 self._config = callback._config(self)
#                 if isinstance(callback, WandbCallback):
#                     self._config['timesteps'] = timesteps
#                     self._config['trajectory_length'] = trajectory_length
#                     self._config['batch_size'] = batch_size
#                     self._config['learning_epochs'] = learning_epochs
#                     self._config['seed'] = seed # Add seed to config to send to wandb for logging
#                     self._config['num_envs'] = num_envs
#                     callback.on_train_begin((self.value_model, self.policy_model,), logs=self._config)
#                     # logger.debug(f'TD3.train on train begin callback complete')
#                 else:
#                     callback.on_train_begin(logs=self._config)

#         try:
#             # instantiate new vec environment
#             env = self._initialize_env(0, num_envs, seed)
#             # for e in env.envs:
#             #     print(e.spec)
#             # logger.debug(f'initiating environment with render {render}')
#         except Exception as e:
#             logger.error(f"Error in PPO.train agent._initialize_env process: {e}", exc_info=True)

#         # set best reward
#         try:
#             best_reward = self.env.reward_range
#         except:
#             best_reward = -np.inf

#         self.trajectory_length = trajectory_length
#         self.num_envs = num_envs
#         self.policy_model.train()
#         self.value_model.train()
#         # timestep = 0
#         self._step = 0
#         all_states = []
#         all_actions = []
#         all_log_probs = []
#         all_rewards = []
#         all_next_states = []
#         all_dones = []
#         # score_history = []
#         episode_scores = [[] for _ in range(num_envs)]  # Track scores for each env
#         # episode_scores = []  # Track scores for each env
#         policy_loss_history = []
#         value_loss_history = []
#         entropy_history = []
#         kl_history = []
#         time_history = []
#         lambda_values = []
#         param_history = []
#         frames = []  # List to store frames for the video
#         self.episodes = np.zeros(self.num_envs) # Tracks current episode for each env
#         episode_lengths = np.zeros(self.num_envs) # Tracks step count for each env
#         scores = np.zeros(self.num_envs) # Tracks current score for each env
#         states, _ = env.reset()

#         # set an episode rendered flag to track if an episode has yet to be rendered
#         episode_rendered = False
#         # track the previous episode number of the first env for rendering
#         prev_episode = self.episodes[0]

#         while self._step < timesteps:
#             self._step += 1 # Increment step count by 1
#             episode_lengths += 1 # increment the step count of each episode of each env by 1
#             dones = []
#             actions, log_probs = self.get_action(states)
#             # print(f'actions:{actions}')
#             if self.distribution == 'beta':
#                 acts = self.action_adapter(actions, env)
#             else:
#                 acts = actions
#             # acts = [self.action_adapter(action) if self.distribution == 'beta' else action for action in actions]
#             # acts = np.reshape(acts, env.action_space.shape)
#             acts = acts.astype(np.float32)
#             acts = np.clip(acts, env.single_action_space.low, env.single_action_space.high)
#             # print(f'acts reshape:{acts.shape}')
#             # print(f'acts:{acts}')
#             acts = acts.tolist()
#             acts = [[float(a) for a in act] for act in acts]
#             # print(f'actions after adapter:{acts}')

#             #DEBUG
#             # print(f'reshaped acts shape:{acts.shape}')

#             # if self.distribution == 'Beta':
#             #     acts = []
#             #     for action in actions:
#             #         print(f'action:{action}')
#             #         print(f'action shape:{action.shape}')
#             #         act = [self.action_adapter(a) for a in action]
#             #         print(f'act:{act}')
#             #         print(f'act shape:{np.array(act).shape}')
#             #         acts.append(act)
#             # else:
#             #     acts = actions

#             #DEBUG
#             # for e in env.envs:
#             #     print(f'continuous:{e.spec}')
#             next_states, rewards, terms, truncs, _ = env.step(acts)
#             #DEBUG
#             # print(f'terms:{terms}, truncs:{truncs}')
#             # Update scores of each episode
#             scores += rewards
#             # print(f'rewards:{rewards.mean()}')
#             self._train_step_config["step_reward"] = rewards.mean()

#             for i, (term, trunc) in enumerate(zip(terms, truncs)):
#                 if term or trunc:
#                     dones.append(True)
#                     # print(f'append true')
#                     episode_scores[i].append(scores[i])  # Store score at end of episode
#                     self._train_step_config["episode_reward"] = scores[i]
#                     scores[i] = 0  # Reset score for this environment
#                     self._train_step_config["episode_length"] = episode_lengths[i]
#                     episode_lengths[i]  = 0 # Resets the step count of the env that returned term/trunc to 0
#                 else:
#                     dones.append(False)
#                     # print(f'append false')

#             # Add frame of first env to frames array if rendering
#             # if render_freq > 0:
#             #     # Capture the frame
#             #     frame = self.env.render()[0]
#             #     # print(f'frame:{frame}')
#             #     frames.append(frame)


#             self.episodes += dones
#             # set episode rendered to false if episode number has changed
#             if prev_episode != self.episodes[0]:
#                 episode_rendered = False
#             # print(f'dones:{dones}')
#             # print(f'episodes:{episodes}')
#             self._train_episode_config['episode'] = self.episodes[0]
#             all_states.append(states)
#             all_actions.append(actions)
#             all_log_probs.append(log_probs)
#             clipped_rewards = [self.clip_reward(reward) for reward in rewards]
#             all_rewards.append(clipped_rewards)
#             all_next_states.append(next_states)
#             all_dones.append(dones)

#             # render episode if first env shows done and first env episode num % render_freq == 0
#             if render_freq > 0 and self.episodes[0] % render_freq == 0 and episode_rendered == False:
#                 print(f"Rendering episode {self.episodes[0]} during training...")
#                 # Call the test function to render an episode
#                 self.test(num_episodes=1, seed=seed, render_freq=1, training=True)
#                 # Add render to wandb log
#                 video_path = os.path.join(self.save_dir, f"renders/train/episode_{self.episodes[0]}.mp4")
#                 # Log the video to wandb
#                 if self.callbacks:
#                     for callback in self.callbacks:
#                         if isinstance(callback, WandbCallback):
#                             wandb.log({"training_video": wandb.Video(video_path, caption="Training process", format="mp4")})
#                 episode_rendered = True
#                 # Switch models back to train mode after rendering
#                 self.policy_model.train()
#                 self.value_model.train()

#             prev_episode = self.episodes[0]

#             env_scores = np.array([
#                 env_score[-1] if len(env_score) > 0 else np.nan
#                 for env_score in episode_scores
#             ])

#             if self._step % self.trajectory_length == 0:
#                 print(f'learning timestep: {self._step}')
#                 trajectory = (all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones)
#                 if self.distribution == 'categorical':
#                     policy_loss, value_loss, entropy, kl, logits = self.learn(trajectory, batch_size, learning_epochs)
#                 else:
#                     policy_loss, value_loss, entropy, kl, param1, param2 = self.learn(trajectory, batch_size, learning_epochs)
#                 self._train_episode_config[f"avg_env_scores"] = np.nanmean(env_scores)
#                 self._train_episode_config["actor_loss"] = policy_loss
#                 self._train_episode_config["critic_loss"] = value_loss
#                 self._train_episode_config["entropy"] = entropy
#                 self._train_episode_config["kl_divergence"] = kl
#                 # self._train_episode_config["lambda"] = lambda_value
#                 if self.distribution == 'categorical':
#                     self._train_episode_config["logits"] = logits.mean()
#                 else:
#                     self._train_episode_config["param1"] = param1.mean()
#                     self._train_episode_config["param2"] = param2.mean()

#                 # check if best reward
#                 avg_score = np.mean([
#                     np.mean(env_score[-avg_num:]) if len(env_score) >= avg_num else np.mean(env_score)
#                     for env_score in episode_scores
#                 ])
#                 if avg_score > best_reward:
#                     best_reward = avg_score
#                     self._train_episode_config["best"] = True
#                     # save model
#                     self.save()
#                 else:
#                     self._train_episode_config["best"] = False

#                 policy_loss_history.append(policy_loss)
#                 value_loss_history.append(value_loss)
#                 entropy_history.append(entropy)
#                 kl_history.append(kl)
#                 # time_history.append(time)
#                 # lambda_values.append(lambda_value)
#                 if self.distribution == 'categorical':
#                     param_history.append(logits)
#                 else:
#                     param_history.append((param1, param2))
#                 all_states = []
#                 all_actions = []
#                 all_log_probs = []
#                 all_rewards = []
#                 all_next_states = []
#                 all_dones = []

#                 if self.callbacks:
#                     for callback in self.callbacks:
#                         callback.on_train_epoch_end(epoch=self._step, logs=self._train_episode_config)

#             states = next_states

#             if self._step % 1000 == 0:
#                 print(f'episode: {self.episodes}; total steps: {self._step}; episodes scores: {env_scores}; avg score: {np.nanmean(env_scores)}')

#             if self.callbacks:
#                 for callback in self.callbacks:
#                     callback.on_train_step_end(step=self._step, logs=self._train_step_config)

#         if self.callbacks:
#             for callback in self.callbacks:
#                 callback.on_train_end(logs=self._train_episode_config)

#         return {
#                 'scores': episode_scores,  # Changed to episode_scores
#                 'policy loss': policy_loss_history,
#                 'value loss': value_loss_history,
#                 'entropy': entropy_history,
#                 'kl': kl_history,
#                 # 'time': time_history,
#                 'lambda': lambda_values,
#                 'params': param_history,
#                 }

#     # def learn(self, trajectory, batch_size, learning_epochs):
#     #     # Unpack trajectory
#     #     all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones = trajectory
#     #     # Flatten the lists of numpy arrays across the num_envs dimension
#     #     states = np.concatenate(all_states, axis=0)
#     #     actions = np.concatenate(all_actions, axis=0)
#     #     log_probs = np.concatenate(all_log_probs, axis=0)
#     #     rewards = np.concatenate(all_rewards, axis=0)
#     #     next_states = np.concatenate(all_next_states, axis=0)
#     #     dones = np.concatenate(all_dones, axis=0)

#     #     # Convert to Tensors
#     #     states = T.tensor(states, dtype=T.float32, device=self.policy_model.device)
#     #     actions = T.tensor(actions, dtype=T.float32, device=self.policy_model.device)
#     #     log_probs = T.tensor(log_probs, dtype=T.float32, device=self.policy_model.device)
#     #     rewards = T.tensor(rewards, dtype=T.float32, device=self.value_model.device).unsqueeze(1)
#     #     next_states = T.tensor(next_states, dtype=T.float32, device=self.policy_model.device)
#     #     dones = T.tensor(dones, dtype=T.int, device=self.policy_model.device)


#     #     # Calculate advantages and returns
#     #     advantages, returns = self.calculate_advantages_and_returns(rewards, states, next_states, dones)

#     #     # advantages = T.tensor(advantages, dtype=T.float32, device=self.policy.device)
#     #     advantages = T.cat(advantages, dim=0)
#     #     advantages = advantages.to(self.policy_model.device, dtype=T.float32)
#     #     returns = T.cat(returns, dim=0)
#     #     returns = returns.to(self.policy_model.device, dtype=T.float32)
#     #     # returns = T.tensor(returns, dtype=T.float32, device=self.value_function.device)
#     #     # advantages = advantages.reshape(-1, 1)
#     #     # returns = returns.reshape(-1, 1)
#     #     # print(f'advantages shape:{advantages.shape}')
#     #     # print(f'returns shape:{returns.shape}')
#     #     # kl_div_loss_fn = T.nn.KLDivLoss(reduction="batchmean", log_target=True)

#     #     # Set previous distribution to none (used for KL divergence calculation)
#     #     prev_dist = None

#     #     num_batches = len(states) // batch_size
#     #     print(f'num batches:{num_batches}')

#     #     # Loop over learning_epochs epochs to train the policy and value functions
#     #     for epoch in range(learning_epochs):
#     #         times = []
#     #         start_time = time.time()
#     #         # Sample mini batch from trajectory
#     #         indices = T.randperm(len(states))
#     #         batches = [indices[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]
#     #         for batch in batches:
#     #             states_batch = states[batch]
#     #             actions_batch = actions[batch]
#     #             log_probs_batch = log_probs[batch]
#     #             rewards_batch = rewards[batch]
#     #             next_states_batch = next_states[batch]
#     #             dones_batch = dones[batch]
#     #             advantages_batch = advantages[batch]
#     #             returns_batch = returns[batch]

#     def learn(self, trajectory, batch_size, learning_epochs):
#         # Unpack trajectory
#         all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones = trajectory

#         # Convert lists to tensors without flattening
#         # This results in tensors of shape (num_steps, num_envs, ...)
#         states = T.stack([T.tensor(s, dtype=T.float32, device=self.policy_model.device) for s in all_states])
#         actions = T.stack([T.tensor(a, dtype=T.float32, device=self.policy_model.device) for a in all_actions])
#         log_probs = T.stack([T.tensor(lp, dtype=T.float32, device=self.policy_model.device) for lp in all_log_probs])
#         rewards = T.stack([T.tensor(r, dtype=T.float32, device=self.value_model.device) for r in all_rewards])
#         next_states = T.stack([T.tensor(ns, dtype=T.float32, device=self.policy_model.device) for ns in all_next_states])
#         dones = T.stack([T.tensor(d, dtype=T.int, device=self.policy_model.device) for d in all_dones])

#         # DEBUG
#         # print(f'states shape:{states.shape}')
#         # print(f'actions shape:{actions.shape}')
#         # print(f'log_probs shape:{log_probs.shape}')
#         # print(f'rewards shape:{rewards.shape}')
#         # print(f'next_states shape:{next_states.shape}')
#         # print(f'dones shape:{dones.shape}')

#         # Now, states.shape = (num_steps, num_envs, observation_space)
#         # Similarly for other variables

#         # Calculate advantages and returns
#         advantages, returns, all_values = self.calculate_advantages_and_returns(rewards, states, next_states, dones)
#         #DEBUG
#         # print(f'advantages shape:{advantages.shape}')
#         # print(f'returns shape:{returns.shape}')

#         # Proceed with the rest of the learning process
#         # Flatten the tensors along the time and environment dimensions for batching
#         num_steps, num_envs = rewards.shape
#         total_samples = num_steps * num_envs

#         # Reshape observations
#         obs_shape = states.shape[2:]  # Get observation shape
#         states = states.reshape(total_samples, *obs_shape)
#         next_states = next_states.reshape(total_samples, *obs_shape)

#         # Reshape tensors for batching
#         all_values = all_values.reshape(total_samples, -1) # Shape: (total_samples, 1)
#         # states = states.reshape(total_samples, -1)       # Shape: (total_samples, observation_space)
#         actions = actions.reshape(total_samples, -1)     # Shape: (total_samples, action_space)
#         log_probs = log_probs.reshape(total_samples, -1) # Shape: (total_samples, action_dim)
#         advantages = advantages.reshape(total_samples, 1) # Shape: (total_samples, 1)
#         returns = returns.reshape(total_samples, 1)      # Shape: (total_samples, 1)
#         #DEBUG
#         # print(f'flatenned states shape:{states.shape}')
#         # print(f'flatenned actions shape:{actions.shape}')
#         # print(f'flatenned log_probs shape:{log_probs.shape}')
#         # print(f'flatenned advantages shape:{advantages.shape}')
#         # print(f'flatenned returns shape:{returns.shape}')

#         # Set previous distribution to none (used for KL divergence calculation)
#         prev_dist = None

#         # Create random indices for shuffling
#         indices = T.randperm(total_samples)
#         num_batches = total_samples // batch_size

#         # Training loop
#         for epoch in range(learning_epochs):
#             for batch_num in range(num_batches):
#                 batch_indices = indices[batch_num * batch_size : (batch_num + 1) * batch_size]
#                 states_batch = states[batch_indices]
#                 actions_batch = actions[batch_indices]
#                 log_probs_batch = log_probs[batch_indices]
#                 advantages_batch = advantages[batch_indices]
#                 returns_batch = returns[batch_indices]
#                 #DEBUG
#                 # print(f'states batch shape:{states_batch.shape}')
#                 # print(f'actions batch shape:{actions_batch.shape}')
#                 # print(f'log_probs batch shape:{log_probs_batch.shape}')
#                 # print(f'advantages batch shape:{advantages_batch.shape}')
#                 # print(f'returns batch shape:{returns_batch.shape}')

#                 # Calculate the policy loss

#                 if self.distribution == 'categorical':
#                     dist, logits = self.policy_model(states_batch)
#                 else:
#                     dist, param1, param2 = self.policy_model(states_batch)
#                 # print(f'dist mean:{dist.loc}')
#                 # print(f'dist var:{dist.scale}')
#                 # print(f'param 1:{param1}')
#                 # print(f'param 2:{param2}')
#                 # dist_time = time.time()
#                 # Create prev_dist by recreating the distribution from the previous step's parameters
#                 if prev_dist is None:
#                     prev_dist = dist

#                 else:
#                     # Recreate prev_dist by passing in the previous parameters
#                     if self.distribution == 'beta':
#                         param1_prev = prev_dist.concentration1.clone().detach()
#                         param2_prev = prev_dist.concentration0.clone().detach()
#                         prev_dist = Beta(param1_prev, param2_prev)
#                     elif self.distribution == 'normal':
#                         param1_prev = prev_dist.loc.clone().detach()
#                         param2_prev = prev_dist.scale.clone().detach()
#                         prev_dist = Normal(param1_prev, param2_prev)
#                     elif self.distribution == 'categorical':
#                         param_prev = prev_dist.logits.clone().detach()
#                         prev_dist = Categorical(logits=param_prev)
#                     else:
#                         raise ValueError(f'Unknown distribution: {self.distribution}')
#                 # dist_delta = time.time() - dist_time
#                 # print(f'dist_delta: {dist_delta}')

#                 # Calculate new log probabilities of actions
#                 new_log_probs = dist.log_prob(actions_batch)
#                 # print(f'new_log_probs shape:{new_log_probs.shape}')
#                 # print(f'new_log_probs:{new_log_probs}')
#                 # print(f'new_log_probs shape:{new_log_probs.sum(axis=-1, keepdim=True).shape}')
#                 # print(f'log_probs shape:{log_probs_batch.sum(axis=-1, keepdim=True).shape}')

#                 # Calculate the ratios of new to old probabilities of actions
#                 prob_ratio = T.exp(new_log_probs.sum(axis=-1, keepdim=True) - log_probs_batch.sum(axis=-1, keepdim=True))
#                 # print(f'prob ratio shape:{prob_ratio.shape}')
#                 # print(f'prob ratio:{prob_ratio}')
#                 # Calculate the surrogate loss
#                 # print(f'advantages batch:{advantages_batch}')

#                 # Calculate the entropy of the distribution
#                 entropy = dist.entropy().sum(axis=-1, keepdims=True).mean()
#                 # print(f'full entropy:{dist.entropy()}')

#                 # Calculate the KL Divergence
#                 kl = kl_divergence(prev_dist, dist).sum(dim=-1, keepdim=True).mean()

#                 surr1 = (prob_ratio * advantages_batch)
#                 # print(f'surr1 shape:{surr1.shape}')
#                 surr2 = (T.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantages_batch)
#                 # Clipped policy loss
#                 surrogate_loss = -T.min(surr1, surr2).mean()
#                 entropy_penalty = -self.entropy_coefficient * entropy
#                 log_diff = new_log_probs - log_probs_batch
#                 kl_penalty = -log_diff.mean()
#                 kl_penalty *= self.kl_coefficient
#                 policy_loss = surrogate_loss + entropy_penalty + kl_penalty

#                 # if self.loss == 'clipped':
#                 #     lambda_value = 1.0
#                 #     entropy_penalty = -self.entropy_coefficient * entropy
#                 #     policy_loss = surrogate_loss + entropy_penalty
#                 # elif self.loss == 'kl':
#                 #     lambda_value = 0.0
#                 #     log_diff = new_log_probs - log_probs_batch
#                 #     kl_penalty = -log_diff.mean()
#                 #     kl_penalty *= self.kl_coefficient
#                 #     policy_loss = surrogate_loss + kl_penalty
#                 # elif self.loss == 'hybrid':
#                 #     # Run lambda param through sigmoid to clamp between 0 and 1
#                 #     lambda_value = T.sigmoid(self.lambda_param)
#                 #     entropy_penalty = -self.entropy_coefficient * entropy
#                 #     log_diff = new_log_probs - log_probs_batch
#                 #     kl_penalty = -log_diff.mean()
#                 #     kl_penalty *= self.kl_coefficient
#                 #     policy_loss = surrogate_loss + entropy_penalty + kl_penalty
#                 # else:
#                 #     raise ValueError(f'Unknown loss: {self.loss}')

#                 # Update the policy
#                 self.policy_model.optimizer.zero_grad()
#                 policy_loss.backward()
#                 # if self.policy_grad_clip is not None:
#                 T.nn.utils.clip_grad_norm_(self.policy_model.parameters(), max_norm=self.policy_grad_clip)
#                 self.policy_model.optimizer.step()

#                 # Update the value function
#                 # value_loss = F.mse_loss(self.value_function(states_batch), returns_batch)
#                 values = self.value_model(states_batch)
#                 value_loss = (values - returns_batch).pow(2).mean()
#                 self.value_model.optimizer.zero_grad()
#                 value_loss.backward()
#                 self.value_model.optimizer.step()
#                 # epoch_time = time.time() - start_time
#                 # times.append((epoch_time, dist_delta))

#                 # set dist as previous dist
#                 prev_dist = dist

#         # if self.callbacks:
#         #     for callback in self.callbacks:
#         #         if isinstance(callback, WandbCallback):
#         #             # Reduce states to 3D embeddings
#         #             reducer = UMAP(n_components=3, random_state=42)
#         #             embeddings = reducer.fit_transform(states.cpu().numpy())  # Shape: (num_samples, 3)
#         #             # Compute the magnitude of the actions
#         #             action_magnitude = np.linalg.norm(actions.cpu().numpy(), axis=1)
#         #             df = pd.DataFrame({
#         #                 'embedding_x': embeddings[:, 0],
#         #                 'embedding_y': embeddings[:, 1],
#         #                 'embedding_z': embeddings[:, 2],
#         #                 'value': all_values.cpu().numpy().flatten(),
#         #                 'action_magnitude': action_magnitude,
#         #                 # If you want to include specific action components:
#         #                 # 'action_component_0': actions[:, 0],
#         #                 # 'action_component_1': actions[:, 1],
#         #                 # ...
#         #             })

#         #             # Create a 3D scatter plot colored by value estimates
#         #             fig_value = px.scatter_3d(
#         #                 df,
#         #                 x='embedding_x',
#         #                 y='embedding_y',
#         #                 z='embedding_z',
#         #                 color='value',
#         #                 title='State Embeddings Colored by Value Function',
#         #                 labels={'embedding_x': 'Embedding X', 'embedding_y': 'Embedding Y', 'embedding_z': 'Embedding Z', 'value': 'Value Estimate'},
#         #                 opacity=0.7
#         #             )
                    
#         #             # Create a 3D scatter plot colored by action magnitude
#         #             fig_action = px.scatter_3d(
#         #                 df,
#         #                 x='embedding_x',
#         #                 y='embedding_y',
#         #                 z='embedding_z',
#         #                 color='action_magnitude',
#         #                 title='State Embeddings Colored by Action Magnitude',
#         #                 labels={'embedding_x': 'Embedding X', 'embedding_y': 'Embedding Y', 'embedding_z': 'Embedding Z', 'action_magnitude': 'Action Magnitude'},
#         #                 opacity=0.7
#         #             )

#         #             # Log the 3D plots
#         #             wandb.log({
#         #                 "Value Function Embeddings 3D": fig_value,
#         #                 "Policy Embeddings 3D": fig_action
#         #             })

#         print(f'Policy Loss: {policy_loss.sum()}')
#         print(f'Value Loss: {value_loss}')
#         print(f'Entropy: {entropy.mean()}')
#         print(f'KL Divergence: {kl.mean()}')
#         # print(f'kl div:{kl_div.mean()}')
#         # if self.loss == 'hybrid':
#         #     print(f'Lambda: {lambda_value}')

#         if self.distribution == 'categorical':
#             return policy_loss, value_loss, entropy.mean(), kl.mean(), logits.detach().cpu().flatten()
#         else:
#             return policy_loss, value_loss, entropy.mean(), kl.mean(), param1.detach().cpu().flatten(), param2.detach().cpu().flatten()

#     def test(self, num_episodes, num_envs:int=1, seed=None, render_freq:int=0, training=False):
#         """
#         Tests the PPO agent in the environment for a specified number of episodes,
#         renders each episode, and saves the renders as video files.

#         Args:
#             num_episodes (int): Number of episodes to test the agent.
#             render_dir (str): Directory to save the rendered video files.

#         Returns:
#             dict: A dictionary containing the scores, entropy, and KL divergence for each episode.
#         """

#         # Set the policy and value function models to evaluation mode
#         self.policy_model.eval()
#         self.value_model.eval()

#         if seed is None:
#             seed = np.random.randint(100)

#         # Set render freq to 0 if None is passed
#         if render_freq == None:
#             render_freq = 0


#         print(f'seed value:{seed}')
#         # Set seeds
#         T.manual_seed(seed)
#         T.cuda.manual_seed(seed)
#         np.random.seed(seed)
#         gym.utils.seeding.np_random.seed = seed

#         # Create the render directory if it doesn't exist
#         # if not os.path.exists(save_dir):
#         #     os.makedirs(save_dir)

#         # if not training:
#         # self.env = self._initialize_env(render_freq)
#         env = self._initialize_env(render_freq, num_envs)
#         if self.callbacks and not training:
#             print('test begin callback if statement fired')
#             for callback in self.callbacks:
#                 self._config = callback._config(self)
#                 if isinstance(callback, WandbCallback):
#                     # Add to config to send to wandb for logging
#                     self._config['seed'] = seed
#                     self._config['num_envs'] = num_envs
#                 callback.on_test_begin(logs=self._config)

#         # episode_scores = [[] for _ in range(num_envs)]  # Track scores for each env
#         # reset step counter
#         step = 0
#         all_scores = []
#         all_log_probs = []

#         for episode in range(num_episodes):
#             if self.callbacks and not training:
#                 for callback in self.callbacks:
#                     callback.on_test_epoch_begin(epoch=step, logs=None)
#             done = False
#             states, _ = env.reset()
#             scores = 0
#             log_probs = []
#             frames = []  # List to store frames for the video

#             while not done:

#                 # Get action and log probability from the current policy
#                 actions, log_prob = self.get_action(states)
#                 # acts = [self.action_adapter(action, env) if self.distribution == 'beta' else action for action in actions]
#                 # acts = np.reshape(acts, env.action_space.shape)
#                 if self.distribution == 'beta':
#                     acts = self.action_adapter(actions, env)
#                 else:
#                     acts = actions
#                 acts = acts.astype(np.float32)
#                 acts = np.clip(acts, env.single_action_space.low, env.single_action_space.high)
#                 acts = acts.tolist()
#                 acts = [[float(a) for a in act] for act in acts]

#                 #  log prob to log probs list
#                 log_probs.append(log_prob)

#                 # Step the environment
#                 next_states, rewards, terms, truncs, _ = env.step(acts)
#                 # Update scores of each episode
#                 scores += rewards

#                 for i, (term, trunc) in enumerate(zip(terms, truncs)):
#                     if term or trunc:
#                         done = True
#                         # print(f'append true')
#                     # else:
#                     #     dones.append(False)

#                 if render_freq > 0:
#                     # Capture the frame
#                     frame = env.render()[0]
#                     # print(f'frame:{frame}')
#                     frames.append(frame)

#                 # Increment step count
#                 step += 1

#                 # Move to the next state
#                 states = next_states

#                 # Add metrics to test step config to log
#                 self._test_step_config['step_reward'] = rewards[0]
#                 if self.callbacks and not training:
#                     for callback in self.callbacks:
#                         callback.on_test_step_end(step=step, logs=self._test_step_config)

#             # Save the video if the episode number is divisible by render_freq
#             if (render_freq > 0) and ((episode + 1) % render_freq == 0):
#                 if training:
#                     print(f'episode number sent to renderer:{self.episodes[0]}')
#                     self.render(frames, self.episodes[0], 'train')
#                 else:
#                     self.render(frames, episode+1, 'test')

#             # Append the results for the episode
#             all_scores.append(scores)  # Store score at end of episode
#             self._test_episode_config["episode_reward"] = scores[0]

#             # Append log probs for the episode to all_log_probs list
#             all_log_probs.append(log_probs)

#             # Log to callbacks
#             if self.callbacks and not training:
#                 for callback in self.callbacks:
#                     callback.on_test_epoch_end(epoch=step, logs=self._test_episode_config)

#             print(f'Episode {episode+1}/{num_episodes} - Score: {all_scores[-1]}')

#             # Reset score for this environment
#             scores = 0
        
#         if self.callbacks and not training:
#             for callback in self.callbacks:
#                 callback.on_test_end(logs=self._test_episode_config)

#         # close the environment
#         env.close()

#         return {
#             'scores': all_scores,
#             'log probs': all_log_probs,
#             # 'entropy': entropy_list,
#             # 'kl_divergence': kl_list
#         }

#     def get_config(self):
#         return {
#                 "agent_type": self.__class__.__name__,
#                 # "env": serialize_env_spec(self.env.spec),
#                 "env": self.env.spec.to_json(),
#                 "policy": self.policy_model.get_config(),
#                 "value_model": self.value_model.get_config(),
#                 "distribution": self.distribution,
#                 "discount": self.discount,
#                 "gae_coefficient": self.gae_coefficient,
#                 "policy_clip": self.policy_clip,
#                 "entropy_coefficient": self.entropy_coefficient,
#                 "loss": self.loss,
#                 "kl_coefficient": self.kl_coefficient,
#                 "normalize_advantages":self.normalize_advantages,
#                 "normalize_values": self.normalize_values,
#                 "normalizer_clip": self.value_norm_clip,
#                 "grad_clip":self.policy_grad_clip,
#                 "reward_clip":self.reward_clip,
#                 "lambda_": self.lambda_,
#                 "callbacks": [callback.get_config() for callback in self.callbacks if self.callbacks is not None],
#                 "save_dir": self.save_dir,
#                 "device": self.device,
#                 # "seed": self.seed,
#             }

#     def save(self, save_dir=None):
#         """Saves the model."""

#         # Change self.save_dir if save_dir
#         # if save_dir is not None:
#         #     self.save_dir = save_dir + "/ddpg/"

#         config = self.get_config()

#         # makes directory if it doesn't exist
#         os.makedirs(self.save_dir, exist_ok=True)

#         # writes and saves JSON file of DDPG agent config
#         with open(self.save_dir + "/config.json", "w", encoding="utf-8") as f:
#             json.dump(config, f, cls=CustomJSONEncoder)

#         # saves policy and value model
#         self.policy_model.save(self.save_dir)
#         self.value_model.save(self.save_dir)

#         # if self.normalize_inputs:
#         #     self.state_normalizer.save_state(self.save_dir + "state_normalizer.npz")

#         # if wandb callback, save wandb config
#         # if self._wandb:
#         #     for callback in self.callbacks:
#         #         if isinstance(callback, rl_callbacks.WandbCallback):
#         #             callback.save(self.save_dir + "/wandb_config.json")


#     @classmethod
#     def load(cls, config, load_weights=True):
#         """Loads the model."""

#         # create EnvSpec from config
#         # env_spec_json = json.dumps(config["env"])
#         # print(f'env spec json: {env_spec_json}')
#         env_spec = gym.envs.registration.EnvSpec.from_json(config["env"])
#         # load policy model
#         policy_model = models.StochasticContinuousPolicy.load(config['save_dir'], load_weights)
#         # load value model
#         value_model = models.ValueModel.load(config['save_dir'], load_weights)
#         # load callbacks
#         callbacks = [rl_callbacks.load(callback_info['class_name'], callback_info['config']) for callback_info in config['callbacks']]

#         # return PPO agent
#         agent = cls(
#             gym.make(env_spec),
#             policy_model = policy_model,
#             value_model = value_model,
#             distribution = config["distribution"],
#             discount=config["discount"],
#             gae_coefficient = config["gae_coefficient"],
#             policy_clip = config["policy_clip"],
#             entropy_coefficient = config["entropy_coefficient"],
#             loss = config["loss"],
#             kl_coefficient = config["kl_coefficient"],
#             normalize_advantages = config["normalize_advantages"],
#             normalize_values = config["normalize_values"],
#             value_normalizer_clip = config["normalizer_clip"],
#             policy_grad_clip = config["grad_clip"],
#             reward_clip = config['reward_clip'],
#             lambda_ = config["lambda_"],
#             callbacks=callbacks,
#             save_dir=config["save_dir"],
#             device=config["device"],
#         )

#         # if agent.normalize_inputs:
#         #     agent.state_normalizer = helper.Normalizer.load_state(config['save_dir'] + "state_normalizer.npz")

#         return agent

# def load_agent_from_config_path(config_path, load_weights=True):
#     """Loads an agent from a config file path."""
#     with open(
#         Path(config_path).joinpath(Path("obj_config.json")), "r", encoding="utf-8"
#     ) as f:
#         config = json.load(f)

#     agent_type = config["agent_type"]

#     # Use globals() to get a reference to the class
#     agent_class = globals().get(agent_type)

#     if agent_class:
#         return agent_class.load(config_path, load_weights)

#     raise ValueError(f"Unknown agent type: {agent_type}")

def load_agent_from_config(config, load_weights=True):
    """Loads an agent from a loaded config file."""
    agent_type = config["agent_type"]

    # Use globals() to get a reference to the class
    agent_class = globals().get(agent_type)

    if agent_class:
        return agent_class.load(config, load_weights)

    raise ValueError(f"Unknown agent type: {agent_type}")


def get_agent_class_from_type(agent_type: str):
    """Builds an agent from a passed agent type str."""

    types = {"Actor Critic": "ActorCritic",
             "Reinforce": "Reinforce",
             "DDPG": "DDPG",
             "HER_DDPG": "HER",
             "HER": "HER",
             "TD3": "TD3",
             "PPO": "PPO",
            }

    # Use globals() to get a reference to the class
    agent_class = globals().get(types[agent_type])

    if agent_class:
        return agent_class

    raise ValueError(f"Unknown agent type: {agent_type}")

# def init_sweep(sweep_config, comm=None):
#     # rank = MPI.COMM_WORLD.Get_rank()
#     if comm is not None:
#         logger.debug(f"Rank {rank} comm detected")
#         rank = comm.Get_rank()
#         logger.debug(f"Global rank {MPI.COMM_WORLD.Get_rank()} set to comm rank {rank}")
#         logger.debug(f"Rank {rank} in {comm.Get_name()}, name {comm.Get_name()}")
    
#     try:
#         # Set the environment variable
#         os.environ['WANDB_DISABLE_SERVICE'] = 'true'
#         # logger.debug(f"{comm.Get_name()}; Rank {rank} WANDB_DISABLE_SERVICE set to true")

#         # Set seeds (Seeds now set in train.  Update each)
#         # random.seed(train_config['seed'])
#         # np.random.seed(train_config['seed'])
#         # T.manual_seed(train_config['seed'])
#         # T.cuda.manual_seed(train_config['seed'])
#         # logger.debug(f'{comm.Get_name()}; Rank {rank} random seeds set')

#         # Only primary process (rank 0) calls wandb.init() to build agent and log data
#         if comm is not None:
#             if rank == 0:
#                 # logger.debug('MPI rank 0 process fired')
#                 # try:
#                 run_number = wandb_support.get_next_run_number(sweep_config["project"])
#                 logger.debug(f"{comm.Get_name()}; Rank {rank} run number set: {run_number}")
                
#                 run = wandb.init(
#                     project=sweep_config["project"],
#                     settings=wandb.Settings(start_method='thread'),
#                     job_type="train",
#                     name=f"train-{run_number}",
#                     tags=["train"],
#                     group=f"group-{run_number}",
#                     # dir=run_dir
#                 )
#                 logger.debug("wandb.init() fired")
#                 wandb_config = dict(wandb.config)
#                 model_type = list(wandb_config.keys())[0]
                
#                 # Wait for configuration to be populated
#                 max_retries = 10
#                 retry_interval = 1  # in seconds

#                 for _ in range(max_retries):
#                     if "model_type" in wandb.config:
#                         break
#                     logger.debug(f"{comm.Get_name()}; Rank {rank} Waiting for wandb.config to be populated...")
#                     time.sleep(retry_interval)

#                 if "model_type" in wandb.config:
#                     logger.debug(f'{comm.Get_name()}; Rank {rank} wandb.config: {wandb.config}')
#                     run.tags = run.tags + (model_type,)
#                 else:
#                     logger.error("wandb.config did not populate with model_type within the expected time", exc_info=True)
                
#                 run.tags = run.tags + (model_type,)
#                 logger.debug(f"{comm.Get_name()}; Rank {rank} run.tag set")
#                 env = gym.make(**{param: value["value"] for param, value in sweep_config["parameters"]["env"]["parameters"].items()})
#                 # save env spec to string
#                 env_spec = env.spec.to_json()
#                 logger.debug(f"{comm.Get_name()}; Rank {rank} env built: {env.spec}")
#                 callbacks = []
#                 callbacks.append(rl_callbacks.WandbCallback(project_name=sweep_config["project"], run_name=f"train-{run_number}", _sweep=True))
#                 logger.debug(f"{comm.Get_name()}; Rank {rank} callbacks created")

#             else:
#                 env_spec = None
#                 callbacks = None
#                 run_number = None
#                 wandb_config = None
            
#             # Use MPI Barrier to sync processes
#             logger.debug(f"{comm.Get_name()}; Rank {rank} init_sweep calling MPI Barrier")
#             comm.Barrier()
#             logger.debug(f"{comm.Get_name()}; Rank {rank} init_sweep MPI Barrier passed")

#             env_spec = comm.bcast(env_spec, root=0)
#             callbacks = comm.bcast(callbacks, root=0)
#             run_number = comm.bcast(run_number, root=0)
#             wandb_config = comm.bcast(wandb_config, root=0)
#             model_type = sweep_config['parameters']['model_type']
#             logger.debug(f"{comm.Get_name()}; Rank {rank} broadcasts complete")

#             agent = get_agent_class_from_type(model_type)
#             logger.debug(f"{comm.Get_name()}; Rank {rank} agent class found. Calling sweep_train")
#             agent.sweep_train(wandb_config, env_spec, callbacks, run_number, comm)
        
#         else:
#             print('comm = None')
#             run_number = wandb_support.get_next_run_number(sweep_config["project"])
#             logger.debug(f"run number set: {run_number}")
#             print(f'run number:{run_number}')
            
#             run = wandb.init(
#                 project=sweep_config["project"],
#                 settings=wandb.Settings(start_method='thread'),
#                 job_type="train",
#                 name=f"train-{run_number}",
#                 tags=["train"],
#                 group=f"group-{run_number}",
#                 # dir=run_dir
#             )
#             logger.debug("wandb.init() fired")
#             wandb_config = dict(wandb.config)
#             print(f'wandb config: {wandb_config}')
#             model_type = wandb_config['model_type']
            
#             # Wait for configuration to be populated
#             max_retries = 10
#             retry_interval = 1  # in seconds

#             for _ in range(max_retries):
#                 if "model_type" in wandb.config:
#                     break
#                 logger.debug(f"Waiting for wandb.config to be populated...")
#                 time.sleep(retry_interval)

#             if "model_type" in wandb.config:
#                 logger.debug(f'wandb.config: {wandb.config}')
#                 run.tags = run.tags + (model_type,)
#             else:
#                 logger.error("wandb.config did not populate with model_type within the expected time", exc_info=True)
            
#             run.tags = run.tags + (model_type,)
#             logger.debug(f"run.tag set")
#             # env = gym.make(**{param: value["value"] for param, value in sweep_config["parameters"]["env"]["parameters"].items()})
#             env_params = {
#                 key.replace("env_", ""): val["value"]
#                 for key, val in sweep_config["parameters"].items()
#                 if key.startswith("env_")
#             }
#             #DEBUG
#             print(f'env_params:{env_params}')
#             env = gym.make(**env_params)
#             # save env spec to string
#             env_spec = env.spec.to_json()
#             logger.debug(f"env built: {env.spec}")
#             callbacks = []
#             callbacks.append(rl_callbacks.WandbCallback(project_name=sweep_config["project"], run_name=f"train-{run_number}", _sweep=True))
#             logger.debug(f"callbacks created")
#             agent = get_agent_class_from_type(model_type)
#             logger.debug(f"agent class found. Calling sweep_train")
#             agent.sweep_train(wandb_config, env_spec, callbacks, run_number)

#     except Exception as e:
#         logger.error(f"Error in rl_agent.init_sweep: {e}", exc_info=True)

# def init_sweep(sweep_config):
#     try:
#         # Set the environment variable
#         os.environ['WANDB_DISABLE_SERVICE'] = 'true'
#         run_number = wandb_support.get_next_run_number(sweep_config["project"])
#         logger.debug(f"run number set: {run_number}")
#         run = wandb.init(
#             project=sweep_config["project"],
#             settings=wandb.Settings(start_method='thread'),
#             job_type="train",
#             name=f"train-{run_number}",
#             tags=["train"],
#             group=f"group-{run_number}",
#         )
#         logger.debug("wandb.init() fired")
#         wandb_config = dict(wandb.config)
#         model_type = list(wandb_config.keys())[0]
#         # Wait for configuration to be populated
#         max_retries = 10
#         retry_interval = 1  # in seconds
#         for _ in range(max_retries):
#             if "model_type" in wandb.config:
#                 break
#             logger.debug("Waiting for wandb.config to be populated...")
#             time.sleep(retry_interval)
#         if "model_type" in wandb.config:
#             logger.debug(f'wandb.config: {wandb.config}')
#             run.tags = run.tags + (model_type,)
#         else:
#             logger.error("wandb.config did not populate with model_type within the expected time", exc_info=True)
#         run.tags = run.tags + (model_type,)
#         logger.debug("run.tag set")
#         # Extract environment parameters from sweep_config
#         env_params = {
#             key.replace("env_", ""): val["value"]
#             for key, val in sweep_config["parameters"].items()
#             if key.startswith("env_")
#         }
#         env = gym.make(**env_params)
#         env_spec = env.spec.to_json()
#         logger.debug(f"env built: {env.spec}")
#         callbacks = []
#         callbacks.append(rl_callbacks.WandbCallback(project_name=sweep_config["project"], run_name=f"train-{run_number}", _sweep=True))
#         logger.debug(f"callbacks created")
#         agent = get_agent_class_from_type(model_type)
#         logger.debug(f"agent class found. Calling sweep_train")
#         agent.sweep_train(wandb_config, env_spec, callbacks, run_number)
#     except Exception as e:
#         logger.error(f"Error in rl_agent.init_sweep: {e}", exc_info=True)

def init_sweep(config):
    try:
        # Extract the model type (stored as a list) from the config.
        model_type_list = config.get("model_type", [])
        if not model_type_list:
            raise ValueError("No model type provided in config.")
        model_type = model_type_list[0]

        # Inject wandb settings into the config if not already provided.
        if "wandb" not in config:
            run_number = wandb_support.get_next_run_number(config["project"])
            config["wandb"] = {
                "project": config["project"],
                "name": f"train-{run_number}",
                "job_type": "train",
                "tags": ["train", model_type],
                "group": f"group-{run_number}",
            }

        # Build the environment.
        env_params = {
            key.replace("env_", ""): config[key]
            for key in config if key.startswith("env_")
        }
        env = gym.make(**env_params)
        env_spec = env.spec.to_json()
        logger.debug(f"Environment built: {env.spec}")

        # Create callbacks (using your custom WandbCallback).
        callbacks = []
        callbacks.append(WandbCallback(
            project_name=config["project"],
            run_name=config["wandb"]["name"],
            _sweep=True
        ))
        logger.debug("Callbacks created")

        # Get the appropriate agent class from the model type.
        agent = get_agent_class_from_type(model_type)
        logger.debug("Agent class found. Calling sweep_train")

        # Call the sweep_train function on the agent with the full config.
        agent.sweep_train(config, env_spec, callbacks, run_number)
    except Exception as e:
        logger.error(f"Error in init_sweep: {e}", exc_info=True)


```

---

## rl_callbacks.py

```python
import os
import json
import numpy as np
import torch as T
import wandb

import wandb_support


class Callback():
    """
    Base class for all callbacks in reinforcement learning.

    Methods:
        on_train_begin(logs): Called at the beginning of training.
        on_train_end(logs): Called at the end of training.
        on_train_epoch_begin(epoch, logs): Called at the beginning of each epoch during training.
        on_train_epoch_end(epoch, logs): Called at the end of each epoch during training.
        on_train_step_begin(logs): Called at the beginning of each training step.
        on_train_step_end(step, logs): Called at the end of each training step.
        on_test_begin(logs): Called at the beginning of testing.
        on_test_end(logs): Called at the end of testing.
        on_test_epoch_begin(epoch, logs): Called at the beginning of each epoch during testing.
        on_test_epoch_end(epoch, logs): Called at the end of each epoch during testing.
        on_test_step_begin(logs): Called at the beginning of each testing step.
        on_test_step_end(step, logs): Called at the end of each testing step.
    """
    
    def on_train_begin(self, logs=None):
        pass

    def on_train_end(self, logs=None):
        pass

    def on_train_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_train_epoch_end(self, epoch: int, logs=None):
        pass

    def on_train_step_begin(self, logs=None):
        pass

    def on_train_step_end(self, step: int, logs=None):
        pass

    def on_test_begin(self, logs=None):
        pass

    def on_test_end(self, logs=None):
        pass

    def on_test_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_test_epoch_end(self, epoch: int, logs=None):
        pass

    def on_test_step_begin(self, logs=None):
        pass

    def on_test_step_end(self, step: int, logs=None):
        pass



class WandbCallback(Callback):
    """
    W&B integration callback for tracking and logging metrics.

    Args:
        project_name (str): Name of the W&B project.
        run_name (str, optional): Name of the specific W&B run.
        chkpt_freq (int): Frequency of saving checkpoints.
        _sweep (bool): Whether this run is part of a W&B sweep.
    """

    def __init__(self, project_name: str, run_name: str = None, chkpt_freq: int = 100, _sweep: bool = False):
        super().__init__()
        self.project_name = project_name
        self.run_name = run_name
        self.chkpt_freq = chkpt_freq
        self._sweep = _sweep
        self.save_dir = None
        self.model_type = None

    def on_train_begin(self, models, logs=None):
        if not self._sweep:
            run_number = wandb_support.get_next_run_number(self.project_name)
            run = wandb.init(
                project=self.project_name,
                name=f"train-{run_number}",
                tags=["train", self.model_type],
                group=f"group-{run_number}",
                job_type="train",
                config=logs,
            )
            self.run_name = run.name
        wandb.watch(models, log='all', log_freq=100, idx=1, log_graph=True)

    def on_train_end(self, logs=None):
        wandb.finish()

    def on_train_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_train_epoch_end(self, epoch: int, logs=None):
        wandb.log(logs, step=epoch)
        if (logs["best"]) & (logs["episode"] % self.chkpt_freq == 0):
            wandb_support.save_model_artifact(self.save_dir, self.project_name, model_is_best=True)

    def on_train_step_begin(self, step: int, logs=None):
        pass

    def on_train_step_end(self, step: int, logs=None):
        wandb.log(logs, step=step)

    def on_test_begin(self, logs=None, run_number: int = None):
        if run_number is None:
            try:
                run_number = wandb_support.get_run_number_from_name(self.run_name)
            except AttributeError:
                run_number = wandb_support.get_next_run_number(self.project_name)
        run = wandb.init(
            project=self.project_name,
            job_type="test",
            tags=["test", self.model_type],
            name=f"test-{run_number}",
            group=f"group-{run_number}",
            config=logs,
        )
        wandb.config.update({"model_type": self.model_type})
        self.run_name = run.name

    def on_test_end(self, logs=None):
        if not self._sweep:
            wandb.finish()

    def on_test_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_test_epoch_end(self, epoch: int, logs=None):
        wandb.log(logs, step=epoch)

    def on_test_step_begin(self, step: int, logs=None):
        pass

    def on_test_step_end(self, step: int, logs=None):
        wandb.log(logs, step=step)

    def _config(self, agent):
        """Configures callback internal state for wandb integration."""
        self.model_type = type(agent).__name__
        self.save_dir = agent.save_dir
        return agent.get_config()

    def get_config(self):
        return {
            'class_name': self.__class__.__name__,
            'config': {
                'project_name': self.project_name,
                'run_name': self.run_name,
                'chkpt_freq': self.chkpt_freq,
                '_sweep': self._sweep
            }
        }

    def save(self, folder: str = "wandb_config.json"):
        """Save model."""
        wandb_config = self.get_config()
        with open(folder, "w", encoding="utf-8") as f:
            json.dump(wandb_config, f)

    @classmethod
    def load(cls, config):
        return cls(**config)

    
class DashCallback(Callback):
    """
    Callback for sending training/testing data to a Dash app.

    Args:
        dash_app_url (str): URL of the Dash app.
    """
    def __init__(self, dash_app_url: str):
        super().__init__()
        self.dash_app_url = dash_app_url
        self._episode_num = 0

    def on_train_begin(self, logs=None):
        self._episode_num = 0

    def on_train_end(self, logs=None):
        pass

    def on_train_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_train_epoch_end(self, epoch: int, logs=None):
        logs = self.convert_values_to_serializable(logs)
        self._episode_num += 1
        logs['episode'] = self._episode_num

        try:
            os.makedirs("assets", exist_ok=True)
            with open("assets/training_data.json", 'w') as f:
                json.dump(logs, f)
        except Exception as e:
            print(f"Failed to save training data to Dash app: {e}")

    def on_train_step_begin(self, step: int, logs=None):
        pass

    def on_train_step_end(self, step: int, logs=None):
        pass

    def on_test_begin(self, logs=None):
        self._episode_num = 0

    def on_test_end(self, logs=None):
        pass

    def on_test_epoch_begin(self, epoch: int, logs=None):
        pass

    def on_test_epoch_end(self, epoch: int, logs=None):
        logs = self.convert_values_to_serializable(logs)
        self._episode_num += 1
        logs['episode'] = self._episode_num

        try:
            with open("assets/testing_data.json", 'w') as f:
                json.dump(logs, f)
        except Exception as e:
            print(f"Failed to save testing data to Dash app: {e}")

    def on_test_step_begin(self, step: int, logs=None):
        pass

    def on_test_step_end(self, step: int, logs=None):
        pass

    def _config(self, agent):
        pass

    # def _get_model_config(self, model):
    #     pass

    def convert_values_to_serializable(self, d: dict):
        for key, value in d.items():
            if isinstance(value, T.Tensor):
                d[key] = value.item() if value.numel() == 1 else value.tolist()
            elif isinstance(value, (np.float32, np.float64)):
                d[key] = float(value)
            elif isinstance(value, (np.int32, np.int64)):
                d[key] = int(value)
            elif isinstance(value, dict):
                self.convert_values_to_serializable(value)
        return d

    def get_config(self):
        return {
            'class_name': self.__class__.__name__,
            'config': {
                'dash_app_url': self.dash_app_url
            }
        }

    def save(self, folder: str = "wandb_config.json"):
        pass

    @classmethod
    def load(cls, config: dict):
        return cls(**config)
    
def load(class_name: str, config: dict):
    """
    Load a callback class from its name and configuration.

    Args:
        class_name (str): Name of the callback class.
        config (dict): Configuration dictionary for the callback.

    Returns:
        Callback: An instance of the callback class.
    """
    types = {
        "WandbCallback": WandbCallback,
        "DashCallback": DashCallback,
    }

    if class_name in types:
        return types[class_name].load(config)

    raise ValueError(f"Unknown callback type: {class_name}")
```

---

## schedulers.py

```python
import torch as T
from torch import optim
from torch.optim import lr_scheduler

class ScheduleWrapper:
    def __init__(self, schedule_config):
        #DEBUG
        # print(f'scheduler config:{schedule_config}')
        if schedule_config is None:
            self.schedule_config = None
            self.scheduler = None
            return
        self.schedule_config = schedule_config
        
        self.param = T.nn.Parameter(T.zeros(1), requires_grad=False)
        self.optimizer = optim.SGD([self.param], lr=1.0)
        
        scheduler_type = schedule_config.get("type", "").lower()
        scheduler_params = schedule_config.get("params", {})
        
        # Map scheduler type to PyTorch's built-in schedulers
        if scheduler_type == "linear":
            self.scheduler = lr_scheduler.LinearLR(self.optimizer, **scheduler_params)
        elif scheduler_type == "step":
            self.scheduler = lr_scheduler.StepLR(self.optimizer, **scheduler_params)
        elif scheduler_type == "cosineannealing":
            self.scheduler = lr_scheduler.CosineAnnealingLR(self.optimizer, **scheduler_params)
        elif scheduler_type == "exponential":
            self.scheduler = lr_scheduler.ExponentialLR(self.optimizer, **scheduler_params)
        else:
            raise ValueError(f"Unsupported scheduler type: {scheduler_type}")

    def step(self):
        if self.scheduler:
            self.scheduler.step()

    def get_factor(self):
        if self.scheduler:
            return self.scheduler.get_last_lr()[0]
        return 1.0
    
    def get_config(self):
        return self.schedule_config

```

---

## streamlit_support.py

```python
"""Adds a custom callback to the keras model to update streamlit elements"""

# imports
from tensorflow.keras.callbacks import Callback


class StreamlitCallback(Callback):
    """Custom callback to update streamlit elements"""

    def __init__(self, progress_bar, terminal_output, num_episodes):
        """Initializes the callback.

        Args:
            progress_bar (streamlit.progress): Streamlit progress bar element.
            terminal_output (streamlit.terminal): Streamlit terminal output element.
            num_episodes (int): Number of episodes to train for.

        """

        super().__init__()
        self.progress_bar = progress_bar
        self.terminal_output = terminal_output
        self.num_episodes = num_episodes
        self.epoch = 0

    def on_batch_end(self, batch, logs=None):
        """Updates the progress bar and terminal output."""

    def On_epoch_begin(self, epoch, logs=None):
        """Updates the progress bar and terminal output."""

    def on_epoch_end(self, epoch, logs=None):
        """Updates the progress bar and terminal output."""
        # udpate epoch number
        self.epoch += 1
        # Update Streamlit elements
        self.progress_bar.progress(self.epoch / self.num_episodes)
        message = f"### Episode {self.epoch}\nEpisode Reward: {logs['episode_reward']}\nAVG Reward: {logs['avg_reward']}"
        self.terminal_output.markdown(message)

    def on_train_begin(self, logs=None):
        """Updates the progress bar and terminal output."""
        self.terminal_output.markdown("### Training in Progress")
        self.epoch = 0

    def on_train_end(self, logs=None):
        """Updates the progress bar and terminal output."""
        self.terminal_output.markdown(
            f"### Training Complete\nFinal Reward: {logs['episode_reward']}"
        )

    def on_test_begin(self, logs=None):
        """Updates the progress bar and terminal output."""
        self.terminal_output.markdown("### Testing in Progress")

    def on_test_end(self, logs=None):
        """Updates the progress bar and terminal output."""
        self.terminal_output.markdown(
            f"### Testing Complete\nFinal Reward: {logs['episode_reward']}"
        )

    def on_test_batch_end(self, batch, logs=None):
        """Updates the progress bar and terminal output."""
        # udpate epoch number
        self.epoch += 1
        # Update Streamlit elements
        self.progress_bar.progress(self.epoch / self.num_episodes)
        self.terminal_output.markdown(
            f"### Episode {self.epoch}\nEpisode Reward: {logs['episode_reward']}"
        )

```

---

## sweep.py

```python
import os
from pathlib import Path
import argparse
import json
import logging
import multiprocessing
import subprocess
import shutil
import uuid
import time

import wandb
import gymnasium as gym
import numpy as np
import torch as T
# from dash_callbacks import run_agent
from wandb_support import get_next_run_number, format_layers
from rl_agents import init_sweep

print('sweep.py called')

# Initialize parser
parser = argparse.ArgumentParser(description='Sweep MPI')
parser.add_argument('--sweep_config', type=str, required=True, help='Path to sweep_config.json to load agent')
parser.add_argument('--num_sweeps', type=str, required=True, help='Number of sweeps to perform')
args = parser.parse_args()
sweep_config_path = args.sweep_config
num_sweeps = args.num_sweeps
print(f'num sweep = {num_sweeps}')
print(f'num sweeps type is {type(num_sweeps)}')
if num_sweeps == 0:
    num_sweeps = None

# Initialize logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

args = parser.parse_args()

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def main(sweep_config, num_sweeps):
    print('main fired...')
    try:
        # config_file_path = 'sweep/sweep_config.json'
        # with open(config_file_path, 'r') as file:
        #     sweep_config = json.load(file)

        # config_file_path = 'sweep/train_config.json'
        # with open(config_file_path, 'r') as file:
        #     train_config = json.load(file)
        # Set extended timeout settings
        # Configure W&B settings
        # settings = {
        #     'timeout':120,          # Increase the timeout (in seconds)
        #     'retry_timedelta':300,  # Set retry delay time
        #     'max_retries':5         # Number of retry attempts
        # }

        print('attempting to create sweep id...')
        sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config["project"])
        print('sweep id created')

        # num_sweep_agents = train_config['num_agents'] if train_config['num_agents'] is not None else 1
        # print(f'num sweep agents:{num_sweep_agents}')

        # if num_sweep_agents > 1:
        #     processes = []
        #     for agent in range(num_sweep_agents):
        #         p = multiprocessing.Process(target=run_agent, args=(sweep_id, sweep_config, train_config))
        #         p.start()
        #         processes.append(p)

        #     for p in processes:
        #         p.join()
        
        # else:
        run_agent(sweep_id, sweep_config, num_sweeps)

    except KeyError as e:
        logger.error(f"KeyError in W&B stream handling: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)

def run_agent(sweep_id, sweep_config, num_sweeps):
    print('run agent fired...')
    wandb.agent(
        sweep_id,
        function=lambda: init_sweep(sweep_config),
        count=num_sweeps,
        project=sweep_config["project"],
    )

# def run_sweep(sweep_config):
#     print('run sweep fired...')

#     try:
#         init_sweep(sweep_config)

#         # if train_config['use_mpi']:
#         #     print('sweep test use mpi fired')
#         #     sweep_config_path = 'sweep/sweep_config.json'
#         #     train_config_path = 'sweep/train_config.json'
#         #     # Construct absolute paths
#         #     # base_dir = Path(__file__).resolve().parent
#         #     # sweep_config_path = base_dir / 'sweep' / 'sweep_config.json'
#         #     # train_config_path = base_dir / 'sweep' / 'train_config.json'
            
#         #     # Ensure the paths exist
#         #     # if not sweep_config_path.exists():
#         #     #     logger.error(f"Sweep config path does not exist: {sweep_config_path}")
#         #     # if not train_config_path.exists():
#         #     #     logger.error(f"Train config path does not exist: {train_config_path}")

#         #     # Find the path to mpirun
#         #     # mpi_path = shutil.which("mpirun")
#         #     # if not mpi_path:
#         #     #     logger.error("mpirun not found in PATH")
#         #     # else:
#         #     #     logger.debug(f"mpirun found at: {mpi_path}")

#         #     # mpi_command = [
#         #     #     mpi_path,
#         #     #     "-np", str(train_config['num_workers']),
#         #     #     "python", str(base_dir / "init_sweep.py"),
#         #     #     "--sweep_config", str(sweep_config_path),
#         #     #     "--train_config", str(train_config_path)
#         #     # ]
#         #     # mpi_command = f"{mpi_path} -np {str(train_config['num_workers'])} python {str(base_dir / 'init_sweep.py')} --sweep_config {str(sweep_config_path)} --train_config {str(train_config_path)}"
        
#         #     try:
#         #         # logger.debug(f"Running MPI command: {' '.join(mpi_command)}")

#         #         # mpi_process = subprocess.Popen(mpi_command, env=os.environ.copy(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)
#         #         command = (
#         #             "mpiexec -np "
#         #             + str(train_config['num_workers'])
#         #             + " python init_sweep.py "
#         #             + "--sweep_config "
#         #             + str(sweep_config_path)
#         #             + " --train_config "
#         #             + str(train_config_path)
#         #         )

#         #         logger.debug(f"Running command: {command}")

#         #         result = subprocess.run(
#         #             command,
#         #             check=True,
#         #             stderr=subprocess.PIPE,
#         #             stdout=subprocess.PIPE,
#         #             universal_newlines=True,
#         #             shell=True
#         #         )

#         #         logger.debug("Standard Output:")
#         #         logger.debug(result.stdout)

#         #         logger.debug("Standard Error:")
#         #         logger.debug(result.stderr)

#         #     except subprocess.CalledProcessError as e:
#         #         logger.error(f"Subprocess failed with return code {e.returncode}")
#         #         logger.error(f"Standard Output: {e.stdout}")
#         #         logger.error(f"Standard Error: {e.stderr}")
#         #     except Exception as e:
#         #         logger.error(f"Error during subprocess execution: {str(e)}")

#         # else:
#             # init_sweep(sweep_config)

#     except Exception as e:
#         logging.error(f"Error during sweep run attempt: {str(e)}")

if __name__ == "__main__":
    try:
        # Set the environment variable
        # os.environ['WANDB_DISABLE_SERVICE'] = 'true'
        logger.debug("sweep.py fired")
        sweep_config = load_config(sweep_config_path)
        print(f'sweep config: {sweep_config}')
        logger.debug("sweep config loaded")
        main(sweep_config, num_sweeps)
    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)

```

---

## sweep_her_mpi.py

```python

import os
import json
import logging
import argparse

import random
import numpy as np
import torch as T
import wandb
from mpi4py import MPI

from rl_agents import load_agent_from_config

parser = argparse.ArgumentParser(description='Sweep MPI')
parser.add_argument('--agent_config', type=str, required=True, help='Path to agent_config.json to load agent')
parser.add_argument('--train_config', type=str, required=True, help='Path to train_config.json to set training params')

args = parser.parse_args()

agent_config_path = args.agent_config
train_config_path = args.train_config

# Initialize logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def train(agent_config, train_config):
    try:
        logger.debug('mpi sweep train fired')

        # Load agent
        try:
            rl_agent = load_agent_from_config(agent_config)
            logger.debug('mpi sweep rl agent loaded')
        except Exception as e:
            logger.error(f"Error loading agent from config: {e}")
            raise

        # Set seeds
        try:
            random.seed(train_config['seed'])
            np.random.seed(train_config['seed'])
            T.manual_seed(train_config['seed'])
            T.cuda.manual_seed(train_config['seed'])
            logger.debug('mpi sweep seeds set')
        except Exception as e:
            logger.error(f"Error setting seeds: {e}")
            raise

        # Initialize wandb
        # try:
        #     if MPI.COMM_WORLD.Get_rank() == 0:
        #         wandb.init()
        #         logger.debug('mpi sweep wandb init called')
        # except Exception as e:
        #     logger.error(f"Error initializing wandb: {e}")
        #     raise

        # Train agent
        try:
            rl_agent.train(
                num_epochs=train_config['num_epochs'],
                num_cycles=train_config['num_cycles'],
                num_episodes=train_config['num_episodes'],
                num_updates=train_config['num_updates'],
                render=False,
                render_freq=0
            )
            logger.debug('mpi sweep training completed')
        except Exception as e:
            logger.error(f"Error during training: {e}")
            raise

    except Exception as e:
        logger.error(f"General error in train function: {e}")
        raise

        
if __name__ == "__main__":

    try:
        agent_config = load_config(agent_config_path)
        train_config = load_config(train_config_path)

        # Set the environment variable
        os.environ['WANDB_DISABLE_SERVICE'] = 'true'

        train(agent_config, train_config)

    except KeyError as e:
        logger.error(f"KeyError in W&B stream handling: {e}")
    except Exception as e:
        logger.error(f"An error occurred: {e}")
```

---

## test.py

```python
import sys
import json
import logging
import argparse
import subprocess

import numpy as np
import torch as T

from rl_agents import load_agent_from_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
parser = argparse.ArgumentParser(description='Test Agent')
parser.add_argument('--agent_config', type=str, required=True, help='Path to the agent configuration file')
parser.add_argument('--test_config', type=str, required=True, help='Path to the test configuration file')

args = parser.parse_args()

agent_config_path = args.agent_config
test_config_path = args.test_config

def test_agent(agent_config, test_config):
    try:
        
        agent_type = agent_config['agent_type']
        load_weights = test_config['load_weights']
        num_episodes = test_config['num_episodes']
        num_envs = test_config['num_envs']
        # render = test_config['render']
        render_freq = test_config.get('render_freq', 0)
        seed = test_config.get('seed', np.random.randint(1000))
        # run_number = test_config['run_number']
        # num_runs = test_config['num_runs']

        assert agent_type in ['Reinforce', 'ActorCritic', 'DDPG', 'TD3', 'HER', 'PPO'], f"Unsupported agent type: {agent_type}"

        if agent_type:
            agent = load_agent_from_config(agent_config, load_weights)
            print('agent config loaded')
            # for i in range(num_runs):
            agent.test(num_episodes, num_envs, seed, render_freq)
            # print(f'testing run {i+1} initiated')

    except KeyError as e:
        logging.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logging.error(str(e))
        raise

    except Exception as e:
        logging.exception("An unexpected error occurred during testing")
        raise

if __name__ == '__main__':
    try:
        with open(agent_config_path, 'r', encoding="utf-8") as f:
            agent_config = json.load(f)

        with open(test_config_path, 'r', encoding="utf-8") as f:
            test_config = json.load(f)

        test_agent(agent_config, test_config)

    except FileNotFoundError as e:
        logging.error(f"Configuration file not found: {str(e)}")

    except json.JSONDecodeError as e:
        logging.error(f"Invalid JSON format in configuration file: {str(e)}")
```

---

## test_per.py

```python
import torch as T
import numpy as np
import matplotlib.pyplot as plt
from buffer import PrioritizedReplayBuffer, SumTree
from env_wrapper import GymnasiumWrapper
import gymnasium as gym
import os
import wandb
from collections import defaultdict

def log_priority_metrics(buffer, step, prefix=''):
    """Log priority-related metrics to wandb"""
    if buffer.priority == 'proportional':
        # Get all priorities from sum tree
        priorities = buffer.sum_tree.tree[buffer.sum_tree.capacity-1:].cpu().numpy()
        valid_priorities = priorities[priorities > 0]  # Filter out zero priorities
        
        metrics = {
            f'{prefix}priority_mean': np.mean(valid_priorities),
            f'{prefix}priority_median': np.median(valid_priorities),
            f'{prefix}priority_std': np.std(valid_priorities),
            f'{prefix}priority_max': float(buffer.sum_tree.max_priority),
            f'{prefix}total_priority': float(buffer.sum_tree.total_priority),
            f'{prefix}beta': float(buffer.beta)
        }
    else:  # rank-based
        priorities = buffer.priorities[:min(buffer.counter, buffer.buffer_size)].cpu().numpy()
        metrics = {
            f'{prefix}priority_mean': np.mean(priorities),
            f'{prefix}priority_median': np.median(priorities),
            f'{prefix}priority_std': np.std(priorities),
            f'{prefix}beta': float(buffer.beta)
        }
    
    wandb.log(metrics, step=step)

def test_sum_tree():
    """Test SumTree functionality"""
    print("\nTesting SumTree...")
    
    # Initialize wandb
    wandb.init(project="per_test", name="sum_tree_test")
    
    # Initialize SumTree
    capacity = 8
    tree = SumTree(capacity, T.device('cpu'))
    
    # Test 1: Basic priority updates
    print("\nTest 1: Basic priority updates")
    indices = T.tensor([0, 1, 2])
    priorities = T.tensor([1.0, 2.0, 3.0])
    tree.update(indices, priorities)
    total = tree.total_priority
    print(f"Total priority after update: {total} (should be 6.0)")
    print(f"Tree structure:\n{tree.tree}")
    
    # Log metrics
    wandb.log({
        'sum_tree/total_priority': total,
        'sum_tree/max_priority': float(tree.max_priority),
        'sum_tree/tree_values': wandb.Histogram(tree.tree.cpu().numpy())
    })
    
    # Test 2: Sampling
    print("\nTest 2: Sampling")
    p_values = T.tensor([1.5, 4.5])  # Should sample indices 1 and 2
    indices, priorities = tree.get(p_values)
    print(f"Sampled indices: {indices}")
    print(f"Sampled priorities: {priorities}")
    
    # Log sampling results
    wandb.log({
        'sum_tree/sampled_indices': wandb.Histogram(indices.cpu().numpy()),
        'sum_tree/sampled_priorities': wandb.Histogram(priorities.cpu().numpy())
    })
    
    # Test 3: Max priority update
    print("\nTest 3: Max priority tracking")
    print(f"Current max priority: {tree.max_priority}")
    tree.update(T.tensor([3]), T.tensor([5.0]))
    print(f"Max priority after update: {tree.max_priority}")
    
    wandb.log({
        'sum_tree/max_priority_after_update': float(tree.max_priority)
    })
    
    wandb.finish()

def test_prioritized_buffer():
    """Test PrioritizedReplayBuffer functionality"""
    print("\nTesting PrioritizedReplayBuffer...")
    
    # Initialize wandb
    wandb.init(project="per_test", name="prioritized_buffer_test")
    
    # Create environment and buffer
    env = gym.make('Pendulum-v1')
    env_spec = gym.spec('Pendulum-v1')
    env_wrapper = GymnasiumWrapper(env_spec)
    buffer_size = 1000
    buffer = PrioritizedReplayBuffer(
        env=env_wrapper,
        buffer_size=buffer_size,
        alpha=0.6,
        beta_start=0.4,
        beta_iter=10000,
        priority='proportional'
    )
    
    # Test 1: Adding transitions
    print("\nTest 1: Adding transitions")
    state = env.reset()[0]
    priorities = []
    sampled_priorities = defaultdict(list)
    
    for i in range(100):
        action = env.action_space.sample()
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # Add transition to buffer
        buffer.add(
            states=np.array([state]),
            actions=np.array([action]),
            rewards=np.array([reward]),
            next_states=np.array([next_state]),
            dones=np.array([done])
        )
        
        if done:
            state = env.reset()[0]
        else:
            state = next_state
            
        # Record current max priority
        if buffer.priority == 'proportional':
            priorities.append(float(buffer.sum_tree.max_priority))
        
        # Log metrics every 10 steps
        if i % 10 == 0:
            log_priority_metrics(buffer, i, prefix='buffer/')
    
    print(f"Buffer size after adding: {min(buffer.counter, buffer.buffer_size)}")
    
    # Test 2: Sampling with importance sampling
    print("\nTest 2: Sampling with importance sampling")
    batch_size = 32
    num_samples = 1000  # Number of samples to collect for distribution analysis
    
    # Collect samples and their priorities
    for _ in range(num_samples):
        samples = buffer.sample(batch_size)
        if buffer.goal_shape is not None:
            states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals, weights, indices = samples
        else:
            states, actions, rewards, next_states, dones, weights, indices = samples
            
        # Get priorities for sampled indices
        if buffer.priority == 'proportional':
            sampled_priorities['proportional'].extend(buffer.sum_tree.tree[buffer.sum_tree.capacity-1+indices].cpu().numpy())
        else:
            sampled_priorities['rank'].extend(buffer.priorities[indices].cpu().numpy())
    
    # Log sampling statistics
    wandb.log({
        'sampling/sampled_priorities_dist': wandb.Histogram(sampled_priorities['proportional' if buffer.priority == 'proportional' else 'rank']),
        'sampling/weights_dist': wandb.Histogram(weights.cpu().numpy()),
        'sampling/weights_mean': float(weights.mean()),
        'sampling/weights_std': float(weights.std())
    })
    
    # Test 3: Priority updates
    print("\nTest 3: Priority updates")
    print(f"Initial beta value: {buffer.beta:.3f}")
    
    # Update priorities and sample again
    new_priorities = T.rand(batch_size, device=buffer.device) * 2.0  # Random priorities between 0 and 2
    buffer.update_priorities(indices, new_priorities)
    
    # Log metrics after priority update
    log_priority_metrics(buffer, 100, prefix='buffer_after_update/')
    
    # Sample after priority update
    if buffer.goal_shape is not None:
        states, actions, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals, weights, indices = buffer.sample(batch_size)
    else:
        states, actions, rewards, next_states, dones, weights, indices = buffer.sample(batch_size)
    
    print(f"\nWeight range after priority update: [{weights.min():.3f}, {weights.max():.3f}]")
    print(f"Updated beta value: {buffer.beta:.3f}")
    
    # Log final metrics
    wandb.log({
        'final/weights_range_min': float(weights.min()),
        'final/weights_range_max': float(weights.max()),
        'final/beta': float(buffer.beta)
    })
    
    # Plot weight distribution
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.hist(weights.cpu().numpy(), bins=20)
    plt.title('Importance Sampling Weights Distribution')
    plt.xlabel('Weight Value')
    plt.ylabel('Count')
    
    plt.subplot(1, 2, 2)
    plt.hist(weights.cpu().numpy(), bins=20)
    plt.title('Weights Distribution After Priority Update')
    plt.xlabel('Weight Value')
    plt.ylabel('Count')
    
    plt.tight_layout()
    
    # Create test_results directory if it doesn't exist
    os.makedirs('test_results', exist_ok=True)
    plt.savefig('test_results/priority_weights_distribution.png')
    plt.close()
    
    # Log the plot to wandb
    wandb.log({
        'plots/weight_distribution': wandb.Image('test_results/priority_weights_distribution.png')
    })
    
    wandb.finish()

if __name__ == "__main__":
    test_sum_tree()
    test_prioritized_buffer() 
```

---

## torch_utils.py

```python
import torch as T
import torch.nn as nn
from torch import optim
import numpy as np


def get_device(device_spec=None):
    """
    Convert any valid device specification to a torch.device object.
    
    Args:
        device_spec: Can be a string ('cuda', 'cpu'), a torch.device object, 
                    or None (defaults to 'cuda' if available, else 'cpu')
                    
    Returns:
        torch.device: The corresponding device object
    """
    if device_spec is None:
        return T.device('cuda' if T.cuda.is_available() else 'cpu')
    elif isinstance(device_spec, str):
        return T.device('cuda' if device_spec == 'cuda' and T.cuda.is_available() else 'cpu')
    elif isinstance(device_spec, T.device):
        return device_spec
    else:
        raise ValueError(f"Unsupported device specification: {device_spec}")

def set_seed(seed: int):
    """
    Set the random seed for reproducibility in PyTorch and NumPy.

    Args:
        seed (int): The seed to set for all random number generators.
    """
    T.manual_seed(seed)
    T.cuda.manual_seed(seed)
    np.random.seed(seed)

def VarianceScaling_(
    tensor: T.Tensor, 
    scale: float = 1.0, 
    mode: str = 'fan_in', 
    distribution: str = 'normal'
):
    """
    Apply variance scaling initialization to a tensor.

    Args:
        tensor (torch.Tensor): The tensor to initialize.
        scale (float): Scaling factor for the initialization. Default is 1.0.
        mode (str): Mode for scaling. Options are 'fan_in', 'fan_out', or 'fan_avg'. Default is 'fan_in'.
        distribution (str): Distribution to use for initialization. Options are 'normal', 'truncated_normal', or 'uniform'. Default is 'normal'.

    Raises:
        ValueError: If mode or distribution is not supported.
    """
    # Validate mode
    if mode not in {'fan_in', 'fan_out', 'fan_avg'}:
        raise ValueError(f"Mode '{mode}' is not supported. Use 'fan_in', 'fan_out', or 'fan_avg'.")
    
    # Compute fan based on mode
    if mode == 'fan_in':
        fan = tensor.size(0)
    elif mode == 'fan_out':
        fan = tensor.size(1)
    else:  # mode == 'fan_avg'
        fan = (tensor.size(0) + tensor.size(1)) / 2

    val = T.sqrt(T.tensor(scale / fan))

    # Apply initialization based on distribution
    with T.no_grad():
        if distribution == 'normal':
            nn.init.normal_(tensor, mean=0.0, std=val)
        elif distribution == 'truncated_normal':
            nn.init.trunc_normal_(tensor, mean=0.0, std=val.item(), a=-2.0 * val.item(), b=2.0 * val.item())
        elif distribution == 'uniform':
            nn.init.uniform_(tensor, -val.item(), val.item())
        else:
            raise ValueError(
                f"Distribution '{distribution}' is not supported. Use 'normal', 'truncated_normal', or 'uniform'."
            )
    
def get_optimizer_by_name(name: str):
    """
    Retrieve an optimizer class by name.

    Args:
        name (str): Name of the optimizer (e.g., 'Adam', 'SGD').

    Returns:
        Optimizer class: The PyTorch optimizer class corresponding to the name.

    Raises:
        ValueError: If the optimizer name is not recognized.
    """
    opts = {
        "Adam": optim.Adam,
        "SGD": optim.SGD,
        "RMSprop": optim.RMSprop,
        "Adagrad": optim.Adagrad,
    }

    if name not in opts:
        raise ValueError(
            f'Optimizer "{name}" is not recognized. Available options: {list(opts.keys())}'
        )

    return opts[name]
```

---

## train.py

```python
import sys
import json
import time
from logging_config import logger
import argparse
import subprocess

import random
import numpy as np
import torch as T
import wandb

from rl_agents import load_agent_from_config

# Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description='Train Agent')
parser.add_argument('--agent_config', type=str, required=True, help='Path to the agent configuration file')
parser.add_argument('--train_config', type=str, required=True, help='Path to the train configuration file')

args = parser.parse_args()

agent_config_path = args.agent_config
train_config_path = args.train_config

def train_agent(agent_config, train_config):

    # wandb_initialized = False  # Track if wandb is initialized
    try:
        agent_type = agent_config['agent_type']
        print(f'agent type:{agent_type}')
        # load_weights = train_config.get('load_weights', False)
        load_weights = train_config.get('load_weights', False)
        # render = train_config.get('render', False)
        render_freq = train_config.get('render_freq', 0)
        save_dir = train_config.get('save_dir', agent_config['save_dir'])
        #DEBUG
        # print(f'training save dir: {save_dir}')
        num_envs = train_config['num_envs']
        seed = train_config.get('seed', np.random.randint(1000))
        run_number = train_config.get('run_number', None)
        num_episodes = train_config['num_episodes']

        assert agent_type in ['Reinforce', 'ActorCritic', 'DDPG', 'TD3', 'HER', 'PPO'], f"Unsupported agent type: {agent_type}"

        if agent_type:
            agent = load_agent_from_config(agent_config, load_weights)

            if agent_type in ['ActorCritic', 'DDPG', 'TD3']:
                agent.train(num_episodes, num_envs, seed, render_freq)

            elif agent_type == 'Reinforce':
                trajectories_per_update = train_config['trajectories_per_update']
                agent.train(num_episodes, num_envs, trajectories_per_update, seed, render_freq)

            elif agent_type == 'HER':
                num_epochs = train_config['num_epochs']
                num_cycles = train_config['num_cycles']
                num_updates = train_config['learning_epochs']
                agent.train(num_epochs, num_cycles, num_episodes, num_updates, render_freq, num_envs, seed)
            
            elif agent_type == 'PPO':
                timesteps = train_config['num_timesteps']
                traj_length = train_config['traj_length']
                batch_size = train_config['batch_size']
                learning_epochs = train_config['learning_epochs']
                agent.train(timesteps, traj_length, batch_size, learning_epochs, num_envs, seed, 10, render_freq, run_number=run_number)

    except KeyError as e:
        logger.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logger.error(str(e))
        raise

    except Exception as e:
        logger.exception("An unexpected error occurred during training")
        raise
    # finally:
    #     # Ensure the WandB run is properly finished if it was initialized
    #     if wandb_initialized:
    #         wandb.finish()
    #         logging.info("WandB run finished")

if __name__ == '__main__':
    try:
        with open(agent_config_path, 'r', encoding="utf-8") as f:
            agent_config = json.load(f)

        with open(train_config_path, 'r', encoding="utf-8") as f:
            train_config = json.load(f)

        train_agent(agent_config, train_config)

    except FileNotFoundError as e:
        logger.error(f"Configuration file not found: {str(e)}")

    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON format in configuration file: {str(e)}")
```

---

## train_ddpg_mpi.py

```python
import sys
import json
from logging_config import logger
import argparse
from mpi4py import MPI

from rl_agents import DDPG

# Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description='Train Agent')
parser.add_argument('--agent_config', type=str, required=True, help='Path to the agent configuration file')
parser.add_argument('--train_config', type=str, required=True, help='Path to the train configuration file')

args = parser.parse_args()

agent_config_path = args.agent_config
train_config_path = args.train_config

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def train_agent(agent_config, train_config):
    # print('mpi train agent fired')
    try:
        agent_type = agent_config['agent_type']
        load_weights = train_config.get('load_weights', False)
        num_episodes = train_config['num_episodes']
        render = train_config.get('render', False)
        render_freq = train_config.get('render_freq', 0)
        save_dir = train_config.get('save_dir', agent_config['save_dir'])
        run_number = train_config.get('run_number', None)

        assert agent_type == 'DDPG', f"Unsupported agent type: {agent_type}"

        if agent_type:
            #DEBUG
            # print(f'if agent passed in mpi')
            agent = DDPG.load(agent_config, load_weights)
            agent.use_mpi = True
            agent.comm = MPI.COMM_WORLD
            agent.rank = MPI.COMM_WORLD.Get_rank()
            logger.error(f'rank:{agent.rank}')
            # print(f'mpi agent built:{agent.get_config()}')
            agent.train(num_episodes, render, render_freq, save_dir, run_number)

    except KeyError as e:
        logger.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logger.error(str(e))
        raise

    except Exception as e:
        logger.exception("An unexpected error occurred during training")
        raise

if __name__ == '__main__':
    # print('train_her_mpi fired')
    try:
        agent_config = load_config(agent_config_path)
        # print(f'mpi agent config loaded:{agent_config}')
        train_config = load_config(train_config_path)
        # print(f'mpi train config loaded:{train_config}')

        train_agent(agent_config, train_config)

    except FileNotFoundError as e:
        logger.error(f"Configuration file not found: {str(e)}")

    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON format in configuration file: {str(e)}")
```

---

## train_her_mpi.py

```python
import sys
import json
import logging
import argparse

from rl_agents import HER

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description='Train Agent')
parser.add_argument('--agent_config', type=str, required=True, help='Path to the agent configuration file')
parser.add_argument('--train_config', type=str, required=True, help='Path to the train configuration file')

args = parser.parse_args()

agent_config_path = args.agent_config
train_config_path = args.train_config

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def train_agent(agent_config, train_config):
    print('mpi train agent fired')
    try:
        agent_type = agent_config['agent_type']
        load_weights = train_config.get('load_weights', False)
        num_epochs = train_config.get('num_epochs', None)
        num_cycles = train_config.get('num_cycles', None)
        num_episodes = train_config['num_episodes']
        num_updates = train_config.get('num_updates', None)
        render = train_config.get('render', False)
        render_freq = train_config.get('render_freq', 0)
        save_dir = train_config.get('save_dir', agent_config['save_dir'])
        run_number = train_config.get('run_number', None)

        assert agent_type == 'HER', f"Unsupported agent type: {agent_type}"
        print(f'mpi assert passed')

        if agent_type:
            #DEBUG
            print(f'if agent passed in mpi')
            agent = HER.load(agent_config, load_weights)
            print(f'mpi agent built:{agent.get_config()}')
            agent.train(num_epochs, num_cycles, num_episodes, num_updates, render, render_freq, save_dir, run_number)

    except KeyError as e:
        logging.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logging.error(str(e))
        raise

    except Exception as e:
        logging.exception("An unexpected error occurred during training")
        raise

if __name__ == '__main__':
    print('train_her_mpi fired')
    try:
        agent_config = load_config(agent_config_path)
        print(f'mpi agent config loaded:{agent_config}')
        train_config = load_config(train_config_path)
        print(f'mpi train config loaded:{train_config}')

        train_agent(agent_config, train_config)

    except FileNotFoundError as e:
        logging.error(f"Configuration file not found: {str(e)}")

    except json.JSONDecodeError as e:
        logging.error(f"Invalid JSON format in configuration file: {str(e)}")
```

---

## train_td3_mpi.py

```python
import sys
import json
from logging_config import logger
import argparse
from mpi4py import MPI

from rl_agents import TD3

# Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description='Train Agent')
parser.add_argument('--agent_config', type=str, required=True, help='Path to the agent configuration file')
parser.add_argument('--train_config', type=str, required=True, help='Path to the train configuration file')

args = parser.parse_args()

agent_config_path = args.agent_config
train_config_path = args.train_config

def load_config(path):
    with open(path, 'r', encoding="utf-8") as f:
        return json.load(f)

def train_agent(agent_config, train_config):
    # print('mpi train agent fired')
    try:
        agent_type = agent_config['agent_type']
        load_weights = train_config.get('load_weights', False)
        num_episodes = train_config['num_episodes']
        render = train_config.get('render', False)
        render_freq = train_config.get('render_freq', 0)
        save_dir = train_config.get('save_dir', agent_config['save_dir'])
        run_number = train_config.get('run_number', None)

        assert agent_type == 'TD3', f"Unsupported agent type: {agent_type}"

        if agent_type:
            #DEBUG
            # print(f'if agent passed in mpi')
            agent = TD3.load(agent_config, load_weights)
            agent.use_mpi = True
            agent.comm = MPI.COMM_WORLD
            agent.rank = MPI.COMM_WORLD.Get_rank()
            logger.error(f'rank:{agent.rank}')
            # print(f'mpi agent built:{agent.get_config()}')
            agent.train(num_episodes, render, render_freq, save_dir, run_number)

    except KeyError as e:
        logger.error(f"Missing configuration parameter: {str(e)}")
        raise

    except AssertionError as e:
        logger.error(str(e))
        raise

    except Exception as e:
        logger.exception("An unexpected error occurred during training")
        raise

if __name__ == '__main__':
    # print('train_her_mpi fired')
    try:
        agent_config = load_config(agent_config_path)
        # print(f'mpi agent config loaded:{agent_config}')
        train_config = load_config(train_config_path)
        # print(f'mpi train config loaded:{train_config}')

        train_agent(agent_config, train_config)

    except FileNotFoundError as e:
        logger.error(f"Configuration file not found: {str(e)}")

    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON format in configuration file: {str(e)}")
```

---

## utils.py

```python
"General utility functions"
import os
import torch as T
import numpy as np
from moviepy.editor import ImageSequenceClip
from env_wrapper import EnvWrapper, GymnasiumWrapper, IsaacSimWrapper
from gymnasium.envs.registration import EnvSpec


def flatten_dict(d: dict, parent_key: str = '', sep: str = '_') -> dict:
    """
    Flatten a nested dictionary.

    Args:
        d (dict): The dictionary to flatten.
        parent_key (str): The base key to use for the current level of recursion (default is '').
        sep (str): The separator between nested keys (default is '_').

    Returns:
        dict: A flattened dictionary with concatenated keys.
    """
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

def render_video(frames: list, episode: int, save_dir: str, context: str = None) -> None:
    """
    Render a video from a list of frames and save it to a file.

    Args:
        frames (list): List of frames to render.
        episode (int): Episode number for naming the output file.
        save_dir (str): Directory to save the rendered video.
        context (str): Context for the video (e.g., 'train', 'test').

    Returns:
        None
    """
    print('rendering episode...')
    if not isinstance(frames, np.ndarray):
        frames = np.array(frames)
    if context == 'train':
        video_path = os.path.join(save_dir, f"renders/train/episode_{episode}.mp4")
    elif context == 'test':
        print('context set to test')
        video_path = os.path.join(save_dir, f"renders/test/episode_{episode}.mp4")
        print(f'video path:{video_path}')
    else:
        video_path = os.path.join(save_dir, f"renders/episode_{episode}.mp4")

    # Ensure the directory exists
    directory = os.path.dirname(video_path)
    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    fps = 30
    clip = ImageSequenceClip(list(frames), fps=fps)
    clip.write_videofile(video_path, codec='libx264')
    print('episode rendered')

def build_env_wrapper_obj(config: dict) -> EnvWrapper:
    """
    Build an environment wrapper object based on the configuration.

    Args:
        config (dict): Configuration dictionary containing environment details.

    Returns:
        EnvWrapper: An instance of the appropriate environment wrapper.

    Raises:
        ValueError: If the wrapper type specified in the config is not recognized.
    """
    if config['type'] == "GymnasiumWrapper":
        env = EnvSpec.from_json(config['env'])
        return GymnasiumWrapper(env)
    elif config['type'] == "IsaacSimWrapper":
        pass
    else:
        raise ValueError(f"Environment wrapper {config['type']} not found")
    
def check_for_inf_or_NaN(value:T.Tensor, label:str):
    if T.any(T.isnan(value)):
        print(f'NAN found in {label}; {value}')
    elif T.any(T.isinf(value)):
        print(f'inf found in {label}; {value}')
```

---

## wandb_support.py

```python
"""Adds support for W&B integration to the rl_agents package."""


import json
from pathlib import Path
import os
import subprocess
import random
from logging_config import logger
import numpy as np
import torch as T
import pandas as pd
import gymnasium as gym
import wandb
from scipy.stats import zscore
import plotly.graph_objs as go

import rl_agents
import utils


def save_model_artifact(file_path: str, project_name: str, model_is_best: bool = False):
    """Save the model to W&B

    Args:
        file_path (str): The path to the model files.
        project_name (str): The name of the project.
        model_is_best (bool): Whether the model is the best model so far.
    """
    artifact = wandb.Artifact(f"{project_name}-{wandb.run.name}", type="model")
    artifact.add_dir(f"{file_path}", name="model")

    if os.path.exists(f"{file_path}/wandb_config.json"):
        artifact.add_file(f"{file_path}/wandb_config.json", name="wandb_config.json")

    aliases = ["latest"]
    if model_is_best:
        aliases.append("best")

    wandb.log_artifact(artifact, aliases=aliases)
    wandb.run.link_artifact(artifact, target_path=project_name)


def load_model_from_artifact(artifact, load_weights: bool = True):
    """
    Load a model from a W&B artifact.

    Args:
        artifact (wandb.Artifact): The artifact containing the model.
        load_weights (bool): Whether to load weights.

    Returns:
        rl_agents.Agent: Loaded RL agent object.
    """
    # Download the artifact files to a directory
    artifact_dir = Path(artifact.download())

    return rl_agents.load_agent_from_config(artifact_dir, load_weights)


def format_layers(sweep_config):
    """formats sweep_config into policy and value layers.

    Args:
        sweep_config (dict): The sweep configuration.

    Returns:
        tuple: The policy layers and value layers.
    """
    agent = sweep_config['model_type']
    model_config = {}
    for model in ['policy', 'value']:
        model_config[model] = {'hidden': []}
        layer_config = []
        for layer_idx in range(1, sweep_config[f"{agent}_{model}_num_layers"] + 1):
            layer = {
                'type': sweep_config[f"{agent}_{model}_{layer_idx}_layer_types"],
                'params': format_dense_layer(sweep_config, agent, model, layer_idx)
            }
            layer_config.append(layer)
        model_config[model]['hidden'] = layer_config
        output_layer = {
            'type': 'dense',
            'params': {
                'kernel': sweep_config[f"{agent}_{model}_output_kernel"],
                'kernel params': get_kernel_params(sweep_config, agent, model, 'output')
            }
        }
        model_config[model]['output'] = [output_layer]
    return model_config
                    
        
        # # get policy layers
        # policy_layers = []
        # if sweep_config[agent_type][f"{agent_type}_policy_num_layers"] > 0:
        #     for layer_num in range(1, sweep_config[agent_type][f"{agent_type}_policy_num_layers"] + 1):
        #         policy_layers.append(
        #             (
        #                 sweep_config[agent_type][f"policy_units_layer_{layer_num}_{agent_type}"],
        #                 sweep_config[agent_type][f"{agent_type}_policy_activation"],
        #                 kernels['policy_hidden_kernel'],
        #             )
        #         )
        # # get value layers
        # value_layers = []
        # if sweep_config[agent_type][f"{agent_type}_value_num_layers"] > 0:
        #     for layer_num in range(1, sweep_config[agent_type][f"{agent_type}_value_num_layers"] + 1):
        #         value_layers.append(
        #             (
        #                 sweep_config[agent_type][f"value_units_layer_{layer_num}_{agent_type}"],
        #                 sweep_config[agent_type][f"{agent_type}_value_activation"],
        #                 kernels['value_hidden_kernel'],
        #             )
        #         )

        # return policy_layers, value_layers, kernels
    
    # elif agent in ["DDPG", "HER_DDPG", "TD3"]:
        
    #     # Get actor CNN layers if present
    #     actor_cnn_layers = []
    #     # Set variable to keep track of last out channel param in conv layer loop to use for batchnorm layer features param
    #     last_out_channels = 0

    #     # Set number of cnn layers to loop over and to check if 0
    #     num_layers = sweep_config[agent][f"{agent}_actor_num_cnn_layers"]
        
    #     # Loop over num layers if not 0
    #     if num_layers > 0:
    #         for layer_num in range(1, num_layers + 1):
    #             layer_type = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_types"]
    #             if layer_type == 'conv':
    #                 # get num filters
    #                 out_channels = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_conv_filters"]
    #                 # update last out channel param
    #                 last_out_channels = out_channels

    #                 # get kernel size
    #                 kernel_size = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_conv_kernel_size"]
    #                 # get stride
    #                 stride = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_conv_strides"]
    #                 # get padding
    #                 padding = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_conv_padding"]
    #                 # get bias
    #                 bias = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_conv_bias"]

    #                 # append to actor_cnn_layers
    #                 actor_cnn_layers.append({layer_type: {"out_channels": out_channels, "kernel_size": kernel_size, "stride": stride, "padding": padding, "bias": bias}})

    #             elif layer_type == 'pool':
    #                 # get pool size
    #                 kernel_size = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_pool_kernel_size"]
    #                 stride = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_pool_strides"]

    #                 # append to actor_cnn_layers
    #                 actor_cnn_layers.append({layer_type: {"kernel_size": kernel_size, "stride": stride}})

    #             elif layer_type == 'dropout':
    #                 # get dropout rate
    #                 rate = sweep_config[agent][f"actor_cnn_layer_{layer_num}_{agent}"][f"{agent}_actor_cnn_layer_{layer_num}_dropout_rate"]

    #                 # append to actor_cnn_layers
    #                 actor_cnn_layers.append({layer_type: {"p": rate}})

    #             elif layer_type == 'batchnorm':
    #                 if last_out_channels == 0:
    #                     raise ValueError("Batchnorm layer must come after a conv layer")
    #                 else:
    #                     num_features = last_out_channels
                    
    #                 # append to actor_cnn_layers
    #                 actor_cnn_layers.append({layer_type: {"num_features": num_features}})

    #     # Get critic CNN layers if present
    #     critic_cnn_layers = []
    #     # Set variable to keep track of last out channel param in conv layer loop to use for batchnorm layer features param
    #     last_out_channels = 0

    #     # Set number of cnn layers to loop over and to check if 0
    #     num_layers = sweep_config[agent][f"{agent}_critic_num_cnn_layers"]
        
    #     # Loop over num layers if not 0
    #     if num_layers > 0:
    #         for layer_num in range(1, num_layers + 1):
    #             layer_type = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_types"]
    #             if layer_type == 'conv':
    #                 # get num filters
    #                 out_channels = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_conv_filters"]
    #                 # update last out channel param
    #                 last_out_channels = out_channels

    #                 # get kernel size
    #                 kernel_size = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_conv_kernel_size"]
    #                 # get stride
    #                 stride = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_conv_strides"]
    #                 # get padding
    #                 padding = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_conv_padding"]
    #                 # get bias
    #                 bias = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_conv_bias"]

    #                 # append to critic_cnn_layers
    #                 critic_cnn_layers.append({layer_type: {"out_channels": out_channels, "kernel_size": kernel_size, "stride": stride, "padding": padding, "bias": bias}})

    #             elif layer_type == 'pool':
    #                 # get pool size
    #                 kernel_size = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_pool_kernel_size"]
    #                 stride = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_pool_strides"]

    #                 # append to critic_cnn_layers
    #                 critic_cnn_layers.append({layer_type: {"kernel_size": kernel_size, "stride": stride}})

    #             elif layer_type == 'dropout':
    #                 # get dropout rate
    #                 rate = sweep_config[agent][f"critic_cnn_layer_{layer_num}_{agent}"][f"{agent}_critic_cnn_layer_{layer_num}_dropout_rate"]

    #                 # append to critic_cnn_layers
    #                 critic_cnn_layers.append({layer_type: {"p": rate}})

    #             elif layer_type == 'batchnorm':
    #                 if last_out_channels == 0:
    #                     raise ValueError("Batchnorm layer must come after a conv layer")
    #                 else:
    #                     num_features = last_out_channels
                    
    #                 # append to critic_cnn_layers
    #                 critic_cnn_layers.append({layer_type: {"num_features": num_features}})

    #     # Create empty dict to store kernel params
    #     kernels = {}
    #     # Create kernel initializer params
    #     for model in ['actor', 'critic']:
    #         for layer in ['hidden', 'output']:
    #             kernel = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_initializer"]
    #             params = {}
    #             if kernel == "constant":
    #                 params["val"] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_value"]

    #             elif kernel == 'variance_scaling':
    #                 params['scale'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_scale"]
    #                 params['mode'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_mode"]
    #                 params['distribution'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_distribution"]

    #             elif kernel == 'normal':
    #                 params['mean'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_mean"]
    #                 params['std'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_stddev"]

    #             elif kernel == 'uniform':
    #                 params['a'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_minval"]
    #                 params['b'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_maxval"]
                
    #             elif kernel == 'truncated_normal':
    #                 params['mean'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_mean"]
    #                 params['std'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_stddev"]

    #             elif kernel == "xavier_uniform":
    #                 params['gain'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_gain"]

    #             elif kernel == "xavier_normal":
    #                 params['gain'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_gain"]

    #             elif kernel == "kaiming_uniform":
    #                 params['mode'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_mode"]

    #             elif kernel == "kaiming_normal":
    #                 params['mode'] = sweep_config[agent][f"{agent}_{model}_{layer}_kernel_{kernel}"][f"{kernel}_mode"]

    #             # Create dict with kernel and params
    #             kernels[f'{model}_{layer}_kernel'] = {kernel:params}

    #     #DEBUG
    #     # print(f'kernels: {kernels}')

        
        
    #     # get actor hidden layers
    #     actor_layers = []
    #     for layer_num in range(1, sweep_config[agent][f"{agent}_actor_num_layers"] + 1):
    #         actor_layers.append(
    #             (
    #                 sweep_config[agent][f"actor_units_layer_{layer_num}_{agent}"],
    #                 sweep_config[agent][f"{agent}_actor_activation"],
    #                 kernels['actor_hidden_kernel'],
    #             )
    #         )
    #     # get critic state hidden layers
    #     critic_state_layers = []
    #     for layer_num in range(1, sweep_config[agent][f"{agent}_critic_state_num_layers"] + 1):
    #         critic_state_layers.append(
    #             (
    #                 sweep_config[agent][f"critic_units_state_layer_{layer_num}_{agent}"],
    #                 sweep_config[agent][f"{agent}_critic_activation"],
    #                 kernels['critic_hidden_kernel'],
    #             )
    #         )
    #     # get critic merged hidden layers
    #     critic_merged_layers = []
    #     for layer_num in range(1, sweep_config[agent][f"{agent}_critic_merged_num_layers"] + 1):
    #         critic_merged_layers.append(
    #             (
    #                 sweep_config[agent][f"critic_units_merged_layer_{layer_num}_{agent}"],
    #                 sweep_config[agent][f"{agent}_critic_activation"],
    #                 kernels['critic_hidden_kernel'],
    #             )
    #         )

    #     return actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels
    
def format_dense_layer(sweep_config, agent, model, layer):
    """Returns dense layer params from wandb config formatted
    for layer config to be used to build RL agent 

    Args:
        sweep_config (wandb config): wandb config
        agent (str): agent type (PPO, TD3, etc...)
        model (str): model type (policy, value)
        layer (int or str): layer of model.  Either int for model hidden layer number or 'output'
    """
    # Create empty dict to store params
    params = {}
    # Get num units
    params['units'] = sweep_config[f"{agent}_{model}_{layer}_num_units"]
    # Get bias term
    params['bias'] = sweep_config[f"{agent}_{model}_{layer}_bias"]
    # Get kernel
    params['kernel'] = sweep_config[f"{agent}_{model}_{layer}_kernel"]
    # Add kernel_params dict to params
    params['kernel params'] = get_kernel_params(sweep_config, agent, model, layer)

    return params
    
def get_kernel_params(sweep_config, agent, model, layer):
    """Returns dict of parameters for the layers kernel 

    Args:
        sweep_config (wandb config): wandb config
        agent (str): agent type to get kernel params from
        model (str): model type to get kernel params from
        layer (int or str): layer to get kernel params from (either int or 'output')
    """
    #DEBUG
    print(f'sweep config passed to get_kernel_params:{sweep_config}')
    # Get kernel
    kernel = sweep_config[f"{agent}_{model}_{layer}_kernel"]
    #DEBUG
    print(f'kernel passed to get_kernel_params:{kernel}')

    # Create empty dict to store kernel params
    kernel_params = {}
    if kernel == "constant":
        kernel_params["val"] = sweep_config[f"{agent}_{model}_{layer}_kernel_value"]

    elif kernel == 'variance_scaling':
        kernel_params['scale'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_scale"]
        kernel_params['mode'] = sweep_config[f"{agent}_{model}_{layer}_kernel_{kernel}_mode"]
        kernel_params['distribution'] = sweep_config[f"{agent}_{model}_{layer}_kernel_distribution"]

    elif kernel == 'normal':
        kernel_params['mean'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_mean"]
        kernel_params['std'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_stddev"]

    elif kernel == 'uniform':
        kernel_params['a'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_minval"]
        kernel_params['b'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_maxval"]
    
    elif kernel == 'truncated_normal':
        kernel_params['mean'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_mean"]
        kernel_params['std'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_stddev"]

    elif kernel == "xavier_uniform":
        kernel_params['gain'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_gain"]

    elif kernel == "xavier_normal":
        kernel_params['gain'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_gain"]

    elif kernel == "kaiming_uniform":
        kernel_params['mode'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_mode"]

    elif kernel == "kaiming_normal":
        kernel_params['mode'] = sweep_config[f"{agent}_{model}_{layer}_{kernel}_mode"]

    return kernel_params


def load_model_from_run(run_name: str, project_name: str, load_weights: bool = True):
    """Loads the model from the specified run.

    Args:
        run_name (str): The name of the run.
        project_name (str): The name of the project.

    Returns:
        rl_agents.Agent: The agent object.
    """

    artifact = get_artifact_from_run(project_name, run_name)

    return load_model_from_artifact(artifact, load_weights)


def hyperparameter_sweep(
    sweep_config,
    train_config,
    # num_sweeps: int,
    # episodes_per_sweep: int,
    # epochs_per_sweep: int = None,
    # cycles_per_sweep: int = None,
    # updates_per_sweep: int = None,
):
    """Runs a hyperparameter sweep of the specified agent.

    Args:
        rl_agent (rl_agents.Agent): The agent to train.
        sweep_config (dict): The sweep configuration.
        num_sweeps (int): The number of sweeps to run.
        episodes_per_sweep (int): The number of episodes to train per sweep.
    """
    #DEBUG
    # print(f'hyperparameter_sweep fired...')
    sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config["project"])
    #DEBUG
    # print(f'sweep id: {sweep_id}')
    wandb.agent(
        sweep_id,
        function=lambda: _run_sweep(sweep_config, train_config,),
        count=train_config['num_sweeps'],
        project=sweep_config["project"],
    )
    # wandb.teardown()

def _run_sweep(sweep_config, train_config):
    """Runs a single sweep of the hyperparameter search.

    Args:
        sweep_config (dict): The sweep configuration.
        episodes_per_sweep (int): The number of episodes to train per sweep.
        save_dir (str): The directory to save the model to.

    Returns:
        dict: The sweep configuration.
    """
    try:
        from rl_callbacks import WandbCallback
        wandb.init()

        if wandb.config.model_type in ["Reinforce", "Actor Critic"]:
            run_number = get_next_run_number(sweep_config["project"])
            print(f'run number:{run_number}')
            train_config['run_number'] = run_number
            run = wandb.init(
                project=sweep_config["project"],
                settings=wandb.Settings(start_method='thread'),
                job_type="train",
                name=f"train-{run_number}",
                tags=["train"],
                group=f"group-{run_number}",
            )
            run.tags = run.tags + (wandb.config.model_type,)
            env = gym.make(**{param: value["value"] for param, value in sweep_config["parameters"]["env"]["parameters"].items()})
            save_dir = train_config.get('save_dir', sweep_config[wandb.config.model_type][f'{wandb.config.model_type}_save_dir'])
            random.seed(train_config['seed'])
            np.random.seed(train_config['seed'])
            T.manual_seed(train_config['seed'])
            T.cuda.manual_seed(train_config['seed'])

            callbacks = []
            if wandb.run:
                print(f'if wandb run fired')
                callbacks.append(WandbCallback(project_name=sweep_config["project"], _sweep=True))
            policy_layers, value_layers = format_layers(wandb.config)
            agent = rl_agents.get_agent_class_from_type(wandb.config.model_type)
            rl_agent = agent.build(
                env=env,
                policy_layers=policy_layers,
                value_layers=value_layers,
                callbacks=[WandbCallback(project_name=sweep_config["project"], _sweep=True)],
                config=wandb.config,
                save_dir=wandb.config.save_dir,
            )
        elif wandb.config.model_type == "DDPG":
            run_number = get_next_run_number(sweep_config["project"])
            print(f'run number:{run_number}')
            train_config['run_number'] = run_number
            run = wandb.init(
                project=sweep_config["project"],
                settings=wandb.Settings(start_method='thread'),
                job_type="train",
                name=f"train-{run_number}",
                tags=["train"],
                group=f"group-{run_number}",
            )
            run.tags = run.tags + (wandb.config.model_type,)
            env = gym.make(**{param: value["value"] for param, value in sweep_config["parameters"]["env"]["parameters"].items()})
            save_dir = train_config.get('save_dir', sweep_config[wandb.config.model_type][f'{wandb.config.model_type}_save_dir'])
            random.seed(train_config['seed'])
            np.random.seed(train_config['seed'])
            T.manual_seed(train_config['seed'])
            T.cuda.manual_seed(train_config['seed'])

            callbacks = []
            if wandb.run:
                print(f'if wandb run fired')
                callbacks.append(WandbCallback(project_name=sweep_config["project"], _sweep=True))
            actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels = format_layers(wandb.config)
            agent = rl_agents.get_agent_class_from_type(wandb.config.model_type)
            rl_agent = agent.build(
                env=env,
                actor_cnn_layers=actor_cnn_layers,
                critic_cnn_layers=critic_cnn_layers,
                actor_layers=actor_layers,
                critic_state_layers=critic_state_layers,
                critic_merged_layers=critic_merged_layers,
                kernels=kernels,
                callbacks=[WandbCallback(project_name=sweep_config["project"], _sweep=True)],
                config=wandb.config,
                save_dir=wandb.config.save_dir,
            )

        elif wandb.config.model_type == "HER_DDPG":
            print('her model creation fired')
            if train_config['use_mpi']:
                print('sweep use_mpi fired')
                mpi_command = [
                    "mpirun", 
                    "-np", str(train_config['num_workers']), 
                    "python", "sweep_mpi.py"
                ]
                
                mpi_process = subprocess.Popen(mpi_command, env=os.environ.copy(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = mpi_process.communicate()

                print("Standard Output:")
                print(stdout.decode())
                
                print("Standard Error:")
                print(stderr.decode())

                if mpi_process.returncode == 0:
                    print("Subprocess completed successfully")
                else:
                    print("Subprocess failed with return code", mpi_process.returncode)
            else:
                print('sweep not using mpi fired')
                print(f'wandb config:{wandb.config}')
                run_number = get_next_run_number(sweep_config["project"])
                print(f'run number:{run_number}')
                train_config['run_number'] = run_number
                run = wandb.init(
                    project=sweep_config["project"],
                    settings=wandb.Settings(start_method='thread'),
                    job_type="train",
                    name=f"train-{run_number}",
                    tags=["train"],
                    group=f"group-{run_number}",
                )
                print('wandb init called')
                run.tags = run.tags + (wandb.config.model_type,)
                print('wandb run tag complete')
                env = gym.make(**{param: value["value"] for param, value in sweep_config["parameters"]["env"]["parameters"].items()})
                print(f'env spec: {env.spec}')
                save_dir = train_config.get('save_dir', wandb.config[wandb.config.model_type][f'{wandb.config.model_type}_save_dir'])
                print(f'save dir set:{save_dir}')
                random.seed(train_config['seed'])
                np.random.seed(train_config['seed'])
                T.manual_seed(train_config['seed'])
                T.cuda.manual_seed(train_config['seed'])

                callbacks = []
                print(f'if wandb run fired')
                callbacks.append(WandbCallback(project_name=sweep_config["project"], run_name=f"train-{run_number}", _sweep=True))
                actor_cnn_layers, critic_cnn_layers, actor_layers, critic_state_layers, critic_merged_layers, kernels = format_layers(wandb.config)
                agent_class = rl_agents.get_agent_class_from_type(wandb.config.model_type)
                rl_agent = agent_class.build(
                    env=env,
                    actor_cnn_layers=actor_cnn_layers,
                    critic_cnn_layers=critic_cnn_layers,
                    actor_layers=actor_layers,
                    critic_state_layers=critic_state_layers,
                    critic_merged_layers=critic_merged_layers,
                    kernels=kernels,
                    callbacks=callbacks,
                    config=wandb.config,
                )
                print(f'agent built:{rl_agent.get_config()}')
                rl_agent.save()
                print(f'agent saved to {rl_agent.save_dir}')
                rl_agent.train(
                    num_epochs=train_config['num_epochs'],
                    num_cycles=train_config['num_cycles'],
                    num_episodes=train_config['num_episodes'],
                    num_updates=train_config['num_updates'],
                    render=False,
                    render_freq=0,
                    save_dir=save_dir,
                    run_number=run_number
                )

    except Exception as e:
        logging.error(f"Error during sweep run attempt: {str(e)}")


def get_run_id_from_name(project_name, run_name):
    """Returns the run ID for the specified run name.

    Args:
        project_name (str): The name of the project.
        run_name (str): The name of the run.

    Returns:
        str: The run ID.
    """

    api = wandb.Api()
    # Fetch all runs in the project
    runs = api.runs(f"{api.default_entity}/{project_name}")
    # Iterate over the runs and find the one with the matching name
    for run in runs:
        if run.name == run_name:
            return run.id

    # If we get here, no run has the given name
    return None


def get_run_number_from_name(run_name):
    """Extracts the run number from the run name.

    Args:
    run_name (str): The run name, e.g., 'train-4'.

    Returns:
    int: The extracted run number.
    """
    try:
        return int(run_name.split("-")[-1])

    except (IndexError, ValueError) as exc:
        raise ValueError(
            "Invalid run name format. Expected format 'train-X' where X is an integer."
        ) from exc


def get_next_run_number(project_name):
    """Returns the next run number for the specified project.

    Args:
        project_name (str): The name of the project.

    Returns:
        int: The next run number.
    """
    api = wandb.Api()
    # Get the list of runs from the project
    runs = api.runs(f"jasonhayes1987/{project_name}")
    if runs:
        # Extract the run numbers and find the maximum
        run_numbers = [int(run.name.split("-")[-1]) for run in runs]
        next_run_number = max(run_numbers) + 1
    else:
        next_run_number = 1

    return next_run_number


def get_run(project_name, run_name):
    """Returns the specified run.

    Args:
    project_name (str): The name of the project.
    run_name (str): The name of the run.

    Returns:
    wandb.Run: The run object.
    """

    api = wandb.Api()
    # get the runs ID
    run_id = get_run_id_from_name(project_name, run_name)

    # Fetch the run using the project and run name
    run_path = f"{api.default_entity}/{project_name}/{run_id}"
    run = api.run(run_path)

    return run


def get_artifact_from_run(
    project_name, run_name, artifact_type: str = "model", version="best"
):
    """Returns the specified artifact from the specified run.

    Args:
    project_name (str): The name of the project.
    run_name (str): The name of the run.
    artifact_type (str): The type of artifact to fetch.
    version (str): The version of the artifact to fetch.

    Returns:
    wandb.Artifact: The artifact object.
    """
    api = wandb.Api()
    # Get the run
    run = get_run(project_name, run_name)

    # Get the list of artifacts linked to this run
    linked_artifacts = run.logged_artifacts()

    # Find the artifact of the specified type
    artifact_name = None
    for artifact in linked_artifacts:
        if artifact.type == artifact_type and version in artifact.aliases:
            artifact_name = artifact.name
            break
    if artifact_name is None:
        raise ValueError("No artifact of the specified type found in the run")

    # Construct the artifact path
    artifact_path = f"{api.default_entity}/{project_name}/{artifact_name}"

    # Fetch the artifact
    artifact = api.artifact(artifact_path, type=artifact_type)

    return artifact


def get_projects():
    """Returns the list of projects."""
    api = wandb.Api()
    projects = api.projects()

    return projects


def get_runs(project_name):
    """Returns the list of runs for the specified project.

    Args:
        project_name (str): The name of the project.

    Returns:
        list: The list of runs.
    """

    api = wandb.Api()

    runs = api.runs(f"{api.default_entity}/{project_name}")

    return runs


def delete_all_runs(project_name, delete_artifacts: bool = True):
    """Deletes all runs for the specified project.

    Args:
    project_name (str): The name of the project.
    delete_artifacts (bool): Whether to delete the artifacts associated with the runs.
    """

    api = wandb.Api()
    wandb.finish()
    runs = api.runs(f"{api.default_entity}/{project_name}")
    for run in runs:
        print(f"Deleting run: {run.name}")
        run.delete()

def delete_all_artifacts(project_name, artifact_type: str = "model"):
    """Deletes all artifacts for the specified project."""
    api = wandb.Api()

    # Fetch all artifacts in the project
    artifacts = api.artifact_collections(project_name='Pendulum-v1', type_name='model')

    for artifact in artifacts:
        # If filtering by type, uncomment the following lines
        # if artifact.type != artifact_type:
        #     continue
        
        print(f"Deleting artifact: {artifact.name}")
        artifact.delete()


## NOT WORKING
# def delete_all_sweeps(project_name):
#     api = wandb.Api()
#     wandb.finish()
#     project = api.project(f"{api.default_entity}/{project_name}")
#     print(f"Deleting all sweeps in project: {project_name}")
#     sweeps = project.sweeps()
#     print(f"sweeps: {sweeps}")
#     for sweep in sweeps:
#         print(f"Deleting sweep: {sweep.id}")
#         sweep.delete()

## NOT WORKING
# def delete_artifacts(project_name, only_empty: bool = True):
#     api = wandb.Api()
#     artifacts = api.artifacts(f"{api.default_entity}/{project_name}/model", per_page=1000)

#     for artifact in artifacts:
#         # Fetch the artifact to get detailed info
#         artifact = artifact.fetch()
#         if only_empty:
#             if len(artifact.manifest.entries) == 0:
#                 print(f"Deleting empty artifact: {artifact.name}")
#                 artifact.delete()
#             else:
#                 print(f"Artifact {artifact.name} is not empty and will not be deleted.")
#         else:
#             print(f"Deleting artifact: {artifact.name}")
#             artifact.delete()


def custom_wandb_init(*args, **kwargs):
    """Initializes a W&B run and prints the run ID."""
    print("Initializing W&B run...")
    run = wandb.init(*args, **kwargs)
    print(f"Run initialized with ID: {run.id}")

    return run


def custom_wandb_finish():
    """Finishes a W&B run."""
    print("Finishing W&B run...")
    wandb.finish()
    print("Run finished.")


def flush_cache():
    """Flushes the W&B cache."""
    api = wandb.Api()
    api.flush()


def get_sweep_from_name(project, sweep_name):
    try:
        api = wandb.Api()
        sweeps = api.project(project).sweeps()
        
        # Filter sweeps by name to find the matching sweep ID
        for sweep in sweeps:
            if sweep.name == sweep_name:
                return sweep
        else:
            print(f"No sweep found with the name '{sweep_name}'.")
    except Exception as e:
        print(f"An error occurred in get_sweep_from_name: {e}")
        return None

def get_sweeps_from_name(project):
    try:
        api = wandb.Api()
        sweeps = api.project(project).sweeps()

        # Filter sweeps by name to find the matching sweep ID
        sweeps_list = [sweep.name for sweep in sweeps]
        return sweeps_list
    except Exception as e:
        print(f"An error occurred in get_sweeps_from_name: {e}")
        return []

def get_runs_from_sweep(project, sweep):
    try:
        api = wandb.Api()
        runs = api.runs(path=project, filters={"sweep": sweep.id})
        return runs
    except Exception as e:
        print(f"An error occurred in get_runs_from_sweep: {e}")

def get_metrics(project: str, sweep_name: str = None):
    try:
        api = wandb.Api()
        # check if sweep is specified and if so, get the runs from the sweep
        if sweep_name is not None:
            sweep = get_sweep_from_name(project, sweep_name)
            runs = get_runs_from_sweep(project, sweep)
        else:
            runs = api.runs(project)

        summary_list, config_list, name_list = [], [], []

        for run in runs:       
            # call ._json_dict to omit large files
            summary_list.append(run.summary._json_dict)

            # remove special values that start with _.
            config_list.append({k: v for k, v in run.config.items() if not k.startswith("_")})

            name_list.append(run.name)

        runs_df = pd.DataFrame(
            {"summary": summary_list, "config": config_list, "name": name_list}
        )

        return runs_df
    except Exception as e:
        print(f"An error occurred in get_metrics: {e}")
        return pd.DataFrame()

def format_metrics(data: pd.DataFrame) -> pd.DataFrame:
    try:
        logger.debug(f'format_metrics: data passed to format metrics: {data}')
        # Parse the 'config' column
        data['config'] = parse_dict_column(data['config'])
        logger.debug(f'format_metrics: data after parse dict column: {data}')

        # Extract hyperparameters and rewards from the dictionaries
        data['avg_reward'] = data['summary'].apply(lambda x: x.get('avg_reward') if isinstance(x, dict) else None)
        logger.debug(f'format_metrics: data after getting avg reward: {data}')
        data.to_csv(f"data.csv")

        # Filter out rows that do not have a 'config_dict' or 'avg reward'
        data_filtered = data.dropna(subset=['config', 'avg_reward'])
        logger.debug(f'format_metrics: data filtered: {data_filtered}')

        # Flatten the 'config_dict'
        hyperparams = data_filtered['config'].apply(lambda x: utils.flatten_dict(x))
        logger.debug(f'format_metrics: hyperparams after utils.flatten_dict: {hyperparams}')
        # parse hyperparameter names to remove dupe parts in name due to wandb config structure
        hyperparams = modify_keys(hyperparams)
        logger.debug(f'format_metrics: hyperparams after modify_keys: {hyperparams}')
        # and create a new DataFrame of hyperparameter values
        data_hyperparams = pd.DataFrame(hyperparams)
        logger.debug(f'format_metrics: data hyperparams columns after flatten config dict: {data_hyperparams.columns}')
        # Join the flattened hyperparameters with the avg reward column
        data_hyperparams = data_hyperparams.join(data_filtered['avg_reward'])
        logger.debug(f'format_metrics: data hyperparams after joining with data_filtered[avg_reward]: {data_hyperparams}')

        return data_hyperparams
    except Exception as e:
        logger.error(f"An error occurred in format_metrics: {e}", exc_info=True)
        return pd.DataFrame()

def calculate_co_occurrence_matrix(data: pd.DataFrame, hyperparameters: list, avg_reward_threshold: int, bins: int, z_scores: bool = False) -> pd.DataFrame:
    try:
        logger.debug(f"calculate_co_occurence_matrix: passed param data: {data}")
        logger.debug(f"calculate_co_occurence_matrix: passed param hyperparameters: {hyperparameters}")
        # create an empty dict to store the bin ranges of each hyperparameter
        bin_ranges = {}
        # drop all columns that arent in hyperparameters list
        data = data[hyperparameters + ['avg_reward']]
        logger.debug(f'calculate_co_occurence_matrix: data:{data}')
        # Filter the DataFrame based on the avg reward_threshold
        data_heatmap = data[data['avg_reward'] >= avg_reward_threshold]
        logger.debug(f'calculate_co_occurence_matrix: data_heatmap created')
        # For continuous variables, bin them into the specified number of bins
        for hp in data_heatmap.columns:
            if data_heatmap[hp].dtype == float and hp not in ['avg_reward']:
                data_heatmap[hp], bin_edges  = pd.cut(data_heatmap[hp], bins, labels=range(bins), retbins=True)
                bin_ranges[hp] = bin_edges
        logger.debug(f'calculate_co_occurence_matrix: bin ranges created')

        # One-hot encode categorical variables
        data_one_hot = pd.get_dummies(data_heatmap.drop('avg_reward', axis=1).astype('category'), dtype=np.int8)
        logger.debug(f'calculate_co_occurence_matrix: categorical data one-hot encoded')
        # Calculate co-occurrence matrix
        co_occurrence_matrix = np.dot(data_one_hot.T, data_one_hot)
        logger.debug(f'calculate_co_occurence_matrix: co-occurrence matrix calculated')

        # calculate z-scores if z_scores is true
        if z_scores:
            # Calculate the z-scores of the co-occurrence counts
            co_occurrence_matrix = zscore(co_occurrence_matrix, axis=None)
            logger.debug(f'calculate_co_occurence_matrix: zscore co-occurrence matrix calculated')
        # Create a DataFrame from the co-occurrence matrix for easier plotting
        co_occurrence_df = pd.DataFrame(co_occurrence_matrix, 
                                        index=data_one_hot.columns, 
                                        columns=data_one_hot.columns)
        logger.debug(f'calculate_co_occurence_matrix: co-occurrence dataframe created')
        co_occurrence_df.to_csv(f"co_occurrence_df.csv")
        logger.debug(f'calculate_co_occurence_matrix: co-occurrance dataframe saved to csv')

        return co_occurrence_df, bin_ranges
    except Exception as e:
        logger.error(f"An error occurred in calculate_co_occurrence_matrix: {e}", exc_info=True)
        return pd.DataFrame(), {}

def plot_co_occurrence_heatmap(co_occurrence_df: pd.DataFrame) -> go.Figure:
    try:
        # Create a layout
        layout = go.Layout(title='Co-occurrence Heatmap')

        # Create the figure
        fig = go.Figure(
            data=
                go.Heatmap(
                    z=co_occurrence_df.values,  # Heatmap values
                    x=co_occurrence_df.columns,  # X-axis categories
                    y=co_occurrence_df.index, # Y-axis categories
                ),
                layout=layout,
            )  

        return fig
    except Exception as e:
        print(f"An error occurred in plot_co_occurrence_heatmap: {e}")
        return go.Figure()

# Function to safely parse a Python dictionary string
def parse_dict_column(column):
    parsed_column = []
    for item in column:
        try:
            # Safely evaluate the dictionary string to a dictionary
            parsed_column.append(item)
        except ValueError:
            # In case of error, append a None or an empty dictionary
            parsed_column.append(None)
    return parsed_column

# Function to flatten the config dictionary and extract specified hyperparameters
def flatten_config(config_dict, hyperparameters):
    flat_config = {}
    for param in hyperparameters:
        # Extract the hyperparameter value from the nested dictionaries
        value = config_dict.get('DDPG', {}).get(param, None)
        # If a hyperparameter is not found, we try to get it from the top level (for env and model_type)
        if value is None:
            value = config_dict.get(param, None)
        flat_config[param] = value
    return flat_config

def fetch_sweep_hyperparameters_single_run(project_name, sweep_name):
    api = wandb.Api()
    sweep = get_sweep_from_name(project_name, sweep_name)
    runs = get_runs_from_sweep(project_name, sweep)
    hyperparameters = set()

    # Attempt to fetch a single run; if no runs, return empty list
    if runs is not None:
        run = runs.__getitem__(0)  # Take the first run from the sweep
        config = run.config
        flattened_config = utils.flatten_dict(config)
        for key in flattened_config.keys():
            if not key.startswith('_') and not key in ['wandb_version', 'state', 'env']:
                # print(f"key before:{key}")
                key = parse_parameter_name(key)
                # print(f"key after:{key}")
                hyperparameters.add(key)

    return list(hyperparameters)

def parse_parameter_name(parameter_string):
    # check if first 2 parts match the next 2 and if so, get rid of first 2 and return the rest
    parts = parameter_string.split('_') # update to split on _ without maxsplit
    if parts[:2] == parts[2:4]: # if the first string is repeated (model_type)
        return '_'.join(parts[2:]) # remove the repeated string and return the rest joined by '_'
                                            
    else:
        return parameter_string
    
def modify_keys(dicts):
    modified_dicts = []
    for d in dicts:
        new_dict = {}
        for key, value in d.items():
            new_key = parse_parameter_name(key)
            new_dict[new_key] = value
        modified_dicts.append(new_dict)
    return modified_dicts

def get_wandb_config_value(config, agent_type, model_type, param_name):
    """Retrieves value from wandb config"""
    #DEBUG
    print()
    return config[f"{agent_type}_{model_type}_{param_name}"]

def get_wandb_config_optimizer_params(config, agent_type, model_type, param_name):
    """Retrieves the parameters of the optimizer from the wandb config"""
    optimizer = get_wandb_config_value(config, agent_type, model_type, param_name)
    params = {}
    if optimizer == "Adam":
        params['weight_decay'] = \
            config[f"{agent_type}_{model_type}_{param_name}_{optimizer}_weight_decay"]
    
    elif optimizer == "Adagrad":
        params['weight_decay'] = \
            config[f"{agent_type}_{model_type}_{param_name}_{optimizer}_weight_decay"]
        params['lr_decay'] = \
            config[f"{agent_type}_{model_type}_{param_name}_{optimizer}_lr_decay"]
    
    elif optimizer == "RMSprop" or optimizer == "SGD":
        params['weight_decay'] = \
            config[f"{agent_type}_{model_type}_{param_name}_{optimizer}_weight_decay"]
        params['momentum'] = \
            config[f"{agent_type}_{model_type}_{param_name}_{optimizer}_momentum"]
    return params


```

---

