{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import umap\n",
    "import pynndescent\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Numba version:\", numba.__version__)\n",
    "print(\"UMAP version:\", umap.__version__)\n",
    "print(\"PyNNDescent version:\", pynndescent.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n",
      "[5ffba6a3ce68:01336] shmem: mmap: an error occurred while determining whether or not /tmp/ompi.5ffba6a3ce68.0/jf.0/580714496/shared_mem_cuda_pool.5ffba6a3ce68 could be created.\n",
      "[5ffba6a3ce68:01336] create_and_attach: unable to create shared memory BTL coordinating structure :: size 134217728 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ale_py\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, StochasticDiscretePolicy\n",
    "import cnn_models\n",
    "from rl_agents import PPO, DDPG, TD3, Reinforce, ActorCritic, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "from helper import Normalizer\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000)\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(100, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonhayes1987\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/src/app/wandb/run-20241023_194801-wrxv3pvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/wrxv3pvr' target=\"_blank\">train-143</a></strong> to <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/wrxv3pvr' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/wrxv3pvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering episode 0.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:0.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_0.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_0.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_0.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-99.85492624]\n",
      "episode: [0. 0. 0. 1.]; total steps: 1000; episodes scores: [          nan           nan           nan -101.03860488]; avg score: -101.03860488181127\n",
      "learning timestep: 2000\n",
      "Policy Loss: -0.09057190269231796\n",
      "Value Loss: 0.8735776543617249\n",
      "Entropy: -0.017818227410316467\n",
      "KL Divergence: 0.01843278668820858\n",
      "episode: [1. 5. 2. 6.]; total steps: 2000; episodes scores: [ -80.32775573 -114.28282535  -97.89942664 -102.67133195]; avg score: -98.79533491880647\n",
      "episode: [1. 5. 2. 8.]; total steps: 3000; episodes scores: [ -80.32775573 -114.28282535  -97.89942664 -112.5987752 ]; avg score: -101.2771957289266\n",
      "learning timestep: 4000\n",
      "Policy Loss: 0.031096884980797768\n",
      "Value Loss: 0.5866125822067261\n",
      "Entropy: -0.030824322253465652\n",
      "KL Divergence: 0.03859567642211914\n",
      "episode: [3. 6. 4. 9.]; total steps: 4000; episodes scores: [-117.39999403  -82.22523912 -112.37502366  -85.06645797]; avg score: -99.26667869271654\n",
      "episode: [4. 6. 5. 9.]; total steps: 5000; episodes scores: [-88.01032089 -82.22523912 -85.26030201 -85.06645797]; avg score: -85.14057999425088\n",
      "learning timestep: 6000\n",
      "Policy Loss: 0.009737580083310604\n",
      "Value Loss: 0.44736891984939575\n",
      "Entropy: -0.03891390562057495\n",
      "KL Divergence: 0.055378258228302\n",
      "episode: [ 4.  8.  7. 10.]; total steps: 6000; episodes scores: [ -88.01032089 -101.86467208 -118.72606031  -75.71606133]; avg score: -96.07927865222467\n",
      "episode: [ 9. 10.  9. 10.]; total steps: 7000; episodes scores: [ -99.31335072 -119.762068    -98.02412569  -75.71606133]; avg score: -98.20390143448705\n",
      "learning timestep: 8000\n",
      "Policy Loss: -0.046214353293180466\n",
      "Value Loss: 0.7062454223632812\n",
      "Entropy: -0.049842242151498795\n",
      "KL Divergence: 0.08205651491880417\n",
      "episode: [14. 10. 15. 11.]; total steps: 8000; episodes scores: [-104.77313092 -119.762068   -100.49025412  -76.52153477]; avg score: -100.38674695333603\n",
      "episode: [14. 13. 15. 14.]; total steps: 9000; episodes scores: [-104.77313092 -103.61334803 -100.49025412 -112.82104237]; avg score: -105.42444386310363\n",
      "learning timestep: 10000\n",
      "Policy Loss: -0.09827970713376999\n",
      "Value Loss: 0.4155082702636719\n",
      "Entropy: -0.07025844603776932\n",
      "KL Divergence: 0.06656265258789062\n",
      "episode: [16. 13. 17. 14.]; total steps: 10000; episodes scores: [ -99.64574054 -103.61334803 -100.48616955 -112.82104237]; avg score: -104.14157512569102\n",
      "episode: [19. 18. 20. 16.]; total steps: 11000; episodes scores: [ -99.8725705   -98.93496061 -103.19835746 -110.08911576]; avg score: -103.0237510819019\n",
      "Rendering episode 20.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:20.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_20.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_20.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_20.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.6433355]\n",
      "learning timestep: 12000\n",
      "Policy Loss: -0.0846797302365303\n",
      "Value Loss: 0.544185221195221\n",
      "Entropy: -0.09277000278234482\n",
      "KL Divergence: 0.09572620689868927\n",
      "episode: [22. 18. 20. 16.]; total steps: 12000; episodes scores: [ -98.77749937  -98.93496061 -103.19835746 -110.08911576]; avg score: -102.74998330133394\n",
      "episode: [25. 19. 22. 18.]; total steps: 13000; episodes scores: [-100.47691421  -75.21154965  -99.50660026  -99.71134226]; avg score: -93.7266015952051\n",
      "learning timestep: 14000\n",
      "Policy Loss: 0.09742354601621628\n",
      "Value Loss: 0.12918342649936676\n",
      "Entropy: -0.11107911169528961\n",
      "KL Divergence: 0.1170923262834549\n",
      "episode: [25. 21. 22. 20.]; total steps: 14000; episodes scores: [-100.47691421  -98.48433425  -99.50660026 -111.32540812]; avg score: -102.44831421038151\n",
      "episode: [27. 21. 27. 29.]; total steps: 15000; episodes scores: [-100.29194199  -98.48433425  -98.70816568  -98.83891411]; avg score: -99.08083900577249\n",
      "learning timestep: 16000\n",
      "Policy Loss: 0.13585157692432404\n",
      "Value Loss: 0.4912380576133728\n",
      "Entropy: -0.13453397154808044\n",
      "KL Divergence: 0.1628701090812683\n",
      "episode: [27. 29. 27. 29.]; total steps: 16000; episodes scores: [-100.29194199  -97.73490669  -98.70816568  -98.83891411]; avg score: -98.89348211504938\n",
      "episode: [30. 29. 29. 30.]; total steps: 17000; episodes scores: [-101.44433251  -97.73490669 -101.55847088  -77.71220205]; avg score: -94.61247803296165\n",
      "learning timestep: 18000\n",
      "Policy Loss: -0.072748102247715\n",
      "Value Loss: 0.34582817554473877\n",
      "Entropy: -0.1908169686794281\n",
      "KL Divergence: 0.22133001685142517\n",
      "episode: [31. 31. 30. 31.]; total steps: 18000; episodes scores: [ -71.71248664 -110.08275912  -70.21354582  -75.45535412]; avg score: -81.86603642572055\n",
      "episode: [35. 31. 30. 31.]; total steps: 19000; episodes scores: [ -99.06210579 -110.08275912  -70.21354582  -75.45535412]; avg score: -88.70344121302446\n",
      "learning timestep: 20000\n",
      "Policy Loss: 0.304129034280777\n",
      "Value Loss: 0.20039188861846924\n",
      "Entropy: -0.19844791293144226\n",
      "KL Divergence: 0.27480635046958923\n",
      "episode: [36. 34. 31. 35.]; total steps: 20000; episodes scores: [-74.73483822 -98.43626386 -68.53511367 -99.26491494]; avg score: -85.24278266889897\n",
      "episode: [36. 35. 32. 35.]; total steps: 21000; episodes scores: [-74.73483822 -70.65138613 -74.88593568 -99.26491494]; avg score: -79.88426874057747\n",
      "Rendering episode 40.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:40.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_40.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_40.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_40.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.73639117]\n",
      "learning timestep: 22000\n",
      "Policy Loss: -0.06146393343806267\n",
      "Value Loss: 0.5491770505905151\n",
      "Entropy: -0.2332465499639511\n",
      "KL Divergence: 0.303682804107666\n",
      "episode: [41. 35. 39. 36.]; total steps: 22000; episodes scores: [-103.076917    -70.65138613  -99.0159316   -69.07000928]; avg score: -85.45356100114452\n",
      "episode: [41. 40. 39. 37.]; total steps: 23000; episodes scores: [-103.076917    -99.43222064  -99.0159316   -68.72426229]; avg score: -92.56233288343375\n",
      "learning timestep: 24000\n",
      "Policy Loss: -0.03518760949373245\n",
      "Value Loss: 0.6722046732902527\n",
      "Entropy: -0.2695491909980774\n",
      "KL Divergence: 0.2790902256965637\n",
      "episode: [45. 42. 46. 41.]; total steps: 24000; episodes scores: [-103.26025268 -102.03156492 -101.68352967  -98.51990562]; avg score: -101.37381322263506\n",
      "episode: [45. 46. 46. 44.]; total steps: 25000; episodes scores: [-103.26025268 -100.80880219 -101.68352967  -99.05856622]; avg score: -101.20278768670543\n",
      "learning timestep: 26000\n",
      "Policy Loss: 0.17605291306972504\n",
      "Value Loss: 0.773408055305481\n",
      "Entropy: -0.2392391413450241\n",
      "KL Divergence: 0.17319755256175995\n",
      "episode: [51. 46. 47. 48.]; total steps: 26000; episodes scores: [-110.28095906 -100.80880219  -65.06134046 -115.87933587]; avg score: -98.00760939431063\n",
      "episode: [51. 48. 49. 49.]; total steps: 27000; episodes scores: [-110.28095906 -116.60828295  -99.96419572  -72.72837355]; avg score: -99.8954528194697\n",
      "learning timestep: 28000\n",
      "Policy Loss: -0.009733596816658974\n",
      "Value Loss: 0.4189784526824951\n",
      "Entropy: -0.31026238203048706\n",
      "KL Divergence: 0.417939156293869\n",
      "episode: [54. 48. 55. 53.]; total steps: 28000; episodes scores: [ -98.68581038 -116.60828295  -98.64793438  -98.85141133]; avg score: -103.1983597597503\n",
      "episode: [55. 54. 55. 54.]; total steps: 29000; episodes scores: [-70.77157038 -97.95977691 -98.64793438 -68.13267194]; avg score: -83.87798840127368\n",
      "learning timestep: 30000\n",
      "Policy Loss: 0.0502881184220314\n",
      "Value Loss: 0.08931265771389008\n",
      "Entropy: -0.40054357051849365\n",
      "KL Divergence: 0.48884695768356323\n",
      "episode: [55. 54. 56. 54.]; total steps: 30000; episodes scores: [-70.77157038 -97.95977691 -73.00244249 -68.13267194]; avg score: -77.46661542970186\n",
      "Rendering episode 60.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:60.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_60.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_60.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_60.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.8959374]\n",
      "episode: [61. 56. 60. 60.]; total steps: 31000; episodes scores: [ -97.70474976 -101.36556225 -101.18869899  -98.63153305]; avg score: -99.72263601405608\n",
      "learning timestep: 32000\n",
      "Policy Loss: 0.206534281373024\n",
      "Value Loss: 0.8253991007804871\n",
      "Entropy: -0.3275437355041504\n",
      "KL Divergence: 0.44670218229293823\n",
      "episode: [73. 59. 63. 67.]; total steps: 32000; episodes scores: [ -99.76154661  -98.26483013 -121.51909062 -120.84675714]; avg score: -110.098056124735\n",
      "episode: [76. 72. 64. 67.]; total steps: 33000; episodes scores: [-100.01585605 -101.46574274  -65.58283186 -120.84675714]; avg score: -96.97779694990908\n",
      "learning timestep: 34000\n",
      "Policy Loss: -0.06292789429426193\n",
      "Value Loss: 0.21908555924892426\n",
      "Entropy: -0.41284775733947754\n",
      "KL Divergence: 0.48999178409576416\n",
      "episode: [79. 74. 65. 69.]; total steps: 34000; episodes scores: [-100.51854348 -101.58319352 -102.60548877  -99.96072386]; avg score: -101.16698740661828\n",
      "Rendering episode 80.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:80.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_80.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_80.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_80.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-101.80572007]\n",
      "episode: [91. 79. 68. 73.]; total steps: 35000; episodes scores: [-101.27822294 -100.64195336 -101.83327306  -97.32836401]; avg score: -100.27045334448789\n",
      "learning timestep: 36000\n",
      "Policy Loss: -0.16398070752620697\n",
      "Value Loss: 0.7818644046783447\n",
      "Entropy: -0.34984922409057617\n",
      "KL Divergence: 0.48864907026290894\n",
      "episode: [91. 86. 68. 81.]; total steps: 36000; episodes scores: [-101.27822294  -99.10845415 -101.83327306  -99.1357926 ]; avg score: -100.33893568917456\n",
      "episode: [98. 86. 76. 81.]; total steps: 37000; episodes scores: [-101.68668452  -99.10845415 -101.16859463  -99.1357926 ]; avg score: -100.27488147566172\n",
      "Rendering episode 100.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:100.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_100.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_100.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.24571092]\n",
      "learning timestep: 38000\n",
      "Policy Loss: -0.006258991546928883\n",
      "Value Loss: 1.0293112993240356\n",
      "Entropy: -0.3691323399543762\n",
      "KL Divergence: 0.49678343534469604\n",
      "episode: [112.  93.  90.  91.]; total steps: 38000; episodes scores: [-100.1737006   -97.88105818 -119.00414115 -102.3305255 ]; avg score: -104.84735635737066\n",
      "episode: [112.  93.  94.  97.]; total steps: 39000; episodes scores: [-100.1737006   -97.88105818 -101.58725874  -99.47621308]; avg score: -99.77955764703125\n",
      "learning timestep: 40000\n",
      "Policy Loss: -0.09746932983398438\n",
      "Value Loss: 0.8506950736045837\n",
      "Entropy: -0.49663203954696655\n",
      "KL Divergence: 0.7422729730606079\n",
      "episode: [117. 100.  97.  98.]; total steps: 40000; episodes scores: [-118.23184692  -98.13095382  -99.69115083  -65.52005155]; avg score: -95.39350078068396\n",
      "Rendering episode 120.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:120.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_120.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_120.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_120.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.56075583]\n",
      "episode: [131. 100. 110. 112.]; total steps: 41000; episodes scores: [-99.7796337  -98.13095382 -98.53050658 -99.25631814]; avg score: -98.92435305619381\n",
      "Rendering episode 140.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:140.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_140.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_140.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_140.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.56075583]\n",
      "learning timestep: 42000\n",
      "Policy Loss: -0.010150564834475517\n",
      "Value Loss: 0.7862306833267212\n",
      "Entropy: -0.3787301480770111\n",
      "KL Divergence: 0.538067102432251\n",
      "episode: [140. 106. 110. 126.]; total steps: 42000; episodes scores: [ -99.25775404 -102.1487068   -98.53050658 -102.08902697]; avg score: -100.50649859629371\n",
      "episode: [140. 106. 116. 139.]; total steps: 43000; episodes scores: [ -99.25775404 -102.1487068   -99.8019578   -97.10548523]; avg score: -99.57847596673544\n",
      "learning timestep: 44000\n",
      "Policy Loss: -0.014195664785802364\n",
      "Value Loss: 0.8111121654510498\n",
      "Entropy: -0.4361358880996704\n",
      "KL Divergence: 0.6021593809127808\n",
      "episode: [151. 116. 118. 153.]; total steps: 44000; episodes scores: [ -97.75491666  -99.82591216 -117.16668114  -97.74066312]; avg score: -103.12204326828802\n",
      "Rendering episode 160.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:160.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_160.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_160.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_160.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-101.3951093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [165. 130. 122. 168.]; total steps: 45000; episodes scores: [ -95.25207575 -100.57278474 -100.86177278 -101.01941314]; avg score: -99.42651160266095\n",
      "learning timestep: 46000\n",
      "Policy Loss: 0.255938321352005\n",
      "Value Loss: 0.830669641494751\n",
      "Entropy: -0.4136633276939392\n",
      "KL Divergence: 0.6997597217559814\n",
      "episode: [176. 145. 135. 181.]; total steps: 46000; episodes scores: [ -98.30949155  -98.50426016 -118.47105396  -99.13527117]; avg score: -103.60501920934365\n",
      "episode: [176. 157. 149. 195.]; total steps: 47000; episodes scores: [-98.30949155 -96.2667027  -98.87847439 -99.53581048]; avg score: -98.24761977851686\n",
      "Rendering episode 180.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:180.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_180.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_180.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_180.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.60072176]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 48000\n",
      "Policy Loss: 0.024178197607398033\n",
      "Value Loss: 1.2553355693817139\n",
      "Entropy: -0.4060301184654236\n",
      "KL Divergence: 0.6466089487075806\n",
      "episode: [187. 159. 152. 209.]; total steps: 48000; episodes scores: [ -97.70066652  -99.22768939 -102.1549143   -99.61781964]; avg score: -99.67527246074347\n",
      "episode: [193. 162. 155. 219.]; total steps: 49000; episodes scores: [ -98.7629263  -100.67606603  -97.65300382  -98.76299801]; avg score: -98.96374854109932\n",
      "learning timestep: 50000\n",
      "Policy Loss: -0.10868334770202637\n",
      "Value Loss: 0.8332064747810364\n",
      "Entropy: -0.507004976272583\n",
      "KL Divergence: 0.7302166819572449\n",
      "episode: [193. 174. 169. 231.]; total steps: 50000; episodes scores: [ -98.7629263  -100.04758049  -98.36158696  -97.74904914]; avg score: -98.730285725633\n",
      "Rendering episode 200.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:200.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_200.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_200.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.7509892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [206. 176. 184. 247.]; total steps: 51000; episodes scores: [-100.38306923  -98.35903515  -98.50268616  -99.92473429]; avg score: -99.29238120981321\n",
      "Rendering episode 220.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:220.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_220.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_220.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_220.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.7509892]\n",
      "learning timestep: 52000\n",
      "Policy Loss: -0.04783152788877487\n",
      "Value Loss: 1.0836371183395386\n",
      "Entropy: -0.5331117510795593\n",
      "KL Divergence: 0.7609465718269348\n",
      "episode: [220. 181. 190. 253.]; total steps: 52000; episodes scores: [-100.26299334 -100.89722552  -99.11853801 -100.31437333]; avg score: -100.14828255087603\n",
      "episode: [233. 194. 191. 254.]; total steps: 53000; episodes scores: [ -99.75987603 -102.52766382  -55.04097809  -60.32194163]; avg score: -79.41261488973738\n",
      "Rendering episode 240.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:240.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_240.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_240.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_240.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-98.07470153]\n",
      "learning timestep: 54000\n",
      "Policy Loss: -0.12846939265727997\n",
      "Value Loss: 0.8972062468528748\n",
      "Entropy: -0.4789048433303833\n",
      "KL Divergence: 0.7139427065849304\n",
      "episode: [249. 196. 205. 268.]; total steps: 54000; episodes scores: [-101.33168741  -96.62330383  -99.55860388  -99.97292202]; avg score: -99.37162928453569\n",
      "episode: [254. 201. 219. 282.]; total steps: 55000; episodes scores: [-98.3493436  -99.08252183 -99.93710978 -99.53014539]; avg score: -99.22478015066156\n",
      "learning timestep: 56000\n",
      "Policy Loss: -0.10200221836566925\n",
      "Value Loss: 0.7011415958404541\n",
      "Entropy: -0.5851373076438904\n",
      "KL Divergence: 0.8165121078491211\n",
      "episode: [256. 216. 233. 297.]; total steps: 56000; episodes scores: [-98.29787187 -98.92835907 -99.4304114  -99.2767305 ]; avg score: -98.9833432103498\n",
      "Rendering episode 260.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:260.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_260.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_260.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_260.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.63647947]\n",
      "episode: [269. 231. 248. 310.]; total steps: 57000; episodes scores: [ -96.21562279  -97.81522627  -99.70741115 -112.74243804]; avg score: -101.62017456065288\n",
      "Rendering episode 280.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:280.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_280.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_280.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_280.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.63647947]\n",
      "learning timestep: 58000\n",
      "Policy Loss: 0.12792494893074036\n",
      "Value Loss: 0.9494129419326782\n",
      "Entropy: -0.5974745154380798\n",
      "KL Divergence: 0.796268880367279\n",
      "episode: [284. 246. 263. 314.]; total steps: 58000; episodes scores: [ -98.18714502  -98.77352565 -100.12947068 -100.77024923]; avg score: -99.46509764682486\n",
      "episode: [290. 260. 268. 317.]; total steps: 59000; episodes scores: [ -97.85465172  -99.68987541 -100.62855595  -97.15555658]; avg score: -98.83215991444125\n",
      "learning timestep: 60000\n",
      "Policy Loss: 0.11166303604841232\n",
      "Value Loss: 1.7440390586853027\n",
      "Entropy: -0.6074353456497192\n",
      "KL Divergence: 0.6759482026100159\n",
      "episode: [290. 276. 269. 320.]; total steps: 60000; episodes scores: [ -97.85465172  -97.39324659  -58.27197389 -100.83293749]; avg score: -88.58820242140024\n",
      "Rendering episode 300.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:300.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_300.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_300.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.82259508]\n",
      "episode: [305. 291. 283. 325.]; total steps: 61000; episodes scores: [-100.59366199  -97.77552327  -99.24185747 -101.98681492]; avg score: -99.89946441138193\n",
      "Rendering episode 320.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:320.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_320.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_320.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_320.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.82259508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 62000\n",
      "Policy Loss: -0.06549153476953506\n",
      "Value Loss: 1.565735101699829\n",
      "Entropy: -0.6927109956741333\n",
      "KL Divergence: 0.8290867209434509\n",
      "episode: [320. 307. 297. 340.]; total steps: 62000; episodes scores: [ -99.60871769  -97.64440285  -99.16263277 -101.25673452]; avg score: -99.41812195946216\n",
      "episode: [335. 322. 313. 355.]; total steps: 63000; episodes scores: [ -96.55195381 -100.54608497 -100.91627541  -99.61377436]; avg score: -99.40702213474628\n",
      "Rendering episode 340.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:340.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_340.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_340.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_340.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-101.38803741]\n",
      "learning timestep: 64000\n",
      "Policy Loss: 0.016720805317163467\n",
      "Value Loss: 1.2367851734161377\n",
      "Entropy: -0.6385188698768616\n",
      "KL Divergence: 1.0850565433502197\n",
      "episode: [352. 335. 326. 368.]; total steps: 64000; episodes scores: [-114.18095343  -99.60369006 -100.60785719 -100.62352055]; avg score: -103.75400530716715\n",
      "episode: [359. 351. 341. 368.]; total steps: 65000; episodes scores: [ -97.30481394 -100.56039038 -101.37734394 -100.62352055]; avg score: -99.96651720348109\n",
      "learning timestep: 66000\n",
      "Policy Loss: -0.01709786430001259\n",
      "Value Loss: 0.8753975629806519\n",
      "Entropy: -0.7194682955741882\n",
      "KL Divergence: 1.0036393404006958\n",
      "episode: [359. 367. 356. 377.]; total steps: 66000; episodes scores: [-97.30481394 -97.74106923 -97.74454368 -99.97812376]; avg score: -98.19213765313708\n",
      "Rendering episode 360.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:360.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_360.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_360.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_360.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.37449731]\n",
      "episode: [374. 374. 357. 387.]; total steps: 67000; episodes scores: [-100.00743928  -99.49535765  -97.72507503  -95.34722164]; avg score: -98.14377340031011\n",
      "Rendering episode 380.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:380.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_380.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_380.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_380.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.37449731]\n",
      "learning timestep: 68000\n",
      "Policy Loss: 0.11178010702133179\n",
      "Value Loss: 1.1275432109832764\n",
      "Entropy: -0.6340888738632202\n",
      "KL Divergence: 1.0688974857330322\n",
      "episode: [388. 384. 363. 387.]; total steps: 68000; episodes scores: [-99.39558407 -96.347625   -97.54763052 -95.34722164]; avg score: -97.15951530856748\n",
      "Rendering episode 400.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:400.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_400.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_400.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-101.34649756]\n",
      "episode: [403. 398. 376. 399.]; total steps: 69000; episodes scores: [-97.68446346 -99.4052258  -99.72743731 -97.47759463]; avg score: -98.57368030061758\n",
      "learning timestep: 70000\n",
      "Policy Loss: -0.12012387812137604\n",
      "Value Loss: 1.2273722887039185\n",
      "Entropy: -0.6713162064552307\n",
      "KL Divergence: 0.8397504091262817\n",
      "episode: [419. 414. 392. 416.]; total steps: 70000; episodes scores: [ -98.43163223 -101.45238683  -98.61881811  -97.99502569]; avg score: -99.12446571502369\n",
      "Rendering episode 420.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:420.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_420.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_420.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_420.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.78012271]\n",
      "episode: [434. 414. 409. 432.]; total steps: 71000; episodes scores: [ -99.28171332 -101.45238683  -99.35717013  -98.6417877 ]; avg score: -99.68326449123785\n",
      "Rendering episode 440.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:440.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_440.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_440.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_440.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.78012271]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 72000\n",
      "Policy Loss: 0.035673219710588455\n",
      "Value Loss: 0.3437882661819458\n",
      "Entropy: -0.6751226186752319\n",
      "KL Divergence: 0.9053938388824463\n",
      "episode: [450. 420. 409. 446.]; total steps: 72000; episodes scores: [ -99.46581291 -100.04467273  -99.35717013  -99.52357938]; avg score: -99.59780878749504\n",
      "Rendering episode 460.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:460.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_460.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_460.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_460.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.20795843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [466. 436. 417. 462.]; total steps: 73000; episodes scores: [-100.30918346 -100.48334763  -99.76531928 -101.83240054]; avg score: -100.59756272477611\n",
      "Rendering episode 480.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:480.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_480.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_480.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_480.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.20795843]\n",
      "learning timestep: 74000\n",
      "Policy Loss: 0.034034308046102524\n",
      "Value Loss: 0.7217748165130615\n",
      "Entropy: -0.8033021688461304\n",
      "KL Divergence: 0.7603370547294617\n",
      "episode: [483. 453. 435. 478.]; total steps: 74000; episodes scores: [ -98.79423163  -99.71793439 -100.38309424  -98.17159593]; avg score: -99.26671404901768\n",
      "Rendering episode 500.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:500.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_500.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_500.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.08097413]\n",
      "episode: [500. 469. 452. 494.]; total steps: 75000; episodes scores: [-100.44357283  -97.47851217 -100.83945542 -100.06822802]; avg score: -99.70744210788655\n",
      "learning timestep: 76000\n",
      "Policy Loss: -0.13444474339485168\n",
      "Value Loss: 0.47512295842170715\n",
      "Entropy: -0.7750436067581177\n",
      "KL Divergence: 0.8874062299728394\n",
      "episode: [517. 484. 468. 508.]; total steps: 76000; episodes scores: [ -99.49327594  -99.76959715 -101.34077992  -98.6699647 ]; avg score: -99.81840442540945\n",
      "Rendering episode 520.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:520.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_520.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_520.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_520.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.60033967]\n",
      "episode: [533. 501. 483. 525.]; total steps: 77000; episodes scores: [ -98.20918691 -101.75849036  -95.16157551  -98.82919176]; avg score: -98.48961113219382\n",
      "Rendering episode 540.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:540.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_540.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_540.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_540.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-100.60033967]\n",
      "learning timestep: 78000\n",
      "Policy Loss: -0.15826795995235443\n",
      "Value Loss: 0.6224267482757568\n",
      "Entropy: -0.7968094348907471\n",
      "KL Divergence: 0.948544979095459\n",
      "episode: [547. 516. 488. 541.]; total steps: 78000; episodes scores: [-98.53574782 -98.74776462 -95.07063356 -98.86303789]; avg score: -97.80429597129363\n",
      "Rendering episode 560.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:560.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_560.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_560.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_560.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-99.62223889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [563. 534. 490. 554.]; total steps: 79000; episodes scores: [-97.39654094 -98.43387608 -97.35378458 -97.12254901]; avg score: -97.57668765138571\n",
      "Rendering episode 580.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:580.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_580.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_580.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_580.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-99.62223889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 80000\n",
      "Policy Loss: -0.12849266827106476\n",
      "Value Loss: 0.9428891539573669\n",
      "Entropy: -0.77274489402771\n",
      "KL Divergence: 1.0987989902496338\n",
      "episode: [580. 551. 508. 554.]; total steps: 80000; episodes scores: [-98.64905453 -99.05439919 -99.96921983 -97.12254901]; avg score: -98.69880563777296\n",
      "episode: [595. 566. 524. 564.]; total steps: 81000; episodes scores: [-98.36057085 -96.6786411  -99.03292781 -98.78943019]; avg score: -98.2153924842955\n",
      "Rendering episode 600.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:600.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_600.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_600.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.83939526]\n",
      "learning timestep: 82000\n",
      "Policy Loss: -0.1891508847475052\n",
      "Value Loss: 0.46155351400375366\n",
      "Entropy: -0.7556453347206116\n",
      "KL Divergence: 0.7523630261421204\n",
      "episode: [611. 584. 540. 579.]; total steps: 82000; episodes scores: [ -97.13601609 -100.26678652  -98.24931073  -94.38970764]; avg score: -97.51045524468505\n",
      "Rendering episode 620.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:620.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_620.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_620.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_620.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.43787986]\n",
      "episode: [627. 602. 557. 596.]; total steps: 83000; episodes scores: [-98.64205475 -98.87351372 -99.27207768 -98.00658655]; avg score: -98.6985581766412\n",
      "Rendering episode 640.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:640.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_640.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_640.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_640.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.43787986]\n",
      "learning timestep: 84000\n",
      "Policy Loss: 0.03358345851302147\n",
      "Value Loss: 0.3743833303451538\n",
      "Entropy: -0.8779285550117493\n",
      "KL Divergence: 0.9766037464141846\n",
      "episode: [642. 615. 574. 611.]; total steps: 84000; episodes scores: [-98.9569698  -98.47388543 -97.10998468 -97.02983831]; avg score: -97.89266955428694\n",
      "episode: [658. 631. 590. 627.]; total steps: 85000; episodes scores: [-98.16360156 -98.06991558 -98.74767193 -98.51788706]; avg score: -98.37476903118116\n",
      "Rendering episode 660.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:660.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_660.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_660.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_660.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.55059938]\n",
      "learning timestep: 86000\n",
      "Policy Loss: -0.12674473226070404\n",
      "Value Loss: 0.4632703959941864\n",
      "Entropy: -0.8300106525421143\n",
      "KL Divergence: 0.8831361532211304\n",
      "episode: [674. 646. 606. 642.]; total steps: 86000; episodes scores: [ -97.1483432   -97.95156368  -99.50463255 -100.05688666]; avg score: -98.66535652335841\n",
      "Rendering episode 680.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:680.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_680.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_680.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_680.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.98601508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [690. 663. 623. 659.]; total steps: 87000; episodes scores: [-99.10974688 -98.55088942 -99.49808454 -95.6529142 ]; avg score: -98.20290875922004\n",
      "Rendering episode 700.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:700.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_700.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_700.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.98601508]\n",
      "learning timestep: 88000\n",
      "Policy Loss: 0.09150655567646027\n",
      "Value Loss: 0.3213774561882019\n",
      "Entropy: -1.023683786392212\n",
      "KL Divergence: 1.3418463468551636\n",
      "episode: [706. 667. 639. 677.]; total steps: 88000; episodes scores: [-98.58925942 -99.5273179  -99.05137104 -98.72944703]; avg score: -98.97434884607559\n",
      "Rendering episode 720.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:720.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_720.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_720.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_720.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.2715636]\n",
      "episode: [722. 671. 655. 693.]; total steps: 89000; episodes scores: [ -97.3765722   -96.03374975 -100.26478616  -99.24232461]; avg score: -98.22935817876163\n",
      "learning timestep: 90000\n",
      "Policy Loss: 0.04853493720293045\n",
      "Value Loss: 0.4091383218765259\n",
      "Entropy: -0.9250882863998413\n",
      "KL Divergence: 1.1622803211212158\n",
      "episode: [737. 686. 669. 709.]; total steps: 90000; episodes scores: [-95.58229359 -98.72837424 -93.61000647 -98.13129246]; avg score: -96.51299168862418\n",
      "episode: [737. 701. 683. 725.]; total steps: 91000; episodes scores: [-95.58229359 -99.99316219 -97.63372723 -97.97951981]; avg score: -97.79717570454316\n",
      "Rendering episode 740.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:740.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_740.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_740.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_740.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-96.97316005]\n",
      "learning timestep: 92000\n",
      "Policy Loss: -0.05542585998773575\n",
      "Value Loss: 0.3925161361694336\n",
      "Entropy: -1.0837022066116333\n",
      "KL Divergence: 1.4181467294692993\n",
      "episode: [744. 717. 699. 741.]; total steps: 92000; episodes scores: [-98.66242314 -98.96041517 -98.08780934 -96.82957597]; avg score: -98.1350559053972\n",
      "episode: [745. 733. 714. 757.]; total steps: 93000; episodes scores: [-97.42094544 -97.97685467 -98.93250384 -95.90394078]; avg score: -97.55856118058568\n",
      "learning timestep: 94000\n",
      "Policy Loss: -0.17499864101409912\n",
      "Value Loss: 0.23841024935245514\n",
      "Entropy: -1.0857057571411133\n",
      "KL Divergence: 1.3609949350357056\n",
      "episode: [752. 748. 730. 773.]; total steps: 94000; episodes scores: [-97.98918016 -96.67257915 -96.72122796 -98.25704691]; avg score: -97.41000854564645\n",
      "Rendering episode 760.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:760.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_760.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_760.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_760.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.02259817]\n",
      "episode: [767. 762. 745. 789.]; total steps: 95000; episodes scores: [-96.75902391 -97.71812874 -99.11150014 -98.90132342]; avg score: -98.1224940507474\n",
      "Rendering episode 780.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:780.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_780.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_780.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_780.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.02259817]\n",
      "learning timestep: 96000\n",
      "Policy Loss: -0.17875491082668304\n",
      "Value Loss: 0.3529725968837738\n",
      "Entropy: -0.9158082008361816\n",
      "KL Divergence: 1.271852731704712\n",
      "episode: [781. 778. 760. 805.]; total steps: 96000; episodes scores: [-94.99932726 -97.53854706 -96.4992644  -97.30347139]; avg score: -96.58515252662123\n",
      "episode: [797. 793. 775. 821.]; total steps: 97000; episodes scores: [-98.96026299 -98.03642701 -96.74400846 -97.97352761]; avg score: -97.92855651689135\n",
      "Rendering episode 800.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:800.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_800.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_800.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-96.75636189]\n",
      "learning timestep: 98000\n",
      "Policy Loss: 0.16041429340839386\n",
      "Value Loss: 0.4734228253364563\n",
      "Entropy: -1.0061044692993164\n",
      "KL Divergence: 1.517354965209961\n",
      "episode: [812. 807. 791. 836.]; total steps: 98000; episodes scores: [-97.77020754 -97.64935439 -98.19261566 -99.34846034]; avg score: -98.24015948280987\n",
      "Rendering episode 820.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:820.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_820.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_820.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_820.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.71529719]\n",
      "episode: [827. 823. 805. 850.]; total steps: 99000; episodes scores: [-99.12790338 -98.05283845 -97.80919815 -97.11608967]; avg score: -98.02650741089539\n",
      "learning timestep: 100000\n",
      "Policy Loss: -0.08612256497144699\n",
      "Value Loss: 0.2965472340583801\n",
      "Entropy: -1.0740771293640137\n",
      "KL Divergence: 1.5980827808380127\n",
      "episode: [836. 839. 818. 865.]; total steps: 100000; episodes scores: [-97.47220107 -97.17829452 -96.5037172  -95.96091644]; avg score: -96.7787823063261\n",
      "episode: [836. 853. 818. 881.]; total steps: 101000; episodes scores: [-97.47220107 -97.49332371 -96.5037172  -94.98340043]; avg score: -96.6131606005809\n",
      "Rendering episode 840.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:840.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_840.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_840.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_840.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.06352688]\n",
      "learning timestep: 102000\n",
      "Policy Loss: 0.17194603383541107\n",
      "Value Loss: 0.172672837972641\n",
      "Entropy: -1.1125617027282715\n",
      "KL Divergence: 1.7180230617523193\n",
      "episode: [850. 867. 827. 895.]; total steps: 102000; episodes scores: [-98.71638283 -96.62951839 -97.73060545 -94.28093812]; avg score: -96.83936119750467\n",
      "episode: [850. 883. 842. 911.]; total steps: 103000; episodes scores: [-98.71638283 -98.6725789  -99.23933609 -98.54415783]; avg score: -98.79311391222771\n",
      "learning timestep: 104000\n",
      "Policy Loss: 0.1619456559419632\n",
      "Value Loss: 0.7394609451293945\n",
      "Entropy: -1.1859396696090698\n",
      "KL Divergence: 1.5947152376174927\n",
      "episode: [857. 899. 858. 926.]; total steps: 104000; episodes scores: [-98.54115938 -94.0388478  -94.89577168 -96.013755  ]; avg score: -95.87238346211177\n",
      "Rendering episode 860.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:860.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_860.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_860.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_860.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.29424024]\n",
      "episode: [871. 914. 872. 931.]; total steps: 105000; episodes scores: [-95.47931441 -96.7568872  -95.99633431 -95.77656093]; avg score: -96.00227421228416\n",
      "Rendering episode 880.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:880.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_880.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_880.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_880.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.29424024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 106000\n",
      "Policy Loss: -0.02312801405787468\n",
      "Value Loss: 0.6654974222183228\n",
      "Entropy: -1.1630983352661133\n",
      "KL Divergence: 1.599853277206421\n",
      "episode: [886. 928. 887. 933.]; total steps: 106000; episodes scores: [-96.37750435 -94.83610672 -97.96179358 -94.6927102 ]; avg score: -95.96702871046682\n",
      "Rendering episode 900.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:900.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_900.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_900.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.01067005]\n",
      "episode: [900. 944. 902. 948.]; total steps: 107000; episodes scores: [-98.42481794 -97.46717736 -97.35090415 -95.06731032]; avg score: -97.07755244382193\n",
      "learning timestep: 108000\n",
      "Policy Loss: -0.1184290200471878\n",
      "Value Loss: 0.3581853210926056\n",
      "Entropy: -1.0353894233703613\n",
      "KL Divergence: 1.4346379041671753\n",
      "episode: [914. 959. 917. 963.]; total steps: 108000; episodes scores: [-94.21083056 -97.91259337 -97.05750417 -96.03528397]; avg score: -96.30405301636644\n",
      "Rendering episode 920.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:920.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_920.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_920.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_920.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.35375502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [928. 973. 931. 978.]; total steps: 109000; episodes scores: [-97.76666262 -99.05320992 -96.93258863 -94.28640901]; avg score: -97.00971754448906\n",
      "Rendering episode 940.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:940.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_940.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_940.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_940.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-97.35375502]\n",
      "learning timestep: 110000\n",
      "Policy Loss: -0.2190115749835968\n",
      "Value Loss: 0.36345791816711426\n",
      "Entropy: -1.1858625411987305\n",
      "KL Divergence: 1.551995038986206\n",
      "episode: [942. 989. 946. 988.]; total steps: 110000; episodes scores: [ -97.2186777  -100.21984788  -97.52433563  -97.19900391]; avg score: -98.04046627725894\n",
      "episode: [ 947. 1002.  961.  988.]; total steps: 111000; episodes scores: [-95.3508882  -97.78399762 -97.17132211 -97.19900391]; avg score: -96.87630296038309\n",
      "learning timestep: 112000\n",
      "Policy Loss: 0.01311310101300478\n",
      "Value Loss: 0.10841189324855804\n",
      "Entropy: -1.4512474536895752\n",
      "KL Divergence: 1.9289895296096802\n",
      "episode: [ 949. 1007.  964. 1000.]; total steps: 112000; episodes scores: [-96.72793354 -96.97927021 -96.08963248 -97.02860346]; avg score: -96.70635992242643\n",
      "Rendering episode 960.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:960.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_960.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_960.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_960.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-96.28652228]\n",
      "episode: [ 962. 1009.  967. 1015.]; total steps: 113000; episodes scores: [-98.40639216 -94.85127379 -98.27446873 -93.47498467]; avg score: -96.25177983391657\n",
      "learning timestep: 114000\n",
      "Policy Loss: -0.0867079347372055\n",
      "Value Loss: 0.34616321325302124\n",
      "Entropy: -1.3435544967651367\n",
      "KL Divergence: 2.2830801010131836\n",
      "episode: [ 976. 1022.  982. 1029.]; total steps: 114000; episodes scores: [-96.43477702 -96.38208644 -97.3691728  -97.55399887]; avg score: -96.9350087829566\n",
      "Rendering episode 980.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:980.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_980.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_980.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_980.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.58862161]\n",
      "episode: [ 991. 1035.  985. 1042.]; total steps: 115000; episodes scores: [-97.07015528 -96.9360153  -95.29151832 -98.50883613]; avg score: -96.95163125509366\n",
      "Rendering episode 1000.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1000.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1000.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1000.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.58862161]\n",
      "learning timestep: 116000\n",
      "Policy Loss: 0.2140197604894638\n",
      "Value Loss: 0.17856121063232422\n",
      "Entropy: -1.3920485973358154\n",
      "KL Divergence: 1.9094419479370117\n",
      "episode: [1004. 1050.  989. 1056.]; total steps: 116000; episodes scores: [-92.4983825  -97.61385798 -96.55578669 -95.47868026]; avg score: -95.53667685762669\n",
      "episode: [1014. 1062. 1001. 1069.]; total steps: 117000; episodes scores: [-95.82153124 -95.55959607 -98.01742165 -94.4709305 ]; avg score: -95.96736986195452\n",
      "learning timestep: 118000\n",
      "Policy Loss: 0.030512845143675804\n",
      "Value Loss: 0.2907506823539734\n",
      "Entropy: -1.4801528453826904\n",
      "KL Divergence: 1.9766616821289062\n",
      "episode: [1014. 1065. 1013. 1083.]; total steps: 118000; episodes scores: [-95.82153124 -93.94472858 -96.06149916 -96.55599186]; avg score: -95.5959377086819\n",
      "episode: [1015. 1068. 1023. 1089.]; total steps: 119000; episodes scores: [-54.22117747 -97.533793   -92.32722119 -94.64257725]; avg score: -84.68119222844223\n",
      "learning timestep: 120000\n",
      "Policy Loss: -0.10221174359321594\n",
      "Value Loss: 0.1826017051935196\n",
      "Entropy: -1.7016792297363281\n",
      "KL Divergence: 1.576762318611145\n",
      "episode: [1016. 1080. 1035. 1089.]; total steps: 120000; episodes scores: [-53.57746443 -93.36030904 -91.34633627 -94.64257725]; avg score: -83.23167174568448\n",
      "episode: [1019. 1081. 1046. 1100.]; total steps: 121000; episodes scores: [-95.91513693 -95.79286862 -95.25228474 -92.63017562]; avg score: -94.89761647853115\n",
      "Rendering episode 1020.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1020.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1020.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1020.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1020.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.6844553]\n",
      "learning timestep: 122000\n",
      "Policy Loss: 0.07361957430839539\n",
      "Value Loss: 0.16343732178211212\n",
      "Entropy: -1.7461590766906738\n",
      "KL Divergence: 1.7201364040374756\n",
      "episode: [1022. 1086. 1046. 1107.]; total steps: 122000; episodes scores: [-95.84547475 -94.81621664 -95.25228474 -93.95823253]; avg score: -94.96805216300797\n",
      "episode: [1034. 1097. 1052. 1107.]; total steps: 123000; episodes scores: [-95.39801567 -96.28052403 -93.27344114 -93.95823253]; avg score: -94.72755334295286\n",
      "Rendering episode 1040.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1040.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1040.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1040.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1040.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-93.69854371]\n",
      "learning timestep: 124000\n",
      "Policy Loss: -0.3329220712184906\n",
      "Value Loss: 0.6264945268630981\n",
      "Entropy: -1.4962297677993774\n",
      "KL Divergence: 2.293818950653076\n",
      "episode: [1047. 1110. 1065. 1117.]; total steps: 124000; episodes scores: [-95.35191413 -94.28450839 -96.79641542 -96.34598343]; avg score: -95.69470534296039\n",
      "episode: [1059. 1121. 1077. 1128.]; total steps: 125000; episodes scores: [-93.69985701 -92.91077936 -95.10642803 -93.83697016]; avg score: -93.88850864253301\n",
      "Rendering episode 1060.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1060.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1060.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1060.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1060.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.80534712]\n",
      "learning timestep: 126000\n",
      "Policy Loss: 0.15901975333690643\n",
      "Value Loss: 0.2523970603942871\n",
      "Entropy: -1.5399198532104492\n",
      "KL Divergence: 2.1139450073242188\n",
      "episode: [1070. 1133. 1088. 1131.]; total steps: 126000; episodes scores: [-93.03747482 -94.77578521 -92.00374633 -93.5336013 ]; avg score: -93.33765191417545\n",
      "episode: [1079. 1145. 1101. 1134.]; total steps: 127000; episodes scores: [-94.10654348 -93.20603946 -93.79410114 -91.19137372]; avg score: -93.07451445071322\n",
      "learning timestep: 128000\n",
      "Policy Loss: -0.03643391653895378\n",
      "Value Loss: 0.24776291847229004\n",
      "Entropy: -1.627225399017334\n",
      "KL Divergence: 2.99267578125\n",
      "episode: [1079. 1155. 1113. 1140.]; total steps: 128000; episodes scores: [-94.10654348 -93.36856332 -96.09627165 -93.3369057 ]; avg score: -94.22707103903375\n",
      "Rendering episode 1080.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1080.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1080.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1080.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1080.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.88836749]\n",
      "episode: [1087. 1168. 1125. 1150.]; total steps: 129000; episodes scores: [-93.42144353 -95.63769555 -94.37671085 -95.64167227]; avg score: -94.76938054897622\n",
      "Rendering episode 1100.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1100.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1100.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1100.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-94.88836749]\n",
      "learning timestep: 130000\n",
      "Policy Loss: 0.06421556323766708\n",
      "Value Loss: 0.41494572162628174\n",
      "Entropy: -1.4316753149032593\n",
      "KL Divergence: 2.1385042667388916\n",
      "episode: [1100. 1180. 1137. 1162.]; total steps: 130000; episodes scores: [-94.11364777 -95.58922401 -94.03951685 -91.9401526 ]; avg score: -93.92063530940996\n",
      "episode: [1110. 1191. 1149. 1174.]; total steps: 131000; episodes scores: [-91.06826333 -93.65969898 -91.4772201  -92.89795936]; avg score: -92.2757854436268\n",
      "Rendering episode 1120.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1120.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1120.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1120.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1120.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-93.88654653]\n",
      "learning timestep: 132000\n",
      "Policy Loss: 0.015653979033231735\n",
      "Value Loss: 0.19060328602790833\n",
      "Entropy: -1.423702359199524\n",
      "KL Divergence: 1.9246312379837036\n",
      "episode: [1120. 1202. 1161. 1185.]; total steps: 132000; episodes scores: [-93.28650912 -93.73817301 -95.20852908 -92.9724539 ]; avg score: -93.8014162780464\n",
      "episode: [1132. 1213. 1172. 1197.]; total steps: 133000; episodes scores: [-93.60761422 -92.91395394 -93.92257986 -93.09064136]; avg score: -93.3836973442994\n",
      "Rendering episode 1140.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1140.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1140.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1140.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1140.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-93.31497049]\n",
      "learning timestep: 134000\n",
      "Policy Loss: -0.030061252415180206\n",
      "Value Loss: 0.14007999002933502\n",
      "Entropy: -1.5322859287261963\n",
      "KL Divergence: 2.0939435958862305\n",
      "episode: [1142. 1225. 1184. 1207.]; total steps: 134000; episodes scores: [-91.5213791  -92.40602564 -94.23052984 -90.71599478]; avg score: -92.21848233814212\n",
      "episode: [1153. 1235. 1194. 1218.]; total steps: 135000; episodes scores: [-94.88091727 -93.49337364 -92.48808679 -91.76207285]; avg score: -93.15611263594714\n",
      "Rendering episode 1160.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1160.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1160.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1160.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1160.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.65317173]\n",
      "learning timestep: 136000\n",
      "Policy Loss: -0.056585393846035004\n",
      "Value Loss: 0.16217274963855743\n",
      "Entropy: -1.5552510023117065\n",
      "KL Divergence: 2.547184705734253\n",
      "episode: [1164. 1245. 1205. 1229.]; total steps: 136000; episodes scores: [-89.75244859 -91.928014   -93.70821027 -92.01491912]; avg score: -91.85089799439768\n",
      "episode: [1175. 1257. 1216. 1240.]; total steps: 137000; episodes scores: [-92.91489379 -94.23035989 -91.72011057 -93.23046084]; avg score: -93.0239562704875\n",
      "Rendering episode 1180.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1180.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1180.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1180.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1180.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.41618193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning timestep: 138000\n",
      "Policy Loss: 0.1240260973572731\n",
      "Value Loss: 0.21507367491722107\n",
      "Entropy: -1.572155475616455\n",
      "KL Divergence: 2.2504916191101074\n",
      "episode: [1185. 1267. 1227. 1251.]; total steps: 138000; episodes scores: [-90.70277646 -92.03960457 -93.21957446 -93.19397051]; avg score: -92.28898149996205\n",
      "episode: [1195. 1278. 1239. 1263.]; total steps: 139000; episodes scores: [-91.62249244 -92.43501071 -91.15334061 -92.84428357]; avg score: -92.01378183353646\n",
      "Rendering episode 1200.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1200.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1200.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1200.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.7459626]\n",
      "learning timestep: 140000\n",
      "Policy Loss: -0.3118324875831604\n",
      "Value Loss: 0.43141651153564453\n",
      "Entropy: -1.5887150764465332\n",
      "KL Divergence: 2.4194188117980957\n",
      "episode: [1205. 1289. 1250. 1274.]; total steps: 140000; episodes scores: [-90.88891468 -91.25225299 -92.4360417  -94.05253925]; avg score: -92.15743715671326\n",
      "episode: [1216. 1300. 1260. 1285.]; total steps: 141000; episodes scores: [-93.52935924 -92.13175587 -91.45956395 -94.12316623]; avg score: -92.81096132391488\n",
      "Rendering episode 1220.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1220.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1220.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1220.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1220.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.81503494]\n",
      "learning timestep: 142000\n",
      "Policy Loss: -0.03382577747106552\n",
      "Value Loss: 0.11200544238090515\n",
      "Entropy: -1.7262473106384277\n",
      "KL Divergence: 2.8967223167419434\n",
      "episode: [1226. 1311. 1271. 1295.]; total steps: 142000; episodes scores: [-92.05998414 -93.11105847 -88.25780781 -92.4306924 ]; avg score: -91.46488570485312\n",
      "episode: [1236. 1322. 1282. 1306.]; total steps: 143000; episodes scores: [-89.28152979 -88.28788974 -90.92222378 -92.16317467]; avg score: -90.16370449621867\n",
      "Rendering episode 1240.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1240.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1240.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1240.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1240.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.5523784]\n",
      "learning timestep: 144000\n",
      "Policy Loss: -0.2537160813808441\n",
      "Value Loss: 0.37213242053985596\n",
      "Entropy: -1.6678167581558228\n",
      "KL Divergence: 2.1280927658081055\n",
      "episode: [1245. 1333. 1293. 1316.]; total steps: 144000; episodes scores: [-91.86959528 -91.68723424 -90.37283791 -91.91249879]; avg score: -91.46054155235723\n",
      "episode: [1257. 1344. 1303. 1326.]; total steps: 145000; episodes scores: [-91.22232046 -92.29951459 -92.13860714 -88.5408389 ]; avg score: -91.05032027278808\n",
      "Rendering episode 1260.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1260.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1260.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1260.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1260.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-92.33869069]\n",
      "learning timestep: 146000\n",
      "Policy Loss: -0.03763780742883682\n",
      "Value Loss: 0.169778972864151\n",
      "Entropy: -1.8646137714385986\n",
      "KL Divergence: 2.7821061611175537\n",
      "episode: [1265. 1354. 1314. 1336.]; total steps: 146000; episodes scores: [-91.32867695 -91.54707523 -93.30975635 -92.53081058]; avg score: -92.17907977697439\n",
      "episode: [1275. 1365. 1323. 1346.]; total steps: 147000; episodes scores: [-91.50389714 -93.71577538 -94.41465608 -91.05649504]; avg score: -92.67270591296271\n",
      "Rendering episode 1280.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1280.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1280.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1280.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1280.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.34696706]\n",
      "learning timestep: 148000\n",
      "Policy Loss: 0.05304597318172455\n",
      "Value Loss: 0.2395435869693756\n",
      "Entropy: -1.7148832082748413\n",
      "KL Divergence: 2.808030128479004\n",
      "episode: [1282. 1375. 1333. 1356.]; total steps: 148000; episodes scores: [-91.23314876 -89.09753401 -90.4037636  -92.18090587]; avg score: -90.7288380585614\n",
      "episode: [1292. 1385. 1344. 1366.]; total steps: 149000; episodes scores: [-89.24175045 -89.39487276 -91.71824087 -91.66813302]; avg score: -90.50574927590895\n",
      "Rendering episode 1300.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1300.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1300.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1300.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.3269419]\n",
      "learning timestep: 150000\n",
      "Policy Loss: -0.27438610792160034\n",
      "Value Loss: 0.27598756551742554\n",
      "Entropy: -1.8507670164108276\n",
      "KL Divergence: 2.187688112258911\n",
      "episode: [1301. 1394. 1354. 1375.]; total steps: 150000; episodes scores: [-92.59482662 -90.94777094 -91.34649413 -90.76681957]; avg score: -91.41397781470138\n",
      "episode: [1310. 1403. 1364. 1386.]; total steps: 151000; episodes scores: [-91.56586794 -92.70474667 -89.02582782 -92.3367623 ]; avg score: -91.40830118221572\n",
      "learning timestep: 152000\n",
      "Policy Loss: -0.022658338770270348\n",
      "Value Loss: 0.23622839152812958\n",
      "Entropy: -1.9777735471725464\n",
      "KL Divergence: 2.7619993686676025\n",
      "episode: [1319. 1413. 1373. 1395.]; total steps: 152000; episodes scores: [-89.57041767 -89.49620832 -93.83517433 -90.24418008]; avg score: -90.78649510307439\n",
      "Rendering episode 1320.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1320.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1320.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1320.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1320.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-90.31173432]\n",
      "episode: [1328. 1423. 1384. 1404.]; total steps: 153000; episodes scores: [-90.22701309 -89.59275838 -91.21229293 -91.17183322]; avg score: -90.5509744020022\n",
      "learning timestep: 154000\n",
      "Policy Loss: 0.3003113269805908\n",
      "Value Loss: 0.1816623955965042\n",
      "Entropy: -1.7947335243225098\n",
      "KL Divergence: 2.4261655807495117\n",
      "episode: [1336. 1433. 1393. 1414.]; total steps: 154000; episodes scores: [-90.93440184 -89.56013626 -90.90365195 -93.4022985 ]; avg score: -91.20012213731789\n",
      "Rendering episode 1340.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1340.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1340.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1340.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1340.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.66966802]\n",
      "episode: [1345. 1442. 1403. 1423.]; total steps: 155000; episodes scores: [-89.368415   -90.94103079 -91.73522431 -92.23574144]; avg score: -91.07010288843125\n",
      "learning timestep: 156000\n",
      "Policy Loss: 0.16742554306983948\n",
      "Value Loss: 0.23948554694652557\n",
      "Entropy: -2.0612549781799316\n",
      "KL Divergence: 2.8421034812927246\n",
      "episode: [1355. 1449. 1413. 1433.]; total steps: 156000; episodes scores: [-91.18494743 -91.61651786 -93.42747067 -91.04546279]; avg score: -91.81859968785166\n",
      "Rendering episode 1360.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1360.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1360.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1360.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1360.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-89.61282807]\n",
      "episode: [1363. 1459. 1422. 1442.]; total steps: 157000; episodes scores: [-89.94539321 -89.81956851 -93.26731322 -90.24257473]; avg score: -90.81871241891042\n",
      "learning timestep: 158000\n",
      "Policy Loss: 0.03535019978880882\n",
      "Value Loss: 0.14957338571548462\n",
      "Entropy: -1.907745361328125\n",
      "KL Divergence: 3.4149246215820312\n",
      "episode: [1373. 1468. 1433. 1452.]; total steps: 158000; episodes scores: [-91.16602617 -88.96740755 -88.95414258 -91.32644791]; avg score: -90.10350605323072\n",
      "Rendering episode 1380.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1380.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1380.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1380.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1380.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-90.54348401]\n",
      "episode: [1382. 1477. 1443. 1461.]; total steps: 159000; episodes scores: [-92.81058313 -90.89607456 -92.12885449 -87.49507339]; avg score: -90.83264639304531\n",
      "learning timestep: 160000\n",
      "Policy Loss: -0.23280216753482819\n",
      "Value Loss: 0.4425647556781769\n",
      "Entropy: -1.8387885093688965\n",
      "KL Divergence: 2.6568286418914795\n",
      "episode: [1391. 1487. 1453. 1471.]; total steps: 160000; episodes scores: [-92.69298627 -93.21904916 -93.19845296 -90.51939614]; avg score: -92.40747113164483\n",
      "Rendering episode 1400.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1400.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1400.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1400.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-89.63368643]\n",
      "episode: [1400. 1497. 1462. 1480.]; total steps: 161000; episodes scores: [-91.43719777 -89.18642514 -89.00246016 -91.33957634]; avg score: -90.24141485459218\n",
      "learning timestep: 162000\n",
      "Policy Loss: 0.03060150519013405\n",
      "Value Loss: 0.2219718098640442\n",
      "Entropy: -2.0926895141601562\n",
      "KL Divergence: 3.4589056968688965\n",
      "episode: [1410. 1505. 1471. 1489.]; total steps: 162000; episodes scores: [-91.61832629 -90.4042949  -89.88815977 -90.44087567]; avg score: -90.58791415917051\n",
      "Rendering episode 1420.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1420.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1420.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1420.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1420.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-89.76404931]\n",
      "episode: [1420. 1515. 1480. 1499.]; total steps: 163000; episodes scores: [-92.27335652 -90.55900029 -91.46983607 -89.56264568]; avg score: -90.96620963948857\n",
      "learning timestep: 164000\n",
      "Policy Loss: -0.06488284468650818\n",
      "Value Loss: 0.22501638531684875\n",
      "Entropy: -1.890417456626892\n",
      "KL Divergence: 2.7573297023773193\n",
      "episode: [1429. 1525. 1490. 1508.]; total steps: 164000; episodes scores: [-90.89773115 -91.45299962 -89.57204769 -91.36053987]; avg score: -90.82082958364379\n",
      "episode: [1438. 1534. 1500. 1518.]; total steps: 165000; episodes scores: [-90.05253467 -91.2322458  -90.93839551 -94.61104998]; avg score: -91.70855649154866\n",
      "Rendering episode 1440.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1440.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1440.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1440.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1440.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-90.86405673]\n",
      "learning timestep: 166000\n",
      "Policy Loss: 0.0414319783449173\n",
      "Value Loss: 0.2279585301876068\n",
      "Entropy: -2.121845245361328\n",
      "KL Divergence: 3.487071990966797\n",
      "episode: [1446. 1544. 1510. 1527.]; total steps: 166000; episodes scores: [-89.58933829 -91.43905238 -89.8588781  -92.32675049]; avg score: -90.80350481587509\n",
      "episode: [1456. 1553. 1520. 1536.]; total steps: 167000; episodes scores: [-90.53767597 -90.20126599 -89.2830839  -90.14385988]; avg score: -90.0414714370612\n",
      "Rendering episode 1460.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1460.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1460.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1460.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1460.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.79702539]\n",
      "learning timestep: 168000\n",
      "Policy Loss: 0.15155787765979767\n",
      "Value Loss: 0.11093145608901978\n",
      "Entropy: -1.9067491292953491\n",
      "KL Divergence: 3.1610469818115234\n",
      "episode: [1465. 1562. 1529. 1546.]; total steps: 168000; episodes scores: [-91.07851082 -90.99389479 -88.61756753 -91.93655423]; avg score: -90.65663184211016\n",
      "episode: [1473. 1571. 1538. 1554.]; total steps: 169000; episodes scores: [-92.29256179 -87.57357061 -90.37420019 -89.22779081]; avg score: -89.86703084753779\n",
      "Rendering episode 1480.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1480.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1480.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1480.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1480.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.52701943]\n",
      "learning timestep: 170000\n",
      "Policy Loss: -0.036059606820344925\n",
      "Value Loss: 0.13244962692260742\n",
      "Entropy: -2.1215643882751465\n",
      "KL Divergence: 2.9384350776672363\n",
      "episode: [1482. 1580. 1548. 1564.]; total steps: 170000; episodes scores: [-90.90280425 -91.12280514 -91.13242366 -90.07228317]; avg score: -90.80757905588315\n",
      "episode: [1491. 1588. 1557. 1573.]; total steps: 171000; episodes scores: [-92.63976319 -88.86594732 -90.14955016 -89.82456862]; avg score: -90.36995732297655\n",
      "Rendering episode 1500.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1500.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1500.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1500.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-91.18901482]\n",
      "learning timestep: 172000\n",
      "Policy Loss: 0.08800585567951202\n",
      "Value Loss: 0.0915999710559845\n",
      "Entropy: -2.0378565788269043\n",
      "KL Divergence: 3.378136157989502\n",
      "episode: [1500. 1598. 1566. 1582.]; total steps: 172000; episodes scores: [-90.64324273 -92.08875914 -88.58035463 -89.97711241]; avg score: -90.32236722751473\n",
      "episode: [1509. 1607. 1575. 1591.]; total steps: 173000; episodes scores: [-89.60004474 -88.7228923  -91.66689028 -90.33155714]; avg score: -90.08034611635219\n",
      "learning timestep: 174000\n",
      "Policy Loss: -0.26254624128341675\n",
      "Value Loss: 0.09844858944416046\n",
      "Entropy: -2.1851017475128174\n",
      "KL Divergence: 3.6747846603393555\n",
      "episode: [1518. 1615. 1584. 1599.]; total steps: 174000; episodes scores: [-88.55234632 -90.48698542 -89.64053329 -90.92225857]; avg score: -89.90053089866181\n",
      "Rendering episode 1520.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1520.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1520.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1520.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1520.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-90.67457237]\n",
      "episode: [1527. 1624. 1593. 1608.]; total steps: 175000; episodes scores: [-90.17320048 -91.33963626 -88.7153461  -89.7454066 ]; avg score: -89.99339735713056\n",
      "learning timestep: 176000\n",
      "Policy Loss: 0.13506032526493073\n",
      "Value Loss: 0.13710734248161316\n",
      "Entropy: -2.082366466522217\n",
      "KL Divergence: 3.533259868621826\n",
      "episode: [1536. 1632. 1601. 1617.]; total steps: 176000; episodes scores: [-91.64550939 -89.12082707 -86.9376235  -88.86193018]; avg score: -89.1414725341297\n",
      "Rendering episode 1540.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1540.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1540.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1540.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1540.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-90.17780862]\n",
      "episode: [1544. 1641. 1610. 1625.]; total steps: 177000; episodes scores: [-91.57441806 -90.32177268 -88.04056221 -89.76548608]; avg score: -89.92555976020995\n",
      "learning timestep: 178000\n",
      "Policy Loss: -0.14530067145824432\n",
      "Value Loss: 0.09285685420036316\n",
      "Entropy: -2.2246155738830566\n",
      "KL Divergence: 2.441056728363037\n",
      "episode: [1552. 1649. 1618. 1633.]; total steps: 178000; episodes scores: [-90.20783791 -93.24992245 -91.70763008 -89.45202874]; avg score: -91.1543547954227\n",
      "Rendering episode 1560.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1560.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1560.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1560.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1560.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-89.04800797]\n",
      "episode: [1561. 1658. 1627. 1642.]; total steps: 179000; episodes scores: [-90.21691794 -90.95539738 -89.03174858 -88.25055353]; avg score: -89.61365435761198\n",
      "learning timestep: 180000\n",
      "Policy Loss: 0.019546395167708397\n",
      "Value Loss: 0.0843757763504982\n",
      "Entropy: -2.237999439239502\n",
      "KL Divergence: 3.3966946601867676\n",
      "episode: [1569. 1666. 1635. 1650.]; total steps: 180000; episodes scores: [-87.67428339 -88.9180246  -88.86139563 -88.72510088]; avg score: -88.54470112720132\n",
      "episode: [1578. 1674. 1644. 1659.]; total steps: 181000; episodes scores: [-90.25643545 -87.96290238 -91.2232186  -88.41739836]; avg score: -89.46498869584128\n",
      "Rendering episode 1580.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1580.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1580.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1580.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1580.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-88.75631122]\n",
      "learning timestep: 182000\n",
      "Policy Loss: 0.17668266594409943\n",
      "Value Loss: 0.10748858749866486\n",
      "Entropy: -2.278834819793701\n",
      "KL Divergence: 3.0338006019592285\n",
      "episode: [1586. 1683. 1652. 1666.]; total steps: 182000; episodes scores: [-90.27324437 -87.33655731 -89.96355383 -86.81711236]; avg score: -88.59761696753776\n",
      "episode: [1595. 1692. 1660. 1674.]; total steps: 183000; episodes scores: [-88.98469233 -89.99507461 -89.75870107 -89.77534833]; avg score: -89.62845408313876\n",
      "Rendering episode 1600.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1600.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1600.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1600.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.85328046]\n",
      "learning timestep: 184000\n",
      "Policy Loss: 0.009596526622772217\n",
      "Value Loss: 0.08102262020111084\n",
      "Entropy: -2.2020864486694336\n",
      "KL Divergence: 4.499589920043945\n",
      "episode: [1603. 1700. 1669. 1683.]; total steps: 184000; episodes scores: [-88.65486309 -87.23505323 -92.98064568 -89.12702575]; avg score: -89.49939693734298\n",
      "episode: [1611. 1708. 1677. 1691.]; total steps: 185000; episodes scores: [-87.47542593 -89.50048838 -91.57660602 -88.67504326]; avg score: -89.30689089670327\n",
      "learning timestep: 186000\n",
      "Policy Loss: 0.05630632862448692\n",
      "Value Loss: 0.08884970098733902\n",
      "Entropy: -2.266240358352661\n",
      "KL Divergence: 4.123047828674316\n",
      "episode: [1619. 1717. 1686. 1699.]; total steps: 186000; episodes scores: [-89.34836785 -88.74514352 -89.79679759 -89.93114772]; avg score: -89.45536416899769\n",
      "Rendering episode 1620.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1620.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1620.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1620.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1620.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.89474994]\n",
      "episode: [1627. 1725. 1694. 1707.]; total steps: 187000; episodes scores: [-88.53953643 -88.21625138 -86.99892605 -87.56900162]; avg score: -87.83092887001997\n",
      "learning timestep: 188000\n",
      "Policy Loss: -0.15220674872398376\n",
      "Value Loss: 0.14867660403251648\n",
      "Entropy: -2.3031814098358154\n",
      "KL Divergence: 3.2404513359069824\n",
      "episode: [1635. 1731. 1702. 1715.]; total steps: 188000; episodes scores: [-87.2547803  -91.6172639  -85.87057821 -93.16559137]; avg score: -89.4770534431017\n",
      "Rendering episode 1640.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1640.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1640.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1640.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1640.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.41239122]\n",
      "episode: [1643. 1738. 1710. 1723.]; total steps: 189000; episodes scores: [-87.19959523 -89.75347084 -87.90269066 -88.25802601]; avg score: -88.2784456835414\n",
      "learning timestep: 190000\n",
      "Policy Loss: 0.22971466183662415\n",
      "Value Loss: 0.11737552285194397\n",
      "Entropy: -2.1295459270477295\n",
      "KL Divergence: 4.142721176147461\n",
      "episode: [1651. 1746. 1718. 1731.]; total steps: 190000; episodes scores: [-89.61961814 -89.15369826 -88.76517901 -88.93183912]; avg score: -89.11758363333222\n",
      "episode: [1659. 1754. 1726. 1739.]; total steps: 191000; episodes scores: [-88.72852686 -89.22169266 -88.63323256 -87.92972499]; avg score: -88.62829426910821\n",
      "Rendering episode 1660.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1660.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1660.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1660.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1660.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.80277838]\n",
      "learning timestep: 192000\n",
      "Policy Loss: 0.05885729938745499\n",
      "Value Loss: 0.11371715366840363\n",
      "Entropy: -2.4399046897888184\n",
      "KL Divergence: 3.4955360889434814\n",
      "episode: [1666. 1762. 1735. 1746.]; total steps: 192000; episodes scores: [-89.1224079  -87.10128889 -89.49710952 -86.77547507]; avg score: -88.12407034580471\n",
      "episode: [1674. 1771. 1743. 1754.]; total steps: 193000; episodes scores: [-89.14081295 -87.7865151  -88.06802213 -87.74412919]; avg score: -88.18486984121986\n",
      "Rendering episode 1680.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1680.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1680.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1680.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1680.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.79881914]\n",
      "learning timestep: 194000\n",
      "Policy Loss: -0.008327854797244072\n",
      "Value Loss: 0.13979870080947876\n",
      "Entropy: -2.2659687995910645\n",
      "KL Divergence: 4.105343818664551\n",
      "episode: [1683. 1779. 1751. 1762.]; total steps: 194000; episodes scores: [-88.51880761 -88.93636743 -87.73334902 -88.59556437]; avg score: -88.44602210929493\n",
      "episode: [1691. 1787. 1759. 1771.]; total steps: 195000; episodes scores: [-87.4166066  -87.63266921 -87.07370231 -86.97467044]; avg score: -87.27441213977353\n",
      "learning timestep: 196000\n",
      "Policy Loss: -0.04028218239545822\n",
      "Value Loss: 0.15257716178894043\n",
      "Entropy: -2.229966640472412\n",
      "KL Divergence: 3.052595376968384\n",
      "episode: [1699. 1796. 1767. 1779.]; total steps: 196000; episodes scores: [-87.11934751 -88.32866142 -89.70131055 -88.49182541]; avg score: -88.41028622139866\n",
      "Rendering episode 1700.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1700.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1700.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1700.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.67861218]\n",
      "episode: [1708. 1804. 1775. 1788.]; total steps: 197000; episodes scores: [-85.62297999 -86.21826671 -88.3821147  -86.87849711]; avg score: -86.77546462638426\n",
      "learning timestep: 198000\n",
      "Policy Loss: -0.18723970651626587\n",
      "Value Loss: 0.1606147289276123\n",
      "Entropy: -2.3773655891418457\n",
      "KL Divergence: 5.68685245513916\n",
      "episode: [1716. 1812. 1784. 1796.]; total steps: 198000; episodes scores: [-86.65083926 -86.76649932 -88.32613058 -86.6433942 ]; avg score: -87.09671583968526\n",
      "Rendering episode 1720.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1720.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1720.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1720.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1720.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-88.13930921]\n",
      "episode: [1724. 1821. 1792. 1804.]; total steps: 199000; episodes scores: [-87.3822614  -87.8962183  -84.9373918  -87.50273911]; avg score: -86.92965265258681\n",
      "learning timestep: 200000\n",
      "Policy Loss: 0.0205231960862875\n",
      "Value Loss: 0.1384216845035553\n",
      "Entropy: -2.3073086738586426\n",
      "KL Divergence: 4.549289703369141\n",
      "episode: [1733. 1829. 1801. 1813.]; total steps: 200000; episodes scores: [-88.61886295 -87.54934988 -90.12296363 -85.65760003]; avg score: -87.98719412460116\n",
      "Rendering episode 1740.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1740.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1740.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1740.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1740.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.46164608]\n",
      "episode: [1741. 1836. 1809. 1821.]; total steps: 201000; episodes scores: [-87.98262842 -89.49882443 -88.60336853 -84.58468004]; avg score: -87.66737535457236\n",
      "learning timestep: 202000\n",
      "Policy Loss: 0.05515426769852638\n",
      "Value Loss: 0.4950297474861145\n",
      "Entropy: -2.2884037494659424\n",
      "KL Divergence: 4.166984558105469\n",
      "episode: [1750. 1844. 1817. 1830.]; total steps: 202000; episodes scores: [-84.98051055 -86.9986858  -87.70411157 -88.4405152 ]; avg score: -87.03095577748229\n",
      "episode: [1759. 1852. 1825. 1838.]; total steps: 203000; episodes scores: [-88.11151386 -84.48696158 -87.43383413 -87.35333112]; avg score: -86.846410173115\n",
      "Rendering episode 1760.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1760.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1760.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1760.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1760.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.78239764]\n",
      "learning timestep: 204000\n",
      "Policy Loss: -0.19492173194885254\n",
      "Value Loss: 0.182217538356781\n",
      "Entropy: -2.2769882678985596\n",
      "KL Divergence: 3.7958154678344727\n",
      "episode: [1767. 1859. 1833. 1845.]; total steps: 204000; episodes scores: [-87.32797195 -88.15315394 -87.64622443 -85.71910077]; avg score: -87.2116127714448\n",
      "episode: [1775. 1867. 1839. 1854.]; total steps: 205000; episodes scores: [-86.72913847 -82.84131724 -85.61219147 -86.72691705]; avg score: -85.47739105841657\n",
      "Rendering episode 1780.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1780.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1780.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1780.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1780.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.83429251]\n",
      "learning timestep: 206000\n",
      "Policy Loss: 0.1588388979434967\n",
      "Value Loss: 0.18245533108711243\n",
      "Entropy: -2.5039358139038086\n",
      "KL Divergence: 4.465455532073975\n",
      "episode: [1783. 1875. 1847. 1861.]; total steps: 206000; episodes scores: [-86.9442521  -87.48902379 -85.69226696 -87.23328383]; avg score: -86.83970666782976\n",
      "episode: [1792. 1883. 1855. 1869.]; total steps: 207000; episodes scores: [-85.44112372 -86.24697171 -85.59738901 -89.24797504]; avg score: -86.63336487092187\n",
      "Rendering episode 1800.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1800.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1800.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1800.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.07643271]\n",
      "learning timestep: 208000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./BipedalWalker/ppo)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: -0.025104381144046783\n",
      "Value Loss: 0.10381957143545151\n",
      "Entropy: -2.4306392669677734\n",
      "KL Divergence: 5.716612815856934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [1800. 1891. 1863. 1877.]; total steps: 208000; episodes scores: [-84.72897337 -86.68076488 -85.27816165 -86.8916765 ]; avg score: -85.89489409919238\n",
      "episode: [1809. 1899. 1872. 1885.]; total steps: 209000; episodes scores: [-86.51005155 -86.85378671 -88.06490059 -86.31853108]; avg score: -86.93681748308241\n",
      "learning timestep: 210000\n",
      "Policy Loss: 0.09919518232345581\n",
      "Value Loss: 0.08014434576034546\n",
      "Entropy: -2.433913230895996\n",
      "KL Divergence: 4.004668235778809\n",
      "episode: [1817. 1908. 1881. 1894.]; total steps: 210000; episodes scores: [-87.96382054 -85.99354514 -88.70649939 -85.60682282]; avg score: -87.06767197238499\n",
      "Rendering episode 1820.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1820.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1820.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1820.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1820.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.62428155]\n",
      "episode: [1825. 1916. 1889. 1902.]; total steps: 211000; episodes scores: [-84.45529367 -85.06321066 -88.28270759 -85.68187398]; avg score: -85.87077147675025\n",
      "learning timestep: 212000\n",
      "Policy Loss: -0.04980874061584473\n",
      "Value Loss: 0.1931457221508026\n",
      "Entropy: -2.4598402976989746\n",
      "KL Divergence: 4.146085739135742\n",
      "episode: [1833. 1924. 1896. 1911.]; total steps: 212000; episodes scores: [-86.83463211 -85.09237709 -89.84502622 -85.59827684]; avg score: -86.8425780638388\n",
      "Rendering episode 1840.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1840.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1840.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1840.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1840.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.20340913]\n",
      "episode: [1842. 1932. 1905. 1919.]; total steps: 213000; episodes scores: [-86.92388502 -84.46269716 -85.089787   -86.75600365]; avg score: -85.80809320913022\n",
      "learning timestep: 214000\n",
      "Policy Loss: -0.06661473214626312\n",
      "Value Loss: 0.11232130229473114\n",
      "Entropy: -2.335789918899536\n",
      "KL Divergence: 5.581724166870117\n",
      "episode: [1851. 1940. 1913. 1927.]; total steps: 214000; episodes scores: [-85.6760889  -85.50928809 -87.44191719 -85.32904794]; avg score: -85.98908552998257\n",
      "episode: [1859. 1948. 1921. 1934.]; total steps: 215000; episodes scores: [-84.02512251 -85.30956812 -86.18248702 -85.46535491]; avg score: -85.24563313989543\n",
      "Rendering episode 1860.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1860.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1860.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1860.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1860.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.45368046]\n",
      "learning timestep: 216000\n",
      "Policy Loss: -0.010346447117626667\n",
      "Value Loss: 0.10071081668138504\n",
      "Entropy: -2.2984766960144043\n",
      "KL Divergence: 5.641991138458252\n",
      "episode: [1867. 1956. 1929. 1943.]; total steps: 216000; episodes scores: [-84.98019615 -85.1751364  -87.9473916  -84.80062534]; avg score: -85.72583737254081\n",
      "episode: [1876. 1964. 1937. 1951.]; total steps: 217000; episodes scores: [-83.42928117 -88.76775016 -86.01736679 -83.75130305]; avg score: -85.49142529353935\n",
      "Rendering episode 1880.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1880.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1880.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1880.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1880.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.96940584]\n",
      "learning timestep: 218000\n",
      "Policy Loss: -0.09837866574525833\n",
      "Value Loss: 0.40227800607681274\n",
      "Entropy: -2.5510735511779785\n",
      "KL Divergence: 5.171655654907227\n",
      "episode: [1884. 1972. 1946. 1959.]; total steps: 218000; episodes scores: [-86.29037709 -84.66140692 -83.71210513 -87.05878997]; avg score: -85.43066977756827\n",
      "episode: [1892. 1980. 1954. 1967.]; total steps: 219000; episodes scores: [-85.42864777 -87.80099613 -87.4822887  -84.73920932]; avg score: -86.36278547994527\n",
      "Rendering episode 1900.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1900.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1900.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1900.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.69503065]\n",
      "learning timestep: 220000\n",
      "Policy Loss: -0.013868795707821846\n",
      "Value Loss: 0.3284358084201813\n",
      "Entropy: -2.5471725463867188\n",
      "KL Divergence: 5.230093002319336\n",
      "episode: [1900. 1989. 1962. 1975.]; total steps: 220000; episodes scores: [-85.54831654 -86.70188575 -84.18596431 -83.82512684]; avg score: -85.0653233616234\n",
      "episode: [1908. 1997. 1970. 1983.]; total steps: 221000; episodes scores: [-79.6166967  -86.73248201 -84.77341532 -85.65705223]; avg score: -84.19491156625514\n",
      "learning timestep: 222000\n",
      "Policy Loss: -0.1005319282412529\n",
      "Value Loss: 0.17196916043758392\n",
      "Entropy: -2.641178607940674\n",
      "KL Divergence: 6.4560770988464355\n",
      "episode: [1916. 2005. 1979. 1991.]; total steps: 222000; episodes scores: [-85.64731674 -86.70181085 -86.20862486 -85.05472996]; avg score: -85.90312060306562\n",
      "Rendering episode 1920.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1920.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1920.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1920.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1920.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-85.0595364]\n",
      "episode: [1924. 2013. 1987. 1998.]; total steps: 223000; episodes scores: [-85.38909995 -82.65112783 -81.90300306 -85.30207232]; avg score: -83.81132578881815\n",
      "learning timestep: 224000\n",
      "Policy Loss: 0.2602660357952118\n",
      "Value Loss: 0.8497000932693481\n",
      "Entropy: -2.567755699157715\n",
      "KL Divergence: 6.4508280754089355\n",
      "episode: [1933. 2021. 1995. 2007.]; total steps: 224000; episodes scores: [-87.96178686 -86.85236753 -83.17232831 -87.4491004 ]; avg score: -86.35889577402702\n",
      "Rendering episode 1940.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1940.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1940.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1940.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1940.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.1609513]\n",
      "episode: [1941. 2029. 2003. 2014.]; total steps: 225000; episodes scores: [-85.58028624 -79.69913018 -92.34315752 -84.44462703]; avg score: -85.5168002433887\n",
      "learning timestep: 226000\n",
      "Policy Loss: 0.1258777529001236\n",
      "Value Loss: 0.2444586157798767\n",
      "Entropy: -2.3150410652160645\n",
      "KL Divergence: 4.559828758239746\n",
      "episode: [1949. 2037. 2010. 2023.]; total steps: 226000; episodes scores: [-85.47423569 -86.07536067 -81.81593018 -86.0441966 ]; avg score: -84.8524307876327\n",
      "episode: [1957. 2046. 2018. 2031.]; total steps: 227000; episodes scores: [-84.32999307 -83.14945315 -86.64070116 -81.72674932]; avg score: -83.96172417574826\n",
      "Rendering episode 1960.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1960.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1960.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1960.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1960.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-87.52888646]\n",
      "learning timestep: 228000\n",
      "Policy Loss: -0.10488192737102509\n",
      "Value Loss: 0.16952168941497803\n",
      "Entropy: -2.6574790477752686\n",
      "KL Divergence: 6.873276710510254\n",
      "episode: [1965. 2053. 2026. 2038.]; total steps: 228000; episodes scores: [-83.92540258 -86.84759176 -85.57344202 -83.90623952]; avg score: -85.063168969094\n",
      "episode: [1972. 2061. 2034. 2046.]; total steps: 229000; episodes scores: [-82.44209366 -76.19450358 -85.84780984 -82.13086558]; avg score: -81.65381816599674\n",
      "Rendering episode 1980.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:1980.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_1980.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_1980.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_1980.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-85.79605022]\n",
      "learning timestep: 230000\n",
      "Policy Loss: 0.04907088354229927\n",
      "Value Loss: 0.26908957958221436\n",
      "Entropy: -2.446119546890259\n",
      "KL Divergence: 5.65444278717041\n",
      "episode: [1980. 2069. 2042. 2054.]; total steps: 230000; episodes scores: [ -82.10667777 -116.57335228  -81.54424554  -84.78288901]; avg score: -91.25179115070665\n",
      "episode: [1988. 2076. 2049. 2062.]; total steps: 231000; episodes scores: [-87.14961723 -78.30226096 -82.2238584  -83.36641668]; avg score: -82.76053831927214\n",
      "learning timestep: 232000\n",
      "Policy Loss: -0.14350809156894684\n",
      "Value Loss: 0.37581539154052734\n",
      "Entropy: -2.6667768955230713\n",
      "KL Divergence: 6.161645412445068\n",
      "episode: [1996. 2083. 2057. 2070.]; total steps: 232000; episodes scores: [-80.46933409 -79.09891885 -86.19236232 -83.02937993]; avg score: -82.1974987965344\n",
      "Rendering episode 2000.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2000.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2000.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2000.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.46375048]\n",
      "episode: [2003. 2091. 2064. 2077.]; total steps: 233000; episodes scores: [-75.30763061 -78.86208278 -84.52617382 -82.33118699]; avg score: -80.2567685494513\n",
      "learning timestep: 234000\n",
      "Policy Loss: -0.03459348902106285\n",
      "Value Loss: 0.7149350643157959\n",
      "Entropy: -2.7107787132263184\n",
      "KL Divergence: 6.768614768981934\n",
      "episode: [2010. 2099. 2071. 2084.]; total steps: 234000; episodes scores: [ -83.95673469  -84.40569397 -106.0932127   -81.64708517]; avg score: -89.0256816317076\n",
      "episode: [2017. 2106. 2079. 2090.]; total steps: 235000; episodes scores: [-76.26358224 -79.62405733 -80.4880265  -78.53512942]; avg score: -78.72769887148854\n",
      "Rendering episode 2020.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2020.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2020.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2020.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2020.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-86.35390915]\n",
      "learning timestep: 236000\n",
      "Policy Loss: -0.17999665439128876\n",
      "Value Loss: 0.41885748505592346\n",
      "Entropy: -2.6778719425201416\n",
      "KL Divergence: 6.14491081237793\n",
      "episode: [2025. 2114. 2086. 2097.]; total steps: 236000; episodes scores: [-79.05253983 -80.98395775 -81.77668411 -81.06834106]; avg score: -80.72038068704043\n",
      "episode: [2031. 2121. 2092. 2104.]; total steps: 237000; episodes scores: [-81.58905036 -82.82760283 -78.34609208 -77.01342761]; avg score: -79.94404322098707\n",
      "learning timestep: 238000\n",
      "Policy Loss: 0.09275289624929428\n",
      "Value Loss: 1.181370735168457\n",
      "Entropy: -2.6906967163085938\n",
      "KL Divergence: 7.539802551269531\n",
      "episode: [2038. 2128. 2099. 2112.]; total steps: 238000; episodes scores: [ -80.33924298 -104.50237654 -108.18830123 -104.09599909]; avg score: -99.28147996067318\n",
      "Rendering episode 2040.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2040.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2040.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2040.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2040.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-80.20947408]\n",
      "episode: [2044. 2135. 2106. 2118.]; total steps: 239000; episodes scores: [-72.07224154 -86.25023791 -72.16204696 -79.29065309]; avg score: -77.44379487268006\n",
      "learning timestep: 240000\n",
      "Policy Loss: -0.07175102084875107\n",
      "Value Loss: 0.4593621492385864\n",
      "Entropy: -2.58217191696167\n",
      "KL Divergence: 6.4634175300598145\n",
      "episode: [2051. 2142. 2112. 2125.]; total steps: 240000; episodes scores: [-82.08803639 -85.32475903 -86.8113769  -70.13248624]; avg score: -81.0891646378475\n",
      "episode: [2057. 2149. 2119. 2132.]; total steps: 241000; episodes scores: [-73.48354492 -78.28990466 -84.8198695  -80.98243752]; avg score: -79.39393914911756\n",
      "Rendering episode 2060.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2060.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2060.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2060.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2060.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-81.86164154]\n",
      "learning timestep: 242000\n",
      "Policy Loss: 0.06055482476949692\n",
      "Value Loss: 0.36141443252563477\n",
      "Entropy: -2.6530022621154785\n",
      "KL Divergence: 6.097825050354004\n",
      "episode: [2064. 2154. 2125. 2138.]; total steps: 242000; episodes scores: [-76.78710715 -77.02939112 -70.38555956 -78.31737536]; avg score: -75.62985829801485\n",
      "episode: [2071. 2160. 2132. 2144.]; total steps: 243000; episodes scores: [-83.52779629 -80.99436087 -79.03037033 -73.43114872]; avg score: -79.24591905214577\n",
      "learning timestep: 244000\n",
      "Policy Loss: 0.10906393080949783\n",
      "Value Loss: 2.1989078521728516\n",
      "Entropy: -2.60471773147583\n",
      "KL Divergence: 7.512774467468262\n",
      "episode: [2078. 2166. 2138. 2152.]; total steps: 244000; episodes scores: [-77.25586217 -82.10373407 -66.50185683 -75.27423211]; avg score: -75.28392129633747\n",
      "Rendering episode 2080.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2080.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2080.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2080.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2080.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-83.32397516]\n",
      "episode: [2085. 2172. 2144. 2158.]; total steps: 245000; episodes scores: [-89.57435937 -77.16634288 -76.46293405 -81.06105007]; avg score: -81.06617159196409\n",
      "learning timestep: 246000\n",
      "Policy Loss: 0.13545185327529907\n",
      "Value Loss: 1.5395252704620361\n",
      "Entropy: -2.7422289848327637\n",
      "KL Divergence: 8.225635528564453\n",
      "episode: [2091. 2179. 2150. 2164.]; total steps: 246000; episodes scores: [-84.10337739 -71.72618096 -74.76354459 -72.87727064]; avg score: -75.86759339688174\n",
      "episode: [2097. 2185. 2156. 2169.]; total steps: 247000; episodes scores: [-65.20425332 -84.94455204 -67.28287559 -61.39406375]; avg score: -69.70643617777573\n",
      "Rendering episode 2100.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2100.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2100.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2100.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-77.4373811]\n",
      "learning timestep: 248000\n",
      "Policy Loss: -0.025390323251485825\n",
      "Value Loss: 2.0203404426574707\n",
      "Entropy: -2.7730283737182617\n",
      "KL Divergence: 6.723309516906738\n",
      "episode: [2105. 2191. 2163. 2174.]; total steps: 248000; episodes scores: [-105.49903668  -77.6066071   -82.99537876  -84.80481916]; avg score: -87.72646042593678\n",
      "episode: [2111. 2197. 2170. 2180.]; total steps: 249000; episodes scores: [-49.11415782 -81.80673256 -81.86593707 -72.71624938]; avg score: -71.37576920982683\n",
      "learning timestep: 250000\n",
      "Policy Loss: 0.0030745433177798986\n",
      "Value Loss: 0.712912380695343\n",
      "Entropy: -2.8624401092529297\n",
      "KL Divergence: 7.7985029220581055\n",
      "episode: [2117. 2201. 2176. 2186.]; total steps: 250000; episodes scores: [ -74.19266712  -49.55145019 -107.04632641  -79.59403725]; avg score: -77.59612024409401\n",
      "Rendering episode 2120.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2120.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2120.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2120.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2120.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-54.44462339]\n",
      "episode: [2123. 2207. 2182. 2192.]; total steps: 251000; episodes scores: [-82.00842152 -77.19840243 -89.22487481 -88.96218197]; avg score: -84.34847018291583\n",
      "learning timestep: 252000\n",
      "Policy Loss: -0.031068839132785797\n",
      "Value Loss: 1.3663667440414429\n",
      "Entropy: -2.8494114875793457\n",
      "KL Divergence: 8.210768699645996\n",
      "episode: [2129. 2212. 2186. 2197.]; total steps: 252000; episodes scores: [-81.99862876 -62.10619751 -77.43127965 -86.46947312]; avg score: -77.00139476277786\n",
      "episode: [2134. 2217. 2191. 2201.]; total steps: 253000; episodes scores: [-65.27811111 -75.3630496  -87.16493629 -76.85464141]; avg score: -76.1651846028793\n",
      "learning timestep: 254000\n",
      "Policy Loss: -0.023702913895249367\n",
      "Value Loss: 0.6205655336380005\n",
      "Entropy: -2.8929836750030518\n",
      "KL Divergence: 8.738736152648926\n",
      "episode: [2139. 2219. 2198. 2205.]; total steps: 254000; episodes scores: [ -63.70401261  -39.50173851 -106.09596365  -60.60647289]; avg score: -67.47704691617501\n",
      "Rendering episode 2140.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2140.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2140.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2140.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2140.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-66.24631928]\n",
      "episode: [2146. 2224. 2203. 2210.]; total steps: 255000; episodes scores: [-80.33294884 -81.20066215 -57.90393383 -64.68052674]; avg score: -71.02951789101974\n",
      "learning timestep: 256000\n",
      "Policy Loss: -0.11751396954059601\n",
      "Value Loss: 0.44945278763771057\n",
      "Entropy: -2.58590030670166\n",
      "KL Divergence: 9.119956970214844\n",
      "episode: [2150. 2229. 2209. 2216.]; total steps: 256000; episodes scores: [-84.16546147 -50.51944671 -70.20867106 -64.27377068]; avg score: -67.29183747824561\n",
      "episode: [2154. 2235. 2215. 2220.]; total steps: 257000; episodes scores: [-35.01211846 -83.26157204 -71.75924062 -74.7566256 ]; avg score: -66.1973891806292\n",
      "Rendering episode 2160.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2160.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2160.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2160.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2160.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-71.11015636]\n",
      "learning timestep: 258000\n",
      "Policy Loss: -0.06671522557735443\n",
      "Value Loss: 1.0436848402023315\n",
      "Entropy: -2.9495341777801514\n",
      "KL Divergence: 10.629786491394043\n",
      "episode: [2160. 2240. 2221. 2226.]; total steps: 258000; episodes scores: [-76.73947999 -80.23866471 -53.71648879 -78.07497226]; avg score: -72.19240143618315\n",
      "episode: [2165. 2244. 2226. 2231.]; total steps: 259000; episodes scores: [-54.20618672 -58.62975948 -78.45761613 -61.88982909]; avg score: -63.29584785549977\n",
      "learning timestep: 260000\n",
      "Policy Loss: 0.27409154176712036\n",
      "Value Loss: 1.3139903545379639\n",
      "Entropy: -2.8295464515686035\n",
      "KL Divergence: 7.754397392272949\n",
      "episode: [2171. 2246. 2230. 2238.]; total steps: 260000; episodes scores: [-38.01994341 -30.94112653 -42.66819013 -73.54859516]; avg score: -46.29446380862251\n",
      "episode: [2176. 2250. 2234. 2244.]; total steps: 261000; episodes scores: [-85.73081665 -56.38668953 -51.21937547 -70.5906162 ]; avg score: -65.98187445935451\n",
      "Rendering episode 2180.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2180.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2180.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2180.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2180.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-83.21067969]\n",
      "learning timestep: 262000\n",
      "Policy Loss: -0.26547059416770935\n",
      "Value Loss: 0.9079775810241699\n",
      "Entropy: -2.9657816886901855\n",
      "KL Divergence: 9.039329528808594\n",
      "episode: [2180. 2257. 2237. 2248.]; total steps: 262000; episodes scores: [-74.15604601 -80.43165558 -56.99862306 -74.22337474]; avg score: -71.45242484696783\n",
      "episode: [2185. 2263. 2242. 2254.]; total steps: 263000; episodes scores: [ -51.51922503  -37.22744694 -102.98634782  -86.50948992]; avg score: -69.5606274278611\n",
      "learning timestep: 264000\n",
      "Policy Loss: 0.1977916657924652\n",
      "Value Loss: 1.0232291221618652\n",
      "Entropy: -2.851471424102783\n",
      "KL Divergence: 8.8829927444458\n",
      "episode: [2191. 2268. 2249. 2260.]; total steps: 264000; episodes scores: [-64.55639943 -64.26448012 -86.48275285 -73.7960952 ]; avg score: -72.27493189941316\n",
      "episode: [2195. 2272. 2255. 2265.]; total steps: 265000; episodes scores: [-37.09069756 -58.51799584 -71.12184225 -73.1744879 ]; avg score: -59.976255885970765\n",
      "Rendering episode 2200.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2200.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2200.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2200.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-71.80451277]\n",
      "learning timestep: 266000\n",
      "Policy Loss: -0.11672326177358627\n",
      "Value Loss: 0.7670583724975586\n",
      "Entropy: -2.9569177627563477\n",
      "KL Divergence: 8.715373992919922\n",
      "episode: [2200. 2278. 2258. 2270.]; total steps: 266000; episodes scores: [-73.45575878 -74.88495008 -34.12234324 -62.79482917]; avg score: -61.31447031674472\n",
      "episode: [2206. 2282. 2262. 2276.]; total steps: 267000; episodes scores: [-56.55112917 -75.14667498 -69.63457593 -64.27158126]; avg score: -66.40099033426043\n",
      "learning timestep: 268000\n",
      "Policy Loss: -0.03019961155951023\n",
      "Value Loss: 1.0317671298980713\n",
      "Entropy: -2.978480100631714\n",
      "KL Divergence: 8.456792831420898\n",
      "episode: [2213. 2287. 2265. 2280.]; total steps: 268000; episodes scores: [-87.02449519 -52.96141996 -52.9265794  -41.00313673]; avg score: -58.478907821446974\n",
      "episode: [2218. 2290. 2269. 2287.]; total steps: 269000; episodes scores: [-72.84120064 -34.65916562 -52.67610143 -89.64918895]; avg score: -62.456414160467205\n",
      "Rendering episode 2220.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2220.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2220.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2220.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2220.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-57.69141554]\n",
      "learning timestep: 270000\n",
      "Policy Loss: -0.2962586283683777\n",
      "Value Loss: 1.1301590204238892\n",
      "Entropy: -2.9765303134918213\n",
      "KL Divergence: 10.78464412689209\n",
      "episode: [2223. 2293. 2274. 2291.]; total steps: 270000; episodes scores: [-65.07518964 -66.16771533 -78.96921472 -31.82873984]; avg score: -60.510214883987025\n",
      "episode: [2228. 2298. 2281. 2297.]; total steps: 271000; episodes scores: [-72.13442626 -35.76964823 -75.53750296 -70.06183274]; avg score: -63.37585254786225\n",
      "learning timestep: 272000\n",
      "Policy Loss: -0.07935737818479538\n",
      "Value Loss: 0.9903631210327148\n",
      "Entropy: -3.067530632019043\n",
      "KL Divergence: 10.675592422485352\n",
      "episode: [2233. 2304. 2286. 2303.]; total steps: 272000; episodes scores: [-40.37361694 -79.04920615 -74.17245622 -87.74626814]; avg score: -70.33538686385688\n",
      "episode: [2239. 2307. 2290. 2308.]; total steps: 273000; episodes scores: [-56.21955703 -68.66016384 -45.63205307 -63.00209121]; avg score: -58.378466288141894\n",
      "Rendering episode 2240.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2240.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2240.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2240.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2240.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-84.13937301]\n",
      "learning timestep: 274000\n",
      "Policy Loss: -0.05996377021074295\n",
      "Value Loss: 1.1033401489257812\n",
      "Entropy: -3.019636631011963\n",
      "KL Divergence: 9.725693702697754\n",
      "episode: [2242. 2310. 2294. 2313.]; total steps: 274000; episodes scores: [-68.79586773 -63.47271558 -35.77785639 -72.96990821]; avg score: -60.25408697736622\n",
      "episode: [2246. 2315. 2298. 2317.]; total steps: 275000; episodes scores: [-68.69813407 -71.12011877 -86.17936211 -65.8548715 ]; avg score: -72.96312161191273\n",
      "learning timestep: 276000\n",
      "Policy Loss: 0.09433932602405548\n",
      "Value Loss: 0.9960113167762756\n",
      "Entropy: -2.9690375328063965\n",
      "KL Divergence: 8.405643463134766\n",
      "episode: [2253. 2319. 2302. 2320.]; total steps: 276000; episodes scores: [-70.89533613 -60.31547573 -52.00761082 -26.18025596]; avg score: -52.34966966398463\n",
      "episode: [2257. 2323. 2307. 2323.]; total steps: 277000; episodes scores: [-75.02114213 -76.63297235 -79.30996043 -80.3283727 ]; avg score: -77.82311190191082\n",
      "Rendering episode 2260.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2260.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2260.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2260.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2260.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-67.68849972]\n",
      "learning timestep: 278000\n",
      "Policy Loss: -0.017837896943092346\n",
      "Value Loss: 0.8031799793243408\n",
      "Entropy: -3.0063531398773193\n",
      "KL Divergence: 8.443724632263184\n",
      "episode: [2260. 2327. 2309. 2329.]; total steps: 278000; episodes scores: [-53.35832447 -61.63731659  19.85596069 -68.58641379]; avg score: -40.93152354128725\n",
      "episode: [2264. 2330. 2312. 2333.]; total steps: 279000; episodes scores: [-28.40332569 -26.38619798 -54.14637602 -57.79700383]; avg score: -41.68322588086983\n",
      "learning timestep: 280000\n",
      "Policy Loss: 0.12857100367546082\n",
      "Value Loss: 1.201434850692749\n",
      "Entropy: -3.216897487640381\n",
      "KL Divergence: 10.305622100830078\n",
      "episode: [2268. 2333. 2317. 2338.]; total steps: 280000; episodes scores: [-51.43737161 -13.68970817 -64.6357489  -86.67913671]; avg score: -54.11049134809119\n",
      "episode: [2271. 2336. 2320. 2341.]; total steps: 281000; episodes scores: [-106.96311543  -31.33912741  -64.82940149  -32.16001895]; avg score: -58.82291581934257\n",
      "learning timestep: 282000\n",
      "Policy Loss: -0.1629713773727417\n",
      "Value Loss: 1.902799129486084\n",
      "Entropy: -3.1631624698638916\n",
      "KL Divergence: 9.280593872070312\n",
      "episode: [2275. 2340. 2320. 2345.]; total steps: 282000; episodes scores: [-61.2759655  -70.34103395 -64.82940149 -51.67198509]; avg score: -62.029596508383754\n",
      "Rendering episode 2280.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2280.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2280.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2280.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2280.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-67.78156971]\n",
      "episode: [2280. 2343. 2325. 2349.]; total steps: 283000; episodes scores: [-63.558221   -49.39928086 -72.44723449 -34.08363394]; avg score: -54.87209257423366\n",
      "learning timestep: 284000\n",
      "Policy Loss: -0.22138208150863647\n",
      "Value Loss: 4.5518646240234375\n",
      "Entropy: -3.0291082859039307\n",
      "KL Divergence: 10.090234756469727\n",
      "episode: [2284. 2346. 2328. 2354.]; total steps: 284000; episodes scores: [-61.05413992 -68.90158054 -30.22986127 -50.34986386]; avg score: -52.63386139866927\n",
      "episode: [2288. 2349. 2333. 2360.]; total steps: 285000; episodes scores: [-56.53779605  -0.33647594 -20.6301005  -74.67078091]; avg score: -38.04378835216133\n",
      "learning timestep: 286000\n",
      "Policy Loss: 0.0009856661781668663\n",
      "Value Loss: 1.4416344165802002\n",
      "Entropy: -3.1171467304229736\n",
      "KL Divergence: 10.8441743850708\n",
      "episode: [2292. 2352. 2338. 2364.]; total steps: 286000; episodes scores: [-17.15978821 -33.04527858 -69.25478196 -62.08575828]; avg score: -45.386401758173356\n",
      "episode: [2298. 2358. 2342. 2370.]; total steps: 287000; episodes scores: [-73.14284752 -74.64124599 -25.61118058 -58.98492516]; avg score: -58.095049813392265\n",
      "Rendering episode 2300.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2300.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2300.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2300.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-55.77588606]\n",
      "learning timestep: 288000\n",
      "Policy Loss: 0.04705946147441864\n",
      "Value Loss: 0.7104421854019165\n",
      "Entropy: -2.9431638717651367\n",
      "KL Divergence: 10.186071395874023\n",
      "episode: [2302. 2361. 2345. 2374.]; total steps: 288000; episodes scores: [-70.78480941 -50.86448476 -31.06956742 -61.48313268]; avg score: -53.55049856776621\n",
      "episode: [2306. 2366. 2351. 2378.]; total steps: 289000; episodes scores: [-58.56867388 -59.74998355 -79.48039064 -70.96573674]; avg score: -67.19119620189768\n",
      "learning timestep: 290000\n",
      "Policy Loss: -0.022963210940361023\n",
      "Value Loss: 1.4024338722229004\n",
      "Entropy: -3.047243595123291\n",
      "KL Divergence: 9.14102554321289\n",
      "episode: [2309. 2370. 2355. 2380.]; total steps: 290000; episodes scores: [-20.40857589 -36.43268533 -33.27523122 -74.75869033]; avg score: -41.218795693197464\n",
      "episode: [2312. 2373. 2359. 2384.]; total steps: 291000; episodes scores: [-33.21424367 -69.83781224 -64.19398847 -85.59042513]; avg score: -63.20911737903812\n",
      "learning timestep: 292000\n",
      "Policy Loss: 0.10566726326942444\n",
      "Value Loss: 1.193694829940796\n",
      "Entropy: -3.1555581092834473\n",
      "KL Divergence: 8.732366561889648\n",
      "episode: [2317. 2377. 2363. 2387.]; total steps: 292000; episodes scores: [-71.00680004 -28.05720491 -36.87182505 -23.78979524]; avg score: -39.931406309448676\n",
      "Rendering episode 2320.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2320.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2320.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2320.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2320.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-33.7148135]\n",
      "episode: [2320. 2380. 2366. 2392.]; total steps: 293000; episodes scores: [-75.77647041   7.0687765  -36.42009233 -57.16005732]; avg score: -40.571960889682686\n",
      "learning timestep: 294000\n",
      "Policy Loss: -0.21538479626178741\n",
      "Value Loss: 1.0147801637649536\n",
      "Entropy: -3.060147762298584\n",
      "KL Divergence: 11.383108139038086\n",
      "episode: [2323. 2384. 2370. 2395.]; total steps: 294000; episodes scores: [-43.43039565 -58.16092634 -69.51782476 -24.19518744]; avg score: -48.8260835494753\n",
      "episode: [2327. 2388. 2373. 2397.]; total steps: 295000; episodes scores: [-75.43346479 -70.22060093 -20.06781846 -58.74749506]; avg score: -56.117344812644646\n",
      "learning timestep: 296000\n",
      "Policy Loss: 0.02000381238758564\n",
      "Value Loss: 1.3270727396011353\n",
      "Entropy: -3.2150235176086426\n",
      "KL Divergence: 12.298633575439453\n",
      "episode: [2332. 2391. 2376. 2400.]; total steps: 296000; episodes scores: [-31.1628696   -3.5400416  -71.97539666 -10.02809582]; avg score: -29.17660092153552\n",
      "episode: [2337. 2396. 2380. 2406.]; total steps: 297000; episodes scores: [-59.04021271 -60.62027125 -67.13201514 -77.71257965]; avg score: -66.12626968632685\n",
      "Rendering episode 2340.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2340.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2340.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2340.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2340.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-52.03243863]\n",
      "learning timestep: 298000\n",
      "Policy Loss: 0.21696186065673828\n",
      "Value Loss: 0.9916747808456421\n",
      "Entropy: -3.1216256618499756\n",
      "KL Divergence: 12.812141418457031\n",
      "episode: [2342. 2401. 2383. 2410.]; total steps: 298000; episodes scores: [-55.20651856 -82.54633718 -62.92000929 -46.43215409]; avg score: -61.77625477604832\n",
      "episode: [2347. 2406. 2387. 2415.]; total steps: 299000; episodes scores: [-77.6378335  -44.62105636 -71.73998102 -60.4691098 ]; avg score: -63.61699516969918\n",
      "learning timestep: 300000\n",
      "Policy Loss: -0.17157305777072906\n",
      "Value Loss: 1.5688683986663818\n",
      "Entropy: -3.108181953430176\n",
      "KL Divergence: 11.163786888122559\n",
      "episode: [2349. 2411. 2390. 2420.]; total steps: 300000; episodes scores: [-38.72866137 -61.30986278 -62.973813   -64.87949238]; avg score: -56.97295738145677\n",
      "episode: [2352. 2413. 2394. 2426.]; total steps: 301000; episodes scores: [-57.36536601  57.13279282 -61.95037539 -71.26756278]; avg score: -33.36262783798574\n",
      "learning timestep: 302000\n",
      "Policy Loss: -0.1915305256843567\n",
      "Value Loss: 1.3063757419586182\n",
      "Entropy: -3.310737133026123\n",
      "KL Divergence: 11.669432640075684\n",
      "episode: [2357. 2415. 2400. 2431.]; total steps: 302000; episodes scores: [-54.45883295 -15.79372056 -70.5778162  -82.72462319]; avg score: -55.88874822549754\n",
      "Rendering episode 2360.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2360.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2360.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2360.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2360.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-26.92411976]\n",
      "episode: [2360. 2418. 2404. 2437.]; total steps: 303000; episodes scores: [-57.01030158 -30.82657203 -23.40822372 -71.52216899]; avg score: -45.69181658078248\n",
      "learning timestep: 304000\n",
      "Policy Loss: -0.4040859043598175\n",
      "Value Loss: 1.4213604927062988\n",
      "Entropy: -3.0500924587249756\n",
      "KL Divergence: 11.714723587036133\n",
      "episode: [2364. 2422. 2407. 2441.]; total steps: 304000; episodes scores: [-51.30167205 -81.89732784 -73.7175324  -14.16873159]; avg score: -55.27131597012547\n",
      "episode: [2368. 2426. 2411. 2443.]; total steps: 305000; episodes scores: [-73.64360446 -80.92185422 -53.09105629 -18.38634449]; avg score: -56.510714866119756\n",
      "learning timestep: 306000\n",
      "Policy Loss: 0.014105039648711681\n",
      "Value Loss: 1.2937642335891724\n",
      "Entropy: -3.0726518630981445\n",
      "KL Divergence: 9.905179977416992\n",
      "episode: [2373. 2430. 2413. 2446.]; total steps: 306000; episodes scores: [-21.78374442 -72.46647483   4.23205399 -18.1924443 ]; avg score: -27.052652387526543\n",
      "episode: [2376. 2435. 2417. 2451.]; total steps: 307000; episodes scores: [-15.4829304  -60.7232068  -49.25796204 -63.96100788]; avg score: -47.3562767797103\n",
      "learning timestep: 308000\n",
      "Policy Loss: -0.06738099455833435\n",
      "Value Loss: 1.0229356288909912\n",
      "Entropy: -3.136495590209961\n",
      "KL Divergence: 12.525821685791016\n",
      "episode: [2379. 2438. 2421. 2457.]; total steps: 308000; episodes scores: [-54.42614141 -41.41939919 -32.79138359 -66.58287919]; avg score: -48.80495084311455\n",
      "Rendering episode 2380.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2380.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2380.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2380.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2380.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-18.94329467]\n",
      "episode: [2382. 2441. 2424. 2461.]; total steps: 309000; episodes scores: [-56.38885501 -23.62064895 -50.92304641 -55.08079569]; avg score: -46.5033365137886\n",
      "learning timestep: 310000\n",
      "Policy Loss: 0.05694533884525299\n",
      "Value Loss: 2.7336390018463135\n",
      "Entropy: -3.1994450092315674\n",
      "KL Divergence: 10.807575225830078\n",
      "episode: [2387. 2445. 2430. 2466.]; total steps: 310000; episodes scores: [-58.72510901 -34.50526454 -76.896078   -29.78016876]; avg score: -49.976655076565834\n",
      "episode: [2391. 2447. 2435. 2468.]; total steps: 311000; episodes scores: [-27.59758567 -44.18953285 -89.68939883   7.34684699]; avg score: -38.53241759066951\n",
      "learning timestep: 312000\n",
      "Policy Loss: -0.017890099436044693\n",
      "Value Loss: 1.1621818542480469\n",
      "Entropy: -3.1685240268707275\n",
      "KL Divergence: 12.861722946166992\n",
      "episode: [2395. 2449. 2440. 2472.]; total steps: 312000; episodes scores: [-67.48867648   0.36549068 -36.93803744 -33.15015053]; avg score: -34.302843444881475\n",
      "episode: [2399. 2450. 2444. 2476.]; total steps: 313000; episodes scores: [-55.28586533  63.73953323 -59.66227212 -63.98519843]; avg score: -28.798450664679017\n",
      "Rendering episode 2400.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2400.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2400.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2400.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-82.17343587]\n",
      "learning timestep: 314000\n",
      "Policy Loss: -0.17469444870948792\n",
      "Value Loss: 0.9984925985336304\n",
      "Entropy: -3.215426206588745\n",
      "KL Divergence: 12.028707504272461\n",
      "episode: [2402. 2454. 2448. 2478.]; total steps: 314000; episodes scores: [-20.18813293 -13.07419605 -64.10215736  14.99662583]; avg score: -20.59196512848094\n",
      "episode: [2405. 2457. 2452. 2480.]; total steps: 315000; episodes scores: [-35.1066685  -58.39262081 -13.64527135 -27.45395181]; avg score: -33.64962811702911\n",
      "learning timestep: 316000\n",
      "Policy Loss: 0.24792762100696564\n",
      "Value Loss: 1.6842412948608398\n",
      "Entropy: -3.2254695892333984\n",
      "KL Divergence: 12.032210350036621\n",
      "episode: [2408. 2459. 2454. 2484.]; total steps: 316000; episodes scores: [ -8.82905761 -45.60683574 -37.4084414  -49.36168665]; avg score: -35.30150534990537\n",
      "episode: [2410. 2460. 2457. 2487.]; total steps: 317000; episodes scores: [-40.88687935 126.31021017 -40.16948821 -25.35229037]; avg score: 4.975388058413229\n",
      "learning timestep: 318000\n",
      "Policy Loss: 0.023282885551452637\n",
      "Value Loss: 0.7992011308670044\n",
      "Entropy: -2.9606289863586426\n",
      "KL Divergence: 10.583291053771973\n",
      "episode: [2413. 2462. 2461. 2489.]; total steps: 318000; episodes scores: [-23.31910783 -25.49886477 -31.17013158 -15.10305248]; avg score: -23.77278916470644\n",
      "episode: [2414. 2465. 2464. 2494.]; total steps: 319000; episodes scores: [ 53.35021007 -61.71984471 -62.53545966 -39.73142758]; avg score: -27.659130469341118\n",
      "learning timestep: 320000\n",
      "Policy Loss: -0.06307560950517654\n",
      "Value Loss: 1.8320603370666504\n",
      "Entropy: -3.171818494796753\n",
      "KL Divergence: 12.341825485229492\n",
      "episode: [2415. 2469. 2467. 2496.]; total steps: 320000; episodes scores: [ -30.6394819   -72.81273553   12.41260392 -101.07314002]; avg score: -48.02818838092949\n",
      "episode: [2418. 2471. 2469. 2498.]; total steps: 321000; episodes scores: [-70.66505958 -35.13714235 -50.57129756 -31.74336266]; avg score: -47.02921553784064\n",
      "Rendering episode 2420.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2420.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2420.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2420.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2420.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-17.05239566]\n",
      "learning timestep: 322000\n",
      "Policy Loss: 0.056038748472929\n",
      "Value Loss: 1.5818195343017578\n",
      "Entropy: -3.238888740539551\n",
      "KL Divergence: 12.5671968460083\n",
      "episode: [2422. 2471. 2472. 2502.]; total steps: 322000; episodes scores: [-73.74464116 -35.13714235 -72.45953042 -70.47593869]; avg score: -62.95431315366473\n",
      "episode: [2424. 2474. 2474. 2503.]; total steps: 323000; episodes scores: [ 43.89105605 -85.83175097 -52.28393078  55.44024686]; avg score: -9.696094709879397\n",
      "learning timestep: 324000\n",
      "Policy Loss: 0.008206816390156746\n",
      "Value Loss: 1.4886035919189453\n",
      "Entropy: -3.300351858139038\n",
      "KL Divergence: 10.15047550201416\n",
      "episode: [2428. 2475. 2478. 2505.]; total steps: 324000; episodes scores: [-33.24581001  84.95201466 -83.1954371  -84.71137403]; avg score: -29.05015161673867\n",
      "episode: [2429. 2477. 2479. 2507.]; total steps: 325000; episodes scores: [ 84.78836423 -67.21984148  41.69871435 -48.04927917]; avg score: 2.8044894801014095\n",
      "learning timestep: 326000\n",
      "Policy Loss: 0.07313939183950424\n",
      "Value Loss: 2.0275588035583496\n",
      "Entropy: -3.2498295307159424\n",
      "KL Divergence: 12.254600524902344\n",
      "episode: [2431. 2479. 2484. 2509.]; total steps: 326000; episodes scores: [ -71.26438265  -65.58364398 -101.63230046  -65.02206044]; avg score: -75.87559688337255\n",
      "episode: [2433. 2480. 2487. 2512.]; total steps: 327000; episodes scores: [-40.96802178 148.80848564 -81.57438024 -20.77800069]; avg score: 1.3720207335260248\n",
      "learning timestep: 328000\n",
      "Policy Loss: 0.1761229932308197\n",
      "Value Loss: 2.1917502880096436\n",
      "Entropy: -3.4323062896728516\n",
      "KL Divergence: 12.225719451904297\n",
      "episode: [2435. 2482. 2489. 2514.]; total steps: 328000; episodes scores: [-75.37273095 -43.04911974 -42.22032972 -50.04764505]; avg score: -52.67245636446742\n",
      "episode: [2437. 2484. 2491. 2518.]; total steps: 329000; episodes scores: [ -99.77194539 -100.22822995  -43.26240078  -41.36088869]; avg score: -71.15586620246386\n",
      "learning timestep: 330000\n",
      "Policy Loss: -0.08318158239126205\n",
      "Value Loss: 1.3500025272369385\n",
      "Entropy: -3.261261463165283\n",
      "KL Divergence: 10.469270706176758\n",
      "episode: [2438. 2489. 2493. 2523.]; total steps: 330000; episodes scores: [ 64.33926964 -69.53238154 -11.22345402 -81.96820731]; avg score: -24.59619330927671\n",
      "Rendering episode 2440.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2440.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2440.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2440.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2440.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [23.31512344]\n",
      "episode: [2441. 2491. 2495. 2523.]; total steps: 331000; episodes scores: [-36.78548891 -49.212993    -8.88431966 -81.96820731]; avg score: -44.212752219045385\n",
      "learning timestep: 332000\n",
      "Policy Loss: -0.10689720511436462\n",
      "Value Loss: 1.4944005012512207\n",
      "Entropy: -3.158954381942749\n",
      "KL Divergence: 11.126771926879883\n",
      "episode: [2444. 2493. 2499. 2525.]; total steps: 332000; episodes scores: [-48.55246748 -27.60185325  -6.629668   -33.77505485]; avg score: -29.139760894488074\n",
      "episode: [2447. 2495. 2501. 2527.]; total steps: 333000; episodes scores: [-64.61909227 -44.11012021 -62.5941452  -50.89160788]; avg score: -55.553741390098345\n",
      "learning timestep: 334000\n",
      "Policy Loss: 0.03611084073781967\n",
      "Value Loss: 0.6485116481781006\n",
      "Entropy: -3.324009418487549\n",
      "KL Divergence: 11.066866874694824\n",
      "episode: [2448. 2495. 2502. 2528.]; total steps: 334000; episodes scores: [-23.34585865 -44.11012021 123.75276379  38.55383536]; avg score: 23.712655072845113\n",
      "episode: [2451. 2496. 2502. 2529.]; total steps: 335000; episodes scores: [-47.19942175 139.73429896 123.75276379 152.58021729]; avg score: 92.21696457113256\n",
      "learning timestep: 336000\n",
      "Policy Loss: 0.10852955281734467\n",
      "Value Loss: 1.142270803451538\n",
      "Entropy: -3.3263943195343018\n",
      "KL Divergence: 13.330018997192383\n",
      "episode: [2453. 2497. 2504. 2530.]; total steps: 336000; episodes scores: [-22.44556199 180.80472557 -23.02077402 296.58497742]; avg score: 107.98084174578479\n",
      "episode: [2455. 2499. 2508. 2533.]; total steps: 337000; episodes scores: [-11.10646129  48.54202058 -29.0508548  -18.35900048]; avg score: -2.493573995684475\n",
      "learning timestep: 338000\n",
      "Policy Loss: 0.08198925107717514\n",
      "Value Loss: 0.6441489458084106\n",
      "Entropy: -3.0563230514526367\n",
      "KL Divergence: 10.029909133911133\n",
      "episode: [2458. 2502. 2511. 2535.]; total steps: 338000; episodes scores: [-50.77844709 -75.24487868 -47.5508293  -43.66445269]; avg score: -54.30965193984346\n",
      "Rendering episode 2460.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2460.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2460.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2460.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2460.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-43.6226432]\n",
      "episode: [2461. 2506. 2514. 2538.]; total steps: 339000; episodes scores: [-57.10229851 -64.76387378 -55.37715541 -55.40977091]; avg score: -58.163274653964216\n",
      "learning timestep: 340000\n",
      "Policy Loss: 0.29544559121131897\n",
      "Value Loss: 1.488443374633789\n",
      "Entropy: -3.0915944576263428\n",
      "KL Divergence: 11.486565589904785\n",
      "episode: [2463. 2509. 2518. 2541.]; total steps: 340000; episodes scores: [ -9.59583963 -64.6984153  -63.07515178 -72.84296165]; avg score: -52.55309208711314\n",
      "episode: [2465. 2511. 2521. 2542.]; total steps: 341000; episodes scores: [-19.2322224    1.71425007 -51.90853786 -21.05560147]; avg score: -22.620527914272134\n",
      "learning timestep: 342000\n",
      "Policy Loss: -0.2786182463169098\n",
      "Value Loss: 1.2228091955184937\n",
      "Entropy: -3.344200611114502\n",
      "KL Divergence: 12.073022842407227\n",
      "episode: [2468. 2513. 2523. 2544.]; total steps: 342000; episodes scores: [-63.65521686 -56.84610577 -16.46137831 -55.72559858]; avg score: -48.172074879179846\n",
      "episode: [2470. 2513. 2525. 2547.]; total steps: 343000; episodes scores: [  8.58228664 -56.84610577 -48.28334642   3.26902983]; avg score: -23.319533930717174\n",
      "learning timestep: 344000\n",
      "Policy Loss: 0.05248117819428444\n",
      "Value Loss: 0.7046366333961487\n",
      "Entropy: -3.3077640533447266\n",
      "KL Divergence: 10.412715911865234\n",
      "episode: [2473. 2514. 2528. 2549.]; total steps: 344000; episodes scores: [-55.57640439 193.10921917  -3.86492061 -28.01008658]; avg score: 26.414451899788936\n",
      "episode: [2476. 2515. 2532. 2552.]; total steps: 345000; episodes scores: [-38.6382597  174.70389624 -55.83087114 -48.52939566]; avg score: 7.926342435454545\n",
      "learning timestep: 346000\n",
      "Policy Loss: -0.09354635328054428\n",
      "Value Loss: 1.0723631381988525\n",
      "Entropy: -3.359431505203247\n",
      "KL Divergence: 14.826136589050293\n",
      "episode: [2479. 2517. 2535. 2555.]; total steps: 346000; episodes scores: [-59.84619469 -71.18591475 -55.39146545 -52.11617728]; avg score: -59.63493804280723\n",
      "Rendering episode 2480.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2480.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2480.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2480.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2480.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-62.56288185]\n",
      "episode: [2482. 2521. 2537. 2557.]; total steps: 347000; episodes scores: [-13.25914578 -59.62953338 -34.27898406  61.7124638 ]; avg score: -11.36379985415708\n",
      "learning timestep: 348000\n",
      "Policy Loss: 0.24781031906604767\n",
      "Value Loss: 1.0013399124145508\n",
      "Entropy: -3.1898794174194336\n",
      "KL Divergence: 12.388435363769531\n",
      "episode: [2484. 2525. 2540. 2560.]; total steps: 348000; episodes scores: [ -3.85324664 -51.87620591 -82.85671815 -35.95071217]; avg score: -43.63422071755787\n",
      "episode: [2487. 2529. 2543. 2563.]; total steps: 349000; episodes scores: [-30.63858818 -84.63824743 -64.56510516 -15.42273523]; avg score: -48.81616900067156\n",
      "learning timestep: 350000\n",
      "Policy Loss: -0.17010559141635895\n",
      "Value Loss: 0.7484577894210815\n",
      "Entropy: -3.276034355163574\n",
      "KL Divergence: 12.590240478515625\n",
      "episode: [2491. 2531. 2545. 2567.]; total steps: 350000; episodes scores: [-66.73425546  18.35804141 -52.84006291 -52.88762267]; avg score: -38.525974904147255\n",
      "episode: [2495. 2534. 2549. 2569.]; total steps: 351000; episodes scores: [-31.78262211 -74.54235138 -33.10751459 -28.09217665]; avg score: -41.881166183718975\n",
      "learning timestep: 352000\n",
      "Policy Loss: 0.027178779244422913\n",
      "Value Loss: 0.78947913646698\n",
      "Entropy: -3.174811840057373\n",
      "KL Divergence: 14.205377578735352\n",
      "episode: [2498. 2536. 2551. 2573.]; total steps: 352000; episodes scores: [-55.18740941 -60.82412301 -35.21384522 -54.60816339]; avg score: -51.45838525932235\n",
      "Rendering episode 2500.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2500.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2500.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2500.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-69.29492941]\n",
      "episode: [2500. 2541. 2555. 2577.]; total steps: 353000; episodes scores: [-60.78945708 -57.08479888 -41.92866866 -23.13480307]; avg score: -45.73443192509856\n",
      "learning timestep: 354000\n",
      "Policy Loss: -0.024492735043168068\n",
      "Value Loss: 1.7698166370391846\n",
      "Entropy: -3.269890069961548\n",
      "KL Divergence: 12.207923889160156\n",
      "episode: [2505. 2543. 2560. 2580.]; total steps: 354000; episodes scores: [ -58.51777571  -19.37115101 -103.75361439  -31.06165824]; avg score: -53.17604983633288\n",
      "episode: [2508. 2546. 2562. 2582.]; total steps: 355000; episodes scores: [-49.25044433 -34.0912821   10.93252464  14.57885759]; avg score: -14.457586051921165\n",
      "learning timestep: 356000\n",
      "Policy Loss: -0.09200388938188553\n",
      "Value Loss: 1.168527603149414\n",
      "Entropy: -3.193516254425049\n",
      "KL Divergence: 11.165458679199219\n",
      "episode: [2510. 2548. 2564. 2586.]; total steps: 356000; episodes scores: [-39.93006623  38.5209777  -12.74250997 -71.29524872]; avg score: -21.361711806166113\n",
      "episode: [2512. 2552. 2568. 2589.]; total steps: 357000; episodes scores: [ 12.04138069 -11.96239113 -30.24478391 -53.96092686]; avg score: -21.03168030352829\n",
      "learning timestep: 358000\n",
      "Policy Loss: -0.32928627729415894\n",
      "Value Loss: 1.0138647556304932\n",
      "Entropy: -3.1363296508789062\n",
      "KL Divergence: 11.384138107299805\n",
      "episode: [2514. 2554. 2572. 2592.]; total steps: 358000; episodes scores: [-67.49810143  -4.75095477 -48.36414799 -66.17368146]; avg score: -46.696721413535855\n",
      "episode: [2516. 2555. 2572. 2595.]; total steps: 359000; episodes scores: [ 81.79956297 103.01992774 -48.36414799 -83.07537585]; avg score: 13.344991717527343\n",
      "learning timestep: 360000\n",
      "Policy Loss: 0.1059906929731369\n",
      "Value Loss: 0.6467781662940979\n",
      "Entropy: -3.1580076217651367\n",
      "KL Divergence: 10.912269592285156\n",
      "episode: [2518. 2556. 2574. 2597.]; total steps: 360000; episodes scores: [-41.88916068 119.62995286 -10.77943785  28.21614638]; avg score: 23.794375177443655\n",
      "Rendering episode 2520.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2520.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2520.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2520.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2520.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [32.11403916]\n",
      "episode: [2521. 2558. 2576. 2598.]; total steps: 361000; episodes scores: [-67.49279915 -78.84193198 -45.18742585  80.04214312]; avg score: -27.870003465597676\n",
      "learning timestep: 362000\n",
      "Policy Loss: -0.24173352122306824\n",
      "Value Loss: 1.1760848760604858\n",
      "Entropy: -3.2043235301971436\n",
      "KL Divergence: 13.411857604980469\n",
      "episode: [2524. 2560. 2579. 2599.]; total steps: 362000; episodes scores: [-55.96409053 -24.77266356 -67.55017342 125.32727303]; avg score: -5.739913621715466\n",
      "episode: [2528. 2564. 2582. 2600.]; total steps: 363000; episodes scores: [  3.69021837 -20.54608541 -50.71899585  65.69193041]; avg score: -0.4707331202237697\n",
      "learning timestep: 364000\n",
      "Policy Loss: -0.0803474560379982\n",
      "Value Loss: 0.5217248201370239\n",
      "Entropy: -3.316645622253418\n",
      "KL Divergence: 11.18496322631836\n",
      "episode: [2530. 2564. 2584. 2604.]; total steps: 364000; episodes scores: [-32.75046696 -20.54608541 -71.66578052 -36.19378421]; avg score: -40.28902927442299\n",
      "episode: [2534. 2566. 2585. 2606.]; total steps: 365000; episodes scores: [-40.16678196  56.22934486  88.5621561  -55.1632516 ]; avg score: 12.365366851182799\n",
      "learning timestep: 366000\n",
      "Policy Loss: -0.125189870595932\n",
      "Value Loss: 1.239516258239746\n",
      "Entropy: -3.303464889526367\n",
      "KL Divergence: 14.411967277526855\n",
      "episode: [2536. 2567. 2588. 2609.]; total steps: 366000; episodes scores: [-61.57240147 -23.53265565 -47.67575186 -30.30818876]; avg score: -40.77224943541221\n",
      "Rendering episode 2540.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2540.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2540.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2540.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2540.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-11.81835005]\n",
      "episode: [2540. 2569. 2591. 2612.]; total steps: 367000; episodes scores: [ -52.69992979   51.66478998 -103.52507932  -50.60667507]; avg score: -38.79172355042199\n",
      "learning timestep: 368000\n",
      "Policy Loss: -0.013600155711174011\n",
      "Value Loss: 1.0306339263916016\n",
      "Entropy: -3.2402608394622803\n",
      "KL Divergence: 14.28085708618164\n",
      "episode: [2542. 2571. 2592. 2616.]; total steps: 368000; episodes scores: [ 39.57598576 -78.42834943  81.22495973 -49.33138328]; avg score: -1.7396968042457424\n",
      "episode: [2544. 2574. 2594. 2618.]; total steps: 369000; episodes scores: [-50.73851136 -35.62918555 -51.61649949  57.87569191]; avg score: -20.027126120799807\n",
      "learning timestep: 370000\n",
      "Policy Loss: -0.11712923645973206\n",
      "Value Loss: 1.305424690246582\n",
      "Entropy: -3.451138734817505\n",
      "KL Divergence: 13.339993476867676\n",
      "episode: [2547. 2577. 2596. 2619.]; total steps: 370000; episodes scores: [-57.42520965 -26.0257983  -50.19974153  62.49677558]; avg score: -17.788493476402195\n",
      "episode: [2550. 2580. 2597. 2621.]; total steps: 371000; episodes scores: [-49.94227638 -54.94098054 299.34084049  63.61544728]; avg score: 64.51825771284993\n",
      "learning timestep: 372000\n",
      "Policy Loss: 0.025544123724102974\n",
      "Value Loss: 1.0170462131500244\n",
      "Entropy: -3.426805257797241\n",
      "KL Divergence: 13.016521453857422\n",
      "episode: [2552. 2584. 2597. 2624.]; total steps: 372000; episodes scores: [ -4.92889961 -46.92994718 299.34084049 -38.7595104 ]; avg score: 52.180620826157764\n",
      "episode: [2553. 2588. 2600. 2627.]; total steps: 373000; episodes scores: [ 78.33623352 -74.7406377  -69.69447053  35.70306271]; avg score: -7.598952998759085\n",
      "learning timestep: 374000\n",
      "Policy Loss: -0.1544950008392334\n",
      "Value Loss: 1.1053647994995117\n",
      "Entropy: -3.3494348526000977\n",
      "KL Divergence: 15.082141876220703\n",
      "episode: [2556. 2590. 2603. 2631.]; total steps: 374000; episodes scores: [ -1.55467531 -14.96221528 -51.88252757  -7.33295594]; avg score: -18.933093525063686\n",
      "episode: [2559. 2592. 2607. 2631.]; total steps: 375000; episodes scores: [-53.77515117 -51.09695984 -71.69834139  -7.33295594]; avg score: -45.975852087475374\n",
      "learning timestep: 376000\n",
      "Policy Loss: -0.106848806142807\n",
      "Value Loss: 0.8731351494789124\n",
      "Entropy: -3.389535427093506\n",
      "KL Divergence: 13.546993255615234\n",
      "episode: [2559. 2594. 2609. 2634.]; total steps: 376000; episodes scores: [-53.77515117 -33.34654025 -11.20912175 -20.33301833]; avg score: -29.665957876750422\n",
      "Rendering episode 2560.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2560.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2560.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2560.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2560.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [44.58406401]\n",
      "episode: [2561. 2595. 2612. 2636.]; total steps: 377000; episodes scores: [ 31.85384218 174.71054969 -12.81330865  71.41184179]; avg score: 66.29073125538028\n",
      "learning timestep: 378000\n",
      "Policy Loss: 0.052685659378767014\n",
      "Value Loss: 0.5351332426071167\n",
      "Entropy: -3.453073024749756\n",
      "KL Divergence: 15.132438659667969\n",
      "episode: [2562. 2595. 2613. 2637.]; total steps: 378000; episodes scores: [133.14368578 174.71054969  90.9841532   50.01337173]; avg score: 112.21294009788451\n",
      "episode: [2564. 2598. 2614. 2640.]; total steps: 379000; episodes scores: [-29.59037016 -14.4017632  101.61891958 -34.2482057 ]; avg score: 5.844645129902769\n",
      "learning timestep: 380000\n",
      "Policy Loss: -0.06932010501623154\n",
      "Value Loss: 1.05727219581604\n",
      "Entropy: -3.4844627380371094\n",
      "KL Divergence: 12.982808113098145\n",
      "episode: [2566. 2599. 2616. 2641.]; total steps: 380000; episodes scores: [-47.87566989  51.70749634 -59.71305334 109.14982137]; avg score: 13.317148622557044\n",
      "episode: [2568. 2600. 2617. 2642.]; total steps: 381000; episodes scores: [ 20.15758362  59.90903179  14.41202528 114.8919698 ]; avg score: 52.34265262157767\n",
      "learning timestep: 382000\n",
      "Policy Loss: 0.07288047671318054\n",
      "Value Loss: 0.641984760761261\n",
      "Entropy: -3.4413013458251953\n",
      "KL Divergence: 13.128856658935547\n",
      "episode: [2569. 2602. 2618. 2644.]; total steps: 382000; episodes scores: [-27.88298459 -39.96152201 167.59279624  28.70236646]; avg score: 32.11266402751877\n",
      "episode: [2571. 2605. 2620. 2646.]; total steps: 383000; episodes scores: [ 86.5083169  -58.66844132 -13.88881012 -34.89740637]; avg score: -5.236585225529506\n",
      "learning timestep: 384000\n",
      "Policy Loss: -0.05134063586592674\n",
      "Value Loss: 1.3950221538543701\n",
      "Entropy: -3.3502979278564453\n",
      "KL Divergence: 13.2831392288208\n",
      "episode: [2573. 2607. 2621. 2649.]; total steps: 384000; episodes scores: [-23.97872807  31.74879475 123.2961096  -45.58080347]; avg score: 21.37134320361201\n",
      "episode: [2575. 2608. 2624. 2649.]; total steps: 385000; episodes scores: [-51.58787553 -78.77147914 -54.74746468 -45.58080347]; avg score: -57.671905705578936\n",
      "learning timestep: 386000\n",
      "Policy Loss: -0.04034190624952316\n",
      "Value Loss: 1.1811437606811523\n",
      "Entropy: -3.4082846641540527\n",
      "KL Divergence: 11.87090015411377\n",
      "episode: [2577. 2611. 2626. 2652.]; total steps: 386000; episodes scores: [ -5.90257142 -45.54040898 -34.99969641   6.16300528]; avg score: -20.069917882869795\n",
      "Rendering episode 2580.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2580.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2580.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2580.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2580.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-66.75222725]\n",
      "episode: [2581. 2612. 2630. 2654.]; total steps: 387000; episodes scores: [ -8.0658952   92.1168838  -30.12886202  56.79217705]; avg score: 27.678575905374153\n",
      "learning timestep: 388000\n",
      "Policy Loss: -0.14878247678279877\n",
      "Value Loss: 1.2924576997756958\n",
      "Entropy: -3.404890537261963\n",
      "KL Divergence: 15.000345230102539\n",
      "episode: [2584. 2613. 2633. 2657.]; total steps: 388000; episodes scores: [-25.1464454  303.98634743 -52.89634886 -15.83777558]; avg score: 52.526444395878286\n",
      "episode: [2585. 2615. 2633. 2660.]; total steps: 389000; episodes scores: [ -9.6534879  -22.58083264 -52.89634886   3.17079094]; avg score: -20.489969615573777\n",
      "learning timestep: 390000\n",
      "Policy Loss: 0.02623763680458069\n",
      "Value Loss: 1.9006186723709106\n",
      "Entropy: -3.4078657627105713\n",
      "KL Divergence: 13.003005981445312\n",
      "episode: [2587. 2617. 2634. 2661.]; total steps: 390000; episodes scores: [-39.89266604 -47.06304087 304.8116667  -55.10888373]; avg score: 40.68676901250646\n",
      "episode: [2590. 2619. 2638. 2664.]; total steps: 391000; episodes scores: [-43.16388746 -24.16998699 -62.37615936 -47.05866646]; avg score: -44.1921750656129\n",
      "learning timestep: 392000\n",
      "Policy Loss: 0.13758434355258942\n",
      "Value Loss: 2.2984633445739746\n",
      "Entropy: -3.5940237045288086\n",
      "KL Divergence: 15.431715965270996\n",
      "episode: [2592. 2622. 2641. 2667.]; total steps: 392000; episodes scores: [-14.72457879  -9.46232608 -91.64853505 -41.02243957]; avg score: -39.21446987465456\n",
      "episode: [2594. 2625. 2644. 2669.]; total steps: 393000; episodes scores: [-53.07817224 -26.4990494  -67.60739737  43.61351861]; avg score: -25.892775099500724\n",
      "learning timestep: 394000\n",
      "Policy Loss: 0.15387924015522003\n",
      "Value Loss: 2.2930922508239746\n",
      "Entropy: -3.5073628425598145\n",
      "KL Divergence: 13.331724166870117\n",
      "episode: [2598. 2627. 2647. 2673.]; total steps: 394000; episodes scores: [-11.43243695 -21.15209202 -13.63008155 -70.52872162]; avg score: -29.18583303601058\n",
      "Rendering episode 2600.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2600.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2600.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2600.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-8.63212497]\n",
      "episode: [2603. 2630. 2652. 2676.]; total steps: 395000; episodes scores: [-45.63181816 -27.82477829 -67.00425178 -64.04590425]; avg score: -51.12668812012176\n",
      "learning timestep: 396000\n",
      "Policy Loss: 0.028653334826231003\n",
      "Value Loss: 2.0674045085906982\n",
      "Entropy: -3.4682085514068604\n",
      "KL Divergence: 14.796457290649414\n",
      "episode: [2607. 2632. 2656. 2679.]; total steps: 396000; episodes scores: [-47.64555211  63.84595147 -62.30902752 -48.48789927]; avg score: -23.649131855470863\n",
      "episode: [2611. 2635. 2657. 2683.]; total steps: 397000; episodes scores: [-61.61535864 -59.55706254 -71.44370997 -51.8009994 ]; avg score: -61.10428263525989\n",
      "learning timestep: 398000\n",
      "Policy Loss: 0.14180020987987518\n",
      "Value Loss: 1.1589481830596924\n",
      "Entropy: -3.343235731124878\n",
      "KL Divergence: 11.502189636230469\n",
      "episode: [2614. 2639. 2660. 2685.]; total steps: 398000; episodes scores: [-29.59092146 -45.32434684 -55.99656358 -44.78123459]; avg score: -43.92326661819252\n",
      "episode: [2618. 2641. 2664. 2688.]; total steps: 399000; episodes scores: [-46.14611249   3.73571875 -33.8901206  -22.9926305 ]; avg score: -24.823286212921158\n",
      "Rendering episode 2620.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2620.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2620.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2620.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2620.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-17.79854939]\n",
      "learning timestep: 400000\n",
      "Policy Loss: 0.15919536352157593\n",
      "Value Loss: 1.5213158130645752\n",
      "Entropy: -3.3795340061187744\n",
      "KL Divergence: 12.479681015014648\n",
      "episode: [2621. 2643. 2668. 2692.]; total steps: 400000; episodes scores: [-33.64673378 -34.81581121 -85.72100692 -22.49628563]; avg score: -44.1699593849221\n",
      "episode: [2624. 2645. 2672. 2693.]; total steps: 401000; episodes scores: [-63.87059696 -12.11386156 -54.70286189 -23.00214795]; avg score: -38.422367089490415\n",
      "learning timestep: 402000\n",
      "Policy Loss: 0.0483916774392128\n",
      "Value Loss: 1.0750854015350342\n",
      "Entropy: -3.495561122894287\n",
      "KL Divergence: 16.060977935791016\n",
      "episode: [2629. 2649. 2677. 2696.]; total steps: 402000; episodes scores: [-64.32606401 -38.71278728 -86.92433881 -74.6651144 ]; avg score: -66.15707612680117\n",
      "episode: [2633. 2649. 2679. 2699.]; total steps: 403000; episodes scores: [-16.38517371 -38.71278728  49.16088051  13.74255793]; avg score: 1.951369361129247\n",
      "learning timestep: 404000\n",
      "Policy Loss: -0.09864204376935959\n",
      "Value Loss: 1.2283625602722168\n",
      "Entropy: -3.4711358547210693\n",
      "KL Divergence: 13.526488304138184\n",
      "episode: [2635. 2652. 2681. 2703.]; total steps: 404000; episodes scores: [-44.511826   -18.8568225  -68.38609676 -72.04716484]; avg score: -50.95047752308707\n",
      "episode: [2639. 2656. 2683. 2705.]; total steps: 405000; episodes scores: [-107.6257704   -83.89048625  -43.26291933  -18.54622345]; avg score: -63.33134986002696\n",
      "Rendering episode 2640.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2640.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2640.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2640.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2640.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-63.16806631]\n",
      "learning timestep: 406000\n",
      "Policy Loss: 0.011574894189834595\n",
      "Value Loss: 1.709397554397583\n",
      "Entropy: -3.235288143157959\n",
      "KL Divergence: 14.87621021270752\n",
      "episode: [2641. 2658. 2686. 2708.]; total steps: 406000; episodes scores: [ 77.14950794   2.00916375 -71.73669988 -65.26981649]; avg score: -14.46196116842669\n",
      "episode: [2644. 2660. 2689. 2710.]; total steps: 407000; episodes scores: [-53.17320991 -43.41962643 -19.15258839 -43.71035237]; avg score: -39.86394427218692\n",
      "learning timestep: 408000\n",
      "Policy Loss: -0.11747930943965912\n",
      "Value Loss: 2.322864532470703\n",
      "Entropy: -3.5075740814208984\n",
      "KL Divergence: 15.330377578735352\n",
      "episode: [2649. 2664. 2690. 2713.]; total steps: 408000; episodes scores: [ -52.16261316  -58.79354867   -5.22300941 -103.34418959]; avg score: -54.88084020681913\n",
      "episode: [2650. 2667. 2692. 2715.]; total steps: 409000; episodes scores: [ 42.11134844 -63.42974376 -83.51216624  53.81011295]; avg score: -12.755112151986893\n",
      "learning timestep: 410000\n",
      "Policy Loss: 0.03688378259539604\n",
      "Value Loss: 1.532659649848938\n",
      "Entropy: -3.573390245437622\n",
      "KL Divergence: 15.311605453491211\n",
      "episode: [2651. 2670. 2695. 2718.]; total steps: 410000; episodes scores: [187.71403119 -21.98447512  12.2755019    8.56254508]; avg score: 46.64190076428668\n",
      "episode: [2655. 2672. 2695. 2720.]; total steps: 411000; episodes scores: [ 42.36511804 -51.26494561  12.2755019   73.24108662]; avg score: 19.154190238325462\n",
      "learning timestep: 412000\n",
      "Policy Loss: 0.008176589384675026\n",
      "Value Loss: 0.704432487487793\n",
      "Entropy: -3.4565417766571045\n",
      "KL Divergence: 13.45431900024414\n",
      "episode: [2657. 2673. 2697. 2722.]; total steps: 412000; episodes scores: [ 41.82073443 -19.85814385  35.12603121  13.27508813]; avg score: 17.590927480923778\n",
      "Rendering episode 2660.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2660.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2660.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2660.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2660.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-11.75007229]\n",
      "episode: [2660. 2676. 2699. 2724.]; total steps: 413000; episodes scores: [-28.72207604 -59.34579788 -35.60492216 -26.41243996]; avg score: -37.52130901084948\n",
      "learning timestep: 414000\n",
      "Policy Loss: 0.18985405564308167\n",
      "Value Loss: 1.4947031736373901\n",
      "Entropy: -3.454803943634033\n",
      "KL Divergence: 14.561664581298828\n",
      "episode: [2663. 2678. 2701. 2725.]; total steps: 414000; episodes scores: [ -5.32057772  26.76020998 -26.08830171  64.7612847 ]; avg score: 15.0281538132766\n",
      "episode: [2666. 2680. 2705. 2728.]; total steps: 415000; episodes scores: [-42.44604152 -52.9315312  -17.87026533 -36.68540192]; avg score: -37.483309993006586\n",
      "learning timestep: 416000\n",
      "Policy Loss: 0.1210094541311264\n",
      "Value Loss: 1.6184483766555786\n",
      "Entropy: -3.6024742126464844\n",
      "KL Divergence: 16.136877059936523\n",
      "episode: [2669. 2682. 2706. 2731.]; total steps: 416000; episodes scores: [-12.39915819   1.84716508 125.24243122 -89.9501289 ]; avg score: 6.185077301393147\n",
      "episode: [2671. 2684. 2708. 2732.]; total steps: 417000; episodes scores: [-63.96338767   2.69439408 -28.87478579  91.52928651]; avg score: 0.34637678283376516\n",
      "learning timestep: 418000\n",
      "Policy Loss: -0.07251790910959244\n",
      "Value Loss: 1.2348458766937256\n",
      "Entropy: -3.573601722717285\n",
      "KL Divergence: 14.191450119018555\n",
      "episode: [2674. 2686. 2710. 2736.]; total steps: 418000; episodes scores: [ -43.11984137   11.01958828  -72.53645262 -107.79066065]; avg score: -53.106841588560634\n",
      "episode: [2675. 2688. 2712. 2739.]; total steps: 419000; episodes scores: [ 26.02986121  12.17414959  65.26106843 -61.81032099]; avg score: 10.413689561072156\n",
      "learning timestep: 420000\n",
      "Policy Loss: -0.007160126697272062\n",
      "Value Loss: 1.0255929231643677\n",
      "Entropy: -3.3872838020324707\n",
      "KL Divergence: 14.892679214477539\n",
      "episode: [2678. 2690. 2713. 2742.]; total steps: 420000; episodes scores: [-39.62739516  30.19688395  72.41274152 -17.73947572]; avg score: 11.31068864822466\n",
      "Rendering episode 2680.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2680.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2680.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2680.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2680.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-9.28350882]\n",
      "episode: [2682. 2694. 2718. 2745.]; total steps: 421000; episodes scores: [-76.7607352  -34.51081114 -85.16374916  17.40526593]; avg score: -44.75750739101803\n",
      "learning timestep: 422000\n",
      "Policy Loss: -0.2795529067516327\n",
      "Value Loss: 2.236124038696289\n",
      "Entropy: -3.4742581844329834\n",
      "KL Divergence: 14.40339183807373\n",
      "episode: [2684. 2697. 2721. 2747.]; total steps: 422000; episodes scores: [-41.63721167  10.53396867 -79.32386171 -15.99155655]; avg score: -31.60466531341917\n",
      "episode: [2686. 2698. 2725. 2749.]; total steps: 423000; episodes scores: [-68.8311908   95.26373901 -37.4513579   21.65981495]; avg score: 2.660251316190797\n",
      "learning timestep: 424000\n",
      "Policy Loss: -0.012925430200994015\n",
      "Value Loss: 1.6728270053863525\n",
      "Entropy: -3.5110344886779785\n",
      "KL Divergence: 15.269951820373535\n",
      "episode: [2689. 2700. 2727. 2752.]; total steps: 424000; episodes scores: [-57.57363251 -37.9457287   55.08684665 -58.77308706]; avg score: -24.80140040670131\n",
      "episode: [2689. 2702. 2730. 2754.]; total steps: 425000; episodes scores: [-57.57363251 -44.59702473 -82.84108642  38.21367027]; avg score: -36.699518348897605\n",
      "learning timestep: 426000\n",
      "Policy Loss: -0.10808815062046051\n",
      "Value Loss: 1.8068509101867676\n",
      "Entropy: -3.561492919921875\n",
      "KL Divergence: 11.288225173950195\n",
      "episode: [2692. 2704. 2733. 2756.]; total steps: 426000; episodes scores: [-74.98975011 -54.16598349 -46.66191394 -47.83048523]; avg score: -55.912033192358585\n",
      "episode: [2694. 2705. 2734. 2757.]; total steps: 427000; episodes scores: [-46.31000611  54.42447711 112.82348246  69.56782395]; avg score: 47.626444353644466\n",
      "learning timestep: 428000\n",
      "Policy Loss: 0.08837656676769257\n",
      "Value Loss: 1.5072438716888428\n",
      "Entropy: -3.503035068511963\n",
      "KL Divergence: 13.439411163330078\n",
      "episode: [2696. 2707. 2735. 2758.]; total steps: 428000; episodes scores: [ 10.84113347   5.77277528  -1.89749036 307.42643076]; avg score: 80.53571228824633\n",
      "episode: [2699. 2710. 2737. 2760.]; total steps: 429000; episodes scores: [-100.89557492   54.57231039  -34.99002693   66.57570763]; avg score: -3.6843959580292065\n",
      "Rendering episode 2700.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2700.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2700.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2700.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [39.25155677]\n",
      "learning timestep: 430000\n",
      "Policy Loss: -0.04992292448878288\n",
      "Value Loss: 1.3637851476669312\n",
      "Entropy: -3.4976935386657715\n",
      "KL Divergence: 12.395109176635742\n",
      "episode: [2703. 2711. 2739. 2761.]; total steps: 430000; episodes scores: [-28.17419146 128.69929175 -69.15323088   6.64923058]; avg score: 9.505275000281472\n",
      "episode: [2704. 2712. 2740. 2762.]; total steps: 431000; episodes scores: [  6.97674724  80.71857338 131.36719101  62.47962946]; avg score: 70.38553527172131\n",
      "learning timestep: 432000\n",
      "Policy Loss: -0.05902840569615364\n",
      "Value Loss: 0.5730433464050293\n",
      "Entropy: -3.6217241287231445\n",
      "KL Divergence: 15.14273738861084\n",
      "episode: [2706. 2713. 2741. 2765.]; total steps: 432000; episodes scores: [-56.22501005  31.82648344 109.62417712 -48.68345496]; avg score: 9.135548888828144\n",
      "episode: [2707. 2716. 2744. 2766.]; total steps: 433000; episodes scores: [307.4335836  -83.63244636 -49.0927201  310.33685662]; avg score: 121.26131844158317\n",
      "learning timestep: 434000\n",
      "Policy Loss: -0.07387859374284744\n",
      "Value Loss: 1.5852123498916626\n",
      "Entropy: -3.6193480491638184\n",
      "KL Divergence: 13.786996841430664\n",
      "episode: [2709. 2718. 2746. 2768.]; total steps: 434000; episodes scores: [-31.69934386 -28.1959186   90.98186112  66.48607918]; avg score: 24.393169459878305\n",
      "episode: [2709. 2721. 2746. 2770.]; total steps: 435000; episodes scores: [-31.69934386   0.58403795  90.98186112  97.01661893]; avg score: 39.220793536364056\n",
      "learning timestep: 436000\n",
      "Policy Loss: 0.06021454185247421\n",
      "Value Loss: 2.039607524871826\n",
      "Entropy: -3.4512133598327637\n",
      "KL Divergence: 15.693159103393555\n",
      "episode: [2710. 2722. 2747. 2770.]; total steps: 436000; episodes scores: [307.32752241 -30.30876928 306.11359488  97.01661893]; avg score: 170.0372417345684\n",
      "episode: [2713. 2723. 2748. 2772.]; total steps: 437000; episodes scores: [ -7.42273683 121.72269145 307.50287279 -12.44118409]; avg score: 102.34041083402302\n",
      "learning timestep: 438000\n",
      "Policy Loss: -0.13969241082668304\n",
      "Value Loss: 1.816496729850769\n",
      "Entropy: -3.734447479248047\n",
      "KL Divergence: 16.384967803955078\n",
      "episode: [2715. 2725. 2749. 2773.]; total steps: 438000; episodes scores: [-17.40350168  -4.72138568 306.27662134 166.33390685]; avg score: 112.62141020771199\n",
      "episode: [2716. 2727. 2751. 2774.]; total steps: 439000; episodes scores: [135.91810392   4.85550637 -34.06035724 120.56208883]; avg score: 56.81883546925149\n",
      "learning timestep: 440000\n",
      "Policy Loss: -0.16889719665050507\n",
      "Value Loss: 0.8961768746376038\n",
      "Entropy: -3.535193920135498\n",
      "KL Divergence: 16.4444580078125\n",
      "episode: [2717. 2728. 2751. 2775.]; total steps: 440000; episodes scores: [ 48.23644562 -13.14702814 -34.06035724 163.00338842]; avg score: 41.008112163593296\n",
      "episode: [2718. 2729. 2752. 2776.]; total steps: 441000; episodes scores: [306.24842064 304.07184884 307.35952219 306.21671592]; avg score: 305.9741268981903\n",
      "Rendering episode 2720.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2720.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2720.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2720.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2720.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [81.14354342]\n",
      "learning timestep: 442000\n",
      "Policy Loss: 0.10499094426631927\n",
      "Value Loss: 5.3012261390686035\n",
      "Entropy: -3.569121837615967\n",
      "KL Divergence: 13.782392501831055\n",
      "episode: [2720. 2730. 2753. 2776.]; total steps: 442000; episodes scores: [-35.37519776 144.38336103 301.36847014 306.21671592]; avg score: 179.14833733404504\n",
      "episode: [2721. 2732. 2754. 2778.]; total steps: 443000; episodes scores: [19.45996765  2.55196947 23.16112763 68.47211009]; avg score: 28.4112937098186\n",
      "learning timestep: 444000\n",
      "Policy Loss: 0.08743393421173096\n",
      "Value Loss: 1.3562724590301514\n",
      "Entropy: -3.561124324798584\n",
      "KL Divergence: 16.396726608276367\n",
      "episode: [2722. 2734. 2755. 2779.]; total steps: 444000; episodes scores: [303.76390067  28.31813666 304.63925215 -60.51333132]; avg score: 144.05198953829742\n",
      "episode: [2723. 2735. 2756. 2780.]; total steps: 445000; episodes scores: [ 62.41836247 305.138253   306.19133525 304.50019458]; avg score: 244.5620363239699\n",
      "learning timestep: 446000\n",
      "Policy Loss: -0.07414513826370239\n",
      "Value Loss: 0.588457465171814\n",
      "Entropy: -3.531190872192383\n",
      "KL Divergence: 15.620128631591797\n",
      "episode: [2724. 2735. 2758. 2783.]; total steps: 446000; episodes scores: [307.79142758 305.138253   -86.02961249 -60.39156164]; avg score: 116.62712661389499\n",
      "episode: [2726. 2737. 2759. 2784.]; total steps: 447000; episodes scores: [ 49.562738     5.61542979  21.95485847 306.7665503 ]; avg score: 95.97489413960548\n",
      "learning timestep: 448000\n",
      "Policy Loss: -0.05126680061221123\n",
      "Value Loss: 1.116402268409729\n",
      "Entropy: -3.4684956073760986\n",
      "KL Divergence: 17.256349563598633\n",
      "episode: [2728. 2740. 2760. 2784.]; total steps: 448000; episodes scores: [ 45.43326843  19.23376629 173.28516836 306.7665503 ]; avg score: 136.17968834666902\n",
      "episode: [2728. 2741. 2761. 2786.]; total steps: 449000; episodes scores: [ 45.43326843  95.52867738 306.78820794  -0.63749743]; avg score: 111.77816407743379\n",
      "learning timestep: 450000\n",
      "Policy Loss: 0.12545815110206604\n",
      "Value Loss: 7.136749744415283\n",
      "Entropy: -3.56364369392395\n",
      "KL Divergence: 14.0336275100708\n",
      "episode: [2730. 2741. 2761. 2787.]; total steps: 450000; episodes scores: [-27.39495393  95.52867738 306.78820794 158.857226  ]; avg score: 133.44478934635725\n",
      "episode: [2731. 2744. 2762. 2788.]; total steps: 451000; episodes scores: [ 82.42476759 -30.99349045 306.53158439 -29.17860501]; avg score: 82.1960641313427\n",
      "learning timestep: 452000\n",
      "Policy Loss: -0.35853520035743713\n",
      "Value Loss: 1.8281452655792236\n",
      "Entropy: -3.632932186126709\n",
      "KL Divergence: 15.432308197021484\n",
      "episode: [2733. 2746. 2765. 2789.]; total steps: 452000; episodes scores: [-52.21833356  67.14869316  31.67625494  65.7796946 ]; avg score: 28.09657728356717\n",
      "episode: [2735. 2746. 2768. 2791.]; total steps: 453000; episodes scores: [-62.94969503  67.14869316 -49.66671581 -85.72206888]; avg score: -32.797446641326445\n",
      "learning timestep: 454000\n",
      "Policy Loss: -0.034365952014923096\n",
      "Value Loss: 3.929525375366211\n",
      "Entropy: -3.579133987426758\n",
      "KL Divergence: 15.932069778442383\n",
      "episode: [2737. 2747. 2770. 2793.]; total steps: 454000; episodes scores: [ 97.94341693 306.60653594  66.04578942 -21.1593507 ]; avg score: 112.35909789793536\n",
      "episode: [2737. 2749. 2772. 2795.]; total steps: 455000; episodes scores: [ 97.94341693  -7.78613574 -29.38863152  -4.84530231]; avg score: 13.980836840658483\n",
      "learning timestep: 456000\n",
      "Policy Loss: 0.09291626513004303\n",
      "Value Loss: 1.5125436782836914\n",
      "Entropy: -3.4401707649230957\n",
      "KL Divergence: 16.335065841674805\n",
      "episode: [2738. 2751. 2772. 2796.]; total steps: 456000; episodes scores: [ 307.28537506 -104.31083153  -29.38863152  113.92821745]; avg score: 71.87853236398625\n",
      "Rendering episode 2740.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2740.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2740.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2740.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2740.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [154.89222749]\n",
      "episode: [2741. 2753. 2775. 2798.]; total steps: 457000; episodes scores: [-53.762463   -17.52785831 -59.65990804 -53.85173606]; avg score: -46.20049135254745\n",
      "learning timestep: 458000\n",
      "Policy Loss: -0.15336903929710388\n",
      "Value Loss: 0.5959327816963196\n",
      "Entropy: -3.3630599975585938\n",
      "KL Divergence: 15.046255111694336\n",
      "episode: [2742. 2755. 2776. 2799.]; total steps: 458000; episodes scores: [ 77.896112   -27.20072835  97.53368133 309.12002724]; avg score: 114.33727305466621\n",
      "episode: [2743. 2757. 2779. 2801.]; total steps: 459000; episodes scores: [175.52026902 -60.67995614 -76.44613307  87.56973166]; avg score: 31.49097786807637\n",
      "learning timestep: 460000\n",
      "Policy Loss: -0.03973975032567978\n",
      "Value Loss: 0.8161295652389526\n",
      "Entropy: -3.5452356338500977\n",
      "KL Divergence: 13.942085266113281\n",
      "episode: [2745. 2760. 2780. 2806.]; total steps: 460000; episodes scores: [-19.70415924   6.0654607  106.659077   -34.93055847]; avg score: 14.522454997272831\n",
      "episode: [2745. 2762. 2781. 2806.]; total steps: 461000; episodes scores: [-19.70415924   7.6788106  306.93022253 -34.93055847]; avg score: 64.99357885526265\n",
      "learning timestep: 462000\n",
      "Policy Loss: -0.2437690794467926\n",
      "Value Loss: 1.1341943740844727\n",
      "Entropy: -3.5617527961730957\n",
      "KL Divergence: 14.12470817565918\n",
      "episode: [2747. 2762. 2782. 2809.]; total steps: 462000; episodes scores: [ 59.91915505   7.6788106  -79.04321524 -34.8224377 ]; avg score: -11.566921824126986\n",
      "episode: [2748. 2763. 2783. 2810.]; total steps: 463000; episodes scores: [ 65.09707347 187.11680444 307.03528713 144.20651799]; avg score: 175.86392075888168\n",
      "learning timestep: 464000\n",
      "Policy Loss: -0.16904717683792114\n",
      "Value Loss: 1.2856824398040771\n",
      "Entropy: -3.51227068901062\n",
      "KL Divergence: 15.758199691772461\n",
      "episode: [2748. 2765. 2784. 2810.]; total steps: 464000; episodes scores: [ 65.09707347 -80.90639623 307.48371149 144.20651799]; avg score: 108.9702266814241\n",
      "episode: [2750. 2768. 2784. 2811.]; total steps: 465000; episodes scores: [-1.65593513e-01 -8.04662059e+01  3.07483711e+02  3.08230068e+02]; avg score: 133.77049504336497\n",
      "learning timestep: 466000\n",
      "Policy Loss: -0.10373032838106155\n",
      "Value Loss: 1.1636426448822021\n",
      "Entropy: -3.545339584350586\n",
      "KL Divergence: 14.719934463500977\n",
      "episode: [2752. 2769. 2785. 2812.]; total steps: 466000; episodes scores: [-80.77962567  97.14789748 304.60892493 304.7122527 ]; avg score: 156.4223623614008\n",
      "episode: [2754. 2770. 2787. 2813.]; total steps: 467000; episodes scores: [-37.63087655 172.32244132 -80.35501249 175.52627468]; avg score: 57.46570674015159\n",
      "learning timestep: 468000\n",
      "Policy Loss: -0.2819021940231323\n",
      "Value Loss: 0.9776289463043213\n",
      "Entropy: -3.590620517730713\n",
      "KL Divergence: 16.722286224365234\n",
      "episode: [2755. 2771. 2788. 2813.]; total steps: 468000; episodes scores: [ 14.05101059 142.21732745 310.54648413 175.52627468]; avg score: 160.585274214269\n",
      "episode: [2756. 2772. 2789. 2814.]; total steps: 469000; episodes scores: [309.80040664  53.57380423 -22.71393467 307.26833846]; avg score: 161.98215366318902\n",
      "learning timestep: 470000\n",
      "Policy Loss: -0.17913055419921875\n",
      "Value Loss: 0.7816153764724731\n",
      "Entropy: -3.565728187561035\n",
      "KL Divergence: 16.04425048828125\n",
      "episode: [2757. 2774. 2790. 2815.]; total steps: 470000; episodes scores: [309.1500967  135.95412671 307.53504145 307.67241896]; avg score: 265.0779209548804\n",
      "episode: [2758. 2776. 2791. 2818.]; total steps: 471000; episodes scores: [180.08302599 -85.49541861  94.65516255 -75.18141284]; avg score: 28.51533927255691\n",
      "learning timestep: 472000\n",
      "Policy Loss: -0.10037913173437119\n",
      "Value Loss: 1.931288480758667\n",
      "Entropy: -3.663421630859375\n",
      "KL Divergence: 18.479747772216797\n",
      "episode: [2759. 2777. 2792. 2819.]; total steps: 472000; episodes scores: [172.47501068  -1.50191838 124.51418642 105.52251968]; avg score: 100.25244959955171\n",
      "Rendering episode 2760.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2760.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2760.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2760.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2760.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [72.44636628]\n",
      "episode: [2761. 2778. 2793. 2820.]; total steps: 473000; episodes scores: [-76.83954178 306.97178261 309.8576404  136.36007134]; avg score: 169.08748814265385\n",
      "learning timestep: 474000\n",
      "Policy Loss: -0.049503426998853683\n",
      "Value Loss: 1.6877238750457764\n",
      "Entropy: -3.421288013458252\n",
      "KL Divergence: 15.884344100952148\n",
      "episode: [2763. 2781. 2794. 2822.]; total steps: 474000; episodes scores: [ 46.4065712  -12.64033529   3.02689307 -44.66979747]; avg score: -1.9691671201935819\n",
      "episode: [2764. 2781. 2796. 2824.]; total steps: 475000; episodes scores: [307.44526029 -12.64033529  28.98855851  91.84064575]; avg score: 103.9085323150985\n",
      "learning timestep: 476000\n",
      "Policy Loss: -0.11571267247200012\n",
      "Value Loss: 1.176340103149414\n",
      "Entropy: -3.4385247230529785\n",
      "KL Divergence: 13.437826156616211\n",
      "episode: [2764. 2783. 2797. 2825.]; total steps: 476000; episodes scores: [307.44526029  35.09118161   1.05739914  -1.34044713]; avg score: 85.56334847796153\n",
      "episode: [2765. 2784. 2798. 2826.]; total steps: 477000; episodes scores: [308.9632421  151.74645455  64.44482912  50.38041412]; avg score: 143.88373497142496\n",
      "learning timestep: 478000\n",
      "Policy Loss: -0.0401943176984787\n",
      "Value Loss: 4.877444267272949\n",
      "Entropy: -3.389535903930664\n",
      "KL Divergence: 15.666343688964844\n",
      "episode: [2766. 2784. 2799. 2828.]; total steps: 478000; episodes scores: [305.22087812 151.74645455 129.33434757 -91.82553985]; avg score: 123.61903509827367\n",
      "episode: [2767. 2786. 2800. 2829.]; total steps: 479000; episodes scores: [113.36857738  92.53636666 162.00777884  25.32453164]; avg score: 98.30931363282428\n",
      "learning timestep: 480000\n",
      "Policy Loss: 0.1328863799571991\n",
      "Value Loss: 1.2242885828018188\n",
      "Entropy: -3.6318936347961426\n",
      "KL Divergence: 14.306278228759766\n",
      "episode: [2768. 2787. 2801. 2830.]; total steps: 480000; episodes scores: [ 33.42386786  -8.53614309  72.9578312  308.80190247]; avg score: 101.66186461059397\n",
      "episode: [2769. 2788. 2803. 2832.]; total steps: 481000; episodes scores: [308.12386119 306.6307091  -41.1684973  -46.69398753]; avg score: 131.7230213652761\n",
      "learning timestep: 482000\n",
      "Policy Loss: -0.08966580778360367\n",
      "Value Loss: 1.151580572128296\n",
      "Entropy: -3.574652671813965\n",
      "KL Divergence: 13.483427047729492\n",
      "episode: [2770. 2788. 2804. 2833.]; total steps: 482000; episodes scores: [145.06750469 306.6307091  309.78741975  62.86361384]; avg score: 206.08731184685038\n",
      "episode: [2773. 2791. 2805. 2835.]; total steps: 483000; episodes scores: [-26.47857447  30.71210137 -40.13937389  22.05849974]; avg score: -3.461836813924057\n",
      "learning timestep: 484000\n",
      "Policy Loss: -0.03004836104810238\n",
      "Value Loss: 4.03574800491333\n",
      "Entropy: -3.3625307083129883\n",
      "KL Divergence: 13.818462371826172\n",
      "episode: [2774. 2793. 2806. 2836.]; total steps: 484000; episodes scores: [ -4.38122768  20.42172883 308.57546729  -7.72056791]; avg score: 79.22385013428818\n",
      "episode: [2777. 2797. 2807. 2837.]; total steps: 485000; episodes scores: [ 30.62193745 -41.01198891  90.52796403 164.85251557]; avg score: 61.24760703630067\n",
      "learning timestep: 486000\n",
      "Policy Loss: 0.2261621206998825\n",
      "Value Loss: 2.8632240295410156\n",
      "Entropy: -3.3567423820495605\n",
      "KL Divergence: 13.0681734085083\n",
      "episode: [2779. 2799. 2808. 2839.]; total steps: 486000; episodes scores: [-24.16784864  71.29769923  79.84113885 -19.94358712]; avg score: 26.756850578716065\n",
      "Rendering episode 2780.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2780.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2780.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2780.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2780.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [162.93875636]\n",
      "episode: [2780. 2801. 2811. 2840.]; total steps: 487000; episodes scores: [ 13.92218401 -38.83651822 -51.07864114 -43.40828956]; avg score: -29.850316227212414\n",
      "learning timestep: 488000\n",
      "Policy Loss: -0.04969726502895355\n",
      "Value Loss: 1.7587928771972656\n",
      "Entropy: -3.4338088035583496\n",
      "KL Divergence: 13.830368995666504\n",
      "episode: [2782. 2804. 2813. 2843.]; total steps: 488000; episodes scores: [-75.72582199 -42.55155604  15.93261685 -12.07937896]; avg score: -28.606035033619825\n",
      "episode: [2783. 2806. 2815. 2844.]; total steps: 489000; episodes scores: [162.40386429  40.79382577 -75.86049226 132.22730129]; avg score: 64.89112477373442\n",
      "learning timestep: 490000\n",
      "Policy Loss: -0.25245401263237\n",
      "Value Loss: 1.4591941833496094\n",
      "Entropy: -3.438976287841797\n",
      "KL Divergence: 13.33250617980957\n",
      "episode: [2786. 2807. 2817. 2846.]; total steps: 490000; episodes scores: [-105.83363427  103.87417691   60.7205922   -16.16048461]; avg score: 10.65016255644678\n",
      "episode: [2788. 2808. 2819. 2847.]; total steps: 491000; episodes scores: [-27.54958997  14.12138094 -25.40144368 311.34541095]; avg score: 68.12893955879645\n",
      "learning timestep: 492000\n",
      "Policy Loss: -0.17174412310123444\n",
      "Value Loss: 0.7350303530693054\n",
      "Entropy: -3.56657338142395\n",
      "KL Divergence: 15.246932983398438\n",
      "episode: [2789. 2810. 2820. 2849.]; total steps: 492000; episodes scores: [310.35620366  32.22670187 152.01168745 -63.1529507 ]; avg score: 107.86041056892657\n",
      "episode: [2791. 2810. 2821. 2851.]; total steps: 493000; episodes scores: [-45.85418615  32.22670187  38.33497122 -18.8575301 ]; avg score: 1.4624892086382228\n",
      "learning timestep: 494000\n",
      "Policy Loss: -0.05408675596117973\n",
      "Value Loss: 1.9930949211120605\n",
      "Entropy: -3.623751401901245\n",
      "KL Divergence: 17.366493225097656\n",
      "episode: [2792. 2811. 2822. 2855.]; total steps: 494000; episodes scores: [310.12160792 180.72493114 310.32865961 -49.81960658]; avg score: 187.83889802472544\n",
      "episode: [2796. 2814. 2823. 2857.]; total steps: 495000; episodes scores: [-54.48001802 -29.76687144 308.79021726 -38.83267815]; avg score: 46.427662412824795\n",
      "learning timestep: 496000\n",
      "Policy Loss: 0.014591552317142487\n",
      "Value Loss: 1.3895905017852783\n",
      "Entropy: -3.57427978515625\n",
      "KL Divergence: 15.270882606506348\n",
      "episode: [2796. 2816. 2826. 2858.]; total steps: 496000; episodes scores: [-54.48001802  -0.60700404 -16.2473412  200.96580635]; avg score: 32.407860771615894\n",
      "Rendering episode 2800.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2800.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2800.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2800.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [52.74078974]\n",
      "episode: [2800. 2817. 2828. 2860.]; total steps: 497000; episodes scores: [-49.15839891  42.57056268  93.96516687  32.51909414]; avg score: 29.974106193550647\n",
      "learning timestep: 498000\n",
      "Policy Loss: 0.04046614468097687\n",
      "Value Loss: 2.3902087211608887\n",
      "Entropy: -3.6207051277160645\n",
      "KL Divergence: 15.245955467224121\n",
      "episode: [2802. 2819. 2829. 2862.]; total steps: 498000; episodes scores: [ -1.63241284 -15.60199282  49.4913245   12.29588886]; avg score: 11.138201923296357\n",
      "episode: [2804. 2822. 2830. 2864.]; total steps: 499000; episodes scores: [-47.43280058  15.34940653  28.14416348  60.36442063]; avg score: 14.106297515029803\n",
      "learning timestep: 500000\n",
      "Policy Loss: 0.06441615521907806\n",
      "Value Loss: 2.226865291595459\n",
      "Entropy: -3.473778486251831\n",
      "KL Divergence: 17.435619354248047\n",
      "episode: [2807. 2824. 2832. 2865.]; total steps: 500000; episodes scores: [-59.87065927  24.01000031  69.49912186 137.62780718]; avg score: 42.81656752254881\n",
      "episode: [2809. 2825. 2833. 2868.]; total steps: 501000; episodes scores: [ 67.6127101   14.76205563 133.9853599    6.9120953 ]; avg score: 55.81805523628932\n",
      "learning timestep: 502000\n",
      "Policy Loss: -0.17739661037921906\n",
      "Value Loss: 1.011897325515747\n",
      "Entropy: -3.5283291339874268\n",
      "KL Divergence: 14.74106216430664\n",
      "episode: [2810. 2826. 2834. 2870.]; total steps: 502000; episodes scores: [ 79.92759728 309.60893853 111.27882824 -40.54632568]; avg score: 115.0672595912095\n",
      "episode: [2812. 2829. 2837. 2872.]; total steps: 503000; episodes scores: [ -7.15077479  30.37957312 -50.29392024 -33.08240515]; avg score: -15.036881765864168\n",
      "learning timestep: 504000\n",
      "Policy Loss: -0.09270933270454407\n",
      "Value Loss: 2.2765650749206543\n",
      "Entropy: -3.387136459350586\n",
      "KL Divergence: 13.212396621704102\n",
      "episode: [2815. 2830. 2839. 2873.]; total steps: 504000; episodes scores: [ -3.71324704  45.85821046 -21.66710757  82.87959367]; avg score: 25.839362379699516\n",
      "episode: [2816. 2832. 2841. 2874.]; total steps: 505000; episodes scores: [-21.20995704  21.20792381 -28.97484877 311.84155525]; avg score: 70.71616831217673\n",
      "learning timestep: 506000\n",
      "Policy Loss: 0.08543182909488678\n",
      "Value Loss: 1.0442149639129639\n",
      "Entropy: -3.5142698287963867\n",
      "KL Divergence: 13.950926780700684\n",
      "episode: [2818. 2834. 2842. 2877.]; total steps: 506000; episodes scores: [-20.4385281  -33.81095529 309.23933583 -41.63952073]; avg score: 53.33758292580947\n",
      "Rendering episode 2820.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2820.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2820.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2820.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2820.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [311.48323205]\n",
      "episode: [2822. 2836. 2844. 2880.]; total steps: 507000; episodes scores: [ -77.08248189    3.02486112   63.00647324 -105.96035115]; avg score: -29.252874668604317\n",
      "learning timestep: 508000\n",
      "Policy Loss: 0.07037211209535599\n",
      "Value Loss: 2.927267551422119\n",
      "Entropy: -3.4710166454315186\n",
      "KL Divergence: 15.163650512695312\n",
      "episode: [2823. 2837. 2845. 2881.]; total steps: 508000; episodes scores: [311.88697108  83.07363812  18.81411579   3.04625231]; avg score: 104.20524432434819\n",
      "episode: [2824. 2840. 2846. 2882.]; total steps: 509000; episodes scores: [152.46072615 -78.21385161 163.29634082 103.87089976]; avg score: 85.3535287799707\n",
      "learning timestep: 510000\n",
      "Policy Loss: -0.020328959450125694\n",
      "Value Loss: 2.225741147994995\n",
      "Entropy: -3.5112524032592773\n",
      "KL Divergence: 17.122455596923828\n",
      "episode: [2827. 2841. 2847. 2883.]; total steps: 510000; episodes scores: [-41.98359407  -8.96674497 309.40816329 182.53753556]; avg score: 110.24883995532942\n",
      "episode: [2830. 2844. 2849. 2885.]; total steps: 511000; episodes scores: [-30.4191033  -57.01381476 -28.81118734 112.44212029]; avg score: -0.9504962752222426\n",
      "learning timestep: 512000\n",
      "Policy Loss: -0.012919877655804157\n",
      "Value Loss: 2.3022539615631104\n",
      "Entropy: -3.8002679347991943\n",
      "KL Divergence: 20.593080520629883\n",
      "episode: [2833. 2844. 2851. 2887.]; total steps: 512000; episodes scores: [-39.9918879  -57.01381476 -52.84677959  40.91785831]; avg score: -27.23365598063232\n",
      "episode: [2834. 2847. 2853. 2891.]; total steps: 513000; episodes scores: [ 63.19243611 -13.70246784 -18.44334078  48.94641347]; avg score: 19.998260239628138\n",
      "learning timestep: 514000\n",
      "Policy Loss: -0.12859699130058289\n",
      "Value Loss: 1.481194257736206\n",
      "Entropy: -3.4866368770599365\n",
      "KL Divergence: 14.145345687866211\n",
      "episode: [2836. 2849. 2856. 2892.]; total steps: 514000; episodes scores: [-107.23942074  -10.82638655  -14.83547244   65.10130538]; avg score: -16.949993587123885\n",
      "episode: [2837. 2851. 2858. 2894.]; total steps: 515000; episodes scores: [311.9417185   -4.4546514  -67.39688238 -61.7228785 ]; avg score: 44.591826554820756\n",
      "Rendering episode 2840.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2840.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2840.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2840.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2840.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-59.3938479]\n",
      "learning timestep: 516000\n",
      "Policy Loss: -0.014265727251768112\n",
      "Value Loss: 2.758664608001709\n",
      "Entropy: -3.5924315452575684\n",
      "KL Divergence: 15.167126655578613\n",
      "episode: [2840. 2855. 2860. 2895.]; total steps: 516000; episodes scores: [-77.39376431 -71.61803482 -76.63502942  24.89503952]; avg score: -50.187947260969\n",
      "episode: [2842. 2858. 2861. 2897.]; total steps: 517000; episodes scores: [-14.45433199 -10.68256704 309.76384457  -2.61205927]; avg score: 70.50372156434406\n",
      "learning timestep: 518000\n",
      "Policy Loss: -0.014389793388545513\n",
      "Value Loss: 1.110002040863037\n",
      "Entropy: -3.538435935974121\n",
      "KL Divergence: 16.79971694946289\n",
      "episode: [2843. 2859. 2862. 2899.]; total steps: 518000; episodes scores: [159.95699827 154.99059121 145.12640893 -79.661916  ]; avg score: 95.10302060178184\n",
      "episode: [2844. 2861. 2863. 2900.]; total steps: 519000; episodes scores: [148.40910721  76.75612003  71.60680314 134.22490129]; avg score: 107.74923291949165\n",
      "learning timestep: 520000\n",
      "Policy Loss: 0.09745420515537262\n",
      "Value Loss: 1.4490861892700195\n",
      "Entropy: -3.5858325958251953\n",
      "KL Divergence: 15.981025695800781\n",
      "episode: [2845. 2862. 2866. 2902.]; total steps: 520000; episodes scores: [184.87005227 119.96651533 -70.19857043 -60.54653229]; avg score: 43.522866219741786\n",
      "episode: [2846. 2864. 2868. 2904.]; total steps: 521000; episodes scores: [ 67.18630129  -1.79181675 -22.33530253  85.54360302]; avg score: 32.150696258833904\n",
      "learning timestep: 522000\n",
      "Policy Loss: -0.14591045677661896\n",
      "Value Loss: 1.7168959379196167\n",
      "Entropy: -3.6347293853759766\n",
      "KL Divergence: 16.27193832397461\n",
      "episode: [2849. 2865. 2868. 2907.]; total steps: 522000; episodes scores: [-56.75701626 311.32180601 -22.33530253   2.20398291]; avg score: 58.60836753088757\n",
      "episode: [2850. 2866. 2872. 2910.]; total steps: 523000; episodes scores: [ 64.68620293  98.95112373 -32.36131803  -1.91977139]; avg score: 32.33905930929302\n",
      "learning timestep: 524000\n",
      "Policy Loss: 0.11089323461055756\n",
      "Value Loss: 1.9758402109146118\n",
      "Entropy: -3.632120132446289\n",
      "KL Divergence: 14.447351455688477\n",
      "episode: [2852. 2867. 2874. 2912.]; total steps: 524000; episodes scores: [-41.12936842 152.05002656   1.74994878  49.97993189]; avg score: 40.662634703274776\n",
      "episode: [2855. 2868. 2875. 2914.]; total steps: 525000; episodes scores: [ 23.37322613 145.43231104 202.2606265   21.46637033]; avg score: 98.13313349891492\n",
      "learning timestep: 526000\n",
      "Policy Loss: -0.20075497031211853\n",
      "Value Loss: 4.265745639801025\n",
      "Entropy: -3.6762280464172363\n",
      "KL Divergence: 18.396665573120117\n",
      "episode: [2858. 2869. 2878. 2918.]; total steps: 526000; episodes scores: [ 44.10167858 310.19439855 -60.01898784 -41.25388681]; avg score: 63.25580062094747\n",
      "Rendering episode 2860.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2860.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2860.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2860.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2860.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [312.4243571]\n",
      "episode: [2860. 2870. 2879. 2920.]; total steps: 527000; episodes scores: [-66.13053534 127.70822214  55.05582521  46.14032357]; avg score: 40.69345889397357\n",
      "learning timestep: 528000\n",
      "Policy Loss: 0.11023762077093124\n",
      "Value Loss: 2.14155912399292\n",
      "Entropy: -3.405825614929199\n",
      "KL Divergence: 17.392244338989258\n",
      "episode: [2862. 2873. 2881. 2922.]; total steps: 528000; episodes scores: [-51.91042161  31.08976983  93.89852742   4.44115189]; avg score: 19.379756881310964\n",
      "episode: [2862. 2874. 2881. 2924.]; total steps: 529000; episodes scores: [-51.91042161  98.01544685  93.89852742 -19.24725735]; avg score: 30.18907382519246\n",
      "learning timestep: 530000\n",
      "Policy Loss: 0.2477133423089981\n",
      "Value Loss: 4.403156757354736\n",
      "Entropy: -3.6704721450805664\n",
      "KL Divergence: 17.653966903686523\n",
      "episode: [2865. 2875. 2882. 2926.]; total steps: 530000; episodes scores: [-67.42311051  53.30477248 309.99837483 -30.08632673]; avg score: 66.44842751648295\n",
      "episode: [2866. 2877. 2885. 2928.]; total steps: 531000; episodes scores: [164.26520461  83.26087909 -75.61958786 -17.84498156]; avg score: 38.51537857043387\n",
      "learning timestep: 532000\n",
      "Policy Loss: 0.03521931916475296\n",
      "Value Loss: 1.0614827871322632\n",
      "Entropy: -3.6999735832214355\n",
      "KL Divergence: 17.883480072021484\n",
      "episode: [2867. 2878. 2887. 2930.]; total steps: 532000; episodes scores: [2.06269005e+02 1.05610040e-01 3.82179288e+00 3.77373547e+01]; avg score: 61.983440623987164\n",
      "episode: [2870. 2879. 2888. 2931.]; total steps: 533000; episodes scores: [-22.82564494  71.72998917 118.70371217  30.02205784]; avg score: 49.407528560760525\n",
      "learning timestep: 534000\n",
      "Policy Loss: -0.014719093218445778\n",
      "Value Loss: 1.9672223329544067\n",
      "Entropy: -3.6234164237976074\n",
      "KL Divergence: 16.43167495727539\n",
      "episode: [2871. 2881. 2890. 2933.]; total steps: 534000; episodes scores: [-4.21821525 45.25443257 53.08908567 14.55084471]; avg score: 27.16903692478796\n",
      "episode: [2873. 2881. 2892. 2934.]; total steps: 535000; episodes scores: [ 52.27369457  45.25443257 -29.02434858 311.17554734]; avg score: 94.91983147775389\n",
      "learning timestep: 536000\n",
      "Policy Loss: -0.10028398036956787\n",
      "Value Loss: 0.8299573659896851\n",
      "Entropy: -3.744873523712158\n",
      "KL Divergence: 17.85063362121582\n",
      "episode: [2875. 2882. 2894. 2935.]; total steps: 536000; episodes scores: [-82.82831716 202.71631522 -41.26783051  51.05383029]; avg score: 32.41849945948043\n",
      "episode: [2876. 2885. 2894. 2936.]; total steps: 537000; episodes scores: [ 73.47373827 -52.15023363 -41.26783051 169.21184281]; avg score: 37.31687923502718\n",
      "learning timestep: 538000\n",
      "Policy Loss: 0.0007706794422119856\n",
      "Value Loss: 1.971585988998413\n",
      "Entropy: -3.570065498352051\n",
      "KL Divergence: 16.97713851928711\n",
      "episode: [2879. 2887. 2896. 2939.]; total steps: 538000; episodes scores: [-104.84051751  123.46257224  -21.38336579  -75.92623734]; avg score: -19.671887097849996\n",
      "Rendering episode 2880.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2880.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2880.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2880.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2880.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [43.86306557]\n",
      "episode: [2881. 2890. 2898. 2941.]; total steps: 539000; episodes scores: [-41.31458263 -58.74850848 -17.71370169  92.77961183]; avg score: -6.24929524447041\n",
      "learning timestep: 540000\n",
      "Policy Loss: -0.05602848157286644\n",
      "Value Loss: 2.1424856185913086\n",
      "Entropy: -3.6149802207946777\n",
      "KL Divergence: 17.458580017089844\n",
      "episode: [2885. 2893. 2900. 2943.]; total steps: 540000; episodes scores: [-72.34116613 -20.09611747   7.45631208 -13.21681431]; avg score: -24.54944645975642\n",
      "episode: [2886. 2894. 2902. 2944.]; total steps: 541000; episodes scores: [ 98.16074286 311.58912827  63.27525111 311.47167439]; avg score: 196.12419915668653\n",
      "learning timestep: 542000\n",
      "Policy Loss: -0.08672332018613815\n",
      "Value Loss: 1.1992177963256836\n",
      "Entropy: -3.677760124206543\n",
      "KL Divergence: 15.639127731323242\n",
      "episode: [2887. 2897. 2903. 2945.]; total steps: 542000; episodes scores: [175.15333434 -18.19881415  71.30724357 -23.54899033]; avg score: 51.178193356939865\n",
      "episode: [2889. 2899. 2907. 2947.]; total steps: 543000; episodes scores: [ 89.73765046  -3.15231506 -52.07038661  -8.17938663]; avg score: 6.583890540750845\n",
      "learning timestep: 544000\n",
      "Policy Loss: -0.1093316376209259\n",
      "Value Loss: 1.347015142440796\n",
      "Entropy: -3.7107505798339844\n",
      "KL Divergence: 16.926956176757812\n",
      "episode: [2890. 2901. 2908. 2949.]; total steps: 544000; episodes scores: [-56.31492541 -27.18677774 313.53474378 -19.12723566]; avg score: 52.72645124183546\n",
      "episode: [2891. 2901. 2910. 2950.]; total steps: 545000; episodes scores: [139.9771899  -27.18677774 -50.63483725  -5.62089846]; avg score: 14.133669112701911\n",
      "learning timestep: 546000\n",
      "Policy Loss: 0.1268528550863266\n",
      "Value Loss: 3.675630807876587\n",
      "Entropy: -3.5419681072235107\n",
      "KL Divergence: 15.710025787353516\n",
      "episode: [2892. 2903. 2913. 2953.]; total steps: 546000; episodes scores: [178.59793991 -48.93080713 -65.35301299 -33.6492447 ]; avg score: 7.666218771464223\n",
      "episode: [2894. 2905. 2916. 2957.]; total steps: 547000; episodes scores: [ 30.01154261  58.09921881 -15.22631911 -48.47237255]; avg score: 6.1030174407006115\n",
      "learning timestep: 548000\n",
      "Policy Loss: 0.054668888449668884\n",
      "Value Loss: 1.2612749338150024\n",
      "Entropy: -3.5462565422058105\n",
      "KL Divergence: 15.350854873657227\n",
      "episode: [2897. 2907. 2917. 2959.]; total steps: 548000; episodes scores: [-15.65283364  -1.24332929 190.53712311 -10.83657518]; avg score: 40.70109625081966\n",
      "episode: [2899. 2910. 2917. 2960.]; total steps: 549000; episodes scores: [ 21.88043877  68.88093833 190.53712311 131.3683777 ]; avg score: 103.16671947619473\n",
      "Rendering episode 2900.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2900.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2900.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2900.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [312.97895009]\n",
      "learning timestep: 550000\n",
      "Policy Loss: 0.3448040783405304\n",
      "Value Loss: 2.5024852752685547\n",
      "Entropy: -3.5920825004577637\n",
      "KL Divergence: 17.613765716552734\n",
      "episode: [2900. 2912. 2919. 2962.]; total steps: 550000; episodes scores: [145.55557999 -11.54395009  96.42475943 -16.4849382 ]; avg score: 53.48786278545875\n",
      "episode: [2900. 2913. 2920. 2963.]; total steps: 551000; episodes scores: [145.55557999  19.17906989 121.13362214 312.4729833 ]; avg score: 149.58531383152393\n",
      "learning timestep: 552000\n",
      "Policy Loss: -0.005189662799239159\n",
      "Value Loss: 1.9822516441345215\n",
      "Entropy: -3.671875\n",
      "KL Divergence: 16.381500244140625\n",
      "episode: [2903. 2914. 2922. 2965.]; total steps: 552000; episodes scores: [  15.25944614  313.71951629   92.30555162 -106.27639508]; avg score: 78.75202974060801\n",
      "episode: [2905. 2916. 2923. 2966.]; total steps: 553000; episodes scores: [-105.20021781   -6.16125166  130.63398809   -4.61023648]; avg score: 3.665570537166964\n",
      "learning timestep: 554000\n",
      "Policy Loss: 0.004847570788115263\n",
      "Value Loss: 1.1548726558685303\n",
      "Entropy: -3.7606725692749023\n",
      "KL Divergence: 18.003400802612305\n",
      "episode: [2906. 2917. 2926. 2968.]; total steps: 554000; episodes scores: [-34.93057489 124.43682759 -60.16797484 -54.39102661]; avg score: -6.263187186906563\n",
      "episode: [2909. 2918. 2927. 2970.]; total steps: 555000; episodes scores: [  -9.58911363  312.17873679 -115.57893452   26.19238905]; avg score: 53.30076942214927\n",
      "learning timestep: 556000\n",
      "Policy Loss: -0.12172648310661316\n",
      "Value Loss: 0.9991275072097778\n",
      "Entropy: -3.5012948513031006\n",
      "KL Divergence: 15.995344161987305\n",
      "episode: [2912. 2919. 2929. 2970.]; total steps: 556000; episodes scores: [  0.4057412  312.01245044  31.34697675  26.19238905]; avg score: 92.48938935763579\n",
      "episode: [2913. 2921. 2930. 2971.]; total steps: 557000; episodes scores: [ 58.28029833  -8.26381165  93.72151541 313.5363547 ]; avg score: 114.31858919796112\n",
      "learning timestep: 558000\n",
      "Policy Loss: -0.011685812845826149\n",
      "Value Loss: 0.6742404699325562\n",
      "Entropy: -3.591505765914917\n",
      "KL Divergence: 15.576435089111328\n",
      "episode: [2914. 2922. 2932. 2973.]; total steps: 558000; episodes scores: [ 17.32575088 -39.16411962 -36.94945721  -9.83431865]; avg score: -17.15553615072387\n",
      "episode: [2915. 2925. 2933. 2974.]; total steps: 559000; episodes scores: [312.44400822 -85.66773089 313.46246809 203.11682327]; avg score: 185.8388921701462\n",
      "learning timestep: 560000\n",
      "Policy Loss: 0.17137578129768372\n",
      "Value Loss: 4.978437423706055\n",
      "Entropy: -3.667557716369629\n",
      "KL Divergence: 15.295919418334961\n",
      "episode: [2916. 2925. 2935. 2975.]; total steps: 560000; episodes scores: [311.04394677 -85.66773089  75.1340923  -18.01833974]; avg score: 70.62299210961088\n",
      "episode: [2918. 2927. 2937. 2977.]; total steps: 561000; episodes scores: [-119.82873473    3.34052593   -8.95291438  -35.13978481]; avg score: -40.145226999228484\n",
      "learning timestep: 562000\n",
      "Policy Loss: 0.34379687905311584\n",
      "Value Loss: 6.778757572174072\n",
      "Entropy: -3.723456859588623\n",
      "KL Divergence: 18.473796844482422\n",
      "episode: [2919. 2929. 2937. 2978.]; total steps: 562000; episodes scores: [312.56088472 -11.92106533  -8.95291438 209.76269437]; avg score: 125.36239984570184\n",
      "Rendering episode 2920.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2920.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2920.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2920.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2920.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [311.93050987]\n",
      "episode: [2920. 2930. 2938. 2980.]; total steps: 563000; episodes scores: [ 311.71768962  311.92856864  311.25684327 -105.59957551]; avg score: 207.32588150171662\n",
      "learning timestep: 564000\n",
      "Policy Loss: 0.17750950157642365\n",
      "Value Loss: 9.655601501464844\n",
      "Entropy: -3.5806362628936768\n",
      "KL Divergence: 17.500728607177734\n",
      "episode: [2921. 2931. 2940. 2981.]; total steps: 564000; episodes scores: [  2.08487006  -9.43692912 -27.42019289 122.9073478 ]; avg score: 22.033773963167228\n",
      "episode: [2923. 2932. 2941. 2981.]; total steps: 565000; episodes scores: [ 36.40520176 311.22372203 311.82719262 122.9073478 ]; avg score: 195.59086605281237\n",
      "learning timestep: 566000\n",
      "Policy Loss: -0.17984892427921295\n",
      "Value Loss: 1.1462900638580322\n",
      "Entropy: -3.6836369037628174\n",
      "KL Divergence: 17.442541122436523\n",
      "episode: [2924. 2934. 2942. 2983.]; total steps: 566000; episodes scores: [312.95605388 -28.67241348  10.55995004  95.94619039]; avg score: 97.69744520625912\n",
      "episode: [2924. 2935. 2944. 2984.]; total steps: 567000; episodes scores: [312.95605388  67.81072043  58.05875203 111.71532151]; avg score: 137.63521196258583\n",
      "learning timestep: 568000\n",
      "Policy Loss: 0.006636529229581356\n",
      "Value Loss: 2.4487690925598145\n",
      "Entropy: -3.4426329135894775\n",
      "KL Divergence: 12.324715614318848\n",
      "episode: [2926. 2937. 2944. 2985.]; total steps: 568000; episodes scores: [-102.46741452   54.48994655   58.05875203  125.26398871]; avg score: 33.836318191699036\n",
      "episode: [2927. 2938. 2945. 2986.]; total steps: 569000; episodes scores: [311.34747645  43.15638147 311.4327511  130.59368603]; avg score: 199.1325737626499\n",
      "learning timestep: 570000\n",
      "Policy Loss: -0.02675567753612995\n",
      "Value Loss: 2.0224080085754395\n",
      "Entropy: -3.614236831665039\n",
      "KL Divergence: 14.216203689575195\n",
      "episode: [2928. 2939. 2947. 2987.]; total steps: 570000; episodes scores: [312.73474055 312.99428476 -34.21297622  78.5031767 ]; avg score: 167.504806447313\n",
      "episode: [2930. 2940. 2948. 2988.]; total steps: 571000; episodes scores: [-22.60994511 110.87824758 168.0728641  311.31161354]; avg score: 141.9131950276115\n",
      "learning timestep: 572000\n",
      "Policy Loss: -0.03205206245183945\n",
      "Value Loss: 1.6729576587677002\n",
      "Entropy: -3.5906052589416504\n",
      "KL Divergence: 17.15627670288086\n",
      "episode: [2932. 2941. 2949. 2989.]; total steps: 572000; episodes scores: [ 29.95775441 309.18545488 142.04317188 313.17104861]; avg score: 198.5893574457214\n",
      "episode: [2932. 2942. 2950. 2990.]; total steps: 573000; episodes scores: [ 29.95775441 313.58662075  12.04705092   7.27312755]; avg score: 90.71613840811906\n",
      "learning timestep: 574000\n",
      "Policy Loss: 0.048892877995967865\n",
      "Value Loss: 5.654038906097412\n",
      "Entropy: -3.504112958908081\n",
      "KL Divergence: 14.702255249023438\n",
      "episode: [2934. 2942. 2951. 2991.]; total steps: 574000; episodes scores: [120.80512914 313.58662075 312.96648327 187.77420078]; avg score: 233.78310848416731\n",
      "episode: [2934. 2943. 2952. 2992.]; total steps: 575000; episodes scores: [120.80512914 313.02217675 151.09940121 312.79571307]; avg score: 224.43060504110022\n",
      "learning timestep: 576000\n",
      "Policy Loss: 0.046531062573194504\n",
      "Value Loss: 7.054841995239258\n",
      "Entropy: -3.4953675270080566\n",
      "KL Divergence: 14.692277908325195\n",
      "episode: [2935. 2944. 2953. 2994.]; total steps: 576000; episodes scores: [311.62289556 311.20004824 121.90461178 -82.49715631]; avg score: 165.5575998184493\n",
      "episode: [2936. 2946. 2954. 2994.]; total steps: 577000; episodes scores: [312.14859234 -28.95467299 312.03354113 -82.49715631]; avg score: 128.1825760427137\n",
      "learning timestep: 578000\n",
      "Policy Loss: -0.04900004342198372\n",
      "Value Loss: 1.0655020475387573\n",
      "Entropy: -3.7312681674957275\n",
      "KL Divergence: 15.37065601348877\n",
      "episode: [2937. 2946. 2955. 2996.]; total steps: 578000; episodes scores: [191.38757548 -28.95467299 310.1414763   34.26775444]; avg score: 126.71053330848389\n",
      "episode: [2939. 2948. 2955. 2997.]; total steps: 579000; episodes scores: [-54.44125566  76.78156044 310.1414763  311.79771582]; avg score: 161.06987422341592\n",
      "Rendering episode 2940.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2940.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2940.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2940.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2940.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.11643427]\n",
      "learning timestep: 580000\n",
      "Policy Loss: -0.0010248234029859304\n",
      "Value Loss: 1.029537320137024\n",
      "Entropy: -3.815364122390747\n",
      "KL Divergence: 18.974611282348633\n",
      "episode: [2940. 2948. 2959. 2998.]; total steps: 580000; episodes scores: [  8.05163869  76.78156044 -40.96197847 126.83319204]; avg score: 42.67610317776502\n",
      "episode: [2941. 2950. 2960. 2998.]; total steps: 581000; episodes scores: [177.70144559  79.64202144   8.17306695 126.83319204]; avg score: 98.08743150737021\n",
      "learning timestep: 582000\n",
      "Policy Loss: -0.050936076790094376\n",
      "Value Loss: 0.862544059753418\n",
      "Entropy: -3.6508328914642334\n",
      "KL Divergence: 14.994202613830566\n",
      "episode: [2942. 2951. 2961. 2999.]; total steps: 582000; episodes scores: [312.24762951 159.56897703 310.65341816 313.65452242]; avg score: 274.03113677936966\n",
      "episode: [2943. 2952. 2962. 3001.]; total steps: 583000; episodes scores: [-23.95203067 313.16096033  56.34538354 -32.48900599]; avg score: 78.26632680216886\n",
      "learning timestep: 584000\n",
      "Policy Loss: -0.0368519090116024\n",
      "Value Loss: 6.167506217956543\n",
      "Entropy: -3.735703468322754\n",
      "KL Divergence: 15.656486511230469\n",
      "episode: [2944. 2952. 2963. 3003.]; total steps: 584000; episodes scores: [ 314.31817014  313.16096033  313.35260295 -104.99808275]; avg score: 208.95841266610117\n",
      "episode: [2946. 2953. 2964. 3004.]; total steps: 585000; episodes scores: [-47.11909785 312.82537879 311.62296544 314.16111981]; avg score: 222.8725915461695\n",
      "learning timestep: 586000\n",
      "Policy Loss: 0.0119453901425004\n",
      "Value Loss: 7.497278213500977\n",
      "Entropy: -3.6747758388519287\n",
      "KL Divergence: 15.507943153381348\n",
      "episode: [2948. 2954. 2965. 3004.]; total steps: 586000; episodes scores: [ 66.02324001 314.39179207 312.39384458 314.16111981]; avg score: 251.74249911833869\n",
      "episode: [2948. 2955. 2965. 3005.]; total steps: 587000; episodes scores: [ 66.02324001 314.09853448 312.39384458 313.89997644]; avg score: 251.60389887605055\n",
      "learning timestep: 588000\n",
      "Policy Loss: -0.05886130779981613\n",
      "Value Loss: 6.581796169281006\n",
      "Entropy: -3.421799898147583\n",
      "KL Divergence: 11.5938138961792\n",
      "episode: [2950. 2956. 2966. 3006.]; total steps: 588000; episodes scores: [ 12.59670307 310.94899131 310.41818226 310.98459555]; avg score: 236.2371180495615\n",
      "episode: [2951. 2958. 2967. 3007.]; total steps: 589000; episodes scores: [100.0227411  -33.36879259 311.04468652 310.96249402]; avg score: 172.16528226366336\n",
      "learning timestep: 590000\n",
      "Policy Loss: 0.20051562786102295\n",
      "Value Loss: 11.273681640625\n",
      "Entropy: -3.577439308166504\n",
      "KL Divergence: 13.811042785644531\n",
      "episode: [2952. 2959. 2968. 3008.]; total steps: 590000; episodes scores: [311.90657795 311.9876432  312.49148718  78.61288054]; avg score: 253.74964721859232\n",
      "episode: [2954. 2960. 2969. 3009.]; total steps: 591000; episodes scores: [-40.49397805  28.82514856 313.48581595 312.46977413]; avg score: 153.57169014868055\n",
      "learning timestep: 592000\n",
      "Policy Loss: -0.17515166103839874\n",
      "Value Loss: 0.6788308620452881\n",
      "Entropy: -3.633279323577881\n",
      "KL Divergence: 15.071216583251953\n",
      "episode: [2955. 2962. 2969. 3010.]; total steps: 592000; episodes scores: [313.5529014  -32.90043988 313.48581595 312.90546852]; avg score: 226.76093649632998\n",
      "episode: [2956. 2963. 2970. 3011.]; total steps: 593000; episodes scores: [-28.22890589 312.56384325 312.54497855 -14.12959744]; avg score: 145.68757961734354\n",
      "learning timestep: 594000\n",
      "Policy Loss: 0.04819355905056\n",
      "Value Loss: 0.9635311961174011\n",
      "Entropy: -3.559018135070801\n",
      "KL Divergence: 13.545477867126465\n",
      "episode: [2958. 2964. 2971. 3012.]; total steps: 594000; episodes scores: [-67.17232509  50.04866576 310.71283245 106.7980179 ]; avg score: 100.09679775239022\n",
      "Rendering episode 2960.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2960.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2960.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2960.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2960.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [43.52451311]\n",
      "episode: [2960. 2965. 2972. 3013.]; total steps: 595000; episodes scores: [116.21402973 313.17882769 312.31630181 311.27964934]; avg score: 263.24720214333587\n",
      "learning timestep: 596000\n",
      "Policy Loss: -0.18701232969760895\n",
      "Value Loss: 2.252796173095703\n",
      "Entropy: -3.730372190475464\n",
      "KL Divergence: 15.468265533447266\n",
      "episode: [2960. 2966. 2973. 3014.]; total steps: 596000; episodes scores: [116.21402973 310.93125488 312.10243747 139.72101848]; avg score: 219.74218513762747\n",
      "episode: [2961. 2966. 2974. 3015.]; total steps: 597000; episodes scores: [311.15013917 310.93125488  63.95479995 311.41236815]; avg score: 249.36214053964085\n",
      "learning timestep: 598000\n",
      "Policy Loss: 0.00048639788292348385\n",
      "Value Loss: 1.299876093864441\n",
      "Entropy: -3.588715076446533\n",
      "KL Divergence: 14.677412986755371\n",
      "episode: [2963. 2968. 2975. 3018.]; total steps: 598000; episodes scores: [-18.08964169  40.23173384 314.21879544 -54.76266509]; avg score: 70.39955562688934\n",
      "episode: [2964. 2969. 2976. 3019.]; total steps: 599000; episodes scores: [115.9158419  168.28433065 311.4409783   34.3344515 ]; avg score: 157.49390058516815\n",
      "learning timestep: 600000\n",
      "Policy Loss: -0.09954759478569031\n",
      "Value Loss: 0.4536508321762085\n",
      "Entropy: -3.7457337379455566\n",
      "KL Divergence: 15.112773895263672\n",
      "episode: [2965. 2969. 2976. 3020.]; total steps: 600000; episodes scores: [170.57887729 168.28433065 311.4409783  311.75245243]; avg score: 240.51415966553708\n",
      "episode: [2966. 2971. 2978. 3021.]; total steps: 601000; episodes scores: [310.39226089 -49.61879219 100.93185067 313.12974534]; avg score: 168.7087661782003\n",
      "learning timestep: 602000\n",
      "Policy Loss: -0.03666307032108307\n",
      "Value Loss: 4.164888381958008\n",
      "Entropy: -3.772036552429199\n",
      "KL Divergence: 20.195619583129883\n",
      "episode: [2967. 2972. 2980. 3023.]; total steps: 602000; episodes scores: [ 46.72191594 312.79428008 108.09931127 -12.31055595]; avg score: 113.826237832189\n",
      "episode: [2970. 2973. 2980. 3025.]; total steps: 603000; episodes scores: [-71.44438975 106.08640651 108.09931127 -16.39028961]; avg score: 31.587759601790744\n",
      "learning timestep: 604000\n",
      "Policy Loss: -0.06465767323970795\n",
      "Value Loss: 0.9053528308868408\n",
      "Entropy: -3.612278461456299\n",
      "KL Divergence: 14.952005386352539\n",
      "episode: [2971. 2974. 2981. 3027.]; total steps: 604000; episodes scores: [312.73912457  94.57681055 314.49221168 -29.92325071]; avg score: 172.97122402356248\n",
      "episode: [2971. 2975. 2983. 3028.]; total steps: 605000; episodes scores: [312.73912457 312.52107625 -57.26678274 314.24202   ]; avg score: 220.5588595198725\n",
      "learning timestep: 606000\n",
      "Policy Loss: -0.1598205864429474\n",
      "Value Loss: 1.3046238422393799\n",
      "Entropy: -3.541907787322998\n",
      "KL Divergence: 15.884315490722656\n",
      "episode: [2975. 2976. 2984. 3030.]; total steps: 606000; episodes scores: [-33.71278666 201.90447175 314.59890567 100.89148029]; avg score: 145.92051776295082\n",
      "episode: [2978. 2978. 2986. 3030.]; total steps: 607000; episodes scores: [-51.10049213 -40.10723016 -33.38571749 100.89148029]; avg score: -5.9254898722192735\n",
      "learning timestep: 608000\n",
      "Policy Loss: 0.10188797116279602\n",
      "Value Loss: 2.8288092613220215\n",
      "Entropy: -3.8417346477508545\n",
      "KL Divergence: 18.609542846679688\n",
      "episode: [2979. 2979. 2988. 3032.]; total steps: 608000; episodes scores: [201.90946159 153.23288019 -47.42477556  50.67604838]; avg score: 89.59840365086494\n",
      "episode: [2979. 2980. 2988. 3034.]; total steps: 609000; episodes scores: [201.90946159 313.96735207 -47.42477556  36.49519021]; avg score: 126.23680707814891\n",
      "Rendering episode 2980.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:2980.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_2980.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_2980.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_2980.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [311.57973316]\n",
      "learning timestep: 610000\n",
      "Policy Loss: 0.04191167652606964\n",
      "Value Loss: 1.2515928745269775\n",
      "Entropy: -3.8003554344177246\n",
      "KL Divergence: 16.310457229614258\n",
      "episode: [2981. 2981. 2990. 3036.]; total steps: 610000; episodes scores: [-21.08405906  67.3848857   -8.46747603  28.38555963]; avg score: 16.55472756003146\n",
      "episode: [2982. 2982. 2992. 3037.]; total steps: 611000; episodes scores: [312.21728442 313.14755229  31.04379454 312.3429361 ]; avg score: 242.18789183705994\n",
      "learning timestep: 612000\n",
      "Policy Loss: 0.03723413869738579\n",
      "Value Loss: 8.570172309875488\n",
      "Entropy: -3.5899219512939453\n",
      "KL Divergence: 14.360932350158691\n",
      "episode: [2984. 2984. 2994. 3039.]; total steps: 612000; episodes scores: [-82.80596034   5.19079032 -45.67949308 -28.72033963]; avg score: -38.00375068170905\n",
      "episode: [2985. 2984. 2995. 3040.]; total steps: 613000; episodes scores: [ 97.90257736   5.19079032 310.69007757 312.91830948]; avg score: 181.67543868212897\n",
      "learning timestep: 614000\n",
      "Policy Loss: -0.19369010627269745\n",
      "Value Loss: 0.7068437337875366\n",
      "Entropy: -3.7296512126922607\n",
      "KL Divergence: 18.731441497802734\n",
      "episode: [2986. 2985. 2996. 3042.]; total steps: 614000; episodes scores: [312.93406644 312.62454785 199.74090378  65.24212401]; avg score: 222.63541051739043\n",
      "episode: [2987. 2987. 2997. 3043.]; total steps: 615000; episodes scores: [311.28020616 -35.52883031 169.64008372 313.6553312 ]; avg score: 189.76169769124772\n",
      "learning timestep: 616000\n",
      "Policy Loss: 0.0664779394865036\n",
      "Value Loss: 3.2213475704193115\n",
      "Entropy: -3.636775016784668\n",
      "KL Divergence: 15.418403625488281\n",
      "episode: [2988. 2989. 2998. 3045.]; total steps: 616000; episodes scores: [ 63.0519058    3.76460061 313.48220089  -7.48293042]; avg score: 93.20394421942719\n",
      "episode: [2989. 2991. 2999. 3046.]; total steps: 617000; episodes scores: [178.00288797  61.02533177  17.84607723 133.16281919]; avg score: 97.50927904118228\n",
      "learning timestep: 618000\n",
      "Policy Loss: 0.1377379298210144\n",
      "Value Loss: 7.9186930656433105\n",
      "Entropy: -3.5595693588256836\n",
      "KL Divergence: 17.496427536010742\n",
      "episode: [2991. 2992. 3000. 3046.]; total steps: 618000; episodes scores: [ 21.78623682 311.89847479 185.02781491 133.16281919]; avg score: 162.9688364298265\n",
      "episode: [2992. 2992. 3001. 3047.]; total steps: 619000; episodes scores: [153.72043919 311.89847479 149.23163032 313.06123406]; avg score: 231.9779445930123\n",
      "learning timestep: 620000\n",
      "Policy Loss: -0.07136744260787964\n",
      "Value Loss: 1.4950921535491943\n",
      "Entropy: -3.6346285343170166\n",
      "KL Divergence: 18.174827575683594\n",
      "episode: [2993. 2994. 3003. 3049.]; total steps: 620000; episodes scores: [  4.05966258  42.10327699 -74.19430544 -26.82139361]; avg score: -13.713189871675187\n",
      "episode: [2997. 2995. 3005. 3050.]; total steps: 621000; episodes scores: [-110.10028868   59.41646792  -39.82099798  107.52320309]; avg score: 4.254596087278472\n",
      "learning timestep: 622000\n",
      "Policy Loss: 0.22129209339618683\n",
      "Value Loss: 1.7767268419265747\n",
      "Entropy: -3.7535247802734375\n",
      "KL Divergence: 16.2564697265625\n",
      "episode: [2998. 2996. 3006. 3053.]; total steps: 622000; episodes scores: [313.25656197  98.52919963  25.7319888  -58.43011926]; avg score: 94.77190778347097\n",
      "episode: [2998. 2998. 3007. 3055.]; total steps: 623000; episodes scores: [313.25656197  57.00487923 182.10704528   0.93520335]; avg score: 138.32592245805913\n",
      "Rendering episode 3000.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3000.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3000.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3000.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [172.0831803]\n",
      "learning timestep: 624000\n",
      "Policy Loss: -0.20259475708007812\n",
      "Value Loss: 0.9130328893661499\n",
      "Entropy: -3.847393274307251\n",
      "KL Divergence: 16.727455139160156\n",
      "episode: [3000. 3000. 3009. 3055.]; total steps: 624000; episodes scores: [128.38494508  83.19263215  45.31562922   0.93520335]; avg score: 64.45710245259576\n",
      "episode: [3003. 3001. 3011. 3056.]; total steps: 625000; episodes scores: [-21.0369755  313.68434402  63.32137967 314.2226411 ]; avg score: 167.54784732349032\n",
      "learning timestep: 626000\n",
      "Policy Loss: 0.04005631431937218\n",
      "Value Loss: 1.1598228216171265\n",
      "Entropy: -3.724675178527832\n",
      "KL Divergence: 16.229778289794922\n",
      "episode: [3004. 3001. 3014. 3057.]; total steps: 626000; episodes scores: [ 31.45503061 313.68434402 -49.19679822 210.43997998]; avg score: 126.59563910081077\n",
      "episode: [3007. 3002. 3014. 3058.]; total steps: 627000; episodes scores: [-47.04926831 309.64733094 -49.19679822 200.91204957]; avg score: 103.57832849546003\n",
      "learning timestep: 628000\n",
      "Policy Loss: 0.0867304801940918\n",
      "Value Loss: 2.084777593612671\n",
      "Entropy: -3.7447142601013184\n",
      "KL Divergence: 17.717487335205078\n",
      "episode: [3009. 3003. 3016. 3060.]; total steps: 628000; episodes scores: [-51.77599838 313.1975682   53.07977944   5.57810418]; avg score: 80.01986336280231\n",
      "episode: [3010. 3005. 3017. 3063.]; total steps: 629000; episodes scores: [310.98686107  17.62939253 312.82302599 -82.06978309]; avg score: 139.842374123449\n",
      "learning timestep: 630000\n",
      "Policy Loss: 0.036531295627355576\n",
      "Value Loss: 3.1882200241088867\n",
      "Entropy: -3.7653210163116455\n",
      "KL Divergence: 16.03272247314453\n",
      "episode: [3010. 3006. 3017. 3064.]; total steps: 630000; episodes scores: [310.98686107  -6.9703208  312.82302599  67.11616816]; avg score: 170.98893360654984\n",
      "episode: [3011. 3007. 3019. 3065.]; total steps: 631000; episodes scores: [313.1405866  312.88185606  41.41945509  72.22929488]; avg score: 184.91779815621422\n",
      "learning timestep: 632000\n",
      "Policy Loss: -0.034470509737730026\n",
      "Value Loss: 0.8314041495323181\n",
      "Entropy: -3.7137885093688965\n",
      "KL Divergence: 15.120014190673828\n",
      "episode: [3012. 3008. 3020. 3067.]; total steps: 632000; episodes scores: [314.16941143 314.1366267  127.46085574  17.51525118]; avg score: 193.32053626264204\n",
      "episode: [3013. 3009. 3021. 3068.]; total steps: 633000; episodes scores: [110.50108766 313.26331125 313.11855128  35.97441352]; avg score: 193.21434092665945\n",
      "learning timestep: 634000\n",
      "Policy Loss: -0.08986159414052963\n",
      "Value Loss: 0.8273026347160339\n",
      "Entropy: -3.836808919906616\n",
      "KL Divergence: 20.058185577392578\n",
      "episode: [3014. 3009. 3023. 3069.]; total steps: 634000; episodes scores: [311.48983735 313.26331125  19.37804434 100.82243667]; avg score: 186.23840740177104\n",
      "episode: [3015. 3010. 3024. 3070.]; total steps: 635000; episodes scores: [103.26926077 313.7408977  311.63888892 136.21991256]; avg score: 216.2172399866177\n",
      "learning timestep: 636000\n",
      "Policy Loss: -0.17173683643341064\n",
      "Value Loss: 0.7155438661575317\n",
      "Entropy: -3.8392608165740967\n",
      "KL Divergence: 19.08014678955078\n",
      "episode: [3016. 3011. 3025. 3071.]; total steps: 636000; episodes scores: [313.45178258 173.30469067  88.80137805 311.61128725]; avg score: 221.79228463751986\n",
      "episode: [3017. 3012. 3026. 3072.]; total steps: 637000; episodes scores: [312.12107297 314.40738736 312.21749699 312.93270035]; avg score: 312.9196644169467\n",
      "learning timestep: 638000\n",
      "Policy Loss: -0.12436019629240036\n",
      "Value Loss: 0.4070234000682831\n",
      "Entropy: -3.6614322662353516\n",
      "KL Divergence: 15.625617980957031\n",
      "episode: [3018. 3014. 3027. 3073.]; total steps: 638000; episodes scores: [143.63431524   7.21853603 112.41870153 314.67364526]; avg score: 144.4862995144826\n",
      "Rendering episode 3020.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3020.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3020.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3020.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3020.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [311.76446133]\n",
      "episode: [3021. 3014. 3028. 3075.]; total steps: 639000; episodes scores: [-109.21607859    7.21853603  311.91475517   69.05452705]; avg score: 69.74293491454598\n",
      "learning timestep: 640000\n",
      "Policy Loss: -0.06153826788067818\n",
      "Value Loss: 2.6806297302246094\n",
      "Entropy: -3.8244895935058594\n",
      "KL Divergence: 16.812273025512695\n",
      "episode: [3022. 3015. 3029. 3075.]; total steps: 640000; episodes scores: [105.06499138 310.90930026 117.34593483  69.05452705]; avg score: 150.59368837986938\n",
      "episode: [3023. 3017. 3030. 3076.]; total steps: 641000; episodes scores: [312.67187921  42.38858577 312.49586406 312.49020358]; avg score: 245.01163315613883\n",
      "learning timestep: 642000\n",
      "Policy Loss: -0.10221210867166519\n",
      "Value Loss: 2.9410884380340576\n",
      "Entropy: -3.5772929191589355\n",
      "KL Divergence: 16.036794662475586\n",
      "episode: [3024. 3018. 3030. 3078.]; total steps: 642000; episodes scores: [203.40299367 -40.85031502 312.49586406 -85.60880928]; avg score: 97.35993335725135\n",
      "episode: [3025. 3020. 3032. 3079.]; total steps: 643000; episodes scores: [-36.23180109 -53.09901034 -48.04388686 127.20542079]; avg score: -2.5423193748979003\n",
      "learning timestep: 644000\n",
      "Policy Loss: 0.04376985877752304\n",
      "Value Loss: 3.3500938415527344\n",
      "Entropy: -3.7288384437561035\n",
      "KL Divergence: 14.59419059753418\n",
      "episode: [3026. 3021. 3033. 3080.]; total steps: 644000; episodes scores: [202.24945339 313.38246599 312.25086054 311.31932911]; avg score: 284.80052725614473\n",
      "episode: [3027. 3022. 3034. 3081.]; total steps: 645000; episodes scores: [312.77001028  61.3787371   41.50170867 312.20317346]; avg score: 181.96340737602992\n",
      "learning timestep: 646000\n",
      "Policy Loss: -0.09185996651649475\n",
      "Value Loss: 0.6099379658699036\n",
      "Entropy: -3.747002124786377\n",
      "KL Divergence: 18.629493713378906\n",
      "episode: [3028. 3024. 3035. 3082.]; total steps: 646000; episodes scores: [313.88569726  66.77846283 311.40531416  66.85792505]; avg score: 189.7318498227379\n",
      "episode: [3029. 3024. 3036. 3083.]; total steps: 647000; episodes scores: [312.58711401  66.77846283 312.43792478 314.19395925]; avg score: 251.49936521942197\n",
      "learning timestep: 648000\n",
      "Policy Loss: -0.10555367916822433\n",
      "Value Loss: 0.29084205627441406\n",
      "Entropy: -3.611734390258789\n",
      "KL Divergence: 18.72347640991211\n",
      "episode: [3029. 3026. 3037. 3084.]; total steps: 648000; episodes scores: [312.58711401  96.0185692  313.68502817  23.22015216]; avg score: 186.37771588438008\n",
      "episode: [3030. 3029. 3037. 3085.]; total steps: 649000; episodes scores: [312.82873755 -12.47418759 313.68502817 313.53406898]; avg score: 231.89341177841516\n",
      "learning timestep: 650000\n",
      "Policy Loss: -0.09017061442136765\n",
      "Value Loss: 1.1294167041778564\n",
      "Entropy: -3.836904525756836\n",
      "KL Divergence: 17.85506820678711\n",
      "episode: [3031. 3030. 3038. 3086.]; total steps: 650000; episodes scores: [311.23362559 116.47062555 315.30088315 202.42612289]; avg score: 236.35781429680628\n",
      "episode: [3033. 3030. 3039. 3087.]; total steps: 651000; episodes scores: [-39.22099877 116.47062555 155.41516781 314.18100302]; avg score: 136.71144940183999\n",
      "learning timestep: 652000\n",
      "Policy Loss: 0.08709341287612915\n",
      "Value Loss: 4.705657958984375\n",
      "Entropy: -3.6336512565612793\n",
      "KL Divergence: 16.31487274169922\n",
      "episode: [3034. 3032. 3040. 3088.]; total steps: 652000; episodes scores: [312.3063364   35.10339166 310.80967392 313.07898015]; avg score: 242.8245955301042\n",
      "episode: [3035. 3033. 3042. 3089.]; total steps: 653000; episodes scores: [312.34161897 314.08315468  36.39340712 196.69712346]; avg score: 214.8788260567684\n",
      "learning timestep: 654000\n",
      "Policy Loss: -0.19338703155517578\n",
      "Value Loss: 0.5551462173461914\n",
      "Entropy: -3.785139560699463\n",
      "KL Divergence: 20.044269561767578\n",
      "episode: [3036. 3034. 3043. 3089.]; total steps: 654000; episodes scores: [-37.42983765 313.69975844 157.11111467 196.69712346]; avg score: 157.51953973087154\n",
      "episode: [3038. 3034. 3044. 3091.]; total steps: 655000; episodes scores: [ 29.63058353 313.69975844 312.09878454 -45.62854991]; avg score: 152.45014414826682\n",
      "learning timestep: 656000\n",
      "Policy Loss: 0.07322288304567337\n",
      "Value Loss: 9.045467376708984\n",
      "Entropy: -3.7196781635284424\n",
      "KL Divergence: 15.852457046508789\n",
      "episode: [3038. 3035. 3045. 3093.]; total steps: 656000; episodes scores: [ 29.63058353 312.11027267 124.23599288 -74.16002167]; avg score: 97.95420685369636\n",
      "Rendering episode 3040.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3040.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3040.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3040.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3040.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [3.24020006]\n",
      "episode: [3040. 3036. 3045. 3095.]; total steps: 657000; episodes scores: [ 89.42275384 313.0920013  124.23599288  -0.50104492]; avg score: 131.56242577393843\n",
      "learning timestep: 658000\n",
      "Policy Loss: -0.18561510741710663\n",
      "Value Loss: 0.37514254450798035\n",
      "Entropy: -3.854501485824585\n",
      "KL Divergence: 17.066360473632812\n",
      "episode: [3040. 3038. 3046. 3095.]; total steps: 658000; episodes scores: [ 89.42275384  16.06197519 312.44352464  -0.50104492]; avg score: 104.35680218700739\n",
      "episode: [3043. 3040. 3048. 3097.]; total steps: 659000; episodes scores: [-18.03189367  12.08059025  50.72108503  93.30938094]; avg score: 34.51979063696787\n",
      "learning timestep: 660000\n",
      "Policy Loss: -0.06252356618642807\n",
      "Value Loss: 0.8693183660507202\n",
      "Entropy: -3.826188564300537\n",
      "KL Divergence: 16.741703033447266\n",
      "episode: [3044. 3041. 3048. 3098.]; total steps: 660000; episodes scores: [139.11931625 146.4415199   50.72108503 -43.44995191]; avg score: 73.20799231739299\n",
      "episode: [3045. 3043. 3049. 3099.]; total steps: 661000; episodes scores: [157.23162999  19.25408345 312.16568193 313.26089863]; avg score: 200.47807350115409\n",
      "learning timestep: 662000\n",
      "Policy Loss: -0.006560022011399269\n",
      "Value Loss: 2.8539481163024902\n",
      "Entropy: -3.8603599071502686\n",
      "KL Divergence: 18.959884643554688\n",
      "episode: [3046. 3044. 3051. 3100.]; total steps: 662000; episodes scores: [171.45988689 312.15876638  51.52591046  91.70987584]; avg score: 156.71360989069962\n",
      "episode: [3047. 3044. 3053. 3101.]; total steps: 663000; episodes scores: [  2.79962176 312.15876638 140.10843773 314.08253309]; avg score: 192.2873397413673\n",
      "learning timestep: 664000\n",
      "Policy Loss: -0.08573286235332489\n",
      "Value Loss: 0.6681258678436279\n",
      "Entropy: -3.665778636932373\n",
      "KL Divergence: 15.193443298339844\n",
      "episode: [3049. 3045. 3055. 3102.]; total steps: 664000; episodes scores: [ 78.96545068 311.54442399  24.81744284 311.6667576 ]; avg score: 181.74851877879686\n",
      "episode: [3051. 3046. 3056. 3103.]; total steps: 665000; episodes scores: [  5.09931387 193.65114483 314.09173979 312.82552163]; avg score: 206.41693002881226\n",
      "learning timestep: 666000\n",
      "Policy Loss: -0.0832088515162468\n",
      "Value Loss: 2.1925501823425293\n",
      "Entropy: -3.9755496978759766\n",
      "KL Divergence: 16.102745056152344\n",
      "episode: [3051. 3047. 3057. 3104.]; total steps: 666000; episodes scores: [  5.09931387 312.86196223  58.26282688 312.53163151]; avg score: 172.188933623114\n",
      "episode: [3055. 3049. 3058. 3105.]; total steps: 667000; episodes scores: [-34.32899419 -43.28362208 314.48016925  66.49500668]; avg score: 75.84063991188908\n",
      "learning timestep: 668000\n",
      "Policy Loss: -0.1215498074889183\n",
      "Value Loss: 2.141732692718506\n",
      "Entropy: -3.824864387512207\n",
      "KL Divergence: 17.31802749633789\n",
      "episode: [3056. 3049. 3059. 3106.]; total steps: 668000; episodes scores: [310.86978818 -43.28362208 314.37167923 313.05598465]; avg score: 223.75345749529893\n",
      "episode: [3056. 3051. 3060. 3107.]; total steps: 669000; episodes scores: [310.86978818 107.57760453 143.06838242  63.15729233]; avg score: 156.16826686536962\n",
      "learning timestep: 670000\n",
      "Policy Loss: -0.03448916971683502\n",
      "Value Loss: 0.5116134285926819\n",
      "Entropy: -3.8191094398498535\n",
      "KL Divergence: 14.678352355957031\n",
      "episode: [3057. 3052. 3061. 3108.]; total steps: 670000; episodes scores: [313.60296231 208.05806879 109.91207671 158.9865618 ]; avg score: 197.63991740144286\n",
      "episode: [3058. 3052. 3062. 3109.]; total steps: 671000; episodes scores: [152.0411497  208.05806879 311.09743856 311.88146536]; avg score: 245.76953059954306\n",
      "learning timestep: 672000\n",
      "Policy Loss: 0.09946194291114807\n",
      "Value Loss: 9.659265518188477\n",
      "Entropy: -3.7437984943389893\n",
      "KL Divergence: 16.12746810913086\n",
      "episode: [3059. 3054. 3063. 3110.]; total steps: 672000; episodes scores: [312.77751213 112.49534285 169.759679   312.1029039 ]; avg score: 226.78385946776993\n",
      "Rendering episode 3060.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3060.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3060.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3060.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3060.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [312.85985278]\n",
      "episode: [3060. 3055. 3064. 3111.]; total steps: 673000; episodes scores: [312.81241478 -37.28114043 173.22501835 312.40090449]; avg score: 190.28929929740048\n",
      "learning timestep: 674000\n",
      "Policy Loss: -0.015437867492437363\n",
      "Value Loss: 1.4573725461959839\n",
      "Entropy: -3.7682225704193115\n",
      "KL Divergence: 20.28260040283203\n",
      "episode: [3061. 3057. 3066. 3112.]; total steps: 674000; episodes scores: [143.61487679  27.84775065  28.3399181    5.57518063]; avg score: 51.34443154162573\n",
      "episode: [3063. 3059. 3067. 3116.]; total steps: 675000; episodes scores: [-76.53112699  67.76320271 168.40919735 -47.49878918]; avg score: 28.035620973306337\n",
      "learning timestep: 676000\n",
      "Policy Loss: 0.09616871178150177\n",
      "Value Loss: 1.6018245220184326\n",
      "Entropy: -3.7444353103637695\n",
      "KL Divergence: 17.575124740600586\n",
      "episode: [3064. 3061. 3069. 3117.]; total steps: 676000; episodes scores: [164.86006931 -62.95761712 -40.21778688 128.26172474]; avg score: 47.48659751262473\n",
      "episode: [3066. 3062. 3071. 3118.]; total steps: 677000; episodes scores: [ 30.29670833  98.9190595  104.91881225 116.38386371]; avg score: 87.6296109502225\n",
      "learning timestep: 678000\n",
      "Policy Loss: 0.00636063888669014\n",
      "Value Loss: 1.3620829582214355\n",
      "Entropy: -3.7314705848693848\n",
      "KL Divergence: 16.14696502685547\n",
      "episode: [3066. 3064. 3072. 3119.]; total steps: 678000; episodes scores: [ 30.29670833  27.92249183 171.62132177  60.56655353]; avg score: 72.60176886618727\n",
      "episode: [3068. 3065. 3075. 3120.]; total steps: 679000; episodes scores: [-22.29746181 115.30506449 -75.68788684 314.24698946]; avg score: 82.89167632303804\n",
      "learning timestep: 680000\n",
      "Policy Loss: -0.1352270245552063\n",
      "Value Loss: 2.1052677631378174\n",
      "Entropy: -4.050302505493164\n",
      "KL Divergence: 17.777040481567383\n",
      "episode: [3069. 3067. 3076. 3121.]; total steps: 680000; episodes scores: [314.68070657 -64.92339765 -45.31595407 314.68230187]; avg score: 129.78091418108284\n",
      "episode: [3070. 3069. 3078. 3122.]; total steps: 681000; episodes scores: [ 314.83536334  -71.99909807 -104.85183707   16.69551113]; avg score: 38.669984830375\n",
      "learning timestep: 682000\n",
      "Policy Loss: -0.1372414380311966\n",
      "Value Loss: 2.4132113456726074\n",
      "Entropy: -3.881810426712036\n",
      "KL Divergence: 18.47589874267578\n",
      "episode: [3071. 3070. 3080. 3123.]; total steps: 682000; episodes scores: [ 51.54876142 165.16072105 -77.77270852 311.81895094]; avg score: 112.68893122453721\n",
      "episode: [3072. 3072. 3081. 3125.]; total steps: 683000; episodes scores: [312.38665275 118.5602297  312.20910543  34.23020696]; avg score: 194.34654871138696\n",
      "learning timestep: 684000\n",
      "Policy Loss: 0.010842564515769482\n",
      "Value Loss: 3.590836524963379\n",
      "Entropy: -3.7624170780181885\n",
      "KL Divergence: 19.237045288085938\n",
      "episode: [3074. 3074. 3081. 3126.]; total steps: 684000; episodes scores: [-36.71921709 -37.26753761 312.20910543 313.13042617]; avg score: 137.83819422454275\n",
      "episode: [3074. 3075. 3083. 3126.]; total steps: 685000; episodes scores: [-36.71921709  20.13196909  63.32252349 313.13042617]; avg score: 89.96642541315863\n",
      "learning timestep: 686000\n",
      "Policy Loss: -0.12016138434410095\n",
      "Value Loss: 3.1675376892089844\n",
      "Entropy: -3.6393885612487793\n",
      "KL Divergence: 19.075714111328125\n",
      "episode: [3076. 3076. 3084. 3128.]; total steps: 686000; episodes scores: [ 96.65591799 311.66108142 313.71685795  -3.51863272]; avg score: 179.6288061570216\n",
      "episode: [3077. 3077. 3085. 3130.]; total steps: 687000; episodes scores: [ 66.9238357   98.5686817   -3.34540113 -62.51848912]; avg score: 24.90715678693288\n",
      "learning timestep: 688000\n",
      "Policy Loss: -0.18344329297542572\n",
      "Value Loss: 3.6070737838745117\n",
      "Entropy: -3.75583553314209\n",
      "KL Divergence: 17.79873275756836\n",
      "episode: [3078. 3078. 3086. 3133.]; total steps: 688000; episodes scores: [ 80.51333607 130.13346956 314.77593338  41.42330587]; avg score: 141.71151122181266\n",
      "Rendering episode 3080.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3080.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3080.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3080.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3080.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.47372602]\n",
      "episode: [3080. 3080. 3087. 3134.]; total steps: 689000; episodes scores: [-20.45406029 -71.3866836   81.51554207 142.22480507]; avg score: 32.97490081215641\n",
      "learning timestep: 690000\n",
      "Policy Loss: -0.05707457289099693\n",
      "Value Loss: 0.9650827646255493\n",
      "Entropy: -3.9221482276916504\n",
      "KL Divergence: 15.804854393005371\n",
      "episode: [3082. 3081. 3089. 3134.]; total steps: 690000; episodes scores: [-51.0040297  313.56178554 -13.54359047 142.22480507]; avg score: 97.8097426094418\n",
      "episode: [3083. 3082. 3091. 3135.]; total steps: 691000; episodes scores: [132.74120099  -0.84225489   9.61971939 313.94344942]; avg score: 113.86552872933134\n",
      "learning timestep: 692000\n",
      "Policy Loss: 0.04648706316947937\n",
      "Value Loss: 4.121382713317871\n",
      "Entropy: -3.749843120574951\n",
      "KL Divergence: 15.989362716674805\n",
      "episode: [3084. 3084. 3092. 3137.]; total steps: 692000; episodes scores: [313.72834564  60.73349647 160.92124544   8.41756981]; avg score: 135.95016433969238\n",
      "episode: [3085. 3085. 3093. 3138.]; total steps: 693000; episodes scores: [315.48375759 314.6019292  312.813348    89.28136533]; avg score: 258.04510002973376\n",
      "learning timestep: 694000\n",
      "Policy Loss: -0.07365325838327408\n",
      "Value Loss: 1.0142126083374023\n",
      "Entropy: -3.7035629749298096\n",
      "KL Divergence: 19.386625289916992\n",
      "episode: [3086. 3086. 3094. 3141.]; total steps: 694000; episodes scores: [314.43380179 -20.24901053 313.9323945  -50.45025497]; avg score: 139.41673269655362\n",
      "episode: [3087. 3088. 3095. 3142.]; total steps: 695000; episodes scores: [313.49347467 -13.25667464  10.11474322  -1.95150455]; avg score: 77.10000967510675\n",
      "learning timestep: 696000\n",
      "Policy Loss: -0.10335086286067963\n",
      "Value Loss: 0.8182797431945801\n",
      "Entropy: -3.805114984512329\n",
      "KL Divergence: 18.158321380615234\n",
      "episode: [3088. 3088. 3096. 3144.]; total steps: 696000; episodes scores: [ 56.25388995 -13.25667464 145.52851626 144.03812344]; avg score: 83.14096375365753\n",
      "episode: [3089. 3090. 3097. 3146.]; total steps: 697000; episodes scores: [314.94404106 -10.9209329  155.56994955  31.03673658]; avg score: 122.65744857312048\n",
      "learning timestep: 698000\n",
      "Policy Loss: -0.05036623030900955\n",
      "Value Loss: 1.9116867780685425\n",
      "Entropy: -3.9742307662963867\n",
      "KL Divergence: 20.466732025146484\n",
      "episode: [3090. 3092. 3098. 3147.]; total steps: 698000; episodes scores: [314.55758969 -84.17466734 313.9211159  314.61689923]; avg score: 214.7302343719197\n",
      "episode: [3091. 3093. 3099. 3148.]; total steps: 699000; episodes scores: [313.94527245  79.95086918 314.69469002 -40.33815716]; avg score: 167.06316862291888\n",
      "learning timestep: 700000\n",
      "Policy Loss: 0.018785793334245682\n",
      "Value Loss: 5.310750484466553\n",
      "Entropy: -3.8319315910339355\n",
      "KL Divergence: 18.367874145507812\n",
      "episode: [3091. 3095. 3100. 3149.]; total steps: 700000; episodes scores: [313.94527245  43.78917207 314.30884799 314.9454597 ]; avg score: 246.74718805303405\n",
      "episode: [3094. 3097. 3101. 3152.]; total steps: 701000; episodes scores: [  0.34212121  37.77174293 201.53760457 -29.95687779]; avg score: 52.42364772893961\n",
      "learning timestep: 702000\n",
      "Policy Loss: -0.2714937925338745\n",
      "Value Loss: 0.5245025157928467\n",
      "Entropy: -3.7586724758148193\n",
      "KL Divergence: 17.881227493286133\n",
      "episode: [3097. 3098. 3102. 3153.]; total steps: 702000; episodes scores: [ -6.59169058 314.92484795 157.50943213 185.01054863]; avg score: 162.71328453303596\n",
      "episode: [3099. 3099. 3103. 3154.]; total steps: 703000; episodes scores: [-39.28310462 -13.82868537 194.71494393  26.41859941]; avg score: 42.005438336695434\n",
      "Rendering episode 3100.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3100.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3100.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3100.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-1.74782058]\n",
      "learning timestep: 704000\n",
      "Policy Loss: -0.05560417100787163\n",
      "Value Loss: 1.430938482284546\n",
      "Entropy: -3.7381935119628906\n",
      "KL Divergence: 14.741450309753418\n",
      "episode: [3101. 3101. 3104. 3155.]; total steps: 704000; episodes scores: [-46.325198    34.63669336  11.62298038 313.71097962]; avg score: 78.4113638410758\n",
      "episode: [3102. 3102. 3105. 3157.]; total steps: 705000; episodes scores: [  2.27427015 314.97191454 155.1369292  -17.31032201]; avg score: 113.7681979730987\n",
      "learning timestep: 706000\n",
      "Policy Loss: -0.052751194685697556\n",
      "Value Loss: 1.532775640487671\n",
      "Entropy: -3.992861747741699\n",
      "KL Divergence: 20.009145736694336\n",
      "episode: [3103. 3103. 3107. 3158.]; total steps: 706000; episodes scores: [314.08022178  71.32930986  90.6859676   76.77362119]; avg score: 138.21728010863148\n",
      "episode: [3104. 3104. 3108. 3159.]; total steps: 707000; episodes scores: [315.0798234   71.32306794 314.55949286 314.76920305]; avg score: 253.93289681241998\n",
      "learning timestep: 708000\n",
      "Policy Loss: -0.03604716807603836\n",
      "Value Loss: 3.7507071495056152\n",
      "Entropy: -3.685941219329834\n",
      "KL Divergence: 15.424741744995117\n",
      "episode: [3105. 3105. 3109. 3160.]; total steps: 708000; episodes scores: [314.21069146 315.01323637 -72.09840003 316.17248864]; avg score: 218.3245041087137\n",
      "episode: [3106. 3107. 3112. 3160.]; total steps: 709000; episodes scores: [314.50600725   1.66316588   7.8599882  316.17248864]; avg score: 160.05041248955155\n",
      "learning timestep: 710000\n",
      "Policy Loss: 0.08773619681596756\n",
      "Value Loss: 7.545988082885742\n",
      "Entropy: -3.9375147819519043\n",
      "KL Divergence: 18.02198028564453\n",
      "episode: [3108. 3109. 3115. 3162.]; total steps: 710000; episodes scores: [ 24.01112589  33.56362214 -36.55412838 -17.73957492]; avg score: 0.8202611831180402\n",
      "episode: [3109. 3110. 3117. 3164.]; total steps: 711000; episodes scores: [316.83888522 200.07020697  50.64691212  -0.34923331]; avg score: 141.8016927499311\n",
      "learning timestep: 712000\n",
      "Policy Loss: 0.038263238966464996\n",
      "Value Loss: 3.362109661102295\n",
      "Entropy: -3.9328997135162354\n",
      "KL Divergence: 15.480753898620605\n",
      "episode: [3109. 3111. 3120. 3166.]; total steps: 712000; episodes scores: [ 316.83888522  315.22826674 -106.64221447  -11.67088271]; avg score: 128.43851369421543\n",
      "episode: [3112. 3111. 3121. 3168.]; total steps: 713000; episodes scores: [  6.82955835 315.22826674 314.51966386 -41.21866515]; avg score: 148.8397059487538\n",
      "learning timestep: 714000\n",
      "Policy Loss: 0.06724204123020172\n",
      "Value Loss: 4.752528190612793\n",
      "Entropy: -3.8209269046783447\n",
      "KL Divergence: 15.996789932250977\n",
      "episode: [3113. 3113. 3122. 3170.]; total steps: 714000; episodes scores: [177.96552515  15.54967647 316.26334165 -48.48984119]; avg score: 115.32217551843712\n",
      "episode: [3115. 3114. 3124. 3171.]; total steps: 715000; episodes scores: [  -6.63045993  314.47139948 -104.55576715  119.1196055 ]; avg score: 80.60119447464504\n",
      "learning timestep: 716000\n",
      "Policy Loss: -0.0399312861263752\n",
      "Value Loss: 1.9575161933898926\n",
      "Entropy: -3.8596723079681396\n",
      "KL Divergence: 21.086299896240234\n",
      "episode: [3118. 3116. 3125. 3172.]; total steps: 716000; episodes scores: [ -39.12815558   59.98117195 -104.87336252  315.60250079]; avg score: 57.89553866160794\n",
      "episode: [3119. 3118. 3129. 3173.]; total steps: 717000; episodes scores: [-38.54590616  -9.53593796 -21.80044257 205.74454904]; avg score: 33.965565589072845\n",
      "Rendering episode 3120.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3120.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3120.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3120.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3120.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [51.97081874]\n",
      "learning timestep: 718000\n",
      "Policy Loss: -0.050487618893384933\n",
      "Value Loss: 0.7386610507965088\n",
      "Entropy: -3.794233798980713\n",
      "KL Divergence: 23.288789749145508\n",
      "episode: [3122. 3121. 3130. 3174.]; total steps: 718000; episodes scores: [-57.76558158 -12.87991497  96.7021013  316.32362199]; avg score: 85.59505668411143\n",
      "episode: [3123. 3123. 3132. 3176.]; total steps: 719000; episodes scores: [315.87081254  37.84693281   1.61140717 -23.54383976]; avg score: 82.94632819068\n",
      "learning timestep: 720000\n",
      "Policy Loss: 0.11954686790704727\n",
      "Value Loss: 2.6645169258117676\n",
      "Entropy: -3.930811882019043\n",
      "KL Divergence: 22.247692108154297\n",
      "episode: [3124. 3124. 3134. 3178.]; total steps: 720000; episodes scores: [ 79.12973626 131.87824197  97.66475726 -76.46157307]; avg score: 58.05279060203411\n",
      "episode: [3127. 3126. 3135. 3180.]; total steps: 721000; episodes scores: [-50.64275968 122.99817173  30.32093671  54.77382424]; avg score: 39.36254325051486\n",
      "learning timestep: 722000\n",
      "Policy Loss: -0.011293699033558369\n",
      "Value Loss: 3.6257448196411133\n",
      "Entropy: -3.83235764503479\n",
      "KL Divergence: 19.91790008544922\n",
      "episode: [3128. 3127. 3137. 3184.]; total steps: 722000; episodes scores: [315.77174021 124.12281411  63.20039453  24.99704987]; avg score: 132.0229996802771\n",
      "episode: [3129. 3129. 3139. 3185.]; total steps: 723000; episodes scores: [ 31.22988035 -57.04398058 -39.68767067 314.7053288 ]; avg score: 62.300889476257986\n",
      "learning timestep: 724000\n",
      "Policy Loss: 0.023563768714666367\n",
      "Value Loss: 1.0056116580963135\n",
      "Entropy: -3.6902825832366943\n",
      "KL Divergence: 16.289127349853516\n",
      "episode: [3130. 3130. 3141. 3187.]; total steps: 724000; episodes scores: [182.4337895  -10.89922045 -34.06847732   8.7536987 ]; avg score: 36.55494760728764\n",
      "episode: [3132. 3131. 3142. 3189.]; total steps: 725000; episodes scores: [ 18.08663991 121.25687876 316.28724435 -60.67454514]; avg score: 98.73905446957856\n",
      "learning timestep: 726000\n",
      "Policy Loss: -0.012997770681977272\n",
      "Value Loss: 3.0870070457458496\n",
      "Entropy: -3.873033046722412\n",
      "KL Divergence: 19.554214477539062\n",
      "episode: [3133. 3133. 3143. 3190.]; total steps: 726000; episodes scores: [ 32.55651456 -16.883224    86.77105911 -30.57277805]; avg score: 17.96789290669265\n",
      "episode: [3134. 3134. 3145. 3191.]; total steps: 727000; episodes scores: [313.47069082 162.08041933 -10.34828707 314.63414428]; avg score: 194.95924183849763\n",
      "learning timestep: 728000\n",
      "Policy Loss: 0.015066547319293022\n",
      "Value Loss: 1.7389848232269287\n",
      "Entropy: -3.651728391647339\n",
      "KL Divergence: 12.645307540893555\n",
      "episode: [3136. 3135. 3145. 3192.]; total steps: 728000; episodes scores: [ -2.8068865  314.97478718 -10.34828707 312.81159565]; avg score: 153.65780231440706\n",
      "episode: [3137. 3136. 3147. 3193.]; total steps: 729000; episodes scores: [315.93845256 106.32175995  31.42355852 314.56691315]; avg score: 192.06267104691386\n",
      "learning timestep: 730000\n",
      "Policy Loss: -0.08582530915737152\n",
      "Value Loss: 4.210129261016846\n",
      "Entropy: -3.939702033996582\n",
      "KL Divergence: 22.150543212890625\n",
      "episode: [3137. 3138. 3149. 3194.]; total steps: 730000; episodes scores: [315.93845256  -7.41865605 -48.98724325 314.08467031]; avg score: 143.40430589273703\n",
      "episode: [3138. 3139. 3149. 3196.]; total steps: 731000; episodes scores: [314.57800377 150.40310878 -48.98724325  32.74853461]; avg score: 112.18560097568111\n",
      "learning timestep: 732000\n",
      "Policy Loss: -0.11344568431377411\n",
      "Value Loss: 0.5524497628211975\n",
      "Entropy: -3.777090549468994\n",
      "KL Divergence: 17.71612548828125\n",
      "episode: [3139. 3140. 3151. 3197.]; total steps: 732000; episodes scores: [313.82188655 -11.60176192 -42.36836318 138.52422632]; avg score: 99.59399694400194\n",
      "Rendering episode 3140.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3140.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3140.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3140.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3140.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [2.24117061]\n",
      "episode: [3141. 3141. 3152. 3198.]; total steps: 733000; episodes scores: [ 29.17447698 314.13389596 171.19177192 146.08947453]; avg score: 165.14740484652583\n",
      "learning timestep: 734000\n",
      "Policy Loss: 0.003293613437563181\n",
      "Value Loss: 7.600711822509766\n",
      "Entropy: -3.713885545730591\n",
      "KL Divergence: 13.961363792419434\n",
      "episode: [3141. 3142. 3154. 3200.]; total steps: 734000; episodes scores: [ 29.17447698 141.81936696   0.43663306  -9.5136114 ]; avg score: 40.47921640026995\n",
      "episode: [3142. 3143. 3154. 3200.]; total steps: 735000; episodes scores: [316.56671256 315.8043376    0.43663306  -9.5136114 ]; avg score: 155.82351795527407\n",
      "learning timestep: 736000\n",
      "Policy Loss: 0.012361516244709492\n",
      "Value Loss: 2.4696950912475586\n",
      "Entropy: -3.6868736743927\n",
      "KL Divergence: 15.213072776794434\n",
      "episode: [3144. 3146. 3155. 3202.]; total steps: 736000; episodes scores: [ 83.42538155 -10.88154792 315.96133736 -51.72772693]; avg score: 84.19436101346768\n",
      "episode: [3144. 3147. 3157. 3204.]; total steps: 737000; episodes scores: [ 83.42538155 201.26371349 -38.76599027  46.50513887]; avg score: 73.10706091218765\n",
      "learning timestep: 738000\n",
      "Policy Loss: -0.21759390830993652\n",
      "Value Loss: 2.0215048789978027\n",
      "Entropy: -3.8293662071228027\n",
      "KL Divergence: 20.703048706054688\n",
      "episode: [3145. 3149. 3158. 3204.]; total steps: 738000; episodes scores: [315.11687104 -17.9608556   72.57235782  46.50513887]; avg score: 104.05837803439596\n",
      "episode: [3146. 3150. 3159. 3206.]; total steps: 739000; episodes scores: [315.55055932  40.44803201 187.61994369 110.30151417]; avg score: 163.48001229784924\n",
      "learning timestep: 740000\n",
      "Policy Loss: -0.0665525570511818\n",
      "Value Loss: 1.4734270572662354\n",
      "Entropy: -3.787621021270752\n",
      "KL Divergence: 15.33108139038086\n",
      "episode: [3148. 3152. 3160. 3207.]; total steps: 740000; episodes scores: [ 14.00773537  16.71712509 312.91502811  81.32530487]; avg score: 106.2412983611678\n",
      "episode: [3148. 3153. 3161. 3208.]; total steps: 741000; episodes scores: [ 14.00773537  64.96445584 313.47844836 314.76784115]; avg score: 176.8046201825492\n",
      "learning timestep: 742000\n",
      "Policy Loss: 0.07431837171316147\n",
      "Value Loss: 6.5404157638549805\n",
      "Entropy: -3.7924041748046875\n",
      "KL Divergence: 18.47602081298828\n",
      "episode: [3149. 3154. 3162. 3208.]; total steps: 742000; episodes scores: [314.81141986 314.15648347 198.12734843 314.76784115]; avg score: 285.46577323000747\n",
      "episode: [3150. 3155. 3164. 3211.]; total steps: 743000; episodes scores: [197.08866726 312.32438716 -25.55952601 -12.0524671 ]; avg score: 117.95026532881924\n",
      "learning timestep: 744000\n",
      "Policy Loss: -0.008954755030572414\n",
      "Value Loss: 0.6454954147338867\n",
      "Entropy: -3.842421531677246\n",
      "KL Divergence: 18.35291290283203\n",
      "episode: [3151. 3156. 3165. 3212.]; total steps: 744000; episodes scores: [155.85430255  73.19195274  62.525094   125.8623122 ]; avg score: 104.35841537216832\n",
      "episode: [3152. 3158. 3166. 3213.]; total steps: 745000; episodes scores: [312.81293365  -4.67456338 314.55764034 176.01268978]; avg score: 199.67717509765157\n",
      "learning timestep: 746000\n",
      "Policy Loss: 0.23995429277420044\n",
      "Value Loss: 9.117159843444824\n",
      "Entropy: -3.85636830329895\n",
      "KL Divergence: 20.408084869384766\n",
      "episode: [3153. 3160. 3168. 3214.]; total steps: 746000; episodes scores: [ 314.0481204  -107.54489597 -104.95301663  314.26567131]; avg score: 103.95396977784725\n",
      "episode: [3155. 3160. 3170. 3215.]; total steps: 747000; episodes scores: [ -40.29672395 -107.54489597 -111.53866471  131.77325961]; avg score: -31.901756255931218\n",
      "learning timestep: 748000\n",
      "Policy Loss: -0.10021685063838959\n",
      "Value Loss: 1.4586923122406006\n",
      "Entropy: -3.8633763790130615\n",
      "KL Divergence: 16.61324691772461\n",
      "episode: [3156. 3162. 3172. 3216.]; total steps: 748000; episodes scores: [171.81623231 -90.87771321  81.29653178 314.20368547]; avg score: 119.10968408921076\n",
      "episode: [3156. 3164. 3172. 3216.]; total steps: 749000; episodes scores: [171.81623231  -8.06956353  81.29653178 314.20368547]; avg score: 139.8117215087342\n",
      "learning timestep: 750000\n",
      "Policy Loss: -0.10768382996320724\n",
      "Value Loss: 0.4346469044685364\n",
      "Entropy: -4.046001434326172\n",
      "KL Divergence: 20.492691040039062\n",
      "episode: [3157. 3165. 3173. 3217.]; total steps: 750000; episodes scores: [314.71762266 313.76029438 315.34693775 316.39207693]; avg score: 315.05423293030435\n",
      "episode: [3159. 3165. 3174. 3219.]; total steps: 751000; episodes scores: [-85.27193671 313.76029438 312.4033078   99.40408812]; avg score: 160.07393839594044\n",
      "Rendering episode 3160.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3160.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3160.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3160.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3160.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [313.51099639]\n",
      "learning timestep: 752000\n",
      "Policy Loss: 0.2566719055175781\n",
      "Value Loss: 9.780878067016602\n",
      "Entropy: -3.8866355419158936\n",
      "KL Divergence: 18.85742950439453\n",
      "episode: [3160. 3166. 3175. 3220.]; total steps: 752000; episodes scores: [314.39396295 313.55110412 315.19953404  65.99545125]; avg score: 252.28501308801657\n",
      "episode: [3161. 3168. 3176. 3221.]; total steps: 753000; episodes scores: [193.14759569 -13.4634623  313.72721144 314.01796425]; avg score: 201.85732726914557\n",
      "learning timestep: 754000\n",
      "Policy Loss: 0.13352692127227783\n",
      "Value Loss: 9.783157348632812\n",
      "Entropy: -3.6932055950164795\n",
      "KL Divergence: 18.10167121887207\n",
      "episode: [3162. 3169. 3178. 3222.]; total steps: 754000; episodes scores: [313.66641808 184.92070301  -1.05626395  13.55436925]; avg score: 127.77130659861403\n",
      "episode: [3163. 3172. 3179. 3223.]; total steps: 755000; episodes scores: [314.54044483 -52.93543968 314.53553459 159.77670091]; avg score: 183.97931016261117\n",
      "learning timestep: 756000\n",
      "Policy Loss: 0.10760113596916199\n",
      "Value Loss: 9.715286254882812\n",
      "Entropy: -3.5128285884857178\n",
      "KL Divergence: 16.07923126220703\n",
      "episode: [3163. 3173. 3180. 3225.]; total steps: 756000; episodes scores: [314.54044483 315.0559543   57.54303815  98.18075435]; avg score: 196.33004790841522\n",
      "episode: [3164. 3174. 3181. 3226.]; total steps: 757000; episodes scores: [314.27217552 314.19167637  61.75048881 109.40463893]; avg score: 199.90474490558708\n",
      "learning timestep: 758000\n",
      "Policy Loss: -0.020325664430856705\n",
      "Value Loss: 8.16751766204834\n",
      "Entropy: -3.6247925758361816\n",
      "KL Divergence: 15.857417106628418\n",
      "episode: [3166. 3175. 3182. 3227.]; total steps: 758000; episodes scores: [-40.65332447 316.15647981 315.89882594 315.67446366]; avg score: 226.76911123804518\n",
      "episode: [3168. 3178. 3183. 3229.]; total steps: 759000; episodes scores: [-76.69764312   3.05737632 316.48753164  16.55262879]; avg score: 64.84997340776755\n",
      "learning timestep: 760000\n",
      "Policy Loss: 0.0525580495595932\n",
      "Value Loss: 2.2640767097473145\n",
      "Entropy: -3.8402388095855713\n",
      "KL Divergence: 18.56230926513672\n",
      "episode: [3171. 3179. 3184. 3231.]; total steps: 760000; episodes scores: [ -6.24994322  98.66339571 115.91764384  33.40390615]; avg score: 60.433750620304195\n",
      "episode: [3173. 3180. 3186. 3233.]; total steps: 761000; episodes scores: [ 74.55699847 176.78061957 -13.58351222 -31.78051119]; avg score: 51.49339865716021\n",
      "learning timestep: 762000\n",
      "Policy Loss: -0.16095098853111267\n",
      "Value Loss: 0.8602955341339111\n",
      "Entropy: -3.5215537548065186\n",
      "KL Divergence: 14.231035232543945\n",
      "episode: [3174. 3182. 3187. 3234.]; total steps: 762000; episodes scores: [-39.92763601 137.32424003 102.79730074  85.62433777]; avg score: 71.45456063026822\n",
      "episode: [3177. 3183. 3189. 3235.]; total steps: 763000; episodes scores: [ 24.48969733 126.1554261   32.73542332 150.86898747]; avg score: 83.56238355425609\n",
      "learning timestep: 764000\n",
      "Policy Loss: -0.010411504656076431\n",
      "Value Loss: 1.139793038368225\n",
      "Entropy: -3.8569834232330322\n",
      "KL Divergence: 19.79796600341797\n",
      "episode: [3179. 3184. 3190. 3238.]; total steps: 764000; episodes scores: [ 23.21104556 157.59027367  60.6089387  -34.86284754]; avg score: 51.636852598855384\n",
      "Rendering episode 3180.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3180.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3180.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3180.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3180.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.95474505]\n",
      "episode: [3180. 3186. 3192. 3240.]; total steps: 765000; episodes scores: [171.07830998  37.33217953 -37.37563807  -0.65157389]; avg score: 42.59581938801093\n",
      "learning timestep: 766000\n",
      "Policy Loss: -0.16776491701602936\n",
      "Value Loss: 1.6904165744781494\n",
      "Entropy: -3.825624465942383\n",
      "KL Divergence: 22.691444396972656\n",
      "episode: [3180. 3187. 3193. 3241.]; total steps: 766000; episodes scores: [171.07830998  81.82787934 315.32116002 316.69697152]; avg score: 221.23108021198993\n",
      "episode: [3181. 3188. 3195. 3242.]; total steps: 767000; episodes scores: [313.47574933 186.39595495 -48.68406161 117.50809759]; avg score: 142.17393506405293\n",
      "learning timestep: 768000\n",
      "Policy Loss: -0.09239150583744049\n",
      "Value Loss: 1.0744431018829346\n",
      "Entropy: -3.8352432250976562\n",
      "KL Divergence: 18.380895614624023\n",
      "episode: [3182. 3189. 3196. 3243.]; total steps: 768000; episodes scores: [318.00955521 316.05720879  59.26453579 314.2295947 ]; avg score: 251.8902236243398\n",
      "episode: [3183. 3191. 3197. 3243.]; total steps: 769000; episodes scores: [151.76985608  39.80442297 315.84688753 314.2295947 ]; avg score: 205.41269032278439\n",
      "learning timestep: 770000\n",
      "Policy Loss: 0.11906497180461884\n",
      "Value Loss: 2.9981095790863037\n",
      "Entropy: -3.903041362762451\n",
      "KL Divergence: 23.522254943847656\n",
      "episode: [3184. 3192. 3199. 3246.]; total steps: 770000; episodes scores: [148.58298275  81.00269378 -26.84447701 -30.38957066]; avg score: 43.087907216571416\n",
      "episode: [3186. 3193. 3200. 3248.]; total steps: 771000; episodes scores: [-51.23827525 315.89805683 315.02572443 -48.20810517]; avg score: 132.8693502124426\n",
      "learning timestep: 772000\n",
      "Policy Loss: 0.11339840292930603\n",
      "Value Loss: 3.565985918045044\n",
      "Entropy: -3.7959775924682617\n",
      "KL Divergence: 15.922774314880371\n",
      "episode: [3187. 3194. 3201. 3249.]; total steps: 772000; episodes scores: [103.45742393 118.33481845 316.04255076 151.75107509]; avg score: 172.39646705679024\n",
      "episode: [3188. 3195. 3204. 3249.]; total steps: 773000; episodes scores: [313.94881416  25.52361153  18.68852006 151.75107509]; avg score: 127.47800521046334\n",
      "learning timestep: 774000\n",
      "Policy Loss: 0.04561294987797737\n",
      "Value Loss: 1.7304017543792725\n",
      "Entropy: -4.010294437408447\n",
      "KL Divergence: 17.634613037109375\n",
      "episode: [3189. 3196. 3205. 3251.]; total steps: 774000; episodes scores: [116.0921368  315.49472657  77.39134608  30.25163282]; avg score: 134.80746056892218\n",
      "episode: [3191. 3198. 3206. 3254.]; total steps: 775000; episodes scores: [ 22.79581049 -66.0222093  315.64383467 -12.51154881]; avg score: 64.9764717623714\n",
      "learning timestep: 776000\n",
      "Policy Loss: -0.12639254331588745\n",
      "Value Loss: 1.979434847831726\n",
      "Entropy: -3.890580654144287\n",
      "KL Divergence: 19.36698341369629\n",
      "episode: [3192. 3200. 3207. 3255.]; total steps: 776000; episodes scores: [314.12330182 -29.54966887 109.25213968  59.48942456]; avg score: 113.32879930037777\n",
      "episode: [3192. 3201. 3208. 3256.]; total steps: 777000; episodes scores: [314.12330182 314.96492516 161.998423   316.10426362]; avg score: 276.7977284001113\n",
      "learning timestep: 778000\n",
      "Policy Loss: 0.21161118149757385\n",
      "Value Loss: 4.306158065795898\n",
      "Entropy: -3.902228355407715\n",
      "KL Divergence: 20.397762298583984\n",
      "episode: [3194. 3202. 3210. 3259.]; total steps: 778000; episodes scores: [109.24263257 315.80825126 -39.07396131 -29.79113374]; avg score: 89.04644719391835\n",
      "episode: [3195. 3204. 3212. 3260.]; total steps: 779000; episodes scores: [316.27015637 -34.12186009 -51.40358236 110.92098458]; avg score: 85.4164246256839\n",
      "learning timestep: 780000\n",
      "Policy Loss: -0.03997958078980446\n",
      "Value Loss: 1.671736717224121\n",
      "Entropy: -3.9410858154296875\n",
      "KL Divergence: 15.38868522644043\n",
      "episode: [3197. 3205. 3213. 3261.]; total steps: 780000; episodes scores: [ 52.57483589 314.09373383  99.19977572 315.92834859]; avg score: 195.44917350772874\n",
      "episode: [3198. 3206. 3215. 3262.]; total steps: 781000; episodes scores: [313.78103527  57.42176684 -40.7531943  313.65963274]; avg score: 161.0273101373661\n",
      "learning timestep: 782000\n",
      "Policy Loss: -0.06960797309875488\n",
      "Value Loss: 1.0329452753067017\n",
      "Entropy: -3.7911946773529053\n",
      "KL Divergence: 20.000333786010742\n",
      "episode: [3198. 3207. 3215. 3262.]; total steps: 782000; episodes scores: [313.78103527 314.31052312 -40.7531943  313.65963274]; avg score: 225.24949920882528\n",
      "episode: [3199. 3208. 3216. 3263.]; total steps: 783000; episodes scores: [316.07208249 315.7503231  210.07665107 313.84055681]; avg score: 288.9349033689179\n",
      "Rendering episode 3200.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3200.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3200.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3200.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [102.19546821]\n",
      "learning timestep: 784000\n",
      "Policy Loss: -0.06745795160531998\n",
      "Value Loss: 3.998114585876465\n",
      "Entropy: -3.8775720596313477\n",
      "KL Divergence: 20.49707794189453\n",
      "episode: [3201. 3209. 3219. 3264.]; total steps: 784000; episodes scores: [ 14.20227151 315.56314759  43.89343935 315.44708147]; avg score: 172.27648497929084\n",
      "episode: [3202. 3210. 3220. 3265.]; total steps: 785000; episodes scores: [314.84404458 315.0023817   76.17338041 315.00368696]; avg score: 255.25587341203698\n",
      "learning timestep: 786000\n",
      "Policy Loss: 0.08601474016904831\n",
      "Value Loss: 7.773627281188965\n",
      "Entropy: -3.871039628982544\n",
      "KL Divergence: 18.575162887573242\n",
      "episode: [3203. 3211. 3221. 3266.]; total steps: 786000; episodes scores: [125.47400009  39.93193402 314.23202707 316.36992776]; avg score: 199.00197223325875\n",
      "episode: [3207. 3212. 3222. 3267.]; total steps: 787000; episodes scores: [-22.22607924 315.45668801  79.95913334 315.69934477]; avg score: 172.22227172113935\n",
      "learning timestep: 788000\n",
      "Policy Loss: 0.026218513026833534\n",
      "Value Loss: 1.005225419998169\n",
      "Entropy: -3.620511293411255\n",
      "KL Divergence: 17.672073364257812\n",
      "episode: [3208. 3214. 3223. 3268.]; total steps: 788000; episodes scores: [  4.44267133  87.88917398 314.43699262  84.81204441]; avg score: 122.89522058403983\n",
      "episode: [3210. 3216. 3226. 3270.]; total steps: 789000; episodes scores: [ 19.77736619 112.7130822  -67.75664063  37.15615817]; avg score: 25.472491482095165\n",
      "learning timestep: 790000\n",
      "Policy Loss: -0.03820866718888283\n",
      "Value Loss: 1.1936976909637451\n",
      "Entropy: -3.9036128520965576\n",
      "KL Divergence: 19.721866607666016\n",
      "episode: [3211. 3217. 3227. 3270.]; total steps: 790000; episodes scores: [314.09545848   4.79489008  81.70321015  37.15615817]; avg score: 109.43742921953086\n",
      "episode: [3212. 3219. 3228. 3272.]; total steps: 791000; episodes scores: [316.06420079 -27.92720548 316.22623136  68.44701782]; avg score: 168.20256111914725\n",
      "learning timestep: 792000\n",
      "Policy Loss: -0.05639170482754707\n",
      "Value Loss: 4.636722564697266\n",
      "Entropy: -3.691662311553955\n",
      "KL Divergence: 18.50572395324707\n",
      "episode: [3214. 3220. 3229. 3273.]; total steps: 792000; episodes scores: [ -1.00181316 315.58580784 315.27962293 316.44538336]; avg score: 236.57725024244587\n",
      "episode: [3216. 3222. 3229. 3274.]; total steps: 793000; episodes scores: [-29.28645298 108.91142867 315.27962293 120.75016276]; avg score: 128.9136903438351\n",
      "learning timestep: 794000\n",
      "Policy Loss: 0.04596465826034546\n",
      "Value Loss: 2.3039941787719727\n",
      "Entropy: -3.855134963989258\n",
      "KL Divergence: 20.612506866455078\n",
      "episode: [3217. 3222. 3231. 3275.]; total steps: 794000; episodes scores: [315.90271701 108.91142867 -13.4893617   54.6187543 ]; avg score: 116.48588457034317\n",
      "episode: [3218. 3223. 3232. 3276.]; total steps: 795000; episodes scores: [312.68282551 315.03616924 115.38205248 316.34360298]; avg score: 264.86116255326255\n",
      "learning timestep: 796000\n",
      "Policy Loss: 0.01892477087676525\n",
      "Value Loss: 4.057070255279541\n",
      "Entropy: -3.7433855533599854\n",
      "KL Divergence: 15.54214859008789\n",
      "episode: [3219. 3224. 3234. 3277.]; total steps: 796000; episodes scores: [117.88493247 315.80874019   7.51766318 123.63706942]; avg score: 141.21210131288754\n",
      "Rendering episode 3220.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3220.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3220.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3220.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3220.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [317.16466773]\n",
      "episode: [3220. 3225. 3235. 3280.]; total steps: 797000; episodes scores: [  98.16807411  189.77130799  133.85372369 -109.37135027]; avg score: 78.10543888121369\n",
      "learning timestep: 798000\n",
      "Policy Loss: 0.22042085230350494\n",
      "Value Loss: 7.09515380859375\n",
      "Entropy: -3.9143600463867188\n",
      "KL Divergence: 19.1748104095459\n",
      "episode: [3222. 3226. 3236. 3280.]; total steps: 798000; episodes scores: [ -20.8158326   316.62100785   41.3056506  -109.37135027]; avg score: 56.934868895134244\n",
      "episode: [3223. 3227. 3238. 3281.]; total steps: 799000; episodes scores: [316.41157135 316.09234473  27.89588257 315.21925773]; avg score: 243.90476409477247\n",
      "learning timestep: 800000\n",
      "Policy Loss: -0.10095246136188507\n",
      "Value Loss: 3.510944128036499\n",
      "Entropy: -3.7940783500671387\n",
      "KL Divergence: 13.184600830078125\n",
      "episode: [3225. 3229. 3239. 3282.]; total steps: 800000; episodes scores: [-104.31833702  -75.19401474  193.9158837   318.30594052]; avg score: 83.17736811543885\n",
      "episode: [3226. 3230. 3241. 3283.]; total steps: 801000; episodes scores: [100.59685479 149.32094585 -48.55507213 314.64066609]; avg score: 129.00084865092595\n",
      "learning timestep: 802000\n",
      "Policy Loss: 0.04236375913023949\n",
      "Value Loss: 1.3424592018127441\n",
      "Entropy: -3.937659978866577\n",
      "KL Divergence: 18.205759048461914\n",
      "episode: [3227. 3231. 3243. 3285.]; total steps: 802000; episodes scores: [ 99.81012496  76.87657363 -16.36470427 -40.2521329 ]; avg score: 30.01746535639756\n",
      "episode: [3228. 3232. 3245. 3286.]; total steps: 803000; episodes scores: [316.6113861  111.511644   -26.24609116 315.93102111]; avg score: 179.45199001094716\n",
      "learning timestep: 804000\n",
      "Policy Loss: -0.054398901760578156\n",
      "Value Loss: 0.8727149963378906\n",
      "Entropy: -3.7629547119140625\n",
      "KL Divergence: 20.519163131713867\n",
      "episode: [3229. 3233. 3246. 3286.]; total steps: 804000; episodes scores: [173.56653996 131.76398724 127.3551548  315.93102111]; avg score: 187.15417577581454\n",
      "episode: [3230. 3234. 3247. 3287.]; total steps: 805000; episodes scores: [317.20943221 315.47486505 316.92041739 316.3171319 ]; avg score: 316.48046163851353\n",
      "learning timestep: 806000\n",
      "Policy Loss: -0.1425095796585083\n",
      "Value Loss: 5.776241302490234\n",
      "Entropy: -3.886965036392212\n",
      "KL Divergence: 19.608673095703125\n",
      "episode: [3232. 3235. 3248. 3289.]; total steps: 806000; episodes scores: [-79.1044024  315.64070621 196.74467241  59.38373369]; avg score: 123.16617747621041\n",
      "episode: [3233. 3236. 3249. 3289.]; total steps: 807000; episodes scores: [113.32248489 190.02672653 316.01241064  59.38373369]; avg score: 169.68633893750393\n",
      "learning timestep: 808000\n",
      "Policy Loss: -0.08073169738054276\n",
      "Value Loss: 1.2998607158660889\n",
      "Entropy: -3.6924209594726562\n",
      "KL Divergence: 15.318069458007812\n",
      "episode: [3235. 3238. 3250. 3290.]; total steps: 808000; episodes scores: [-35.73501062 -43.48065471 313.88215936 316.53049807]; avg score: 137.79924802562763\n",
      "episode: [3235. 3239. 3251. 3292.]; total steps: 809000; episodes scores: [-35.73501062  84.67238915 316.65317459  57.77656976]; avg score: 105.84178072004016\n",
      "learning timestep: 810000\n",
      "Policy Loss: -0.14777785539627075\n",
      "Value Loss: 1.3090258836746216\n",
      "Entropy: -3.8078110218048096\n",
      "KL Divergence: 20.84525489807129\n",
      "episode: [3236. 3240. 3251. 3294.]; total steps: 810000; episodes scores: [315.17481355 315.4343536  316.65317459 -49.58173908]; avg score: 224.42015066588328\n",
      "episode: [3237. 3241. 3252. 3295.]; total steps: 811000; episodes scores: [315.18775736 316.19856713 178.46095749 141.86677543]; avg score: 237.9285143523642\n",
      "learning timestep: 812000\n",
      "Policy Loss: 0.24195928871631622\n",
      "Value Loss: 14.704936981201172\n",
      "Entropy: -3.756153106689453\n",
      "KL Divergence: 19.187950134277344\n",
      "episode: [3239. 3241. 3254. 3297.]; total steps: 812000; episodes scores: [  5.42047812 316.19856713  59.65412208   6.69249828]; avg score: 96.99141640090825\n",
      "Rendering episode 3240.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3240.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3240.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3240.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3240.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [313.75932665]\n",
      "episode: [3240. 3243. 3255. 3298.]; total steps: 813000; episodes scores: [315.61507336 162.98975276 315.90252399 314.62377906]; avg score: 277.2827822902907\n",
      "learning timestep: 814000\n",
      "Policy Loss: 0.15310369431972504\n",
      "Value Loss: 8.212224960327148\n",
      "Entropy: -3.8587310314178467\n",
      "KL Divergence: 20.203052520751953\n",
      "episode: [3241. 3243. 3255. 3300.]; total steps: 814000; episodes scores: [ 27.77126264 162.98975276 315.90252399 -34.00049386]; avg score: 118.16576137986914\n",
      "episode: [3242. 3244. 3256. 3301.]; total steps: 815000; episodes scores: [314.36655909 314.91032452 314.81988178 315.37905801]; avg score: 314.86895584840215\n",
      "learning timestep: 816000\n",
      "Policy Loss: -0.12478954344987869\n",
      "Value Loss: 1.0597491264343262\n",
      "Entropy: -3.6003808975219727\n",
      "KL Divergence: 16.35373878479004\n",
      "episode: [3243. 3245. 3257. 3302.]; total steps: 816000; episodes scores: [313.76886058 137.50146582 190.77597401   7.03092538]; avg score: 162.26930645032732\n",
      "episode: [3244. 3247. 3258. 3303.]; total steps: 817000; episodes scores: [315.17210552  67.03608348 314.92430595 316.88311403]; avg score: 253.50390224327555\n",
      "learning timestep: 818000\n",
      "Policy Loss: 0.3056168258190155\n",
      "Value Loss: 14.235538482666016\n",
      "Entropy: -3.6274073123931885\n",
      "KL Divergence: 14.738759994506836\n",
      "episode: [3245. 3248. 3261. 3304.]; total steps: 818000; episodes scores: [ 77.02030871 314.28449609 -80.24670456 315.19086958]; avg score: 156.5622424567731\n",
      "episode: [3246. 3248. 3261. 3305.]; total steps: 819000; episodes scores: [315.21370874 314.28449609 -80.24670456 188.56466906]; avg score: 184.45404233232512\n",
      "learning timestep: 820000\n",
      "Policy Loss: -0.05014556273818016\n",
      "Value Loss: 0.7912781238555908\n",
      "Entropy: -3.7461628913879395\n",
      "KL Divergence: 14.957122802734375\n",
      "episode: [3247. 3250. 3262. 3306.]; total steps: 820000; episodes scores: [315.80276279  47.47043218 317.16584055 315.79199381]; avg score: 249.05775733344473\n",
      "episode: [3247. 3251. 3263. 3307.]; total steps: 821000; episodes scores: [315.80276279 314.03004192 315.91310332 314.85136562]; avg score: 315.1493184137551\n",
      "learning timestep: 822000\n",
      "Policy Loss: 0.032807473093271255\n",
      "Value Loss: 3.7603726387023926\n",
      "Entropy: -3.868165969848633\n",
      "KL Divergence: 18.307636260986328\n",
      "episode: [3249. 3252. 3264. 3307.]; total steps: 822000; episodes scores: [109.63731713 -38.5688999  313.82525667 314.85136562]; avg score: 174.93625987933467\n",
      "episode: [3250. 3253. 3265. 3311.]; total steps: 823000; episodes scores: [ 315.49307295  314.68996412  316.41703471 -114.37775279]; avg score: 208.05557974830236\n",
      "learning timestep: 824000\n",
      "Policy Loss: 0.003124882932752371\n",
      "Value Loss: 1.1371135711669922\n",
      "Entropy: -3.9676690101623535\n",
      "KL Divergence: 18.939027786254883\n",
      "episode: [3250. 3254. 3266. 3312.]; total steps: 824000; episodes scores: [315.49307295 315.53363691 210.04324928 316.13421958]; avg score: 289.30104467917454\n",
      "episode: [3252. 3255. 3267. 3312.]; total steps: 825000; episodes scores: [  4.95595392 316.94803156 316.17617138 316.13421958]; avg score: 238.55359411001203\n",
      "learning timestep: 826000\n",
      "Policy Loss: -0.195209801197052\n",
      "Value Loss: 0.669074296951294\n",
      "Entropy: -3.784686326980591\n",
      "KL Divergence: 17.81281089782715\n",
      "episode: [3254. 3257. 3267. 3313.]; total steps: 826000; episodes scores: [-112.75249336 -104.5735566   316.17617138  313.68121225]; avg score: 103.13283341633789\n",
      "episode: [3255. 3258. 3268. 3314.]; total steps: 827000; episodes scores: [316.39298819 317.74999133 315.61998777 315.89430109]; avg score: 316.4143170934799\n",
      "learning timestep: 828000\n",
      "Policy Loss: -0.032184407114982605\n",
      "Value Loss: 0.8829997181892395\n",
      "Entropy: -3.8251829147338867\n",
      "KL Divergence: 16.81719970703125\n",
      "episode: [3256. 3260. 3269. 3316.]; total steps: 828000; episodes scores: [314.04283734  76.99439524 194.31463702  44.18659095]; avg score: 157.38461513915226\n",
      "episode: [3257. 3261. 3270. 3316.]; total steps: 829000; episodes scores: [162.98636447 313.46892447 144.44283984  44.18659095]; avg score: 166.2711799332387\n",
      "learning timestep: 830000\n",
      "Policy Loss: -0.07834430038928986\n",
      "Value Loss: 0.3199794888496399\n",
      "Entropy: -3.792555332183838\n",
      "KL Divergence: 16.221372604370117\n",
      "episode: [3258. 3261. 3271. 3318.]; total steps: 830000; episodes scores: [  74.31854535  313.46892447  315.38318427 -107.78716214]; avg score: 148.84587298898896\n",
      "episode: [3259. 3263. 3272. 3319.]; total steps: 831000; episodes scores: [314.64650893   7.47439146 314.97152345 314.65615194]; avg score: 237.93714394464777\n",
      "Rendering episode 3260.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3260.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3260.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3260.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3260.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.80308717]\n",
      "learning timestep: 832000\n",
      "Policy Loss: -0.09838683903217316\n",
      "Value Loss: 0.803752601146698\n",
      "Entropy: -4.046159744262695\n",
      "KL Divergence: 19.57610511779785\n",
      "episode: [3260. 3264. 3273. 3320.]; total steps: 832000; episodes scores: [315.44498766 314.85753079 202.67279039 316.0305895 ]; avg score: 287.2514745866919\n",
      "episode: [3261. 3265. 3274. 3321.]; total steps: 833000; episodes scores: [172.31674199 313.75776302 314.51136856 314.08009768]; avg score: 278.66649281469466\n",
      "learning timestep: 834000\n",
      "Policy Loss: -0.12138630449771881\n",
      "Value Loss: 0.38279685378074646\n",
      "Entropy: -3.951089382171631\n",
      "KL Divergence: 20.82535743713379\n",
      "episode: [3264. 3266. 3276. 3323.]; total steps: 834000; episodes scores: [-75.50907041 178.43488889 -40.29913721  48.80601123]; avg score: 27.858173125986667\n",
      "episode: [3265. 3266. 3277. 3323.]; total steps: 835000; episodes scores: [315.29962227 178.43488889 118.29643554  48.80601123]; avg score: 165.20923948310215\n",
      "learning timestep: 836000\n",
      "Policy Loss: -0.08490162342786789\n",
      "Value Loss: 0.26519691944122314\n",
      "Entropy: -3.89617919921875\n",
      "KL Divergence: 19.26103973388672\n",
      "episode: [3266. 3268. 3278. 3324.]; total steps: 836000; episodes scores: [313.48305064  27.35259374 316.29859953 315.59981764]; avg score: 243.18351538836322\n",
      "episode: [3267. 3269. 3279. 3325.]; total steps: 837000; episodes scores: [190.67544522 181.37069373 313.2645734  313.9155664 ]; avg score: 249.8065696880797\n",
      "learning timestep: 838000\n",
      "Policy Loss: -0.09012659639120102\n",
      "Value Loss: 1.4308867454528809\n",
      "Entropy: -3.968573570251465\n",
      "KL Divergence: 19.742259979248047\n",
      "episode: [3268. 3270. 3280. 3326.]; total steps: 838000; episodes scores: [182.52339311 315.74350807 201.67937922 315.00114841]; avg score: 253.7368572032932\n",
      "episode: [3269. 3271. 3281. 3327.]; total steps: 839000; episodes scores: [312.12519295 315.114111   107.48038532 314.70765183]; avg score: 262.35683527592755\n",
      "learning timestep: 840000\n",
      "Policy Loss: -0.14512519538402557\n",
      "Value Loss: 1.7300530672073364\n",
      "Entropy: -3.928175449371338\n",
      "KL Divergence: 17.72159194946289\n",
      "episode: [3270. 3271. 3282. 3328.]; total steps: 840000; episodes scores: [-11.45651125 315.114111   313.77450121 314.52988419]; avg score: 232.9904962892163\n",
      "episode: [3272. 3274. 3282. 3329.]; total steps: 841000; episodes scores: [-44.41544156  30.4461257  313.77450121 145.52306984]; avg score: 111.33206379713458\n",
      "learning timestep: 842000\n",
      "Policy Loss: 0.1550649106502533\n",
      "Value Loss: 12.38526439666748\n",
      "Entropy: -3.7615232467651367\n",
      "KL Divergence: 17.63723373413086\n",
      "episode: [3274. 3274. 3283. 3330.]; total steps: 842000; episodes scores: [ 84.75850535  30.4461257  314.19966763 315.62918822]; avg score: 186.25837172378317\n",
      "episode: [3275. 3275. 3284. 3330.]; total steps: 843000; episodes scores: [-27.7348197  314.82674488 314.90731417 315.62918822]; avg score: 229.40710689393086\n",
      "learning timestep: 844000\n",
      "Policy Loss: 0.06825514137744904\n",
      "Value Loss: 10.431238174438477\n",
      "Entropy: -3.9723665714263916\n",
      "KL Divergence: 17.087078094482422\n",
      "episode: [3277. 3278. 3286. 3332.]; total steps: 844000; episodes scores: [36.08528991 19.72643217 35.09171441 56.6716486 ]; avg score: 36.89377126970585\n",
      "episode: [3277. 3278. 3286. 3334.]; total steps: 845000; episodes scores: [ 36.08528991  19.72643217  35.09171441 -40.19624886]; avg score: 12.676796905596264\n",
      "Rendering episode 3280.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3280.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3280.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3280.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3280.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [315.88779047]\n",
      "learning timestep: 846000\n",
      "Policy Loss: -0.08296018093824387\n",
      "Value Loss: 1.607154130935669\n",
      "Entropy: -3.7294375896453857\n",
      "KL Divergence: 15.114433288574219\n",
      "episode: [3280. 3280. 3287. 3335.]; total steps: 846000; episodes scores: [ 13.0474368  107.22245982 316.33359166 316.14191414]; avg score: 188.1863506057817\n",
      "episode: [3281. 3282. 3288. 3338.]; total steps: 847000; episodes scores: [315.22936524 105.04025706 316.58928817 -13.45674953]; avg score: 180.8505402339406\n",
      "learning timestep: 848000\n",
      "Policy Loss: -0.051155541092157364\n",
      "Value Loss: 2.0659196376800537\n",
      "Entropy: -4.014094352722168\n",
      "KL Divergence: 17.09247589111328\n",
      "episode: [3282. 3284. 3289. 3339.]; total steps: 848000; episodes scores: [ 80.2795621   -9.27627008 122.61105846 164.6701755 ]; avg score: 89.57113149689285\n",
      "episode: [3283. 3285. 3290. 3340.]; total steps: 849000; episodes scores: [316.92180464 -58.71035408 177.37770766 316.23935353]; avg score: 187.9571279395573\n",
      "learning timestep: 850000\n",
      "Policy Loss: -0.03580829128623009\n",
      "Value Loss: 2.7222156524658203\n",
      "Entropy: -3.780360698699951\n",
      "KL Divergence: 16.562049865722656\n",
      "episode: [3284. 3286. 3292. 3341.]; total steps: 850000; episodes scores: [316.76480054 316.83185792 -26.34702573 144.00539586]; avg score: 187.8137571472538\n",
      "episode: [3285. 3287. 3294. 3342.]; total steps: 851000; episodes scores: [314.84872638 315.0014947  -34.55771375 314.7739316 ]; avg score: 227.51660973364284\n",
      "learning timestep: 852000\n",
      "Policy Loss: -0.0337236188352108\n",
      "Value Loss: 3.3904170989990234\n",
      "Entropy: -3.8391401767730713\n",
      "KL Divergence: 17.39423179626465\n",
      "episode: [3286. 3288. 3295. 3343.]; total steps: 852000; episodes scores: [ 15.03296285 315.32866016 314.57144157 316.66803698]; avg score: 240.40027539114405\n",
      "episode: [3287. 3292. 3297. 3344.]; total steps: 853000; episodes scores: [316.09482959 -56.10792791  18.27989171 -38.51835377]; avg score: 59.93710990429591\n",
      "learning timestep: 854000\n",
      "Policy Loss: 0.12897619605064392\n",
      "Value Loss: 3.359468698501587\n",
      "Entropy: -3.8771543502807617\n",
      "KL Divergence: 18.474842071533203\n",
      "episode: [3289. 3293. 3297. 3345.]; total steps: 854000; episodes scores: [ 12.93859097 114.30541026  18.27989171 314.5046737 ]; avg score: 115.00714165956087\n",
      "episode: [3289. 3294. 3299. 3346.]; total steps: 855000; episodes scores: [  12.93859097  315.21362942 -109.01972516   66.4520384 ]; avg score: 71.39613340719062\n",
      "learning timestep: 856000\n",
      "Policy Loss: 0.21608027815818787\n",
      "Value Loss: 13.264796257019043\n",
      "Entropy: -3.9138643741607666\n",
      "KL Divergence: 18.629714965820312\n",
      "episode: [3293. 3295. 3300. 3347.]; total steps: 856000; episodes scores: [-115.30125706  114.20483503  313.73986879  315.07482252]; avg score: 156.92956731963298\n",
      "episode: [3295. 3296. 3301. 3348.]; total steps: 857000; episodes scores: [157.26553273 209.17150062  90.25905021 315.96006475]; avg score: 193.1640370759789\n",
      "learning timestep: 858000\n",
      "Policy Loss: -0.05494467914104462\n",
      "Value Loss: 2.2099955081939697\n",
      "Entropy: -4.0200581550598145\n",
      "KL Divergence: 21.856399536132812\n",
      "episode: [3296. 3297. 3303. 3349.]; total steps: 858000; episodes scores: [157.216458   -31.43482072  39.07838156 314.30395805]; avg score: 119.7909942219843\n",
      "episode: [3296. 3298. 3304. 3350.]; total steps: 859000; episodes scores: [157.216458   315.92100458   6.7331086  133.2545255 ]; avg score: 153.28127417029702\n",
      "learning timestep: 860000\n",
      "Policy Loss: -0.11310524493455887\n",
      "Value Loss: 0.38571110367774963\n",
      "Entropy: -3.8526744842529297\n",
      "KL Divergence: 19.138736724853516\n",
      "episode: [3297. 3300. 3306. 3351.]; total steps: 860000; episodes scores: [314.64137825 147.48854067  52.29812974 317.26566612]; avg score: 207.92342869685143\n",
      "episode: [3298. 3300. 3307. 3352.]; total steps: 861000; episodes scores: [315.98484877 147.48854067 167.79962777  91.74776519]; avg score: 180.75519559973137\n",
      "learning timestep: 862000\n",
      "Policy Loss: 0.05095605179667473\n",
      "Value Loss: 7.435517311096191\n",
      "Entropy: -3.964621067047119\n",
      "KL Divergence: 20.315650939941406\n",
      "episode: [3299. 3302. 3307. 3353.]; total steps: 862000; episodes scores: [315.46471034 115.5926628  167.79962777 316.3635158 ]; avg score: 228.80512917724428\n",
      "Rendering episode 3300.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3300.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3300.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3300.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [209.32849262]\n",
      "episode: [3302. 3302. 3308. 3356.]; total steps: 863000; episodes scores: [-112.35647812  115.5926628   315.18845615   -6.68548582]; avg score: 77.93478875339719\n",
      "learning timestep: 864000\n",
      "Policy Loss: -0.1357293576002121\n",
      "Value Loss: 0.5657939314842224\n",
      "Entropy: -3.6806278228759766\n",
      "KL Divergence: 14.042125701904297\n",
      "episode: [3303. 3305. 3309. 3357.]; total steps: 864000; episodes scores: [315.51447016 -42.28189113 188.01127062 315.65746011]; avg score: 194.22532743942116\n",
      "episode: [3304. 3305. 3310. 3358.]; total steps: 865000; episodes scores: [314.23499478 -42.28189113 315.40811843 317.44431734]; avg score: 226.2013848553931\n",
      "learning timestep: 866000\n",
      "Policy Loss: -0.1091180071234703\n",
      "Value Loss: 0.36060142517089844\n",
      "Entropy: -3.974686622619629\n",
      "KL Divergence: 18.633848190307617\n",
      "episode: [3305. 3308. 3311. 3359.]; total steps: 866000; episodes scores: [202.62026073  47.38257324 315.96605593 316.79950821]; avg score: 220.6920995262176\n",
      "episode: [3306. 3309. 3312. 3359.]; total steps: 867000; episodes scores: [316.2073983  314.75831728 316.23578568 316.79950821]; avg score: 316.0002523654862\n",
      "learning timestep: 868000\n",
      "Policy Loss: 0.12390245497226715\n",
      "Value Loss: 6.897319793701172\n",
      "Entropy: -4.057986259460449\n",
      "KL Divergence: 19.835617065429688\n",
      "episode: [3306. 3310. 3313. 3360.]; total steps: 868000; episodes scores: [316.2073983  189.29611718  84.84706416 316.77131809]; avg score: 226.78047443301895\n",
      "episode: [3308. 3311. 3314. 3361.]; total steps: 869000; episodes scores: [-116.5820437   129.40181564  161.69576762  316.68631625]; avg score: 122.80046395163586\n",
      "learning timestep: 870000\n",
      "Policy Loss: -0.007232610136270523\n",
      "Value Loss: 1.712473750114441\n",
      "Entropy: -3.792482852935791\n",
      "KL Divergence: 18.46548080444336\n",
      "episode: [3309. 3312. 3316. 3362.]; total steps: 870000; episodes scores: [317.12195726 317.79679797  76.61107925 317.06742432]; avg score: 257.1493147007418\n",
      "episode: [3310. 3313. 3317. 3363.]; total steps: 871000; episodes scores: [141.03280123 -15.39703575 -73.70973069 189.52875569]; avg score: 60.36369762161911\n",
      "learning timestep: 872000\n",
      "Policy Loss: 0.1492956131696701\n",
      "Value Loss: 5.965086460113525\n",
      "Entropy: -4.079980850219727\n",
      "KL Divergence: 19.68922996520996\n",
      "episode: [3311. 3314. 3321. 3364.]; total steps: 872000; episodes scores: [316.84153544 316.80213436 -55.38483392 316.76729609]; avg score: 223.75653299273557\n",
      "episode: [3312. 3315. 3322. 3366.]; total steps: 873000; episodes scores: [317.28620047 317.913531   317.18317848  -2.80463541]; avg score: 237.3945686363022\n",
      "learning timestep: 874000\n",
      "Policy Loss: -0.1503724753856659\n",
      "Value Loss: 2.0456109046936035\n",
      "Entropy: -4.150144577026367\n",
      "KL Divergence: 19.74742889404297\n",
      "episode: [3313. 3316. 3323. 3366.]; total steps: 874000; episodes scores: [315.06844099 315.98526577 317.39831405  -2.80463541]; avg score: 236.411846352596\n",
      "episode: [3315. 3317. 3324. 3367.]; total steps: 875000; episodes scores: [ 52.11856523 315.64370355  85.46183733 315.34866647]; avg score: 192.14319314610492\n",
      "learning timestep: 876000\n",
      "Policy Loss: 0.15257692337036133\n",
      "Value Loss: 11.531489372253418\n",
      "Entropy: -4.059250354766846\n",
      "KL Divergence: 18.1292724609375\n",
      "episode: [3316. 3318. 3325. 3368.]; total steps: 876000; episodes scores: [161.88678716 207.45124774 316.46331274 315.67114143]; avg score: 250.36812226759423\n",
      "episode: [3317. 3318. 3326. 3369.]; total steps: 877000; episodes scores: [314.98145257 207.45124774  15.98038406 182.57178949]; avg score: 180.2462184648004\n",
      "learning timestep: 878000\n",
      "Policy Loss: -0.1819009780883789\n",
      "Value Loss: 0.8761248588562012\n",
      "Entropy: -3.8908097743988037\n",
      "KL Divergence: 19.43241310119629\n",
      "episode: [3318. 3319. 3327. 3370.]; total steps: 878000; episodes scores: [316.48018242 316.72409202 317.54837317 133.6257418 ]; avg score: 271.09459735470426\n",
      "episode: [3318. 3320. 3329. 3371.]; total steps: 879000; episodes scores: [316.48018242 317.09827955  79.6420332  166.83531199]; avg score: 220.01395178970307\n",
      "learning timestep: 880000\n",
      "Policy Loss: -0.15603703260421753\n",
      "Value Loss: 0.5497300624847412\n",
      "Entropy: -4.000566482543945\n",
      "KL Divergence: 15.259653091430664\n",
      "episode: [3319. 3321. 3329. 3372.]; total steps: 880000; episodes scores: [198.86439888 317.62910799  79.6420332  315.24988975]; avg score: 227.84635745425328\n",
      "Rendering episode 3320.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3320.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3320.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3320.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3320.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [316.05488639]\n",
      "episode: [3320. 3322. 3330. 3373.]; total steps: 881000; episodes scores: [315.36193265 186.58002447 315.69663848 316.95085677]; avg score: 283.64736309291425\n",
      "learning timestep: 882000\n",
      "Policy Loss: -0.01297600008547306\n",
      "Value Loss: 7.447146415710449\n",
      "Entropy: -3.575497627258301\n",
      "KL Divergence: 18.161828994750977\n",
      "episode: [3321. 3323. 3331. 3375.]; total steps: 882000; episodes scores: [126.81497042 317.96986744 316.02413894 -51.70857114]; avg score: 177.2751014152225\n",
      "episode: [3323. 3325. 3333. 3376.]; total steps: 883000; episodes scores: [ 46.23082142 -44.64763563 -51.40594799 315.51279159]; avg score: 66.42250734696492\n",
      "learning timestep: 884000\n",
      "Policy Loss: 0.07409293204545975\n",
      "Value Loss: 1.2975196838378906\n",
      "Entropy: -4.094635963439941\n",
      "KL Divergence: 19.910797119140625\n",
      "episode: [3325. 3328. 3334. 3376.]; total steps: 884000; episodes scores: [  9.41091792  46.44979223 317.40804794 315.51279159]; avg score: 172.19538741897145\n",
      "episode: [3326. 3328. 3335. 3378.]; total steps: 885000; episodes scores: [193.01943078  46.44979223 315.99210009 130.35506263]; avg score: 171.45409643104162\n",
      "learning timestep: 886000\n",
      "Policy Loss: 0.1291811168193817\n",
      "Value Loss: 3.3888449668884277\n",
      "Entropy: -3.721987724304199\n",
      "KL Divergence: 17.851470947265625\n",
      "episode: [3326. 3329. 3337. 3381.]; total steps: 886000; episodes scores: [193.01943078 315.49973752  53.32520716  11.17980903]; avg score: 143.2560461188825\n",
      "episode: [3330. 3331. 3338. 3383.]; total steps: 887000; episodes scores: [-16.49469928  94.19828425  88.80146184 -53.43992115]; avg score: 28.26628141283385\n",
      "learning timestep: 888000\n",
      "Policy Loss: 0.3937006890773773\n",
      "Value Loss: 5.196717739105225\n",
      "Entropy: -3.998609781265259\n",
      "KL Divergence: 20.191551208496094\n",
      "episode: [3332. 3332. 3340. 3385.]; total steps: 888000; episodes scores: [100.32817834 317.1677762  128.6465416   52.25283338]; avg score: 149.59883237931837\n",
      "episode: [3333. 3333. 3342. 3386.]; total steps: 889000; episodes scores: [317.33661457 -29.98452385   2.22806402  19.44917382]; avg score: 77.25733213793809\n",
      "learning timestep: 890000\n",
      "Policy Loss: -0.13079142570495605\n",
      "Value Loss: 0.5830965042114258\n",
      "Entropy: -3.9007086753845215\n",
      "KL Divergence: 21.593975067138672\n",
      "episode: [3333. 3335. 3343. 3387.]; total steps: 890000; episodes scores: [317.33661457  -2.98205857 316.25737448 316.45891905]; avg score: 236.76771238015334\n",
      "episode: [3335. 3337. 3344. 3389.]; total steps: 891000; episodes scores: [ -7.4392327   -9.82353179 -40.11499391 -75.13664998]; avg score: -33.12860209671873\n",
      "learning timestep: 892000\n",
      "Policy Loss: -0.12186630815267563\n",
      "Value Loss: 2.9553232192993164\n",
      "Entropy: -3.942600965499878\n",
      "KL Divergence: 19.247459411621094\n",
      "episode: [3337. 3338. 3345. 3391.]; total steps: 892000; episodes scores: [-18.39264287 145.19242927 316.2542038   46.9555579 ]; avg score: 122.50238702535546\n",
      "episode: [3338. 3340. 3346. 3392.]; total steps: 893000; episodes scores: [189.72172585 123.91768603 316.60347134 -43.8501334 ]; avg score: 146.59818745630193\n",
      "learning timestep: 894000\n",
      "Policy Loss: -0.10023721307516098\n",
      "Value Loss: 3.2609944343566895\n",
      "Entropy: -3.6503257751464844\n",
      "KL Divergence: 21.337383270263672\n",
      "episode: [3339. 3341. 3347. 3394.]; total steps: 894000; episodes scores: [316.04282916   0.61517476 316.69259543 -23.10237678]; avg score: 152.56205564376936\n",
      "Rendering episode 3340.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3340.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3340.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3340.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3340.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [-25.30076916]\n",
      "episode: [3341. 3342. 3348. 3395.]; total steps: 895000; episodes scores: [ 46.46932494 205.45708742 155.9945589  162.94397296]; avg score: 142.71623605450586\n",
      "learning timestep: 896000\n",
      "Policy Loss: -0.059974756091833115\n",
      "Value Loss: 1.985439419746399\n",
      "Entropy: -3.582726001739502\n",
      "KL Divergence: 18.77236557006836\n",
      "episode: [3342. 3344. 3350. 3396.]; total steps: 896000; episodes scores: [315.87562217 -20.68573151  14.1197552  315.27228362]; avg score: 156.14548236915076\n",
      "episode: [3342. 3345. 3351. 3397.]; total steps: 897000; episodes scores: [315.87562217 316.78292302 173.44648943 149.31922303]; avg score: 238.8560644116557\n",
      "learning timestep: 898000\n",
      "Policy Loss: -0.10807856917381287\n",
      "Value Loss: 1.7477765083312988\n",
      "Entropy: -3.784639358520508\n",
      "KL Divergence: 21.261131286621094\n",
      "episode: [3345. 3346. 3352. 3399.]; total steps: 898000; episodes scores: [-53.30975799 121.8067275  -12.57592367   9.48328536]; avg score: 16.35108279833222\n",
      "episode: [3347. 3347. 3353. 3401.]; total steps: 899000; episodes scores: [ 83.25789118  73.64661538 316.32477636  -8.90710102]; avg score: 116.0805454737928\n",
      "learning timestep: 900000\n",
      "Policy Loss: 0.05866837128996849\n",
      "Value Loss: 6.361771583557129\n",
      "Entropy: -3.6711361408233643\n",
      "KL Divergence: 18.816499710083008\n",
      "episode: [3349. 3348. 3354. 3402.]; total steps: 900000; episodes scores: [ 33.05512614 317.44257875 316.86616167 -59.66903048]; avg score: 151.9237090188812\n",
      "episode: [3350. 3349. 3356. 3403.]; total steps: 901000; episodes scores: [ 120.93498265  315.12952297 -116.21391745  317.37972173]; avg score: 159.30757747512678\n",
      "learning timestep: 902000\n",
      "Policy Loss: -0.09125201404094696\n",
      "Value Loss: 1.0448215007781982\n",
      "Entropy: -3.534316062927246\n",
      "KL Divergence: 18.173280715942383\n",
      "episode: [3351. 3350. 3357. 3405.]; total steps: 902000; episodes scores: [315.87339562  50.59508951 201.37580035 -14.05003139]; avg score: 138.44856352198923\n",
      "episode: [3352. 3352. 3358. 3407.]; total steps: 903000; episodes scores: [317.3092897  -36.28972018 119.66047906 109.56757975]; avg score: 127.56190708227595\n",
      "learning timestep: 904000\n",
      "Policy Loss: -0.12532728910446167\n",
      "Value Loss: 1.2473478317260742\n",
      "Entropy: -3.7970361709594727\n",
      "KL Divergence: 18.980243682861328\n",
      "episode: [3353. 3353. 3359. 3407.]; total steps: 904000; episodes scores: [ 48.05177837  81.99818002 188.01514711 109.56757975]; avg score: 106.90817131116208\n",
      "episode: [3354. 3355. 3360. 3408.]; total steps: 905000; episodes scores: [ 316.65753249 -106.64568287  155.42136378  313.79045069]; avg score: 169.80591602415515\n",
      "learning timestep: 906000\n",
      "Policy Loss: 0.2672150135040283\n",
      "Value Loss: 19.781455993652344\n",
      "Entropy: -3.691317319869995\n",
      "KL Divergence: 14.187829971313477\n",
      "episode: [3355. 3356. 3361. 3409.]; total steps: 906000; episodes scores: [158.65032604 317.42061107 317.01788285 207.53927122]; avg score: 250.1570227926838\n",
      "episode: [3356. 3357. 3363. 3410.]; total steps: 907000; episodes scores: [ 51.34711196  86.58806438 -14.89755946 112.33134924]; avg score: 58.84224153205247\n",
      "learning timestep: 908000\n",
      "Policy Loss: -0.052095118910074234\n",
      "Value Loss: 1.0756736993789673\n",
      "Entropy: -3.687788963317871\n",
      "KL Divergence: 18.560745239257812\n",
      "episode: [3357. 3360. 3364. 3412.]; total steps: 908000; episodes scores: [315.62448043   4.95608629 315.3079046  -15.65069758]; avg score: 155.05944343491453\n",
      "episode: [3359. 3361. 3366. 3414.]; total steps: 909000; episodes scores: [-47.52177739 315.49189682 168.73487935 -52.26528115]; avg score: 96.10992940559586\n",
      "Rendering episode 3360.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3360.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3360.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3360.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3360.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [315.80973522]\n",
      "learning timestep: 910000\n",
      "Policy Loss: -0.018132606521248817\n",
      "Value Loss: 0.7848454713821411\n",
      "Entropy: -3.918271541595459\n",
      "KL Divergence: 15.33383560180664\n",
      "episode: [3360. 3362. 3367. 3415.]; total steps: 910000; episodes scores: [ 80.0584432  -79.9060917   31.05810377  96.04182875]; avg score: 31.813071004714832\n",
      "episode: [3362. 3363. 3368. 3416.]; total steps: 911000; episodes scores: [ 71.90851265 313.59603951  53.80898886 315.46220929]; avg score: 188.69393757917575\n",
      "learning timestep: 912000\n",
      "Policy Loss: 0.05033503845334053\n",
      "Value Loss: 3.610290050506592\n",
      "Entropy: -3.619378089904785\n",
      "KL Divergence: 15.034618377685547\n",
      "episode: [3362. 3364. 3369. 3417.]; total steps: 912000; episodes scores: [ 71.90851265 314.6282142  315.67740681 316.87793223]; avg score: 254.77301647518595\n",
      "episode: [3363. 3365. 3370. 3420.]; total steps: 913000; episodes scores: [315.40371209 152.764743   206.61288436  47.06822646]; avg score: 180.4623914801567\n",
      "learning timestep: 914000\n",
      "Policy Loss: 0.029098987579345703\n",
      "Value Loss: 0.4575985074043274\n",
      "Entropy: -3.76301908493042\n",
      "KL Divergence: 18.4549560546875\n",
      "episode: [3365. 3367. 3371. 3421.]; total steps: 914000; episodes scores: [-17.98437542 124.91904903 315.16952535 109.83224496]; avg score: 132.98411097909514\n",
      "episode: [3366. 3368. 3372. 3422.]; total steps: 915000; episodes scores: [314.75574743 -22.10927745 203.8021937   54.55462447]; avg score: 137.75082203557338\n",
      "learning timestep: 916000\n",
      "Policy Loss: -0.13001693785190582\n",
      "Value Loss: 0.9957016110420227\n",
      "Entropy: -3.8987088203430176\n",
      "KL Divergence: 19.998043060302734\n",
      "episode: [3367. 3369. 3373. 3423.]; total steps: 916000; episodes scores: [316.30825282 314.38660248 316.01905732 316.11701671]; avg score: 315.70773233142233\n",
      "episode: [3368. 3370. 3374. 3424.]; total steps: 917000; episodes scores: [316.94008477 314.82201748 126.1807816  316.27140338]; avg score: 268.5535718051677\n",
      "learning timestep: 918000\n",
      "Policy Loss: -0.03782651573419571\n",
      "Value Loss: 0.9977867603302002\n",
      "Entropy: -3.6643471717834473\n",
      "KL Divergence: 18.117137908935547\n",
      "episode: [3369. 3373. 3375. 3425.]; total steps: 918000; episodes scores: [ 153.49176527 -104.89849329  314.17320102   28.90924216]; avg score: 97.91892878730812\n",
      "episode: [3369. 3374. 3376. 3427.]; total steps: 919000; episodes scores: [153.49176527 130.28107764 316.60099684  82.70641792]; avg score: 170.77006441915927\n",
      "learning timestep: 920000\n",
      "Policy Loss: 0.08380649983882904\n",
      "Value Loss: 3.5773377418518066\n",
      "Entropy: -3.8616185188293457\n",
      "KL Divergence: 16.07085418701172\n",
      "episode: [3371. 3375. 3376. 3427.]; total steps: 920000; episodes scores: [ 13.22429912 315.26445672 316.60099684  82.70641792]; avg score: 181.9490426519219\n",
      "episode: [3372. 3375. 3377. 3428.]; total steps: 921000; episodes scores: [315.48600765 315.26445672 315.43788854 314.13116158]; avg score: 315.0798786239461\n",
      "learning timestep: 922000\n",
      "Policy Loss: -0.0719185620546341\n",
      "Value Loss: 5.144952297210693\n",
      "Entropy: -3.7956655025482178\n",
      "KL Divergence: 15.638320922851562\n",
      "episode: [3373. 3377. 3378. 3429.]; total steps: 922000; episodes scores: [ 20.97727129  77.91605545 315.7199828  316.48726464]; avg score: 182.77514354360653\n",
      "episode: [3374. 3378. 3380. 3430.]; total steps: 923000; episodes scores: [314.17278996 316.0485847  -71.74578621 315.78328428]; avg score: 218.56471818579598\n",
      "learning timestep: 924000\n",
      "Policy Loss: 0.13365231454372406\n",
      "Value Loss: 11.785202026367188\n",
      "Entropy: -3.8916635513305664\n",
      "KL Divergence: 19.007862091064453\n",
      "episode: [3375. 3378. 3381. 3432.]; total steps: 924000; episodes scores: [314.01160943 316.0485847  316.40559383 -65.72250863]; avg score: 220.18581983336782\n",
      "episode: [3376. 3379. 3382. 3434.]; total steps: 925000; episodes scores: [ 315.16295859  315.01911161  313.38585506 -109.59650678]; avg score: 208.49285462090114\n",
      "learning timestep: 926000\n",
      "Policy Loss: -0.016899188980460167\n",
      "Value Loss: 3.4889090061187744\n",
      "Entropy: -3.765815496444702\n",
      "KL Divergence: 14.060478210449219\n",
      "episode: [3377. 3380. 3383. 3434.]; total steps: 926000; episodes scores: [ 314.23611482  315.7180774   314.73065872 -109.59650678]; avg score: 208.77208603921554\n",
      "episode: [3378. 3382. 3384. 3436.]; total steps: 927000; episodes scores: [315.39468113 -42.31801612  -5.35866309   6.69334985]; avg score: 68.60283794124298\n",
      "learning timestep: 928000\n",
      "Policy Loss: -0.02181215211749077\n",
      "Value Loss: 0.8557130694389343\n",
      "Entropy: -3.952974557876587\n",
      "KL Divergence: 19.049942016601562\n",
      "episode: [3378. 3383. 3385. 3437.]; total steps: 928000; episodes scores: [315.39468113 314.52271481 316.116044   315.79302364]; avg score: 315.4566158939951\n",
      "Rendering episode 3380.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3380.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3380.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3380.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3380.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.93765283]\n",
      "episode: [3380. 3384. 3387. 3438.]; total steps: 929000; episodes scores: [ 87.50717285 152.98746075 -47.46608924  61.34447865]; avg score: 63.593255750082974\n",
      "learning timestep: 930000\n",
      "Policy Loss: 0.09568861871957779\n",
      "Value Loss: 8.465245246887207\n",
      "Entropy: -3.7158150672912598\n",
      "KL Divergence: 16.94952964782715\n",
      "episode: [3381. 3386. 3388. 3439.]; total steps: 930000; episodes scores: [-13.25285215 142.34562335 114.1906342  314.99011747]; avg score: 139.5683807161621\n",
      "episode: [3382. 3387. 3389. 3440.]; total steps: 931000; episodes scores: [314.78091788 193.02194152 315.55608555 314.97528298]; avg score: 284.58355698112416\n",
      "learning timestep: 932000\n",
      "Policy Loss: 0.10725779086351395\n",
      "Value Loss: 6.036057949066162\n",
      "Entropy: -3.7694549560546875\n",
      "KL Divergence: 15.070615768432617\n",
      "episode: [3383. 3387. 3390. 3441.]; total steps: 932000; episodes scores: [144.20812739 193.02194152 182.61334608 315.91505829]; avg score: 208.9396183196398\n",
      "episode: [3384. 3388. 3391. 3442.]; total steps: 933000; episodes scores: [316.18309885 316.73679899  97.07310466  24.90031192]; avg score: 188.72332860421304\n",
      "learning timestep: 934000\n",
      "Policy Loss: -0.13043391704559326\n",
      "Value Loss: 0.5488569736480713\n",
      "Entropy: -3.8414933681488037\n",
      "KL Divergence: 18.08915138244629\n",
      "episode: [3387. 3391. 3392. 3443.]; total steps: 934000; episodes scores: [ 68.26704289  14.21797484 314.80471824 316.08041579]; avg score: 178.3425379387918\n",
      "episode: [3387. 3392. 3393. 3445.]; total steps: 935000; episodes scores: [ 68.26704289  15.12473418 316.25585611 -15.49563181]; avg score: 96.03800034317388\n",
      "learning timestep: 936000\n",
      "Policy Loss: -0.17830759286880493\n",
      "Value Loss: 1.1804795265197754\n",
      "Entropy: -3.9101779460906982\n",
      "KL Divergence: 21.016958236694336\n",
      "episode: [3388. 3393. 3394. 3446.]; total steps: 936000; episodes scores: [315.65674637  94.42184634 135.91545914  35.67707472]; avg score: 145.4177816420328\n",
      "episode: [3389. 3395. 3395. 3447.]; total steps: 937000; episodes scores: [316.08011993  -5.72919643 135.00155639 315.12431108]; avg score: 190.11919774252425\n",
      "learning timestep: 938000\n",
      "Policy Loss: -0.04722427576780319\n",
      "Value Loss: 5.394569396972656\n",
      "Entropy: -3.974968433380127\n",
      "KL Divergence: 19.518869400024414\n",
      "episode: [3391. 3396. 3397. 3450.]; total steps: 938000; episodes scores: [ -12.28400659  317.07812356  -59.00804186 -105.60825   ]; avg score: 35.04445627869865\n",
      "episode: [3392. 3397. 3398. 3451.]; total steps: 939000; episodes scores: [ 36.2841825  315.37835748 -29.78604467 315.05709128]; avg score: 159.2333966442109\n",
      "learning timestep: 940000\n",
      "Policy Loss: -0.032327864319086075\n",
      "Value Loss: 1.3268998861312866\n",
      "Entropy: -3.9389851093292236\n",
      "KL Divergence: 21.241226196289062\n",
      "episode: [3393. 3397. 3399. 3451.]; total steps: 940000; episodes scores: [316.37347382 315.37835748 316.62680309 315.05709128]; avg score: 315.85893141809385\n",
      "episode: [3394. 3398. 3400. 3453.]; total steps: 941000; episodes scores: [316.73608067 315.01257097 156.07462722  93.11111961]; avg score: 220.23359961934574\n",
      "learning timestep: 942000\n",
      "Policy Loss: 0.13635683059692383\n",
      "Value Loss: 17.524925231933594\n",
      "Entropy: -3.9279837608337402\n",
      "KL Divergence: 19.281063079833984\n",
      "episode: [3395. 3399. 3402. 3454.]; total steps: 942000; episodes scores: [316.60004362 316.555949    71.1620933  315.05025181]; avg score: 254.84208443043883\n",
      "episode: [3396. 3401. 3403. 3454.]; total steps: 943000; episodes scores: [ 148.52885184 -111.11048936  -38.21856686  315.05025181]; avg score: 78.56251185448015\n",
      "learning timestep: 944000\n",
      "Policy Loss: -0.08098176121711731\n",
      "Value Loss: 0.4519689679145813\n",
      "Entropy: -3.9456872940063477\n",
      "KL Divergence: 20.188283920288086\n",
      "episode: [3397. 3402. 3404. 3455.]; total steps: 944000; episodes scores: [314.34481368 315.90329382 191.58747708 313.44916368]; avg score: 283.8211870655863\n",
      "episode: [3398. 3405. 3405. 3456.]; total steps: 945000; episodes scores: [ 79.24653823 -75.03027214 315.01974278 316.89934782]; avg score: 159.0338391703017\n",
      "learning timestep: 946000\n",
      "Policy Loss: 0.028673306107521057\n",
      "Value Loss: 3.4454634189605713\n",
      "Entropy: -3.8808202743530273\n",
      "KL Divergence: 21.184993743896484\n",
      "episode: [3399. 3406. 3406. 3457.]; total steps: 946000; episodes scores: [315.17508732 317.99647613 315.54001894 315.54712267]; avg score: 316.06467626377\n",
      "Rendering episode 3400.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3400.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3400.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3400.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [315.49335962]\n",
      "episode: [3400. 3407. 3407. 3458.]; total steps: 947000; episodes scores: [316.72524026 143.1263531   29.27451047 192.94567492]; avg score: 170.51794468614787\n",
      "learning timestep: 948000\n",
      "Policy Loss: -0.07818951457738876\n",
      "Value Loss: 0.5850096940994263\n",
      "Entropy: -3.8072075843811035\n",
      "KL Divergence: 19.95567512512207\n",
      "episode: [3401. 3408. 3409. 3460.]; total steps: 948000; episodes scores: [313.93339123 316.59829233 117.98504565 -53.0659114 ]; avg score: 173.86270445112635\n",
      "episode: [3402. 3409. 3409. 3461.]; total steps: 949000; episodes scores: [  4.94323629 315.33594349 117.98504565 315.55622047]; avg score: 188.45511147741794\n",
      "learning timestep: 950000\n",
      "Policy Loss: -0.003778832731768489\n",
      "Value Loss: 1.9230012893676758\n",
      "Entropy: -3.847870111465454\n",
      "KL Divergence: 22.7615909576416\n",
      "episode: [3403. 3410. 3410. 3462.]; total steps: 950000; episodes scores: [210.27420172  83.29703476 316.70352772 206.96539741]; avg score: 204.31004040350027\n",
      "episode: [3405. 3411. 3411. 3464.]; total steps: 951000; episodes scores: [ 10.36280446 314.875186   314.95412163  53.9718315 ]; avg score: 173.54098589679273\n",
      "learning timestep: 952000\n",
      "Policy Loss: -0.067214734852314\n",
      "Value Loss: 3.1176044940948486\n",
      "Entropy: -3.831681251525879\n",
      "KL Divergence: 16.173580169677734\n",
      "episode: [3406. 3413. 3412. 3466.]; total steps: 952000; episodes scores: [315.8265686  -33.49263437 315.37155195 -21.88007281]; avg score: 143.95635334228325\n",
      "episode: [3406. 3414. 3414. 3467.]; total steps: 953000; episodes scores: [315.8265686    4.31394374 -32.2699127  316.22177144]; avg score: 151.02309276881414\n",
      "learning timestep: 954000\n",
      "Policy Loss: -0.12439341843128204\n",
      "Value Loss: 0.4952246844768524\n",
      "Entropy: -3.770724296569824\n",
      "KL Divergence: 18.40816879272461\n",
      "episode: [3408. 3416. 3415. 3469.]; total steps: 954000; episodes scores: [ 14.14173066  63.33931296 314.89847123  26.00669205]; avg score: 104.59655172820241\n",
      "episode: [3409. 3417. 3416. 3470.]; total steps: 955000; episodes scores: [314.64360489 209.6592607   63.72355489 316.19593139]; avg score: 226.0555879678565\n",
      "learning timestep: 956000\n",
      "Policy Loss: 0.056889668107032776\n",
      "Value Loss: 4.948521614074707\n",
      "Entropy: -3.977656841278076\n",
      "KL Divergence: 18.316265106201172\n",
      "episode: [3410. 3418. 3417. 3471.]; total steps: 956000; episodes scores: [316.39205373 315.89964125 316.06174448  -9.50292904]; avg score: 234.71262760535058\n",
      "episode: [3411. 3418. 3418. 3472.]; total steps: 957000; episodes scores: [316.38811557 315.89964125 187.14842353 316.15484551]; avg score: 283.8977564682165\n",
      "learning timestep: 958000\n",
      "Policy Loss: 0.09494142234325409\n",
      "Value Loss: 12.001192092895508\n",
      "Entropy: -3.8642168045043945\n",
      "KL Divergence: 19.018001556396484\n",
      "episode: [3412. 3420. 3419. 3474.]; total steps: 958000; episodes scores: [317.55382811 102.43289844 314.07572922  53.22050271]; avg score: 196.82073961852603\n",
      "episode: [3412. 3423. 3421. 3475.]; total steps: 959000; episodes scores: [317.55382811  93.76760146  93.39574979 316.31756016]; avg score: 205.25868487940272\n",
      "learning timestep: 960000\n",
      "Policy Loss: 0.028149057179689407\n",
      "Value Loss: 1.185567021369934\n",
      "Entropy: -3.740589141845703\n",
      "KL Divergence: 17.433551788330078\n",
      "episode: [3413. 3424. 3422. 3475.]; total steps: 960000; episodes scores: [ 314.50347743 -107.01340172  316.56278431  316.31756016]; avg score: 210.09260504477763\n",
      "episode: [3415. 3427. 3424. 3477.]; total steps: 961000; episodes scores: [  2.09914309 -29.32586024 -59.01625797 147.78859448]; avg score: 15.38640483911864\n",
      "learning timestep: 962000\n",
      "Policy Loss: -0.15682373940944672\n",
      "Value Loss: 2.8114871978759766\n",
      "Entropy: -3.679945468902588\n",
      "KL Divergence: 15.701377868652344\n",
      "episode: [3416. 3428. 3425. 3477.]; total steps: 962000; episodes scores: [315.36339927  73.36460568 316.23908287 147.78859448]; avg score: 213.1889205728208\n",
      "episode: [3417. 3429. 3427. 3480.]; total steps: 963000; episodes scores: [ -2.47328726 146.36180048 -57.37848481 -56.48807032]; avg score: 7.505489522719342\n",
      "Rendering episode 3420.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3420.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3420.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3420.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3420.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [314.89965085]\n",
      "learning timestep: 964000\n",
      "Policy Loss: 0.08691410720348358\n",
      "Value Loss: 3.349274158477783\n",
      "Entropy: -4.062839031219482\n",
      "KL Divergence: 20.035083770751953\n",
      "episode: [3421. 3431. 3429. 3482.]; total steps: 964000; episodes scores: [-54.99852448 -33.48058432 -29.77327024  97.713798  ]; avg score: -5.134645262367375\n",
      "episode: [3422. 3432. 3431. 3483.]; total steps: 965000; episodes scores: [ 87.81923426 315.58086994 -31.52869425 315.71488572]; avg score: 171.89657391613852\n",
      "learning timestep: 966000\n",
      "Policy Loss: 0.014023419469594955\n",
      "Value Loss: 2.3408071994781494\n",
      "Entropy: -3.7772445678710938\n",
      "KL Divergence: 19.579517364501953\n",
      "episode: [3424. 3433. 3432. 3483.]; total steps: 966000; episodes scores: [-42.26073575 315.38895704 156.18209139 315.71488572]; avg score: 186.25629960151196\n",
      "episode: [3425. 3433. 3433. 3486.]; total steps: 967000; episodes scores: [316.58244125 315.38895704 315.77666043  21.33686816]; avg score: 242.27123171900558\n",
      "learning timestep: 968000\n",
      "Policy Loss: -0.05514299124479294\n",
      "Value Loss: 4.521271228790283\n",
      "Entropy: -3.9295644760131836\n",
      "KL Divergence: 20.772789001464844\n",
      "episode: [3426. 3434. 3434. 3486.]; total steps: 968000; episodes scores: [316.7481718  316.36878283 316.04753185  21.33686816]; avg score: 242.6253386602031\n",
      "episode: [3426. 3436. 3434. 3487.]; total steps: 969000; episodes scores: [316.7481718   75.98009769 316.04753185 317.07096651]; avg score: 256.4616919658629\n",
      "learning timestep: 970000\n",
      "Policy Loss: 0.010242227464914322\n",
      "Value Loss: 3.7857093811035156\n",
      "Entropy: -3.9646732807159424\n",
      "KL Divergence: 14.674263000488281\n",
      "episode: [3428. 3437. 3435. 3490.]; total steps: 970000; episodes scores: [  54.64189919  193.42409868  314.36104136 -105.60997399]; avg score: 114.20426630914433\n",
      "episode: [3429. 3437. 3437. 3491.]; total steps: 971000; episodes scores: [317.20048748 193.42409868  88.7922418  315.38087083]; avg score: 228.6994246973483\n",
      "learning timestep: 972000\n",
      "Policy Loss: 0.017723448574543\n",
      "Value Loss: 2.594107151031494\n",
      "Entropy: -3.888859987258911\n",
      "KL Divergence: 17.764738082885742\n",
      "episode: [3430. 3439. 3437. 3493.]; total steps: 972000; episodes scores: [206.25017654 112.89456465  88.7922418  -63.38934816]; avg score: 86.13690870571904\n",
      "episode: [3431. 3440. 3438. 3494.]; total steps: 973000; episodes scores: [315.6020685  314.84112236 315.87810246 317.2161937 ]; avg score: 315.884371752758\n",
      "learning timestep: 974000\n",
      "Policy Loss: -0.08476746082305908\n",
      "Value Loss: 2.0830936431884766\n",
      "Entropy: -4.075442314147949\n",
      "KL Divergence: 22.088428497314453\n",
      "episode: [3433. 3441. 3440. 3495.]; total steps: 974000; episodes scores: [-32.40700496  91.32881157  17.22981607 -41.7457391 ]; avg score: 8.601470892664347\n",
      "episode: [3434. 3442. 3441. 3496.]; total steps: 975000; episodes scores: [160.14186904 319.60470061 317.70984704 315.74973612]; avg score: 278.3015382042171\n",
      "learning timestep: 976000\n",
      "Policy Loss: 0.1965618133544922\n",
      "Value Loss: 17.637483596801758\n",
      "Entropy: -3.7184441089630127\n",
      "KL Divergence: 21.786027908325195\n",
      "episode: [3436. 3443. 3442. 3498.]; total steps: 976000; episodes scores: [ 63.95530379 316.2159764  114.10596218 -55.17243757]; avg score: 109.77620119768356\n",
      "episode: [3438. 3444. 3443. 3499.]; total steps: 977000; episodes scores: [ 73.48047042  42.02784859 315.90303982 316.69659136]; avg score: 187.02698754581417\n",
      "learning timestep: 978000\n",
      "Policy Loss: 0.010959237813949585\n",
      "Value Loss: 0.8767046332359314\n",
      "Entropy: -3.7932910919189453\n",
      "KL Divergence: 18.90713882446289\n",
      "episode: [3439. 3445. 3446. 3500.]; total steps: 978000; episodes scores: [146.82863491 162.10005895 -19.15008379 110.89128814]; avg score: 100.16747455198777\n",
      "Rendering episode 3440.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3440.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3440.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3440.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3440.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [317.74082209]\n",
      "episode: [3440. 3447. 3449. 3501.]; total steps: 979000; episodes scores: [317.90687816  18.3218594  -34.92299195 122.59574715]; avg score: 105.97537318938814\n",
      "learning timestep: 980000\n",
      "Policy Loss: 0.038586027920246124\n",
      "Value Loss: 3.8156585693359375\n",
      "Entropy: -3.9633066654205322\n",
      "KL Divergence: 20.802865982055664\n",
      "episode: [3441. 3449. 3450. 3503.]; total steps: 980000; episodes scores: [197.08912431  -6.08862445 137.47727282  97.49558754]; avg score: 106.4933400533734\n",
      "episode: [3442. 3450. 3451. 3503.]; total steps: 981000; episodes scores: [ 62.7209011  317.81521993 316.63948781  97.49558754]; avg score: 198.66779909335654\n",
      "learning timestep: 982000\n",
      "Policy Loss: 0.1340305060148239\n",
      "Value Loss: 1.9921256303787231\n",
      "Entropy: -3.9898931980133057\n",
      "KL Divergence: 22.447311401367188\n",
      "episode: [3443. 3452. 3452. 3505.]; total steps: 982000; episodes scores: [ 54.43652721  95.60401714 315.6577064    1.88646369]; avg score: 116.89617861025067\n",
      "episode: [3445. 3453. 3453. 3507.]; total steps: 983000; episodes scores: [-32.53463593 -13.35637473 317.51330533 -11.97772978]; avg score: 64.91114121957722\n",
      "learning timestep: 984000\n",
      "Policy Loss: -0.08779320120811462\n",
      "Value Loss: 0.9815378785133362\n",
      "Entropy: -3.942476987838745\n",
      "KL Divergence: 21.798053741455078\n",
      "episode: [3446. 3455. 3453. 3509.]; total steps: 984000; episodes scores: [317.60232808  83.29446924 317.51330533 -69.10798139]; avg score: 162.3255303122297\n",
      "episode: [3448. 3456. 3454. 3511.]; total steps: 985000; episodes scores: [-18.97288761  -8.96104088 316.97543838 -35.20550982]; avg score: 63.459000020461914\n",
      "learning timestep: 986000\n",
      "Policy Loss: -0.09080184251070023\n",
      "Value Loss: 1.5733280181884766\n",
      "Entropy: -3.69115948677063\n",
      "KL Divergence: 18.625659942626953\n",
      "episode: [3450. 3458. 3455. 3514.]; total steps: 986000; episodes scores: [-18.77582348  22.33138488 315.33995464  55.53112723]; avg score: 93.60666081774309\n",
      "episode: [3451. 3459. 3458. 3516.]; total steps: 987000; episodes scores: [178.7215256  317.2563801   59.81772259  27.97097636]; avg score: 145.94165116104276\n",
      "learning timestep: 988000\n",
      "Policy Loss: 1.1910218745470047e-05\n",
      "Value Loss: 6.764796257019043\n",
      "Entropy: -3.8753843307495117\n",
      "KL Divergence: 23.043798446655273\n",
      "episode: [3453. 3460. 3459. 3518.]; total steps: 988000; episodes scores: [  5.05579706  83.13284966 191.26765731  17.49402616]; avg score: 74.23758254934201\n",
      "episode: [3454. 3463. 3460. 3519.]; total steps: 989000; episodes scores: [ 90.88743292  61.31656598 -46.94113543   0.9938857 ]; avg score: 26.56418729227686\n",
      "learning timestep: 990000\n",
      "Policy Loss: 0.023582715541124344\n",
      "Value Loss: 2.2531161308288574\n",
      "Entropy: -3.95005464553833\n",
      "KL Divergence: 22.561607360839844\n",
      "episode: [3455. 3464. 3462. 3521.]; total steps: 990000; episodes scores: [317.3359329  213.83355872 -20.43938963 103.87058266]; avg score: 153.6501711609514\n",
      "episode: [3456. 3465. 3465. 3522.]; total steps: 991000; episodes scores: [88.67169754 26.0752042  66.81379037 66.16058499]; avg score: 61.93031927482942\n",
      "learning timestep: 992000\n",
      "Policy Loss: 0.058180250227451324\n",
      "Value Loss: 1.8330280780792236\n",
      "Entropy: -3.675778865814209\n",
      "KL Divergence: 21.94503402709961\n",
      "episode: [3458. 3467. 3465. 3525.]; total steps: 992000; episodes scores: [ 61.33209579  68.4548896   66.81379037 -30.85975824]; avg score: 41.43525438230651\n",
      "episode: [3459. 3469. 3467. 3527.]; total steps: 993000; episodes scores: [ 101.04388499   19.90177412  -26.18806389 -104.46258778]; avg score: -2.426248139724418\n",
      "Rendering episode 3460.0 during training...\n",
      "seed value:42\n",
      "episode number sent to renderer:3460.0\n",
      "rendering episode...\n",
      "Moviepy - Building video BipedalWalker/ppo/renders/train/episode_3460.0.mp4.\n",
      "Moviepy - Writing video BipedalWalker/ppo/renders/train/episode_3460.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready BipedalWalker/ppo/renders/train/episode_3460.0.mp4\n",
      "episode rendered\n",
      "Episode 1/1 - Score: [182.42518424]\n",
      "learning timestep: 994000\n",
      "Policy Loss: 0.10131906718015671\n",
      "Value Loss: 3.121669292449951\n",
      "Entropy: -3.8008174896240234\n",
      "KL Divergence: 22.508853912353516\n",
      "episode: [3461. 3470. 3468. 3527.]; total steps: 994000; episodes scores: [  75.62734458  316.29980166  202.74966529 -104.46258778]; avg score: 122.55355594026153\n",
      "episode: [3462. 3471. 3469. 3529.]; total steps: 995000; episodes scores: [  85.47222923  316.37270242   76.05232748 -105.44546949]; avg score: 93.11294740960201\n",
      "learning timestep: 996000\n",
      "Policy Loss: -0.004829706624150276\n",
      "Value Loss: 3.143052101135254\n",
      "Entropy: -3.8219408988952637\n",
      "KL Divergence: 17.429058074951172\n",
      "episode: [3463. 3472. 3472. 3530.]; total steps: 996000; episodes scores: [315.80129728 -34.08754096  -4.74484256 316.22482518]; avg score: 148.2984347353988\n",
      "episode: [3464. 3474. 3473. 3532.]; total steps: 997000; episodes scores: [316.2854788  -47.35522474 -33.79273602  46.70430571]; avg score: 70.46045593486105\n",
      "learning timestep: 998000\n",
      "Policy Loss: 0.12172634899616241\n",
      "Value Loss: 10.802040100097656\n",
      "Entropy: -3.8703651428222656\n",
      "KL Divergence: 18.153369903564453\n",
      "episode: [3465. 3475. 3474. 3533.]; total steps: 998000; episodes scores: [ 70.51733225 315.1156512  315.71951118  27.09872883]; avg score: 182.11280586652524\n",
      "episode: [3466. 3476. 3475. 3534.]; total steps: 999000; episodes scores: [316.51375543 316.13645623 315.29649054 318.28141889]; avg score: 316.55703027413387\n",
      "learning timestep: 1000000\n",
      "Policy Loss: -0.051326923072338104\n",
      "Value Loss: 1.7870274782180786\n",
      "Entropy: -3.760805606842041\n",
      "KL Divergence: 20.29871368408203\n",
      "episode: [3467. 3477. 3477. 3535.]; total steps: 1000000; episodes scores: [194.9107279  314.20444179 -19.12796167 317.25708901]; avg score: 201.81107425451833\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>action_0</td><td>██▆▃▁▄▄▁▅▄▄▁▆▃▃▅▄▅▂▆▃▅▆▃█▁▄▅▄▇▆▄▇▆▆▅▂▇▆█</td></tr><tr><td>action_1</td><td>▁▅▂▇▅▅▆▃▆▆█▆▄█▅█▅▅▅▇▇▅▅▃█▆▆▅▅▅▃▃▄██▇▃▃▄█</td></tr><tr><td>action_2</td><td>▅▃▄▇▃▄▆▄▁▅▅▇▁█▂▁▄▄▃▃▅█▆▇▆▆▆▆▆█▅▅▅▆▇▇▅▇▅█</td></tr><tr><td>action_3</td><td>▅▇▅█▂▂▆▂▁▃▆█▁█▃▇▄▃█▅▁▇▂▁▅▂▂▅▃▃▅██▂▃▂██▄▅</td></tr><tr><td>actor_loss</td><td>▄▄▅▃▇▆▅▂▄▄█▄▅▆▅▅▅▁▁▄▂▅▂▆▅▆▅▃▄▅▅▅▄▄▃▃▃▆▅▄</td></tr><tr><td>advantages</td><td>█▁▃▄▂▆█▄▅▄▄▂▂▅▅▅▄▅▄▃▅▃▅▄▄▂▃▅▅▅▅▅▆▅▅▃▅▄▃▆</td></tr><tr><td>avg_env_scores</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▂▅▅▂▂▃▄▃▆▄▄█▆▆█▃▆▆▅</td></tr><tr><td>critic_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▂▁▂▂▄▂▂▁▁▂▁█▇▂</td></tr><tr><td>entropy</td><td>██▅▅▆▄▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>episode</td><td>▁▁▂▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>episode_length</td><td>██▁▁▁▁▁▁▁▁▁▂▂▂▃▂▃▅▅▇▃▂▃▂▄▆▆▆▂▂▂▄▆▂▃▆▆▄▆▄</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▄▁███▄▅▄▂█▃▂▂█▄███▅█▅█▃█▄</td></tr><tr><td>kl_divergence</td><td>▁▁▁▁▂▂▂▂▂▃▄▅▄▆▆▅▅▆▅▆▆▆▆▆▅▇▆▇▇▆▆███▇▇█▇▆▇</td></tr><tr><td>param1</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███</td></tr><tr><td>param2</td><td>▁▁▁▁▂▅▅▆▅▅▆▅▆▇▇▇▆▇▇▇▇▇██▇▇▇▇▇█▇█▇▇▇█▇▇▇█</td></tr><tr><td>returns</td><td>▁▂▁▂▂▂▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇███████████</td></tr><tr><td>step_reward</td><td>▁▁▄▂▃▄▅▃▅▄▅▅▅▅▄▇▃▅▆▆▅▅▆▇▄▅▅▆▅▅▇▆▅▇█▇▇▇▅▄</td></tr><tr><td>values</td><td>▁▁▁▂▂▃▃▃▃▃▅▅▅▅▆▇▇▇▇▇▇▇▇▇██▇█▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>action_0</td><td>-0.94963</td></tr><tr><td>action_1</td><td>0.19428</td></tr><tr><td>action_2</td><td>-0.39075</td></tr><tr><td>action_3</td><td>0.54067</td></tr><tr><td>actor_loss</td><td>-0.05133</td></tr><tr><td>advantages</td><td>0.0</td></tr><tr><td>avg_env_scores</td><td>201.81107</td></tr><tr><td>best</td><td>False</td></tr><tr><td>critic_loss</td><td>1.78703</td></tr><tr><td>entropy</td><td>-3.76081</td></tr><tr><td>episode</td><td>3467</td></tr><tr><td>episode_length</td><td>1171</td></tr><tr><td>episode_reward</td><td>314.20444</td></tr><tr><td>kl_divergence</td><td>20.29871</td></tr><tr><td>param1</td><td>13.18257</td></tr><tr><td>param2</td><td>10.49646</td></tr><tr><td>returns</td><td>26.06729</td></tr><tr><td>step_reward</td><td>0.2841</td></tr><tr><td>values</td><td>26.86747</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train-143</strong> at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/wrxv3pvr' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/wrxv3pvr</a><br/> View project at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a><br/>Synced 5 W&B file(s), 0 media file(s), 142 artifact file(s) and 176 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241023_194801-wrxv3pvr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "# env_id = 'Humanoid-v5'\n",
    "# env_id = \"Reacher-v5\"\n",
    "# env_id = \"Walker2d-v5\"\n",
    "# env_id = 'ALE/SpaceInvaders-ram-v5'\n",
    "# env_id = \"CarRacing-v2\"\n",
    "\n",
    "timesteps = 1_000_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 4\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.001\n",
    "loss = 'hybrid'\n",
    "kl_coeff = 0.0\n",
    "normalize_advantages = True\n",
    "normalize_values = False\n",
    "norm_clip = np.inf\n",
    "grad_clip = 40.0\n",
    "reward_clip = 1.0\n",
    "lambda_ = 0.0\n",
    "distribution = 'beta'\n",
    "device = 'cuda'\n",
    "\n",
    "# Render Settings\n",
    "render_freq = 20\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'BipedalWalker-v3'\n",
    "run_name = None\n",
    "callbacks = [WandbCallback(project_name, run_name)]\n",
    "# callbacks = []\n",
    "\n",
    "seed = 42\n",
    "env = gym.make(env_id)\n",
    "\n",
    "save_dir = \"BipedalWalker\"\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "\n",
    "# Build policy model\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "layer_config = [\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 32, 'kernel_size': (8, 8), 'stride': 4, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (4, 4), 'stride': 2, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (3, 3), 'stride': 1, 'padding': 0}},\n",
    "    # {'type': 'flatten'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "]\n",
    "output_layer_kernel = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "policy = StochasticContinuousPolicy(env, layer_config, output_layer_kernel, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, layer_config, output_layer_kernel, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff,\n",
    "          loss=loss, kl_coefficient=kl_coeff, normalize_advantages=normalize_advantages, normalize_values=normalize_values, value_normalizer_clip=norm_clip, policy_grad_clip=grad_clip,\n",
    "          reward_clip=reward_clip, lambda_=lambda_, callbacks=callbacks, save_dir=save_dir,device=device)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs, seed=seed, render_freq=render_freq)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vec = ppo._initialize_env(100, 2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in env_vec.envs:\n",
    "    print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'ALE/SpaceInvaders-ram-v5'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/walker2d/ppo/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker = PPO.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanoid.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = walker.test(10, render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.001, 0.101, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
