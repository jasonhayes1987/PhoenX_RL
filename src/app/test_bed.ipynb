{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import umap\n",
    "import pynndescent\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Numba version:\", numba.__version__)\n",
    "print(\"UMAP version:\", umap.__version__)\n",
    "print(\"PyNNDescent version:\", pynndescent.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ale_py\n",
    "\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium.vector import VectorEnv, SyncVectorEnv\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, CriticModel, StochasticDiscretePolicy\n",
    "from rl_agents import PPO, DDPG, Reinforce, ActorCritic, TD3, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "# from helper import Normalizer\n",
    "from buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from noise import NormalNoise\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "import dash_utils\n",
    "from env_wrapper import EnvWrapper, GymnasiumWrapper\n",
    "from schedulers import ScheduleWrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mujoco version: {mujoco.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v4')\n",
    "env_spec = env.spec\n",
    "wrap_env = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.env.env.initial_qpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_env.env = wrap_env._initialize_env(num_envs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = wrap_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if T.cuda.is_available():\n",
    "        return T.device('cuda')\n",
    "    else:\n",
    "        return T.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "# env = gym.make('Pendulum-v1')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='BipedalWalker-v3', entry_point='gymnasium.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, disable_env_checker=False, kwargs={'render_mode': None}, namespace=None, name='BipedalWalker', version=3, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "for e in env_wrap.env.envs:\n",
    "    print(e.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorModel(\n",
       "  (layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=24, out_features=400, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "    (dense_2): Linear(in_features=400, out_features=300, bias=True)\n",
       "    (relu_3): ReLU()\n",
       "  )\n",
       "  (output_layer): ModuleDict(\n",
       "    (actor_mu): Linear(in_features=300, out_features=4, bias=True)\n",
       "    (actor_pi): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticModel(\n",
       "  (layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=24, out_features=400, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (merged_layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=404, out_features=300, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (output_layer): ModuleDict(\n",
       "    (state_action_value): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10000, 24)\n"
     ]
    }
   ],
   "source": [
    "# replay_buffer = ReplayBuffer(env_wrap, 100000, device=device)\n",
    "replay_buffer = PrioritizedReplayBuffer(env_wrap, 10000, update_freq=10, device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, stddev=0.2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'PrioritizedReplayBuffer',\n",
       " 'config': {'env': '{\"type\": \"GymnasiumWrapper\", \"env\": \"{\\\\\"id\\\\\": \\\\\"BipedalWalker-v3\\\\\", \\\\\"entry_point\\\\\": \\\\\"gymnasium.envs.box2d.bipedal_walker:BipedalWalker\\\\\", \\\\\"reward_threshold\\\\\": 300, \\\\\"nondeterministic\\\\\": false, \\\\\"max_episode_steps\\\\\": 1600, \\\\\"order_enforce\\\\\": true, \\\\\"disable_env_checker\\\\\": false, \\\\\"kwargs\\\\\": {}, \\\\\"additional_wrappers\\\\\": [], \\\\\"vector_entry_point\\\\\": null}\", \"wrappers\": null}',\n",
       "  'buffer_size': 10000,\n",
       "  'alpha': 0.6,\n",
       "  'beta_start': 0.4,\n",
       "  'beta_iter': 100000,\n",
       "  'priority': 'proportional',\n",
       "  'goal_shape': None,\n",
       "  'epsilon': 1e-06,\n",
       "  'update_freq': 10,\n",
       "  'device': 'cuda'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'NormalNoise',\n",
       " 'config': {'shape': (1, 4),\n",
       "  'mean': 0.0,\n",
       "  'stddev': 0.20000000298023224,\n",
       "  'device': 'cuda'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = DDPG(env=env_wrap,\n",
    "                actor_model=actor,\n",
    "                critic_model=critic,\n",
    "                replay_buffer=replay_buffer,\n",
    "                discount=0.99,\n",
    "                tau=0.005,\n",
    "                action_epsilon=0.2,\n",
    "                noise=noise,\n",
    "                warmup=200,\n",
    "                callbacks=[rl_callbacks.WandbCallback('BipedalWalker-v3')],\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticModel(\n",
       "  (layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=24, out_features=400, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (merged_layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=404, out_features=300, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (output_layer): ModuleDict(\n",
       "    (state_action_value): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticModel(\n",
       "  (layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=24, out_features=400, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (merged_layers): ModuleDict(\n",
       "    (dense_0): Linear(in_features=404, out_features=300, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       "  (output_layer): ModuleDict(\n",
       "    (state_action_value): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonhayes1987\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/src/app/wandb/run-20250322_221545-jxp9log3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/jxp9log3' target=\"_blank\">train-148</a></strong> to <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/jxp9log3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/jxp9log3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback.on_train_begin called\n",
      "Adding trajectory to replay buffer: step 58, counter 58\n",
      "Environment 10: Episode 1, Score -106.83995833677737, Avg_Score -106.83995833677737\n",
      "Adding trajectory to replay buffer: step 66, counter 124\n",
      "Environment 7: Episode 2, Score -113.07360722889503, Avg_Score -109.9567827828362\n",
      "Adding trajectory to replay buffer: step 66, counter 190\n",
      "Environment 13: Episode 3, Score -107.198983888946, Avg_Score -109.0375164848728\n",
      "Adding trajectory to replay buffer: step 67, counter 257\n",
      "Environment 1: Episode 4, Score -101.61562837587825, Avg_Score -107.18204445762416\n",
      "Adding trajectory to replay buffer: step 68, counter 325\n",
      "Environment 8: Episode 5, Score -99.78311157903478, Avg_Score -105.70225788190628\n",
      "Adding trajectory to replay buffer: step 70, counter 395\n",
      "Environment 5: Episode 6, Score -106.40563232402255, Avg_Score -105.81948695559232\n",
      "Adding trajectory to replay buffer: step 78, counter 473\n",
      "Environment 2: Episode 7, Score -113.4137676453221, Avg_Score -106.90438419698229\n",
      "Adding trajectory to replay buffer: step 79, counter 552\n",
      "Environment 11: Episode 8, Score -117.64571641074183, Avg_Score -108.24705072370223\n",
      "Adding trajectory to replay buffer: step 84, counter 636\n",
      "Environment 0: Episode 9, Score -100.09489642631107, Avg_Score -107.34125580176988\n",
      "Adding trajectory to replay buffer: step 84, counter 720\n",
      "Environment 14: Episode 10, Score -98.81048260606205, Avg_Score -106.4881784821991\n",
      "Adding trajectory to replay buffer: step 102, counter 822\n",
      "Environment 4: Episode 11, Score -98.8207573127613, Avg_Score -105.79114019406839\n",
      "Adding trajectory to replay buffer: step 124, counter 878\n",
      "Environment 8: Episode 12, Score -98.8275342685661, Avg_Score -105.21083970027654\n",
      "Adding trajectory to replay buffer: step 166, counter 942\n",
      "Environment 4: Episode 13, Score -114.87276385866106, Avg_Score -105.95406463553688\n",
      "Adding trajectory to replay buffer: step 166, counter 1024\n",
      "Environment 14: Episode 14, Score -100.39202372838758, Avg_Score -105.55677599931194\n",
      "Adding trajectory to replay buffer: step 171, counter 1195\n",
      "Environment 12: Episode 15, Score -127.19753929369617, Avg_Score -106.99949355227088\n",
      "Adding trajectory to replay buffer: step 178, counter 1307\n",
      "Environment 7: Episode 16, Score -97.28713430998917, Avg_Score -106.39247109962827\n",
      "Adding trajectory to replay buffer: step 195, counter 1432\n",
      "Environment 5: Episode 17, Score -124.09525501620645, Avg_Score -107.43381133001522\n",
      "WARNING: Large max priority increase: 1.00e+00 -> 1.58e+01\n",
      "Adding trajectory to replay buffer: step 240, counter 1594\n",
      "Environment 2: Episode 18, Score -127.64738814567033, Avg_Score -108.55678781977385\n",
      "Adding trajectory to replay buffer: step 241, counter 1711\n",
      "Environment 8: Episode 19, Score -104.0129071129368, Avg_Score -108.31763620362453\n",
      "Adding trajectory to replay buffer: step 260, counter 1904\n",
      "Environment 1: Episode 20, Score -127.1921646248727, Avg_Score -109.26136262468694\n",
      "Adding trajectory to replay buffer: step 272, counter 2110\n",
      "Environment 13: Episode 21, Score -121.41835503413073, Avg_Score -109.84026702513665\n",
      "Adding trajectory to replay buffer: step 274, counter 2189\n",
      "Environment 5: Episode 22, Score -119.17263049698272, Avg_Score -110.26446536476602\n",
      "Adding trajectory to replay buffer: step 281, counter 2299\n",
      "Environment 12: Episode 23, Score -109.91504924338206, Avg_Score -110.24927335948846\n",
      "Adding trajectory to replay buffer: step 282, counter 2403\n",
      "Environment 7: Episode 24, Score -106.07484801676807, Avg_Score -110.07533897020845\n",
      "Adding trajectory to replay buffer: step 284, counter 2687\n",
      "Environment 15: Episode 25, Score -123.40699786893693, Avg_Score -110.60860532615757\n",
      "Adding trajectory to replay buffer: step 287, counter 2890\n",
      "Environment 0: Episode 26, Score -119.6111153715249, Avg_Score -110.95485571251787\n",
      "Adding trajectory to replay buffer: step 287, counter 3177\n",
      "Environment 3: Episode 27, Score -125.65681611397434, Avg_Score -111.49937276442367\n",
      "Adding trajectory to replay buffer: step 289, counter 3408\n",
      "Environment 10: Episode 28, Score -116.87313318735019, Avg_Score -111.69129277952818\n",
      "Adding trajectory to replay buffer: step 291, counter 3699\n",
      "Environment 6: Episode 29, Score -122.93464359801149, Avg_Score -112.07899453188966\n",
      "Adding trajectory to replay buffer: step 291, counter 3824\n",
      "Environment 14: Episode 30, Score -117.062190582318, Avg_Score -112.24510106690396\n",
      "Adding trajectory to replay buffer: step 296, counter 3879\n",
      "Environment 8: Episode 31, Score -117.45638968520922, Avg_Score -112.41320715136541\n",
      "Adding trajectory to replay buffer: step 300, counter 3939\n",
      "Environment 2: Episode 32, Score -112.8722980350523, Avg_Score -112.42755374148062\n",
      "Adding trajectory to replay buffer: step 302, counter 4075\n",
      "Environment 4: Episode 33, Score -116.38568724364849, Avg_Score -112.54749718094025\n",
      "Adding trajectory to replay buffer: step 303, counter 4118\n",
      "Environment 1: Episode 34, Score -116.1071465127257, Avg_Score -112.65219274952219\n",
      "Adding trajectory to replay buffer: step 303, counter 4342\n",
      "Environment 11: Episode 35, Score -126.08232475122017, Avg_Score -113.03591080671356\n",
      "Adding trajectory to replay buffer: step 313, counter 4655\n",
      "Environment 9: Episode 36, Score -129.29629958197694, Avg_Score -113.4875882726931\n",
      "Adding trajectory to replay buffer: step 316, counter 4699\n",
      "Environment 13: Episode 37, Score -122.51398209859804, Avg_Score -113.73154486258241\n",
      "Adding trajectory to replay buffer: step 317, counter 4742\n",
      "Environment 5: Episode 38, Score -120.60779274171838, Avg_Score -113.91249875413862\n",
      "Adding trajectory to replay buffer: step 325, counter 4786\n",
      "Environment 12: Episode 39, Score -119.35268737491965, Avg_Score -114.05199077005608\n",
      "Adding trajectory to replay buffer: step 335, counter 4834\n",
      "Environment 0: Episode 40, Score -120.09924825474508, Avg_Score -114.20317220717331\n",
      "Adding trajectory to replay buffer: step 341, counter 4879\n",
      "Environment 8: Episode 41, Score -122.9408379164425, Avg_Score -114.41628600496037\n",
      "Adding trajectory to replay buffer: step 341, counter 4936\n",
      "Environment 15: Episode 42, Score -115.19451277103151, Avg_Score -114.43481521367634\n",
      "Adding trajectory to replay buffer: step 343, counter 4979\n",
      "Environment 2: Episode 43, Score -122.22414901218875, Avg_Score -114.61596251131617\n",
      "Adding trajectory to replay buffer: step 346, counter 5022\n",
      "Environment 11: Episode 44, Score -120.22747182081082, Avg_Score -114.74349681380468\n",
      "Adding trajectory to replay buffer: step 347, counter 5066\n",
      "Environment 1: Episode 45, Score -120.4426236310179, Avg_Score -114.87014407640943\n",
      "Adding trajectory to replay buffer: step 347, counter 5111\n",
      "Environment 4: Episode 46, Score -121.02156927068283, Avg_Score -115.00387071106753\n",
      "Adding trajectory to replay buffer: step 348, counter 5168\n",
      "Environment 6: Episode 47, Score -116.53260306683245, Avg_Score -115.03639693140296\n",
      "Adding trajectory to replay buffer: step 350, counter 5229\n",
      "Environment 10: Episode 48, Score -114.11627559759529, Avg_Score -115.01722773694864\n",
      "Adding trajectory to replay buffer: step 353, counter 5300\n",
      "Environment 7: Episode 49, Score -117.87206119618564, Avg_Score -115.07548964428\n",
      "Adding trajectory to replay buffer: step 361, counter 5348\n",
      "Environment 9: Episode 50, Score -122.74883051262672, Avg_Score -115.22895646164694\n",
      "Adding trajectory to replay buffer: step 363, counter 5394\n",
      "Environment 5: Episode 51, Score -114.8417738626576, Avg_Score -115.22136464598049\n",
      "Adding trajectory to replay buffer: step 365, counter 5434\n",
      "Environment 12: Episode 52, Score -116.84696994550401, Avg_Score -115.25262628635593\n",
      "Adding trajectory to replay buffer: step 369, counter 5487\n",
      "Environment 13: Episode 53, Score -117.31508110558552, Avg_Score -115.29154052822818\n",
      "Adding trajectory to replay buffer: step 374, counter 5570\n",
      "Environment 14: Episode 54, Score -110.4788598862042, Avg_Score -115.20241681263515\n",
      "Adding trajectory to replay buffer: step 397, counter 5626\n",
      "Environment 15: Episode 55, Score -115.02231195180987, Avg_Score -115.19914217880196\n",
      "Adding trajectory to replay buffer: step 399, counter 5690\n",
      "Environment 0: Episode 56, Score -111.25652474916353, Avg_Score -115.12873829612985\n",
      "Adding trajectory to replay buffer: step 435, counter 5775\n",
      "Environment 10: Episode 57, Score -102.44661338652298, Avg_Score -114.90624487666305\n",
      "Adding trajectory to replay buffer: step 445, counter 5859\n",
      "Environment 9: Episode 58, Score -101.2165098862201, Avg_Score -114.67021496303472\n",
      "Adding trajectory to replay buffer: step 454, counter 5914\n",
      "Environment 0: Episode 59, Score -126.00444007521185, Avg_Score -114.8623204734106\n",
      "Adding trajectory to replay buffer: step 500, counter 5979\n",
      "Environment 10: Episode 60, Score -111.07464399702722, Avg_Score -114.79919253213755\n",
      "Adding trajectory to replay buffer: step 516, counter 6152\n",
      "Environment 2: Episode 61, Score -138.0565122155386, Avg_Score -115.18046006793101\n",
      "Adding trajectory to replay buffer: step 517, counter 6224\n",
      "Environment 9: Episode 62, Score -114.97813378365277, Avg_Score -115.17719674076524\n",
      "Adding trajectory to replay buffer: step 527, counter 6377\n",
      "Environment 14: Episode 63, Score -135.97322781121483, Avg_Score -115.50729247204222\n",
      "Adding trajectory to replay buffer: step 539, counter 6462\n",
      "Environment 0: Episode 64, Score -104.83047390257256, Avg_Score -115.34046718189425\n",
      "Adding trajectory to replay buffer: step 539, counter 6632\n",
      "Environment 13: Episode 65, Score -129.74714427783093, Avg_Score -115.56210836798559\n",
      "Adding trajectory to replay buffer: step 547, counter 6831\n",
      "Environment 6: Episode 66, Score -98.39187564851343, Avg_Score -115.30195332678146\n",
      "Adding trajectory to replay buffer: step 555, counter 7040\n",
      "Environment 11: Episode 67, Score -141.0878300902128, Avg_Score -115.68681715907148\n",
      "Adding trajectory to replay buffer: step 558, counter 7251\n",
      "Environment 4: Episode 68, Score -116.29546693165801, Avg_Score -115.69576789102129\n",
      "Adding trajectory to replay buffer: step 561, counter 7525\n",
      "Environment 3: Episode 69, Score -115.16540391359788, Avg_Score -115.68808145656588\n",
      "Adding trajectory to replay buffer: step 561, counter 7745\n",
      "Environment 8: Episode 70, Score -137.15864803226307, Avg_Score -115.9948038362187\n",
      "Adding trajectory to replay buffer: step 566, counter 7964\n",
      "Environment 1: Episode 71, Score -138.5591879018769, Avg_Score -116.31261206249557\n",
      "Adding trajectory to replay buffer: step 568, counter 8015\n",
      "Environment 9: Episode 72, Score -108.57974550455312, Avg_Score -116.20521113807973\n",
      "Adding trajectory to replay buffer: step 569, counter 8221\n",
      "Environment 5: Episode 73, Score -110.78249118329894, Avg_Score -116.13092730308274\n",
      "Adding trajectory to replay buffer: step 572, counter 8440\n",
      "Environment 7: Episode 74, Score -134.69681622301476, Avg_Score -116.38181769389263\n",
      "Adding trajectory to replay buffer: step 575, counter 8650\n",
      "Environment 12: Episode 75, Score -135.14021331817906, Avg_Score -116.63192963554978\n",
      "Adding trajectory to replay buffer: step 576, counter 8726\n",
      "Environment 10: Episode 76, Score -104.20267584439678, Avg_Score -116.4683868225083\n",
      "Adding trajectory to replay buffer: step 585, counter 8795\n",
      "Environment 2: Episode 77, Score -104.26125515464072, Avg_Score -116.30985264500352\n",
      "Adding trajectory to replay buffer: step 586, counter 8854\n",
      "Environment 14: Episode 78, Score -104.4725785105781, Avg_Score -116.1580927202032\n",
      "Adding trajectory to replay buffer: step 588, counter 8903\n",
      "Environment 0: Episode 79, Score -106.62968486748078, Avg_Score -116.0374799625738\n",
      "Adding trajectory to replay buffer: step 588, counter 8952\n",
      "Environment 13: Episode 80, Score -106.96830760223605, Avg_Score -115.92411530806957\n",
      "Adding trajectory to replay buffer: step 597, counter 9152\n",
      "Environment 15: Episode 81, Score -138.3190829881726, Avg_Score -116.20059639054\n",
      "Adding trajectory to replay buffer: step 599, counter 9204\n",
      "Environment 6: Episode 82, Score -107.91109213363939, Avg_Score -116.09950487521193\n",
      "Adding trajectory to replay buffer: step 615, counter 9253\n",
      "Environment 1: Episode 83, Score -107.02368124320607, Avg_Score -115.99015760253718\n",
      "Adding trajectory to replay buffer: step 618, counter 9302\n",
      "Environment 5: Episode 84, Score -106.14227170239141, Avg_Score -115.87292086563068\n",
      "Adding trajectory to replay buffer: step 627, counter 9371\n",
      "Environment 4: Episode 85, Score -108.79018994803045, Avg_Score -115.78959461954126\n",
      "Adding trajectory to replay buffer: step 629, counter 9439\n",
      "Environment 8: Episode 86, Score -102.13195603853092, Avg_Score -115.63078486859928\n",
      "Adding trajectory to replay buffer: step 631, counter 9515\n",
      "Environment 11: Episode 87, Score -104.6178670745803, Avg_Score -115.50419960659906\n",
      "Adding trajectory to replay buffer: step 632, counter 9572\n",
      "Environment 12: Episode 88, Score -103.24205934210433, Avg_Score -115.36485710359344\n",
      "Adding trajectory to replay buffer: step 640, counter 9640\n",
      "Environment 7: Episode 89, Score -104.49071724996126, Avg_Score -115.2426757569234\n",
      "Adding trajectory to replay buffer: step 641, counter 9713\n",
      "Environment 9: Episode 90, Score -107.17864823955794, Avg_Score -115.1530754511749\n",
      "Adding trajectory to replay buffer: step 642, counter 9769\n",
      "Environment 14: Episode 91, Score -104.11962944229258, Avg_Score -115.03182879173664\n",
      "Adding trajectory to replay buffer: step 648, counter 9832\n",
      "Environment 2: Episode 92, Score -103.71044370899598, Avg_Score -114.9087702582286\n",
      "Adding trajectory to replay buffer: step 653, counter 9897\n",
      "Environment 13: Episode 93, Score -105.21004739131965, Avg_Score -114.80448291557367\n",
      "Adding trajectory to replay buffer: step 655, counter 9953\n",
      "Environment 6: Episode 94, Score -106.27875591798872, Avg_Score -114.71378369219511\n",
      "Adding trajectory to replay buffer: step 660, counter 10037\n",
      "Environment 10: Episode 95, Score -110.09883042673953, Avg_Score -114.66520523676925\n",
      "Adding trajectory to replay buffer: step 662, counter 10138\n",
      "Environment 3: Episode 96, Score -132.32906957377006, Avg_Score -114.84920382361302\n",
      "Adding trajectory to replay buffer: step 669, counter 10189\n",
      "Environment 5: Episode 97, Score -106.67103533767568, Avg_Score -114.76489280829406\n",
      "Adding trajectory to replay buffer: step 674, counter 10248\n",
      "Environment 1: Episode 98, Score -103.10322285916222, Avg_Score -114.64589617616008\n",
      "Adding trajectory to replay buffer: step 675, counter 10326\n",
      "Environment 15: Episode 99, Score -104.824603263857, Avg_Score -114.54669119724792\n",
      "Adding trajectory to replay buffer: step 682, counter 10376\n",
      "Environment 12: Episode 100, Score -103.92529985338822, Avg_Score -114.44047728380933\n",
      "Adding trajectory to replay buffer: step 684, counter 10472\n",
      "Environment 0: Episode 101, Score -130.15514252096042, Avg_Score -114.67362912565113\n",
      "Adding trajectory to replay buffer: step 689, counter 10530\n",
      "Environment 11: Episode 102, Score -103.82293244706467, Avg_Score -114.58112237783281\n",
      "Adding trajectory to replay buffer: step 694, counter 10595\n",
      "Environment 8: Episode 103, Score -103.7513826242257, Avg_Score -114.54664636518562\n",
      "Adding trajectory to replay buffer: step 694, counter 10647\n",
      "Environment 14: Episode 104, Score -103.83899878890688, Avg_Score -114.56888006931595\n",
      "Adding trajectory to replay buffer: step 695, counter 10715\n",
      "Environment 4: Episode 105, Score -103.78314699669927, Avg_Score -114.60888042349258\n",
      "Adding trajectory to replay buffer: step 707, counter 10782\n",
      "Environment 7: Episode 106, Score -106.96678240226457, Avg_Score -114.614491924275\n",
      "Adding trajectory to replay buffer: step 716, counter 10850\n",
      "Environment 2: Episode 107, Score -103.0180724430941, Avg_Score -114.5105349722527\n",
      "Adding trajectory to replay buffer: step 718, counter 10906\n",
      "Environment 3: Episode 108, Score -102.93236391444754, Avg_Score -114.36340144728976\n",
      "Adding trajectory to replay buffer: step 724, counter 10956\n",
      "Environment 1: Episode 109, Score -107.2297797021304, Avg_Score -114.43475028004795\n",
      "Adding trajectory to replay buffer: step 725, counter 11028\n",
      "Environment 13: Episode 110, Score -108.43736560287886, Avg_Score -114.53101911001615\n",
      "Adding trajectory to replay buffer: step 728, counter 11096\n",
      "Environment 10: Episode 111, Score -102.7880034153716, Avg_Score -114.57069157104225\n",
      "Adding trajectory to replay buffer: step 734, counter 11189\n",
      "Environment 9: Episode 112, Score -104.49181516202664, Avg_Score -114.62733437997684\n",
      "Adding trajectory to replay buffer: step 736, counter 11270\n",
      "Environment 6: Episode 113, Score -101.92171974123146, Avg_Score -114.49782393880254\n",
      "Adding trajectory to replay buffer: step 741, counter 11342\n",
      "Environment 5: Episode 114, Score -102.45117424626463, Avg_Score -114.5184154439813\n",
      "Adding trajectory to replay buffer: step 741, counter 11401\n",
      "Environment 12: Episode 115, Score -109.13492827207658, Avg_Score -114.33778933376512\n",
      "Adding trajectory to replay buffer: step 743, counter 11469\n",
      "Environment 15: Episode 116, Score -103.408094495425, Avg_Score -114.39899893561947\n",
      "Adding trajectory to replay buffer: step 744, counter 11518\n",
      "Environment 4: Episode 117, Score -108.01282426661625, Avg_Score -114.23817462812356\n",
      "Adding trajectory to replay buffer: step 757, counter 11581\n",
      "Environment 14: Episode 118, Score -107.13233123050382, Avg_Score -114.03302405897189\n",
      "Adding trajectory to replay buffer: step 767, counter 11654\n",
      "Environment 8: Episode 119, Score -107.76434922032493, Avg_Score -114.0705384800458\n",
      "Adding trajectory to replay buffer: step 770, counter 11735\n",
      "Environment 11: Episode 120, Score -108.40831592385533, Avg_Score -113.88269999303562\n",
      "Adding trajectory to replay buffer: step 772, counter 11783\n",
      "Environment 1: Episode 121, Score -107.15016538523138, Avg_Score -113.7400180965466\n",
      "Adding trajectory to replay buffer: step 775, counter 11830\n",
      "Environment 10: Episode 122, Score -106.26240111338409, Avg_Score -113.61091580271064\n",
      "Adding trajectory to replay buffer: step 779, counter 11902\n",
      "Environment 7: Episode 123, Score -102.73160287665513, Avg_Score -113.53908133904335\n",
      "Adding trajectory to replay buffer: step 793, counter 11961\n",
      "Environment 9: Episode 124, Score -104.26694301710712, Avg_Score -113.52100228904678\n",
      "Adding trajectory to replay buffer: step 793, counter 12013\n",
      "Environment 12: Episode 125, Score -107.0840567879832, Avg_Score -113.35777287823721\n",
      "Adding trajectory to replay buffer: step 794, counter 12089\n",
      "Environment 3: Episode 126, Score -105.63087377711138, Avg_Score -113.21797046229307\n",
      "Adding trajectory to replay buffer: step 794, counter 12139\n",
      "Environment 4: Episode 127, Score -107.341357548759, Avg_Score -113.03481587664089\n",
      "Adding trajectory to replay buffer: step 798, counter 12212\n",
      "Environment 13: Episode 128, Score -105.22285376701566, Avg_Score -112.91831308243756\n",
      "Adding trajectory to replay buffer: step 806, counter 12334\n",
      "Environment 0: Episode 129, Score -132.32352635441347, Avg_Score -113.01220191000158\n",
      "Adding trajectory to replay buffer: step 810, counter 12403\n",
      "Environment 5: Episode 130, Score -102.96574525011704, Avg_Score -112.8712374566796\n",
      "Adding trajectory to replay buffer: step 818, counter 12485\n",
      "Environment 6: Episode 131, Score -102.81221038297254, Avg_Score -112.72479566365722\n",
      "Adding trajectory to replay buffer: step 818, counter 12546\n",
      "Environment 14: Episode 132, Score -104.37677340661051, Avg_Score -112.6398404173728\n",
      "Adding trajectory to replay buffer: step 822, counter 12598\n",
      "Environment 11: Episode 133, Score -106.60807895217525, Avg_Score -112.54206433445806\n",
      "Adding trajectory to replay buffer: step 823, counter 12649\n",
      "Environment 1: Episode 134, Score -107.09836955099802, Avg_Score -112.45197656484079\n",
      "Adding trajectory to replay buffer: step 832, counter 12738\n",
      "Environment 15: Episode 135, Score -107.40865844082211, Avg_Score -112.2652399017368\n",
      "Adding trajectory to replay buffer: step 843, counter 12814\n",
      "Environment 8: Episode 136, Score -101.74585817163313, Avg_Score -111.98973548763335\n",
      "Adding trajectory to replay buffer: step 849, counter 12870\n",
      "Environment 9: Episode 137, Score -105.46588188617429, Avg_Score -111.8192544855091\n",
      "Adding trajectory to replay buffer: step 857, counter 12921\n",
      "Environment 0: Episode 138, Score -106.81664830235144, Avg_Score -111.68134304111544\n",
      "Adding trajectory to replay buffer: step 857, counter 12984\n",
      "Environment 3: Episode 139, Score -103.44561542739223, Avg_Score -111.52227232164016\n",
      "Adding trajectory to replay buffer: step 857, counter 13048\n",
      "Environment 12: Episode 140, Score -102.01794476945885, Avg_Score -111.3414592867873\n",
      "Adding trajectory to replay buffer: step 861, counter 13134\n",
      "Environment 10: Episode 141, Score -102.4426398577299, Avg_Score -111.13647730620019\n",
      "Adding trajectory to replay buffer: step 867, counter 13222\n",
      "Environment 7: Episode 142, Score -103.4949819132636, Avg_Score -111.01948199762249\n",
      "Adding trajectory to replay buffer: step 879, counter 13385\n",
      "Environment 2: Episode 143, Score -142.4083262069163, Avg_Score -111.22132376956978\n",
      "Adding trajectory to replay buffer: step 880, counter 13467\n",
      "Environment 13: Episode 144, Score -103.94345398696139, Avg_Score -111.05848359123127\n",
      "Adding trajectory to replay buffer: step 881, counter 13538\n",
      "Environment 5: Episode 145, Score -103.07085125820214, Avg_Score -110.88476586750313\n",
      "Adding trajectory to replay buffer: step 887, counter 13607\n",
      "Environment 6: Episode 146, Score -105.30812841810597, Avg_Score -110.72763145897737\n",
      "Adding trajectory to replay buffer: step 888, counter 13677\n",
      "Environment 14: Episode 147, Score -105.37438350969119, Avg_Score -110.61604926340597\n",
      "Adding trajectory to replay buffer: step 892, counter 13747\n",
      "Environment 11: Episode 148, Score -104.0220113791867, Avg_Score -110.51510662122188\n",
      "Adding trajectory to replay buffer: step 902, counter 13826\n",
      "Environment 1: Episode 149, Score -104.6177779729298, Avg_Score -110.38256378898932\n",
      "Adding trajectory to replay buffer: step 902, counter 13896\n",
      "Environment 15: Episode 150, Score -104.30015319923575, Avg_Score -110.19807701585542\n",
      "Adding trajectory to replay buffer: step 922, counter 13961\n",
      "Environment 3: Episode 151, Score -105.99696710509062, Avg_Score -110.10962894827975\n",
      "Adding trajectory to replay buffer: step 923, counter 14027\n",
      "Environment 0: Episode 152, Score -106.44205969245856, Avg_Score -110.0055798457493\n",
      "Adding trajectory to replay buffer: step 924, counter 14102\n",
      "Environment 9: Episode 153, Score -104.88949890306188, Avg_Score -109.88132402372405\n",
      "Adding trajectory to replay buffer: step 927, counter 14148\n",
      "Environment 5: Episode 154, Score -105.40773952623022, Avg_Score -109.8306128201243\n",
      "Adding trajectory to replay buffer: step 931, counter 14200\n",
      "Environment 2: Episode 155, Score -104.92796894503385, Avg_Score -109.72966939005653\n",
      "Adding trajectory to replay buffer: step 931, counter 14274\n",
      "Environment 12: Episode 156, Score -105.8692953188088, Avg_Score -109.67579709575298\n",
      "Adding trajectory to replay buffer: step 932, counter 14326\n",
      "Environment 13: Episode 157, Score -106.09777688059832, Avg_Score -109.71230873069375\n",
      "Adding trajectory to replay buffer: step 939, counter 14404\n",
      "Environment 10: Episode 158, Score -108.28055940108125, Avg_Score -109.78294922584234\n",
      "Adding trajectory to replay buffer: step 940, counter 14457\n",
      "Environment 6: Episode 159, Score -103.52247728708137, Avg_Score -109.55812959796106\n",
      "Adding trajectory to replay buffer: step 945, counter 14535\n",
      "Environment 7: Episode 160, Score -104.17567999859713, Avg_Score -109.48913995797676\n",
      "Adding trajectory to replay buffer: step 970, counter 14603\n",
      "Environment 15: Episode 161, Score -106.14197890680951, Avg_Score -109.16999462488948\n",
      "Adding trajectory to replay buffer: step 972, counter 14673\n",
      "Environment 1: Episode 162, Score -103.89380038683613, Avg_Score -109.05915129092132\n",
      "Adding trajectory to replay buffer: step 980, counter 14859\n",
      "Environment 4: Episode 163, Score -143.0517521086689, Avg_Score -109.12993653389589\n",
      "Adding trajectory to replay buffer: step 980, counter 14912\n",
      "Environment 5: Episode 164, Score -109.50978257613133, Avg_Score -109.17672962063149\n",
      "Adding trajectory to replay buffer: step 982, counter 14962\n",
      "Environment 13: Episode 165, Score -105.41743908816825, Avg_Score -108.93343256873484\n",
      "Adding trajectory to replay buffer: step 984, counter 15058\n",
      "Environment 14: Episode 166, Score -105.52081251740083, Avg_Score -109.00472193742372\n",
      "Adding trajectory to replay buffer: step 985, counter 15200\n",
      "Environment 8: Episode 167, Score -142.08401235571193, Avg_Score -109.01468376007871\n",
      "Adding trajectory to replay buffer: step 988, counter 15249\n",
      "Environment 10: Episode 168, Score -106.68223068254875, Avg_Score -108.91855139758762\n",
      "Adding trajectory to replay buffer: step 992, counter 15319\n",
      "Environment 3: Episode 169, Score -103.35144552671599, Avg_Score -108.8004118137188\n",
      "Adding trajectory to replay buffer: step 994, counter 15389\n",
      "Environment 9: Episode 170, Score -103.15588892211207, Avg_Score -108.46038422261728\n",
      "Adding trajectory to replay buffer: step 999, counter 15457\n",
      "Environment 12: Episode 171, Score -104.67616355744066, Avg_Score -108.12155397917293\n",
      "Adding trajectory to replay buffer: step 1016, counter 15528\n",
      "Environment 7: Episode 172, Score -101.94312687905739, Avg_Score -108.05518779291796\n",
      "Adding trajectory to replay buffer: step 1019, counter 15577\n",
      "Environment 15: Episode 173, Score -106.8242535533489, Avg_Score -108.01560541661846\n",
      "Adding trajectory to replay buffer: step 1020, counter 15674\n",
      "Environment 0: Episode 174, Score -112.49475427860021, Avg_Score -107.7935847971743\n",
      "Adding trajectory to replay buffer: step 1025, counter 15768\n",
      "Environment 2: Episode 175, Score -104.40780501316425, Avg_Score -107.48626071412414\n",
      "Adding trajectory to replay buffer: step 1036, counter 15912\n",
      "Environment 11: Episode 176, Score -139.62758678336317, Avg_Score -107.8405098235138\n",
      "Adding trajectory to replay buffer: step 1037, counter 15964\n",
      "Environment 8: Episode 177, Score -107.12981281508071, Avg_Score -107.8691954001182\n",
      "Adding trajectory to replay buffer: step 1042, counter 16026\n",
      "Environment 4: Episode 178, Score -103.12989190380895, Avg_Score -107.85576853405053\n",
      "Adding trajectory to replay buffer: step 1042, counter 16128\n",
      "Environment 6: Episode 179, Score -133.4796033094073, Avg_Score -108.1242677184698\n",
      "Adding trajectory to replay buffer: step 1050, counter 16186\n",
      "Environment 3: Episode 180, Score -104.88451078880888, Avg_Score -108.10342975033551\n",
      "Adding trajectory to replay buffer: step 1051, counter 16265\n",
      "Environment 1: Episode 181, Score -107.0881327928851, Avg_Score -107.79112024838265\n",
      "Adding trajectory to replay buffer: step 1052, counter 16333\n",
      "Environment 14: Episode 182, Score -103.31286038577929, Avg_Score -107.74513793090405\n",
      "Adding trajectory to replay buffer: step 1054, counter 16393\n",
      "Environment 9: Episode 183, Score -103.45672547917688, Avg_Score -107.70946837326375\n",
      "Adding trajectory to replay buffer: step 1055, counter 16468\n",
      "Environment 5: Episode 184, Score -104.91206920559797, Avg_Score -107.69716634829581\n",
      "Adding trajectory to replay buffer: step 1067, counter 16536\n",
      "Environment 12: Episode 185, Score -102.30176395644806, Avg_Score -107.63228208837998\n",
      "Adding trajectory to replay buffer: step 1070, counter 16586\n",
      "Environment 0: Episode 186, Score -107.37771031449425, Avg_Score -107.68473963113962\n",
      "Adding trajectory to replay buffer: step 1076, counter 16646\n",
      "Environment 7: Episode 187, Score -102.84651462773358, Avg_Score -107.66702610667114\n",
      "Adding trajectory to replay buffer: step 1076, counter 16734\n",
      "Environment 10: Episode 188, Score -102.22819074720641, Avg_Score -107.65688742072216\n",
      "Adding trajectory to replay buffer: step 1077, counter 16786\n",
      "Environment 2: Episode 189, Score -108.00387395538452, Avg_Score -107.69201898777638\n",
      "Adding trajectory to replay buffer: step 1086, counter 16836\n",
      "Environment 11: Episode 190, Score -108.3055857448345, Avg_Score -107.70328836282914\n",
      "Adding trajectory to replay buffer: step 1096, counter 16950\n",
      "Environment 13: Episode 191, Score -138.12830134019256, Avg_Score -108.04337508180814\n",
      "Adding trajectory to replay buffer: step 1100, counter 17013\n",
      "Environment 8: Episode 192, Score -102.90881767215828, Avg_Score -108.03535882143974\n",
      "Adding trajectory to replay buffer: step 1105, counter 17064\n",
      "Environment 9: Episode 193, Score -106.38017688607408, Avg_Score -108.04706011638727\n",
      "Adding trajectory to replay buffer: step 1106, counter 17151\n",
      "Environment 15: Episode 194, Score -106.6187256690686, Avg_Score -108.05045981389807\n",
      "Adding trajectory to replay buffer: step 1116, counter 17225\n",
      "Environment 6: Episode 195, Score -104.42651111678903, Avg_Score -107.99373662079857\n",
      "Adding trajectory to replay buffer: step 1120, counter 17295\n",
      "Environment 3: Episode 196, Score -105.3630490509326, Avg_Score -107.72407641557021\n",
      "Adding trajectory to replay buffer: step 1123, counter 17342\n",
      "Environment 10: Episode 197, Score -107.30972000358689, Avg_Score -107.73046326222934\n",
      "Adding trajectory to replay buffer: step 1124, counter 17389\n",
      "Environment 2: Episode 198, Score -106.03171278955601, Avg_Score -107.75974816153327\n",
      "Adding trajectory to replay buffer: step 1125, counter 17438\n",
      "Environment 7: Episode 199, Score -107.84913455485739, Avg_Score -107.78999347444328\n",
      "Adding trajectory to replay buffer: step 1127, counter 17523\n",
      "Environment 4: Episode 200, Score -105.57866058026255, Avg_Score -107.80652708171199\n",
      "Adding trajectory to replay buffer: step 1134, counter 17606\n",
      "Environment 1: Episode 201, Score -104.48394819256043, Avg_Score -107.549815138428\n",
      "Adding trajectory to replay buffer: step 1138, counter 17692\n",
      "Environment 14: Episode 202, Score -106.46075480237083, Avg_Score -107.57619336198105\n",
      "Adding trajectory to replay buffer: step 1145, counter 17782\n",
      "Environment 5: Episode 203, Score -105.97320217372601, Avg_Score -107.59841155747607\n",
      "Adding trajectory to replay buffer: step 1148, counter 17860\n",
      "Environment 0: Episode 204, Score -101.92281836783265, Avg_Score -107.57924975326532\n",
      "Adding trajectory to replay buffer: step 1149, counter 17923\n",
      "Environment 11: Episode 205, Score -102.895800860911, Avg_Score -107.57037629190745\n",
      "Adding trajectory to replay buffer: step 1150, counter 18006\n",
      "Environment 12: Episode 206, Score -112.22063359030213, Avg_Score -107.62291480378785\n",
      "Adding trajectory to replay buffer: step 1151, counter 18061\n",
      "Environment 13: Episode 207, Score -104.75012437835895, Avg_Score -107.6402353231405\n",
      "Adding trajectory to replay buffer: step 1152, counter 18108\n",
      "Environment 9: Episode 208, Score -103.21781089268997, Avg_Score -107.64308979292291\n",
      "Adding trajectory to replay buffer: step 1156, counter 18164\n",
      "Environment 8: Episode 209, Score -103.14935845220649, Avg_Score -107.60228558042367\n",
      "Adding trajectory to replay buffer: step 1160, counter 18218\n",
      "Environment 15: Episode 210, Score -101.2381754387406, Avg_Score -107.53029367878229\n",
      "Adding trajectory to replay buffer: step 1164, counter 18266\n",
      "Environment 6: Episode 211, Score -102.5875141066201, Avg_Score -107.52828878569477\n",
      "Adding trajectory to replay buffer: step 1169, counter 18315\n",
      "Environment 3: Episode 212, Score -102.77286945224802, Avg_Score -107.51109932859701\n",
      "Adding trajectory to replay buffer: step 1171, counter 18363\n",
      "Environment 10: Episode 213, Score -103.69452036930434, Avg_Score -107.52882733487775\n",
      "Adding trajectory to replay buffer: step 1173, counter 18412\n",
      "Environment 2: Episode 214, Score -101.95464991363013, Avg_Score -107.5238620915514\n",
      "Adding trajectory to replay buffer: step 1177, counter 18462\n",
      "Environment 4: Episode 215, Score -102.9173510561778, Avg_Score -107.46168631939241\n",
      "Adding trajectory to replay buffer: step 1180, counter 18517\n",
      "Environment 7: Episode 216, Score -101.33517792526136, Avg_Score -107.44095715369077\n",
      "Adding trajectory to replay buffer: step 1183, counter 18566\n",
      "Environment 1: Episode 217, Score -103.08693144616423, Avg_Score -107.39169822548625\n",
      "Adding trajectory to replay buffer: step 1187, counter 18615\n",
      "Environment 14: Episode 218, Score -105.01167692728279, Avg_Score -107.37049168245403\n",
      "Adding trajectory to replay buffer: step 1195, counter 18665\n",
      "Environment 5: Episode 219, Score -102.3856191395869, Avg_Score -107.31670438164663\n",
      "Adding trajectory to replay buffer: step 1198, counter 18711\n",
      "Environment 9: Episode 220, Score -103.06741926340386, Avg_Score -107.26329541504211\n",
      "Adding trajectory to replay buffer: step 1198, counter 18760\n",
      "Environment 11: Episode 221, Score -104.06469285328625, Avg_Score -107.23244068972264\n",
      "Adding trajectory to replay buffer: step 1199, counter 18811\n",
      "Environment 0: Episode 222, Score -102.27332922409227, Avg_Score -107.19254997082973\n",
      "Adding trajectory to replay buffer: step 1201, counter 18862\n",
      "Environment 12: Episode 223, Score -102.37669544004028, Avg_Score -107.18900089646358\n",
      "Adding trajectory to replay buffer: step 1206, counter 18917\n",
      "Environment 13: Episode 224, Score -101.41540336566419, Avg_Score -107.16048549994916\n",
      "Adding trajectory to replay buffer: step 1212, counter 18973\n",
      "Environment 8: Episode 225, Score -101.93711646711267, Avg_Score -107.10901609674045\n",
      "Adding trajectory to replay buffer: step 1215, counter 19024\n",
      "Environment 6: Episode 226, Score -102.68440092699169, Avg_Score -107.07955136823925\n",
      "Adding trajectory to replay buffer: step 1218, counter 19073\n",
      "Environment 3: Episode 227, Score -103.10328376380478, Avg_Score -107.03717063038972\n",
      "Adding trajectory to replay buffer: step 1220, counter 19122\n",
      "Environment 10: Episode 228, Score -103.4141713040142, Avg_Score -107.01908380575969\n",
      "Adding trajectory to replay buffer: step 1224, counter 19173\n",
      "Environment 2: Episode 229, Score -102.22412569996466, Avg_Score -106.71808979921519\n",
      "Adding trajectory to replay buffer: step 1228, counter 19221\n",
      "Environment 7: Episode 230, Score -103.3047646025413, Avg_Score -106.7214799927394\n",
      "Adding trajectory to replay buffer: step 1233, counter 19277\n",
      "Environment 4: Episode 231, Score -100.93421163092181, Avg_Score -106.7027000052189\n",
      "Adding trajectory to replay buffer: step 1239, counter 19329\n",
      "Environment 14: Episode 232, Score -103.05431847696627, Avg_Score -106.68947545592246\n",
      "Adding trajectory to replay buffer: step 1242, counter 19373\n",
      "Environment 11: Episode 233, Score -102.73601000893055, Avg_Score -106.65075476649001\n",
      "Adding trajectory to replay buffer: step 1245, counter 19419\n",
      "Environment 0: Episode 234, Score -103.6292410572966, Avg_Score -106.616063481553\n",
      "Adding trajectory to replay buffer: step 1247, counter 19471\n",
      "Environment 5: Episode 235, Score -102.98832732611088, Avg_Score -106.57186017040587\n",
      "Adding trajectory to replay buffer: step 1248, counter 19536\n",
      "Environment 1: Episode 236, Score -101.93042262010897, Avg_Score -106.57370581489063\n",
      "Adding trajectory to replay buffer: step 1254, counter 19589\n",
      "Environment 12: Episode 237, Score -104.12547887137346, Avg_Score -106.56030178474262\n",
      "Adding trajectory to replay buffer: step 1255, counter 19638\n",
      "Environment 13: Episode 238, Score -104.19198959125019, Avg_Score -106.53405519763164\n",
      "Adding trajectory to replay buffer: step 1259, counter 19737\n",
      "Environment 15: Episode 239, Score -104.35176376966263, Avg_Score -106.54311668105433\n",
      "Adding trajectory to replay buffer: step 1260, counter 19785\n",
      "Environment 8: Episode 240, Score -104.00614798363547, Avg_Score -106.56299871319611\n",
      "Adding trajectory to replay buffer: step 1266, counter 19853\n",
      "Environment 9: Episode 241, Score -105.02691509341449, Avg_Score -106.58884146555296\n",
      "Adding trajectory to replay buffer: step 1268, counter 19903\n",
      "Environment 3: Episode 242, Score -102.89511851453533, Avg_Score -106.58284283156569\n",
      "Adding trajectory to replay buffer: step 1271, counter 19954\n",
      "Environment 10: Episode 243, Score -104.8662416196776, Avg_Score -106.20742198569332\n",
      "Adding trajectory to replay buffer: step 1274, counter 20000\n",
      "Environment 7: Episode 244, Score -103.03476864268383, Avg_Score -106.19833513225052\n",
      "Adding trajectory to replay buffer: step 1276, counter 20052\n",
      "Environment 2: Episode 245, Score -101.75929348583644, Avg_Score -106.18521955452687\n",
      "Adding trajectory to replay buffer: step 1283, counter 20120\n",
      "Environment 6: Episode 246, Score -102.84128793132025, Avg_Score -106.16055114965903\n",
      "Adding trajectory to replay buffer: step 1287, counter 20174\n",
      "Environment 4: Episode 247, Score -103.97607211838042, Avg_Score -106.14656803574591\n",
      "Adding trajectory to replay buffer: step 1289, counter 20221\n",
      "Environment 11: Episode 248, Score -103.25677964210696, Avg_Score -106.13891571837512\n",
      "Adding trajectory to replay buffer: step 1292, counter 20274\n",
      "Environment 14: Episode 249, Score -102.4366917686568, Avg_Score -106.11710485633239\n",
      "Adding trajectory to replay buffer: step 1299, counter 20319\n",
      "Environment 12: Episode 250, Score -111.3676588648862, Avg_Score -106.1877799129889\n",
      "Adding trajectory to replay buffer: step 1301, counter 20373\n",
      "Environment 5: Episode 251, Score -108.09843453275785, Avg_Score -106.20879458726559\n",
      "Adding trajectory to replay buffer: step 1303, counter 20421\n",
      "Environment 13: Episode 252, Score -113.2075599871766, Avg_Score -106.27644959021278\n",
      "Adding trajectory to replay buffer: step 1312, counter 20474\n",
      "Environment 15: Episode 253, Score -125.15267278225036, Avg_Score -106.47908132900467\n",
      "Adding trajectory to replay buffer: step 1320, counter 20526\n",
      "Environment 3: Episode 254, Score -123.1737313839964, Avg_Score -106.65674124758236\n",
      "Adding trajectory to replay buffer: step 1320, counter 20572\n",
      "Environment 7: Episode 255, Score -115.39318675435396, Avg_Score -106.76139342567554\n",
      "Adding trajectory to replay buffer: step 1322, counter 20628\n",
      "Environment 9: Episode 256, Score -123.30890402698518, Avg_Score -106.93578951275731\n",
      "Adding trajectory to replay buffer: step 1322, counter 20679\n",
      "Environment 10: Episode 257, Score -113.32050604343864, Avg_Score -107.00801680438573\n",
      "Adding trajectory to replay buffer: step 1350, counter 20730\n",
      "Environment 12: Episode 258, Score -121.10645874961094, Avg_Score -107.13627579787104\n",
      "Adding trajectory to replay buffer: step 1352, counter 20790\n",
      "Environment 14: Episode 259, Score -116.46773697244066, Avg_Score -107.26572839472463\n",
      "Adding trajectory to replay buffer: step 1353, counter 20840\n",
      "Environment 13: Episode 260, Score -126.2355386446435, Avg_Score -107.4863269811851\n",
      "Adding trajectory to replay buffer: step 1361, counter 20956\n",
      "Environment 0: Episode 261, Score -113.24898683557086, Avg_Score -107.55739706047272\n",
      "Adding trajectory to replay buffer: step 1368, counter 21012\n",
      "Environment 15: Episode 262, Score -124.39176713620748, Avg_Score -107.76237672796643\n",
      "Adding trajectory to replay buffer: step 1371, counter 21061\n",
      "Environment 9: Episode 263, Score -126.15848737175266, Avg_Score -107.59344408059728\n",
      "Adding trajectory to replay buffer: step 1371, counter 21110\n",
      "Environment 10: Episode 264, Score -126.78624560179624, Avg_Score -107.76620871085392\n",
      "Adding trajectory to replay buffer: step 1376, counter 21166\n",
      "Environment 7: Episode 265, Score -125.36938179217402, Avg_Score -107.96572813789396\n",
      "Adding trajectory to replay buffer: step 1399, counter 21215\n",
      "Environment 12: Episode 266, Score -127.79813853677115, Avg_Score -108.18850139808768\n",
      "Adding trajectory to replay buffer: step 1401, counter 21264\n",
      "Environment 14: Episode 267, Score -126.11156313260521, Avg_Score -108.02877690585662\n",
      "Adding trajectory to replay buffer: step 1408, counter 21319\n",
      "Environment 13: Episode 268, Score -124.49665171623137, Avg_Score -108.20692111619344\n",
      "Adding trajectory to replay buffer: step 1414, counter 21372\n",
      "Environment 0: Episode 269, Score -126.60130811539912, Avg_Score -108.43941974208026\n",
      "Adding trajectory to replay buffer: step 1417, counter 21421\n",
      "Environment 15: Episode 270, Score -118.32744816693044, Avg_Score -108.59113533452846\n",
      "Adding trajectory to replay buffer: step 1421, counter 21471\n",
      "Environment 10: Episode 271, Score -125.98132643864429, Avg_Score -108.80418696334048\n",
      "Adding trajectory to replay buffer: step 1422, counter 21522\n",
      "Environment 9: Episode 272, Score -126.33735887759615, Avg_Score -109.04812928332586\n",
      "Adding trajectory to replay buffer: step 1424, counter 21570\n",
      "Environment 7: Episode 273, Score -118.931574839777, Avg_Score -109.16920249619014\n",
      "Adding trajectory to replay buffer: step 1457, counter 21613\n",
      "Environment 0: Episode 274, Score -112.05650076094084, Avg_Score -109.16481996101352\n",
      "Adding trajectory to replay buffer: step 1458, counter 21654\n",
      "Environment 15: Episode 275, Score -111.06895270039887, Avg_Score -109.23143143788589\n",
      "Adding trajectory to replay buffer: step 1460, counter 21690\n",
      "Environment 7: Episode 276, Score -109.38752796190667, Avg_Score -108.92903084967132\n",
      "Adding trajectory to replay buffer: step 1464, counter 21746\n",
      "Environment 13: Episode 277, Score -114.55224162804708, Avg_Score -109.00325513780098\n",
      "Adding trajectory to replay buffer: step 1466, counter 21790\n",
      "Environment 9: Episode 278, Score -116.5287039478151, Avg_Score -109.13724325824103\n",
      "Adding trajectory to replay buffer: step 1468, counter 21982\n",
      "Environment 2: Episode 279, Score -119.55038272475326, Avg_Score -108.9979510523945\n",
      "Adding trajectory to replay buffer: step 1482, counter 22065\n",
      "Environment 12: Episode 280, Score -128.90040456923782, Avg_Score -109.23810999019878\n",
      "Adding trajectory to replay buffer: step 1497, counter 22261\n",
      "Environment 5: Episode 281, Score -136.34587088270854, Avg_Score -109.53068737109702\n",
      "Adding trajectory to replay buffer: step 1498, counter 22499\n",
      "Environment 8: Episode 282, Score -142.942283288567, Avg_Score -109.92698160012489\n",
      "Adding trajectory to replay buffer: step 1499, counter 22577\n",
      "Environment 10: Episode 283, Score -127.15567455007695, Avg_Score -110.16397109083387\n",
      "Adding trajectory to replay buffer: step 1501, counter 22791\n",
      "Environment 4: Episode 284, Score -143.65981016684728, Avg_Score -110.55144850044637\n",
      "Adding trajectory to replay buffer: step 1501, counter 22891\n",
      "Environment 14: Episode 285, Score -126.34003597566677, Avg_Score -110.79183122063856\n",
      "Adding trajectory to replay buffer: step 1502, counter 23104\n",
      "Environment 11: Episode 286, Score -147.70960134074528, Avg_Score -111.19515013090106\n",
      "Adding trajectory to replay buffer: step 1503, counter 23287\n",
      "Environment 3: Episode 287, Score -140.94019596367048, Avg_Score -111.57608694426045\n",
      "Adding trajectory to replay buffer: step 1505, counter 23334\n",
      "Environment 15: Episode 288, Score -104.0664391541928, Avg_Score -111.59446942833033\n",
      "Adding trajectory to replay buffer: step 1509, counter 23595\n",
      "Environment 1: Episode 289, Score -145.27702949012007, Avg_Score -111.96720098367767\n",
      "Adding trajectory to replay buffer: step 1513, counter 23651\n",
      "Environment 0: Episode 290, Score -101.23505581650946, Avg_Score -111.89649568439441\n",
      "Adding trajectory to replay buffer: step 1514, counter 23697\n",
      "Environment 2: Episode 291, Score -104.42148765602087, Avg_Score -111.5594275475527\n",
      "Adding trajectory to replay buffer: step 1515, counter 23929\n",
      "Environment 6: Episode 292, Score -143.3127742791982, Avg_Score -111.96346711362308\n",
      "Adding trajectory to replay buffer: step 1515, counter 23978\n",
      "Environment 9: Episode 293, Score -103.02462076529054, Avg_Score -111.92991155241526\n",
      "Adding trajectory to replay buffer: step 1517, counter 24035\n",
      "Environment 7: Episode 294, Score -101.55541230396337, Avg_Score -111.87927841876419\n",
      "Adding trajectory to replay buffer: step 1522, counter 24093\n",
      "Environment 13: Episode 295, Score -105.45328346592002, Avg_Score -111.88954614225551\n",
      "Adding trajectory to replay buffer: step 1531, counter 24142\n",
      "Environment 12: Episode 296, Score -103.59391303972714, Avg_Score -111.87185478214343\n",
      "Adding trajectory to replay buffer: step 1547, counter 24191\n",
      "Environment 8: Episode 297, Score -103.46988281334254, Avg_Score -111.83345641024101\n",
      "Adding trajectory to replay buffer: step 1552, counter 24244\n",
      "Environment 10: Episode 298, Score -101.80264970955687, Avg_Score -111.791165779441\n",
      "Adding trajectory to replay buffer: step 1553, counter 24294\n",
      "Environment 3: Episode 299, Score -101.78110329910088, Avg_Score -111.73048546688344\n",
      "Adding trajectory to replay buffer: step 1553, counter 24346\n",
      "Environment 14: Episode 300, Score -110.97889905692311, Avg_Score -111.78448785165007\n",
      "Adding trajectory to replay buffer: step 1555, counter 24396\n",
      "Environment 15: Episode 301, Score -102.26604689084986, Avg_Score -111.76230883863295\n",
      "Adding trajectory to replay buffer: step 1556, counter 24455\n",
      "Environment 5: Episode 302, Score -100.89776319237488, Avg_Score -111.70667892253299\n",
      "Adding trajectory to replay buffer: step 1556, counter 24509\n",
      "Environment 11: Episode 303, Score -101.66773984032932, Avg_Score -111.66362429919904\n",
      "Adding trajectory to replay buffer: step 1557, counter 24565\n",
      "Environment 4: Episode 304, Score -106.84844755331304, Avg_Score -111.71288059105382\n",
      "Adding trajectory to replay buffer: step 1563, counter 24613\n",
      "Environment 9: Episode 305, Score -103.66699304694497, Avg_Score -111.72059251291415\n",
      "Adding trajectory to replay buffer: step 1564, counter 24668\n",
      "Environment 1: Episode 306, Score -104.88130948019898, Avg_Score -111.64719927181315\n",
      "Adding trajectory to replay buffer: step 1566, counter 24712\n",
      "Environment 13: Episode 307, Score -103.00049710793793, Avg_Score -111.62970299910894\n",
      "Adding trajectory to replay buffer: step 1569, counter 24766\n",
      "Environment 6: Episode 308, Score -105.1007770395608, Avg_Score -111.64853266057762\n",
      "Adding trajectory to replay buffer: step 1569, counter 24818\n",
      "Environment 7: Episode 309, Score -103.07563798136823, Avg_Score -111.64779545586924\n",
      "Adding trajectory to replay buffer: step 1576, counter 24881\n",
      "Environment 0: Episode 310, Score -105.59155055767508, Avg_Score -111.6913292070586\n",
      "Adding trajectory to replay buffer: step 1584, counter 24951\n",
      "Environment 2: Episode 311, Score -105.76744093722291, Avg_Score -111.72312847536463\n",
      "Adding trajectory to replay buffer: step 1588, counter 25008\n",
      "Environment 12: Episode 312, Score -101.76579674594787, Avg_Score -111.71305774830162\n",
      "Adding trajectory to replay buffer: step 1598, counter 25059\n",
      "Environment 8: Episode 313, Score -104.78145048014044, Avg_Score -111.72392704941\n",
      "Adding trajectory to replay buffer: step 1602, counter 25108\n",
      "Environment 3: Episode 314, Score -104.14225833786713, Avg_Score -111.74580313365237\n",
      "Adding trajectory to replay buffer: step 1604, counter 25159\n",
      "Environment 14: Episode 315, Score -102.59613049728547, Avg_Score -111.74259092806345\n",
      "Adding trajectory to replay buffer: step 1605, counter 25207\n",
      "Environment 4: Episode 316, Score -103.52726062449689, Avg_Score -111.76451175505579\n",
      "Adding trajectory to replay buffer: step 1605, counter 25260\n",
      "Environment 10: Episode 317, Score -101.59440222716952, Avg_Score -111.74958646286585\n",
      "Adding trajectory to replay buffer: step 1607, counter 25312\n",
      "Environment 15: Episode 318, Score -102.14150363021716, Avg_Score -111.72088472989519\n",
      "Adding trajectory to replay buffer: step 1610, counter 25366\n",
      "Environment 5: Episode 319, Score -101.18251527610188, Avg_Score -111.70885369126034\n",
      "Adding trajectory to replay buffer: step 1612, counter 25422\n",
      "Environment 11: Episode 320, Score -101.02782641313908, Avg_Score -111.6884577627577\n",
      "Adding trajectory to replay buffer: step 1615, counter 25468\n",
      "Environment 7: Episode 321, Score -102.81624357874878, Avg_Score -111.6759732700123\n",
      "Adding trajectory to replay buffer: step 1615, counter 25520\n",
      "Environment 9: Episode 322, Score -102.12447770928591, Avg_Score -111.67448475486424\n",
      "Adding trajectory to replay buffer: step 1620, counter 25576\n",
      "Environment 1: Episode 323, Score -102.23311214699969, Avg_Score -111.67304892193383\n",
      "Adding trajectory to replay buffer: step 1623, counter 25630\n",
      "Environment 6: Episode 324, Score -102.45348585004857, Avg_Score -111.68342974677768\n",
      "Adding trajectory to replay buffer: step 1628, counter 25682\n",
      "Environment 0: Episode 325, Score -101.87434896447448, Avg_Score -111.68280207175128\n",
      "Adding trajectory to replay buffer: step 1635, counter 25751\n",
      "Environment 13: Episode 326, Score -103.44880227672247, Avg_Score -111.69044608524858\n",
      "Adding trajectory to replay buffer: step 1642, counter 25809\n",
      "Environment 2: Episode 327, Score -107.33917327844972, Avg_Score -111.73280498039503\n",
      "Adding trajectory to replay buffer: step 1643, counter 25854\n",
      "Environment 8: Episode 328, Score -103.72804966880008, Avg_Score -111.73594376404291\n",
      "Adding trajectory to replay buffer: step 1649, counter 25915\n",
      "Environment 12: Episode 329, Score -101.2815562705677, Avg_Score -111.72651806974892\n",
      "Adding trajectory to replay buffer: step 1651, counter 25961\n",
      "Environment 10: Episode 330, Score -103.07055678379784, Avg_Score -111.7241759915615\n",
      "Adding trajectory to replay buffer: step 1652, counter 26011\n",
      "Environment 3: Episode 331, Score -103.60857049299403, Avg_Score -111.7509195801822\n",
      "Adding trajectory to replay buffer: step 1656, counter 26060\n",
      "Environment 15: Episode 332, Score -104.0186535759978, Avg_Score -111.76056293117252\n",
      "Adding trajectory to replay buffer: step 1661, counter 26117\n",
      "Environment 14: Episode 333, Score -101.2406675958829, Avg_Score -111.74560950704203\n",
      "Adding trajectory to replay buffer: step 1662, counter 26167\n",
      "Environment 11: Episode 334, Score -102.91406931525034, Avg_Score -111.7384577896216\n",
      "Adding trajectory to replay buffer: step 1663, counter 26220\n",
      "Environment 5: Episode 335, Score -102.18370586854716, Avg_Score -111.73041157504596\n",
      "Adding trajectory to replay buffer: step 1666, counter 26271\n",
      "Environment 7: Episode 336, Score -103.4981878248124, Avg_Score -111.746089227093\n",
      "Adding trajectory to replay buffer: step 1668, counter 26324\n",
      "Environment 9: Episode 337, Score -102.48908445146246, Avg_Score -111.72972528289388\n",
      "Adding trajectory to replay buffer: step 1671, counter 26390\n",
      "Environment 4: Episode 338, Score -102.0295485598867, Avg_Score -111.70810087258025\n",
      "Adding trajectory to replay buffer: step 1677, counter 26439\n",
      "Environment 0: Episode 339, Score -103.70388875457893, Avg_Score -111.70162212242944\n",
      "Adding trajectory to replay buffer: step 1679, counter 26495\n",
      "Environment 6: Episode 340, Score -105.46378889684254, Avg_Score -111.71619853156149\n",
      "Adding trajectory to replay buffer: step 1681, counter 26556\n",
      "Environment 1: Episode 341, Score -104.26024684229182, Avg_Score -111.70853184905027\n",
      "Adding trajectory to replay buffer: step 1688, counter 26609\n",
      "Environment 13: Episode 342, Score -101.76609289734066, Avg_Score -111.69724159287829\n",
      "Adding trajectory to replay buffer: step 1693, counter 26660\n",
      "Environment 2: Episode 343, Score -102.57994842613985, Avg_Score -111.67437866094295\n",
      "Adding trajectory to replay buffer: step 1696, counter 26713\n",
      "Environment 8: Episode 344, Score -102.2104070218218, Avg_Score -111.66613504473429\n",
      "Adding trajectory to replay buffer: step 1696, counter 26758\n",
      "Environment 10: Episode 345, Score -103.44777305813754, Avg_Score -111.68301984045735\n",
      "Adding trajectory to replay buffer: step 1702, counter 26808\n",
      "Environment 3: Episode 346, Score -103.26127505007324, Avg_Score -111.68721971164487\n",
      "Adding trajectory to replay buffer: step 1705, counter 26864\n",
      "Environment 12: Episode 347, Score -104.70427831806863, Avg_Score -111.69450177364178\n",
      "Adding trajectory to replay buffer: step 1707, counter 26915\n",
      "Environment 15: Episode 348, Score -101.6869241172215, Avg_Score -111.67880321839287\n",
      "Adding trajectory to replay buffer: step 1709, counter 26963\n",
      "Environment 14: Episode 349, Score -103.35477794673666, Avg_Score -111.68798408017368\n",
      "Adding trajectory to replay buffer: step 1713, counter 27014\n",
      "Environment 11: Episode 350, Score -102.57808573128159, Avg_Score -111.60008834883769\n",
      "Adding trajectory to replay buffer: step 1716, counter 27067\n",
      "Environment 5: Episode 351, Score -102.63179449833308, Avg_Score -111.54542194849343\n",
      "Adding trajectory to replay buffer: step 1717, counter 27118\n",
      "Environment 7: Episode 352, Score -102.6218662833261, Avg_Score -111.43956501145493\n",
      "Adding trajectory to replay buffer: step 1717, counter 27167\n",
      "Environment 9: Episode 353, Score -102.40136761538281, Avg_Score -111.21205195978622\n",
      "Adding trajectory to replay buffer: step 1721, counter 27217\n",
      "Environment 4: Episode 354, Score -103.10116217954271, Avg_Score -111.0113262677417\n",
      "Adding trajectory to replay buffer: step 1728, counter 27264\n",
      "Environment 1: Episode 355, Score -103.36494596633625, Avg_Score -110.89104385986151\n",
      "Adding trajectory to replay buffer: step 1728, counter 27313\n",
      "Environment 6: Episode 356, Score -103.48499471172752, Avg_Score -110.69280476670895\n",
      "Adding trajectory to replay buffer: step 1731, counter 27367\n",
      "Environment 0: Episode 357, Score -101.4683089670092, Avg_Score -110.57428279594465\n",
      "Adding trajectory to replay buffer: step 1737, counter 27411\n",
      "Environment 2: Episode 358, Score -102.92303748505194, Avg_Score -110.39244858329907\n",
      "Adding trajectory to replay buffer: step 1743, counter 27466\n",
      "Environment 13: Episode 359, Score -103.01421115460619, Avg_Score -110.25791332512071\n",
      "Adding trajectory to replay buffer: step 1753, counter 27517\n",
      "Environment 3: Episode 360, Score -101.72099571267391, Avg_Score -110.01276789580099\n",
      "Adding trajectory to replay buffer: step 1754, counter 27575\n",
      "Environment 8: Episode 361, Score -104.67014263012074, Avg_Score -109.9269794537465\n",
      "Adding trajectory to replay buffer: step 1754, counter 27633\n",
      "Environment 10: Episode 362, Score -104.96080789432364, Avg_Score -109.73266986132765\n",
      "Adding trajectory to replay buffer: step 1754, counter 27678\n",
      "Environment 14: Episode 363, Score -116.50783113816009, Avg_Score -109.63616329899172\n",
      "Adding trajectory to replay buffer: step 1756, counter 27729\n",
      "Environment 12: Episode 364, Score -103.90889644083815, Avg_Score -109.40738980738213\n",
      "Adding trajectory to replay buffer: step 1758, counter 27780\n",
      "Environment 15: Episode 365, Score -102.41879023425281, Avg_Score -109.1778838918029\n",
      "Adding trajectory to replay buffer: step 1765, counter 27829\n",
      "Environment 5: Episode 366, Score -102.97246603289557, Avg_Score -108.92962716676415\n",
      "Adding trajectory to replay buffer: step 1766, counter 27878\n",
      "Environment 7: Episode 367, Score -103.58197076195106, Avg_Score -108.70433124305762\n",
      "Adding trajectory to replay buffer: step 1766, counter 27927\n",
      "Environment 9: Episode 368, Score -102.83724546902069, Avg_Score -108.48773718058551\n",
      "Adding trajectory to replay buffer: step 1766, counter 27980\n",
      "Environment 11: Episode 369, Score -104.18722396687915, Avg_Score -108.26359633910032\n",
      "Adding trajectory to replay buffer: step 1769, counter 28028\n",
      "Environment 4: Episode 370, Score -102.61636517103591, Avg_Score -108.10648550914136\n",
      "Adding trajectory to replay buffer: step 1774, counter 28074\n",
      "Environment 1: Episode 371, Score -104.55948107433008, Avg_Score -107.89226705549821\n",
      "Adding trajectory to replay buffer: step 1778, counter 28124\n",
      "Environment 6: Episode 372, Score -104.26688935615556, Avg_Score -107.6715623602838\n",
      "Adding trajectory to replay buffer: step 1780, counter 28173\n",
      "Environment 0: Episode 373, Score -102.23292985646924, Avg_Score -107.50457591045073\n",
      "Adding trajectory to replay buffer: step 1791, counter 28227\n",
      "Environment 2: Episode 374, Score -102.12434233326651, Avg_Score -107.40525432617397\n",
      "Adding trajectory to replay buffer: step 1797, counter 28281\n",
      "Environment 13: Episode 375, Score -101.36993082533962, Avg_Score -107.30826410742338\n",
      "Adding trajectory to replay buffer: step 1802, counter 28329\n",
      "Environment 8: Episode 376, Score -102.70468572053872, Avg_Score -107.2414356850097\n",
      "Adding trajectory to replay buffer: step 1802, counter 28377\n",
      "Environment 10: Episode 377, Score -103.39831284940306, Avg_Score -107.12989639722329\n",
      "Adding trajectory to replay buffer: step 1806, counter 28429\n",
      "Environment 14: Episode 378, Score -101.97904255880655, Avg_Score -106.98439978333322\n",
      "Adding trajectory to replay buffer: step 1807, counter 28483\n",
      "Environment 3: Episode 379, Score -102.77074038827729, Avg_Score -106.81660335996847\n",
      "Adding trajectory to replay buffer: step 1807, counter 28534\n",
      "Environment 12: Episode 380, Score -102.82344460013012, Avg_Score -106.55583376027738\n",
      "Adding trajectory to replay buffer: step 1808, counter 28584\n",
      "Environment 15: Episode 381, Score -103.64206667934047, Avg_Score -106.2287957182437\n",
      "Adding trajectory to replay buffer: step 1813, counter 28631\n",
      "Environment 7: Episode 382, Score -102.87406999865796, Avg_Score -105.8281135853446\n",
      "Adding trajectory to replay buffer: step 1814, counter 28679\n",
      "Environment 9: Episode 383, Score -102.73270585011008, Avg_Score -105.58388389834494\n",
      "Adding trajectory to replay buffer: step 1814, counter 28727\n",
      "Environment 11: Episode 384, Score -102.57360566434326, Avg_Score -105.1730218533199\n",
      "Adding trajectory to replay buffer: step 1816, counter 28778\n",
      "Environment 5: Episode 385, Score -102.08385660179394, Avg_Score -104.93046005958118\n",
      "Adding trajectory to replay buffer: step 1819, counter 28828\n",
      "Environment 4: Episode 386, Score -103.08124261308834, Avg_Score -104.48417647230461\n",
      "Adding trajectory to replay buffer: step 1824, counter 28874\n",
      "Environment 6: Episode 387, Score -102.7331094330959, Avg_Score -104.10210560699886\n",
      "Adding trajectory to replay buffer: step 1825, counter 28925\n",
      "Environment 1: Episode 388, Score -102.21157062774276, Avg_Score -104.08355692173438\n",
      "Adding trajectory to replay buffer: step 1827, counter 28972\n",
      "Environment 0: Episode 389, Score -102.72910916630613, Avg_Score -103.65807771849624\n",
      "Adding trajectory to replay buffer: step 1842, counter 29023\n",
      "Environment 2: Episode 390, Score -102.52245617516711, Avg_Score -103.67095172208283\n",
      "Adding trajectory to replay buffer: step 1851, counter 29068\n",
      "Environment 14: Episode 391, Score -116.44459855251696, Avg_Score -103.79118283104779\n",
      "Adding trajectory to replay buffer: step 1858, counter 29129\n",
      "Environment 13: Episode 392, Score -122.80129828477715, Avg_Score -103.5860680711036\n",
      "Adding trajectory to replay buffer: step 1860, counter 29187\n",
      "Environment 10: Episode 393, Score -102.11499971978863, Avg_Score -103.57697186064857\n",
      "Adding trajectory to replay buffer: step 1861, counter 29246\n",
      "Environment 8: Episode 394, Score -100.85323921788297, Avg_Score -103.56995012978777\n",
      "Adding trajectory to replay buffer: step 1864, counter 29296\n",
      "Environment 9: Episode 395, Score -102.15650460364918, Avg_Score -103.53698234116501\n",
      "Adding trajectory to replay buffer: step 1864, counter 29352\n",
      "Environment 15: Episode 396, Score -104.30873585689672, Avg_Score -103.54413056933676\n",
      "Adding trajectory to replay buffer: step 1865, counter 29410\n",
      "Environment 12: Episode 397, Score -103.96826792439384, Avg_Score -103.54911442044724\n",
      "Adding trajectory to replay buffer: step 1866, counter 29462\n",
      "Environment 11: Episode 398, Score -103.25567581548479, Avg_Score -103.56364468150652\n",
      "Adding trajectory to replay buffer: step 1869, counter 29515\n",
      "Environment 5: Episode 399, Score -102.17265294354782, Avg_Score -103.56756017795097\n",
      "Adding trajectory to replay buffer: step 1871, counter 29567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models/ddpg)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 4: Episode 400, Score -102.55721708240608, Avg_Score -103.48334335820579\n",
      "Adding trajectory to replay buffer: step 1871, counter 29614\n",
      "Environment 6: Episode 401, Score -103.1458569126632, Avg_Score -103.49214145842392\n",
      "Adding trajectory to replay buffer: step 1872, counter 29661\n",
      "Environment 1: Episode 402, Score -102.71622319867586, Avg_Score -103.51032605848694\n",
      "Adding trajectory to replay buffer: step 1884, counter 29718\n",
      "Environment 0: Episode 403, Score -101.28732665014763, Avg_Score -103.50652192658512\n",
      "Adding trajectory to replay buffer: step 1894, counter 29770\n",
      "Environment 2: Episode 404, Score -105.40145426520891, Avg_Score -103.49205199370408\n",
      "Adding trajectory to replay buffer: step 1894, counter 29851\n",
      "Environment 7: Episode 405, Score -101.4583309568502, Avg_Score -103.46996537280314\n",
      "Adding trajectory to replay buffer: step 1902, counter 29895\n",
      "Environment 13: Episode 406, Score -104.3403686504319, Avg_Score -103.46455596450545\n",
      "Adding trajectory to replay buffer: step 1902, counter 29946\n",
      "Environment 14: Episode 407, Score -102.19680206552025, Avg_Score -103.45651901408128\n",
      "Adding trajectory to replay buffer: step 1909, counter 29995\n",
      "Environment 10: Episode 408, Score -103.08180529017622, Avg_Score -103.43632929658743\n",
      "Adding trajectory to replay buffer: step 1912, counter 30041\n",
      "Environment 11: Episode 409, Score -105.92232502686791, Avg_Score -103.46479616704242\n",
      "Adding trajectory to replay buffer: step 1913, counter 30090\n",
      "Environment 9: Episode 410, Score -102.71068724520131, Avg_Score -103.43598753391768\n",
      "Adding trajectory to replay buffer: step 1916, counter 30142\n",
      "Environment 15: Episode 411, Score -103.05692601122838, Avg_Score -103.40888238465776\n",
      "Adding trajectory to replay buffer: step 1918, counter 30199\n",
      "Environment 8: Episode 412, Score -104.0194342221953, Avg_Score -103.43141875942021\n",
      "Adding trajectory to replay buffer: step 1919, counter 30249\n",
      "Environment 5: Episode 413, Score -103.00293517791418, Avg_Score -103.41363360639794\n",
      "Adding trajectory to replay buffer: step 1920, counter 30304\n",
      "Environment 12: Episode 414, Score -104.29499799354126, Avg_Score -103.41516100295472\n",
      "Adding trajectory to replay buffer: step 1922, counter 30355\n",
      "Environment 6: Episode 415, Score -102.61421603659494, Avg_Score -103.4153418583478\n",
      "Adding trajectory to replay buffer: step 1925, counter 30408\n",
      "Environment 1: Episode 416, Score -101.88403639434898, Avg_Score -103.39890961604632\n",
      "Adding trajectory to replay buffer: step 1925, counter 30462\n",
      "Environment 4: Episode 417, Score -101.30604390859729, Avg_Score -103.39602603286059\n",
      "Adding trajectory to replay buffer: step 1926, counter 30581\n",
      "Environment 3: Episode 418, Score -133.00401563656766, Avg_Score -103.70465115292413\n",
      "Adding trajectory to replay buffer: step 1936, counter 30633\n",
      "Environment 0: Episode 419, Score -101.47217818027984, Avg_Score -103.7075477819659\n",
      "Adding trajectory to replay buffer: step 1945, counter 30684\n",
      "Environment 2: Episode 420, Score -102.5144739833381, Avg_Score -103.7224142576679\n",
      "Adding trajectory to replay buffer: step 1949, counter 30739\n",
      "Environment 7: Episode 421, Score -101.07097833912137, Avg_Score -103.70496160527162\n",
      "Adding trajectory to replay buffer: step 1954, counter 30791\n",
      "Environment 13: Episode 422, Score -102.49384329050655, Avg_Score -103.70865526108383\n",
      "Adding trajectory to replay buffer: step 1956, counter 30834\n",
      "Environment 9: Episode 423, Score -104.03447735921232, Avg_Score -103.72666891320594\n",
      "Adding trajectory to replay buffer: step 1959, counter 30884\n",
      "Environment 10: Episode 424, Score -102.74248818080127, Avg_Score -103.72955893651346\n",
      "Adding trajectory to replay buffer: step 1961, counter 30933\n",
      "Environment 11: Episode 425, Score -102.27756835311216, Avg_Score -103.73359113039986\n",
      "Adding trajectory to replay buffer: step 1967, counter 30998\n",
      "Environment 14: Episode 426, Score -101.64745379126506, Avg_Score -103.71557764554527\n",
      "Adding trajectory to replay buffer: step 1967, counter 31049\n",
      "Environment 15: Episode 427, Score -101.75931176247323, Avg_Score -103.65977903038551\n",
      "Adding trajectory to replay buffer: step 1971, counter 31095\n",
      "Environment 4: Episode 428, Score -103.15765502945457, Avg_Score -103.65407508399204\n",
      "Adding trajectory to replay buffer: step 1972, counter 31149\n",
      "Environment 8: Episode 429, Score -101.58571382312414, Avg_Score -103.6571166595176\n",
      "Adding trajectory to replay buffer: step 1973, counter 31203\n",
      "Environment 5: Episode 430, Score -101.47975427071067, Avg_Score -103.64120863438673\n",
      "Adding trajectory to replay buffer: step 1975, counter 31253\n",
      "Environment 1: Episode 431, Score -103.32054007506184, Avg_Score -103.63832833020743\n",
      "Adding trajectory to replay buffer: step 1975, counter 31302\n",
      "Environment 3: Episode 432, Score -102.29371133858835, Avg_Score -103.62107890783332\n",
      "Adding trajectory to replay buffer: step 1979, counter 31359\n",
      "Environment 6: Episode 433, Score -101.596904285963, Avg_Score -103.62464127473415\n",
      "Adding trajectory to replay buffer: step 1984, counter 31423\n",
      "Environment 12: Episode 434, Score -104.54914198389476, Avg_Score -103.64099200142059\n",
      "Adding trajectory to replay buffer: step 1985, counter 31472\n",
      "Environment 0: Episode 435, Score -102.90194619775936, Avg_Score -103.64817440471268\n",
      "Adding trajectory to replay buffer: step 1994, counter 31517\n",
      "Environment 7: Episode 436, Score -103.94096435042222, Avg_Score -103.6526021699688\n",
      "Adding trajectory to replay buffer: step 2003, counter 31575\n",
      "Environment 2: Episode 437, Score -100.5056944765132, Avg_Score -103.63276827021932\n",
      "Adding trajectory to replay buffer: step 2005, counter 31624\n",
      "Environment 9: Episode 438, Score -102.59247609803919, Avg_Score -103.63839754560084\n",
      "Adding trajectory to replay buffer: step 2005, counter 31675\n",
      "Environment 13: Episode 439, Score -101.87498318250539, Avg_Score -103.62010848988012\n",
      "Adding trajectory to replay buffer: step 2016, counter 31724\n",
      "Environment 14: Episode 440, Score -100.95942229831654, Avg_Score -103.57506482389485\n",
      "Adding trajectory to replay buffer: step 2018, counter 31783\n",
      "Environment 10: Episode 441, Score -103.57721294169873, Avg_Score -103.56823448488895\n",
      "Adding trajectory to replay buffer: step 2019, counter 31835\n",
      "Environment 15: Episode 442, Score -101.4895793508372, Avg_Score -103.56546934942389\n",
      "Adding trajectory to replay buffer: step 2020, counter 31894\n",
      "Environment 11: Episode 443, Score -102.15155459917213, Avg_Score -103.56118541115418\n",
      "Adding trajectory to replay buffer: step 2021, counter 31944\n",
      "Environment 4: Episode 444, Score -103.29542584810468, Avg_Score -103.572035599417\n",
      "Adding trajectory to replay buffer: step 2021, counter 31993\n",
      "Environment 8: Episode 445, Score -103.34174075404555, Avg_Score -103.57097527637609\n",
      "Adding trajectory to replay buffer: step 2024, counter 32044\n",
      "Environment 5: Episode 446, Score -102.4370566478651, Avg_Score -103.562733092354\n",
      "Adding trajectory to replay buffer: step 2030, counter 32099\n",
      "Environment 1: Episode 447, Score -102.31200141877402, Avg_Score -103.53881032336102\n",
      "Adding trajectory to replay buffer: step 2031, counter 32151\n",
      "Environment 6: Episode 448, Score -103.09556169652443, Avg_Score -103.55289669915406\n",
      "Adding trajectory to replay buffer: step 2035, counter 32211\n",
      "Environment 3: Episode 449, Score -101.20867523362425, Avg_Score -103.53143567202292\n",
      "Adding trajectory to replay buffer: step 2037, counter 32263\n",
      "Environment 0: Episode 450, Score -102.46384778733118, Avg_Score -103.53029329258341\n",
      "Adding trajectory to replay buffer: step 2042, counter 32311\n",
      "Environment 7: Episode 451, Score -103.75857936831315, Avg_Score -103.54156114128322\n",
      "Adding trajectory to replay buffer: step 2056, counter 32383\n",
      "Environment 12: Episode 452, Score -103.7842371868541, Avg_Score -103.55318485031849\n",
      "Adding trajectory to replay buffer: step 2059, counter 32437\n",
      "Environment 9: Episode 453, Score -102.4418421174766, Avg_Score -103.55358959533943\n",
      "Adding trajectory to replay buffer: step 2059, counter 32491\n",
      "Environment 13: Episode 454, Score -102.81053940026896, Avg_Score -103.55068336754668\n",
      "Adding trajectory to replay buffer: step 2063, counter 32538\n",
      "Environment 14: Episode 455, Score -102.25335145956588, Avg_Score -103.539567422479\n",
      "Adding trajectory to replay buffer: step 2066, counter 32585\n",
      "Environment 15: Episode 456, Score -103.65530383210691, Avg_Score -103.54127051368279\n",
      "Adding trajectory to replay buffer: step 2067, counter 32632\n",
      "Environment 11: Episode 457, Score -102.84283088407976, Avg_Score -103.5550157328535\n",
      "Adding trajectory to replay buffer: step 2071, counter 32682\n",
      "Environment 4: Episode 458, Score -102.96991242145002, Avg_Score -103.55548448221747\n",
      "Adding trajectory to replay buffer: step 2072, counter 32751\n",
      "Environment 2: Episode 459, Score -101.01521304962287, Avg_Score -103.53549450116762\n",
      "Adding trajectory to replay buffer: step 2072, counter 32799\n",
      "Environment 5: Episode 460, Score -104.35495285167706, Avg_Score -103.56183407255764\n",
      "Adding trajectory to replay buffer: step 2075, counter 32856\n",
      "Environment 10: Episode 461, Score -102.05882401410739, Avg_Score -103.5357208863975\n",
      "Adding trajectory to replay buffer: step 2078, counter 32913\n",
      "Environment 8: Episode 462, Score -101.2968646246319, Avg_Score -103.4990814537006\n",
      "Adding trajectory to replay buffer: step 2081, counter 32963\n",
      "Environment 6: Episode 463, Score -103.10283810628951, Avg_Score -103.36503152338193\n",
      "Adding trajectory to replay buffer: step 2083, counter 33009\n",
      "Environment 0: Episode 464, Score -103.76163404309376, Avg_Score -103.3635588994045\n",
      "Adding trajectory to replay buffer: step 2085, counter 33064\n",
      "Environment 1: Episode 465, Score -100.96893690368161, Avg_Score -103.3490603660988\n",
      "Adding trajectory to replay buffer: step 2104, counter 33133\n",
      "Environment 3: Episode 466, Score -102.76379291821458, Avg_Score -103.346973634952\n",
      "Adding trajectory to replay buffer: step 2107, counter 33184\n",
      "Environment 12: Episode 467, Score -102.72326428347453, Avg_Score -103.33838657016724\n",
      "Adding trajectory to replay buffer: step 2108, counter 33250\n",
      "Environment 7: Episode 468, Score -104.53898392471733, Avg_Score -103.35540395472422\n",
      "Adding trajectory to replay buffer: step 2109, counter 33300\n",
      "Environment 9: Episode 469, Score -107.95863819149447, Avg_Score -103.3931180969704\n",
      "Adding trajectory to replay buffer: step 2116, counter 33357\n",
      "Environment 13: Episode 470, Score -102.60537951426271, Avg_Score -103.39300824040266\n",
      "Adding trajectory to replay buffer: step 2118, counter 33412\n",
      "Environment 14: Episode 471, Score -107.53374521221593, Avg_Score -103.42275088178154\n",
      "Adding trajectory to replay buffer: step 2119, counter 33465\n",
      "Environment 15: Episode 472, Score -101.8029537095651, Avg_Score -103.39811152531561\n",
      "Adding trajectory to replay buffer: step 2120, counter 33513\n",
      "Environment 2: Episode 473, Score -103.3149277462146, Avg_Score -103.40893150421307\n",
      "Adding trajectory to replay buffer: step 2122, counter 33564\n",
      "Environment 4: Episode 474, Score -102.41490772571787, Avg_Score -103.41183715813759\n",
      "Adding trajectory to replay buffer: step 2123, counter 33620\n",
      "Environment 11: Episode 475, Score -105.07967825488353, Avg_Score -103.44893463243302\n",
      "Adding trajectory to replay buffer: step 2124, counter 33672\n",
      "Environment 5: Episode 476, Score -101.91669209787156, Avg_Score -103.44105469620634\n",
      "Adding trajectory to replay buffer: step 2132, counter 33719\n",
      "Environment 1: Episode 477, Score -102.50496184478203, Avg_Score -103.43212118616015\n",
      "Adding trajectory to replay buffer: step 2134, counter 33770\n",
      "Environment 0: Episode 478, Score -102.17916759881997, Avg_Score -103.43412243656029\n",
      "Adding trajectory to replay buffer: step 2135, counter 33827\n",
      "Environment 8: Episode 479, Score -101.85636126375198, Avg_Score -103.42497864531504\n",
      "Adding trajectory to replay buffer: step 2137, counter 33883\n",
      "Environment 6: Episode 480, Score -101.74078824131811, Avg_Score -103.4141520817269\n",
      "Adding trajectory to replay buffer: step 2138, counter 33946\n",
      "Environment 10: Episode 481, Score -101.22089322951793, Avg_Score -103.38994034722869\n",
      "Adding trajectory to replay buffer: step 2155, counter 33997\n",
      "Environment 3: Episode 482, Score -101.64686789160719, Avg_Score -103.37766832615821\n",
      "Adding trajectory to replay buffer: step 2159, counter 34049\n",
      "Environment 12: Episode 483, Score -103.11137168388814, Avg_Score -103.38145498449596\n",
      "Adding trajectory to replay buffer: step 2160, counter 34101\n",
      "Environment 7: Episode 484, Score -103.33475383624807, Avg_Score -103.38906646621501\n",
      "Adding trajectory to replay buffer: step 2160, counter 34145\n",
      "Environment 13: Episode 485, Score -103.07005060139919, Avg_Score -103.39892840621108\n",
      "Adding trajectory to replay buffer: step 2161, counter 34187\n",
      "Environment 15: Episode 486, Score -104.34946641210963, Avg_Score -103.41161064420129\n",
      "Adding trajectory to replay buffer: step 2164, counter 34242\n",
      "Environment 9: Episode 487, Score -101.92309708684931, Avg_Score -103.40351052073882\n",
      "Adding trajectory to replay buffer: step 2164, counter 34283\n",
      "Environment 11: Episode 488, Score -104.58222309947533, Avg_Score -103.42721704545613\n",
      "Adding trajectory to replay buffer: step 2167, counter 34332\n",
      "Environment 14: Episode 489, Score -102.64763709144046, Avg_Score -103.42640232470747\n",
      "Adding trajectory to replay buffer: step 2174, counter 34386\n",
      "Environment 2: Episode 490, Score -101.84620713050974, Avg_Score -103.41963983426089\n",
      "Adding trajectory to replay buffer: step 2177, counter 34439\n",
      "Environment 5: Episode 491, Score -102.10289911303917, Avg_Score -103.27622283986608\n",
      "Adding trajectory to replay buffer: step 2182, counter 34499\n",
      "Environment 4: Episode 492, Score -100.63399568725036, Avg_Score -103.05454981389083\n",
      "Adding trajectory to replay buffer: step 2184, counter 34551\n",
      "Environment 1: Episode 493, Score -103.50589662303217, Avg_Score -103.06845878292324\n",
      "Adding trajectory to replay buffer: step 2186, counter 34603\n",
      "Environment 0: Episode 494, Score -102.04412588188114, Avg_Score -103.08036764956323\n",
      "Adding trajectory to replay buffer: step 2190, counter 34658\n",
      "Environment 8: Episode 495, Score -107.29480990024578, Avg_Score -103.1317507025292\n",
      "Adding trajectory to replay buffer: step 2192, counter 34712\n",
      "Environment 10: Episode 496, Score -101.74760318935166, Avg_Score -103.10613937585373\n",
      "Adding trajectory to replay buffer: step 2193, counter 34768\n",
      "Environment 6: Episode 497, Score -107.82279689060213, Avg_Score -103.14468466551581\n",
      "Adding trajectory to replay buffer: step 2207, counter 34820\n",
      "Environment 3: Episode 498, Score -104.21617273174971, Avg_Score -103.15428963467845\n",
      "Adding trajectory to replay buffer: step 2208, counter 34869\n",
      "Environment 12: Episode 499, Score -103.81256394582304, Avg_Score -103.1706887447012\n",
      "Adding trajectory to replay buffer: step 2211, counter 34916\n",
      "Environment 11: Episode 500, Score -103.74551324842436, Avg_Score -103.1825717063614\n",
      "Adding trajectory to replay buffer: step 2212, counter 34961\n",
      "Environment 14: Episode 501, Score -103.91520442166428, Avg_Score -103.1902651814514\n",
      "Adding trajectory to replay buffer: step 2214, counter 35014\n",
      "Environment 15: Episode 502, Score -102.59829507827588, Avg_Score -103.18908590024739\n",
      "Adding trajectory to replay buffer: step 2215, counter 35065\n",
      "Environment 9: Episode 503, Score -103.31261146546342, Avg_Score -103.20933874840055\n",
      "Adding trajectory to replay buffer: step 2216, counter 35121\n",
      "Environment 13: Episode 504, Score -101.18011770785694, Avg_Score -103.16712538282704\n",
      "Adding trajectory to replay buffer: step 2218, counter 35179\n",
      "Environment 7: Episode 505, Score -100.89950374485987, Avg_Score -103.16153711070712\n",
      "Adding trajectory to replay buffer: step 2223, counter 35228\n",
      "Environment 2: Episode 506, Score -103.58567628421572, Avg_Score -103.15399018704494\n",
      "Adding trajectory to replay buffer: step 2230, counter 35281\n",
      "Environment 5: Episode 507, Score -102.98443501712195, Avg_Score -103.16186651656099\n",
      "Adding trajectory to replay buffer: step 2234, counter 35329\n",
      "Environment 0: Episode 508, Score -102.97305150829256, Avg_Score -103.16077897874214\n",
      "Adding trajectory to replay buffer: step 2235, counter 35382\n",
      "Environment 4: Episode 509, Score -101.24641359472585, Avg_Score -103.11401986442074\n",
      "Adding trajectory to replay buffer: step 2235, counter 35427\n",
      "Environment 8: Episode 510, Score -103.10298538916621, Avg_Score -103.1179428458604\n",
      "Adding trajectory to replay buffer: step 2239, counter 35482\n",
      "Environment 1: Episode 511, Score -101.14243390601067, Avg_Score -103.09879792480822\n",
      "Adding trajectory to replay buffer: step 2241, counter 35531\n",
      "Environment 10: Episode 512, Score -102.3371300973439, Avg_Score -103.08197488355968\n",
      "Adding trajectory to replay buffer: step 2250, counter 35588\n",
      "Environment 6: Episode 513, Score -103.92556551257451, Avg_Score -103.0912011869063\n",
      "Adding trajectory to replay buffer: step 2256, counter 35636\n",
      "Environment 12: Episode 514, Score -102.09973676897275, Avg_Score -103.06924857466062\n",
      "Adding trajectory to replay buffer: step 2261, counter 35690\n",
      "Environment 3: Episode 515, Score -101.85719320282153, Avg_Score -103.06167834632286\n",
      "Adding trajectory to replay buffer: step 2264, counter 35738\n",
      "Environment 13: Episode 516, Score -102.9202465329844, Avg_Score -103.0720404477092\n",
      "Adding trajectory to replay buffer: step 2266, counter 35792\n",
      "Environment 14: Episode 517, Score -101.80918650670473, Avg_Score -103.07707187369031\n",
      "Adding trajectory to replay buffer: step 2268, counter 35845\n",
      "Environment 9: Episode 518, Score -104.00449673413733, Avg_Score -102.787076684666\n",
      "Adding trajectory to replay buffer: step 2273, counter 35904\n",
      "Environment 15: Episode 519, Score -100.70360948006685, Avg_Score -102.77939099766387\n",
      "Adding trajectory to replay buffer: step 2276, counter 35957\n",
      "Environment 2: Episode 520, Score -104.27553623754406, Avg_Score -102.79700162020593\n",
      "Adding trajectory to replay buffer: step 2278, counter 36017\n",
      "Environment 7: Episode 521, Score -105.24446736618938, Avg_Score -102.83873651047662\n",
      "Adding trajectory to replay buffer: step 2283, counter 36089\n",
      "Environment 11: Episode 522, Score -103.03184542787758, Avg_Score -102.84411653185032\n",
      "Adding trajectory to replay buffer: step 2290, counter 36145\n",
      "Environment 0: Episode 523, Score -101.03552998112887, Avg_Score -102.8141270580695\n",
      "Adding trajectory to replay buffer: step 2290, counter 36196\n",
      "Environment 1: Episode 524, Score -102.8149757485427, Avg_Score -102.81485193374688\n",
      "Adding trajectory to replay buffer: step 2290, counter 36256\n",
      "Environment 5: Episode 525, Score -100.57898007341474, Avg_Score -102.79786605094989\n",
      "Adding trajectory to replay buffer: step 2294, counter 36315\n",
      "Environment 4: Episode 526, Score -100.50210284450216, Avg_Score -102.78641254148226\n",
      "Adding trajectory to replay buffer: step 2297, counter 36377\n",
      "Environment 8: Episode 527, Score -101.83630460642588, Avg_Score -102.7871824699218\n",
      "Adding trajectory to replay buffer: step 2298, counter 36434\n",
      "Environment 10: Episode 528, Score -102.6299455587895, Avg_Score -102.78190537521513\n",
      "Adding trajectory to replay buffer: step 2299, counter 36483\n",
      "Environment 6: Episode 529, Score -102.85400390936434, Avg_Score -102.79458827607756\n",
      "Adding trajectory to replay buffer: step 2306, counter 36533\n",
      "Environment 12: Episode 530, Score -101.80897588693827, Avg_Score -102.79788049223981\n",
      "Adding trajectory to replay buffer: step 2311, counter 36583\n",
      "Environment 3: Episode 531, Score -102.43492114842682, Avg_Score -102.78902430297344\n",
      "Adding trajectory to replay buffer: step 2311, counter 36630\n",
      "Environment 13: Episode 532, Score -103.80089031328261, Avg_Score -102.80409609272039\n",
      "Adding trajectory to replay buffer: step 2313, counter 36675\n",
      "Environment 9: Episode 533, Score -103.38629608709738, Avg_Score -102.82199001073175\n",
      "Adding trajectory to replay buffer: step 2318, counter 36727\n",
      "Environment 14: Episode 534, Score -101.75316977637883, Avg_Score -102.7940302886566\n",
      "Adding trajectory to replay buffer: step 2326, counter 36780\n",
      "Environment 15: Episode 535, Score -101.62749507588335, Avg_Score -102.78128577743784\n",
      "Adding trajectory to replay buffer: step 2332, counter 36836\n",
      "Environment 2: Episode 536, Score -100.92497753761833, Avg_Score -102.75112590930979\n",
      "Adding trajectory to replay buffer: step 2338, counter 36884\n",
      "Environment 5: Episode 537, Score -103.6631816710333, Avg_Score -102.782700781255\n",
      "Adding trajectory to replay buffer: step 2341, counter 36942\n",
      "Environment 11: Episode 538, Score -104.13319308771317, Avg_Score -102.79810795115176\n",
      "Adding trajectory to replay buffer: step 2344, counter 36989\n",
      "Environment 8: Episode 539, Score -116.22415400502769, Avg_Score -102.94159965937699\n",
      "Adding trajectory to replay buffer: step 2345, counter 37056\n",
      "Environment 7: Episode 540, Score -102.21862919912674, Avg_Score -102.95419172838508\n",
      "Adding trajectory to replay buffer: step 2347, counter 37113\n",
      "Environment 1: Episode 541, Score -100.92823076179127, Avg_Score -102.92770190658601\n",
      "Adding trajectory to replay buffer: step 2348, counter 37167\n",
      "Environment 4: Episode 542, Score -106.78677340014342, Avg_Score -102.98067384707905\n",
      "Adding trajectory to replay buffer: step 2349, counter 37218\n",
      "Environment 10: Episode 543, Score -101.77700780619371, Avg_Score -102.97692837914927\n",
      "Adding trajectory to replay buffer: step 2350, counter 37269\n",
      "Environment 6: Episode 544, Score -102.22010037314395, Avg_Score -102.96617512439967\n",
      "Adding trajectory to replay buffer: step 2353, counter 37332\n",
      "Environment 0: Episode 545, Score -101.13043974097694, Avg_Score -102.944062114269\n",
      "Adding trajectory to replay buffer: step 2353, counter 37379\n",
      "Environment 12: Episode 546, Score -102.34789350293639, Avg_Score -102.9431704828197\n",
      "Adding trajectory to replay buffer: step 2359, counter 37427\n",
      "Environment 3: Episode 547, Score -102.75444646435727, Avg_Score -102.94759493327552\n",
      "Adding trajectory to replay buffer: step 2362, counter 37476\n",
      "Environment 9: Episode 548, Score -102.76182828947715, Avg_Score -102.94425759920505\n",
      "Adding trajectory to replay buffer: step 2363, counter 37528\n",
      "Environment 13: Episode 549, Score -102.50744598110828, Avg_Score -102.9572453066799\n",
      "Adding trajectory to replay buffer: step 2367, counter 37577\n",
      "Environment 14: Episode 550, Score -102.81054465290158, Avg_Score -102.96071227533557\n",
      "Adding trajectory to replay buffer: step 2373, counter 37624\n",
      "Environment 15: Episode 551, Score -102.25938295027862, Avg_Score -102.94572031115524\n",
      "Adding trajectory to replay buffer: step 2385, counter 37671\n",
      "Environment 5: Episode 552, Score -103.75148322169422, Avg_Score -102.94539277150365\n",
      "Adding trajectory to replay buffer: step 2389, counter 37728\n",
      "Environment 2: Episode 553, Score -101.53958400264196, Avg_Score -102.9363701903553\n",
      "Adding trajectory to replay buffer: step 2390, counter 37773\n",
      "Environment 7: Episode 554, Score -103.30361691851417, Avg_Score -102.94130096553774\n",
      "Adding trajectory to replay buffer: step 2390, counter 37822\n",
      "Environment 11: Episode 555, Score -102.42271773298768, Avg_Score -102.94299462827198\n",
      "Adding trajectory to replay buffer: step 2393, counter 37867\n",
      "Environment 4: Episode 556, Score -103.20311491809599, Avg_Score -102.93847273913187\n",
      "Adding trajectory to replay buffer: step 2395, counter 37915\n",
      "Environment 1: Episode 557, Score -103.56569005425646, Avg_Score -102.94570133083363\n",
      "Adding trajectory to replay buffer: step 2395, counter 37966\n",
      "Environment 8: Episode 558, Score -102.43353207056224, Avg_Score -102.94033752732476\n",
      "Adding trajectory to replay buffer: step 2400, counter 38013\n",
      "Environment 0: Episode 559, Score -102.9784593173104, Avg_Score -102.95996999000161\n",
      "Adding trajectory to replay buffer: step 2400, counter 38064\n",
      "Environment 10: Episode 560, Score -102.61647445361565, Avg_Score -102.94258520602101\n",
      "Adding trajectory to replay buffer: step 2401, counter 38115\n",
      "Environment 6: Episode 561, Score -103.99053279862056, Avg_Score -102.96190229386613\n",
      "Adding trajectory to replay buffer: step 2404, counter 38157\n",
      "Environment 9: Episode 562, Score -104.55955868000723, Avg_Score -102.99452923441987\n",
      "Adding trajectory to replay buffer: step 2410, counter 38208\n",
      "Environment 3: Episode 563, Score -107.30077297647111, Avg_Score -103.0365085831217\n",
      "Adding trajectory to replay buffer: step 2411, counter 38266\n",
      "Environment 12: Episode 564, Score -103.8609134382018, Avg_Score -103.03750137707279\n",
      "Adding trajectory to replay buffer: step 2415, counter 38318\n",
      "Environment 13: Episode 565, Score -103.118680344671, Avg_Score -103.0589988114827\n",
      "Adding trajectory to replay buffer: step 2415, counter 38366\n",
      "Environment 14: Episode 566, Score -103.22512626857186, Avg_Score -103.06361214498625\n",
      "Adding trajectory to replay buffer: step 2425, counter 38418\n",
      "Environment 15: Episode 567, Score -102.30165536027773, Avg_Score -103.05939605575428\n",
      "Adding trajectory to replay buffer: step 2432, counter 38465\n",
      "Environment 5: Episode 568, Score -105.63335281121917, Avg_Score -103.0703397446193\n",
      "Adding trajectory to replay buffer: step 2438, counter 38514\n",
      "Environment 2: Episode 569, Score -102.49924794677707, Avg_Score -103.01574584217212\n",
      "Adding trajectory to replay buffer: step 2441, counter 38565\n",
      "Environment 7: Episode 570, Score -106.87872343405584, Avg_Score -103.05847928137007\n",
      "Adding trajectory to replay buffer: step 2445, counter 38620\n",
      "Environment 11: Episode 571, Score -102.02325326195484, Avg_Score -103.00337436186749\n",
      "Adding trajectory to replay buffer: step 2447, counter 38674\n",
      "Environment 4: Episode 572, Score -102.2543080930002, Avg_Score -103.00788790570182\n",
      "Adding trajectory to replay buffer: step 2449, counter 38728\n",
      "Environment 1: Episode 573, Score -102.89323506986102, Avg_Score -103.00367097893832\n",
      "Adding trajectory to replay buffer: step 2449, counter 38776\n",
      "Environment 6: Episode 574, Score -102.39077085226464, Avg_Score -103.00342961020377\n",
      "Adding trajectory to replay buffer: step 2455, counter 38831\n",
      "Environment 0: Episode 575, Score -104.80009859404652, Avg_Score -103.00063381359541\n",
      "Adding trajectory to replay buffer: step 2457, counter 38888\n",
      "Environment 10: Episode 576, Score -102.67592331550209, Avg_Score -103.00822612577171\n",
      "Adding trajectory to replay buffer: step 2458, counter 38942\n",
      "Environment 9: Episode 577, Score -101.31816889464979, Avg_Score -102.99635819627038\n",
      "Adding trajectory to replay buffer: step 2461, counter 38988\n",
      "Environment 14: Episode 578, Score -105.25726654415205, Avg_Score -103.0271391857237\n",
      "Adding trajectory to replay buffer: step 2462, counter 39040\n",
      "Environment 3: Episode 579, Score -106.83651275403798, Avg_Score -103.07694070062658\n",
      "Adding trajectory to replay buffer: step 2462, counter 39091\n",
      "Environment 12: Episode 580, Score -102.36826348304935, Avg_Score -103.08321545304389\n",
      "Adding trajectory to replay buffer: step 2465, counter 39161\n",
      "Environment 8: Episode 581, Score -102.65676950361518, Avg_Score -103.09757421578487\n",
      "Adding trajectory to replay buffer: step 2468, counter 39214\n",
      "Environment 13: Episode 582, Score -102.9410565217423, Avg_Score -103.11051610208622\n",
      "Adding trajectory to replay buffer: step 2477, counter 39266\n",
      "Environment 15: Episode 583, Score -104.71311835501218, Avg_Score -103.12653356879747\n",
      "Adding trajectory to replay buffer: step 2481, counter 39315\n",
      "Environment 5: Episode 584, Score -103.76661302188225, Avg_Score -103.13085216065382\n",
      "Adding trajectory to replay buffer: step 2491, counter 39361\n",
      "Environment 11: Episode 585, Score -103.28223339905404, Avg_Score -103.13297398863037\n",
      "Adding trajectory to replay buffer: step 2496, counter 39416\n",
      "Environment 7: Episode 586, Score -101.80490757295179, Avg_Score -103.10752840023879\n",
      "Adding trajectory to replay buffer: step 2497, counter 39464\n",
      "Environment 1: Episode 587, Score -103.56166875612115, Avg_Score -103.12391411693152\n",
      "Adding trajectory to replay buffer: step 2497, counter 39512\n",
      "Environment 6: Episode 588, Score -103.84250900471272, Avg_Score -103.1165169759839\n",
      "Adding trajectory to replay buffer: step 2498, counter 39563\n",
      "Environment 4: Episode 589, Score -102.0233215794582, Avg_Score -103.11027382086408\n",
      "Adding trajectory to replay buffer: step 2503, counter 39611\n",
      "Environment 0: Episode 590, Score -103.7923822165324, Avg_Score -103.12973557172431\n",
      "Adding trajectory to replay buffer: step 2505, counter 39658\n",
      "Environment 9: Episode 591, Score -104.2566102645211, Avg_Score -103.15127268323911\n",
      "Adding trajectory to replay buffer: step 2505, counter 39706\n",
      "Environment 10: Episode 592, Score -102.81872052934331, Avg_Score -103.17311993166003\n",
      "Adding trajectory to replay buffer: step 2511, counter 39755\n",
      "Environment 12: Episode 593, Score -102.92690777131233, Avg_Score -103.16733004314283\n",
      "Adding trajectory to replay buffer: step 2512, counter 39806\n",
      "Environment 14: Episode 594, Score -103.81657813127401, Avg_Score -103.18505456563676\n",
      "Adding trajectory to replay buffer: step 2513, counter 39857\n",
      "Environment 3: Episode 595, Score -103.10138171234479, Avg_Score -103.14312028375774\n",
      "Adding trajectory to replay buffer: step 2518, counter 39910\n",
      "Environment 8: Episode 596, Score -101.42747395717167, Avg_Score -103.13991899143595\n",
      "Adding trajectory to replay buffer: step 2518, counter 39960\n",
      "Environment 13: Episode 597, Score -102.194129593481, Avg_Score -103.08363231846475\n",
      "Adding trajectory to replay buffer: step 2527, counter 40006\n",
      "Environment 5: Episode 598, Score -103.36844420579696, Avg_Score -103.0751550332052\n",
      "Adding trajectory to replay buffer: step 2536, counter 40065\n",
      "Environment 15: Episode 599, Score -103.4049944946319, Avg_Score -103.07107933869331\n",
      "Adding trajectory to replay buffer: step 2539, counter 40113\n",
      "Environment 11: Episode 600, Score -102.76776845186328, Avg_Score -103.0613018907277\n",
      "Adding trajectory to replay buffer: step 2540, counter 40215\n",
      "Environment 2: Episode 601, Score -103.3847784261989, Avg_Score -103.05599763077305\n",
      "Adding trajectory to replay buffer: step 2545, counter 40263\n",
      "Environment 6: Episode 602, Score -102.7501251016104, Avg_Score -103.0575159310064\n",
      "Adding trajectory to replay buffer: step 2545, counter 40312\n",
      "Environment 7: Episode 603, Score -102.26531754137079, Avg_Score -103.04704299176547\n",
      "Adding trajectory to replay buffer: step 2547, counter 40362\n",
      "Environment 1: Episode 604, Score -102.09715998010027, Avg_Score -103.05621341448789\n",
      "Adding trajectory to replay buffer: step 2550, counter 40414\n",
      "Environment 4: Episode 605, Score -102.0221109377363, Avg_Score -103.06743948641666\n",
      "Adding trajectory to replay buffer: step 2555, counter 40456\n",
      "Environment 3: Episode 606, Score -104.40611490654014, Avg_Score -103.0756438726399\n",
      "Adding trajectory to replay buffer: step 2555, counter 40506\n",
      "Environment 10: Episode 607, Score -102.51126773476197, Avg_Score -103.0709121998163\n",
      "Adding trajectory to replay buffer: step 2557, counter 40560\n",
      "Environment 0: Episode 608, Score -101.5705115638928, Avg_Score -103.0568868003723\n",
      "Adding trajectory to replay buffer: step 2557, counter 40612\n",
      "Environment 9: Episode 609, Score -101.94631905609494, Avg_Score -103.06388585498598\n",
      "Adding trajectory to replay buffer: step 2559, counter 40659\n",
      "Environment 14: Episode 610, Score -103.66362841547156, Avg_Score -103.06949228524905\n",
      "Adding trajectory to replay buffer: step 2564, counter 40712\n",
      "Environment 12: Episode 611, Score -103.36267612723695, Avg_Score -103.0916947074613\n",
      "Adding trajectory to replay buffer: step 2568, counter 40762\n",
      "Environment 8: Episode 612, Score -101.82883190943984, Avg_Score -103.08661172558226\n",
      "Adding trajectory to replay buffer: step 2568, counter 40812\n",
      "Environment 13: Episode 613, Score -102.51845926773544, Avg_Score -103.07254066313388\n",
      "Adding trajectory to replay buffer: step 2578, counter 40863\n",
      "Environment 5: Episode 614, Score -102.82087600660076, Avg_Score -103.07975205551016\n",
      "Adding trajectory to replay buffer: step 2592, counter 40919\n",
      "Environment 15: Episode 615, Score -102.77391396825264, Avg_Score -103.08891926316446\n",
      "Adding trajectory to replay buffer: step 2595, counter 40967\n",
      "Environment 1: Episode 616, Score -102.78160955206616, Avg_Score -103.08753289335529\n",
      "Adding trajectory to replay buffer: step 2596, counter 41008\n",
      "Environment 10: Episode 617, Score -104.67970285405778, Avg_Score -103.11623805682885\n",
      "Adding trajectory to replay buffer: step 2597, counter 41060\n",
      "Environment 6: Episode 618, Score -101.94974170919197, Avg_Score -103.09569050657937\n",
      "Adding trajectory to replay buffer: step 2598, counter 41118\n",
      "Environment 2: Episode 619, Score -101.15841069492573, Avg_Score -103.10023851872795\n",
      "Adding trajectory to replay buffer: step 2604, counter 41183\n",
      "Environment 11: Episode 620, Score -100.84399544280892, Avg_Score -103.06592311078059\n",
      "Adding trajectory to replay buffer: step 2606, counter 41232\n",
      "Environment 9: Episode 621, Score -104.65363809680008, Avg_Score -103.0600148180867\n",
      "Adding trajectory to replay buffer: step 2611, counter 41293\n",
      "Environment 4: Episode 622, Score -102.70799377557573, Avg_Score -103.05677630156366\n",
      "Adding trajectory to replay buffer: step 2612, counter 41350\n",
      "Environment 3: Episode 623, Score -101.44860124036546, Avg_Score -103.06090701415602\n",
      "Adding trajectory to replay buffer: step 2612, counter 41403\n",
      "Environment 14: Episode 624, Score -101.79320544965317, Avg_Score -103.05068931116712\n",
      "Adding trajectory to replay buffer: step 2614, counter 41449\n",
      "Environment 8: Episode 625, Score -103.90989568164449, Avg_Score -103.08399846724944\n",
      "Adding trajectory to replay buffer: step 2616, counter 41501\n",
      "Environment 12: Episode 626, Score -102.5526065197084, Avg_Score -103.1045035040015\n",
      "Adding trajectory to replay buffer: step 2617, counter 41550\n",
      "Environment 13: Episode 627, Score -102.00756103572343, Avg_Score -103.10621606829446\n",
      "Adding trajectory to replay buffer: step 2618, counter 41611\n",
      "Environment 0: Episode 628, Score -100.90867055032123, Avg_Score -103.08900331820976\n",
      "Adding trajectory to replay buffer: step 2620, counter 41686\n",
      "Environment 7: Episode 629, Score -101.9885620590173, Avg_Score -103.08034889970631\n",
      "Adding trajectory to replay buffer: step 2627, counter 41735\n",
      "Environment 5: Episode 630, Score -103.10149558578618, Avg_Score -103.09327409669477\n",
      "Adding trajectory to replay buffer: step 2644, counter 41782\n",
      "Environment 6: Episode 631, Score -103.37598884707441, Avg_Score -103.10268477368125\n",
      "Adding trajectory to replay buffer: step 2646, counter 41832\n",
      "Environment 10: Episode 632, Score -103.62988150241475, Avg_Score -103.10097468557258\n",
      "Adding trajectory to replay buffer: step 2647, counter 41884\n",
      "Environment 1: Episode 633, Score -102.62690887638958, Avg_Score -103.0933808134655\n",
      "Adding trajectory to replay buffer: step 2648, counter 41940\n",
      "Environment 15: Episode 634, Score -103.30312628877536, Avg_Score -103.10888037858946\n",
      "Adding trajectory to replay buffer: step 2658, counter 41994\n",
      "Environment 11: Episode 635, Score -102.26070920232776, Avg_Score -103.11521251985393\n",
      "Adding trajectory to replay buffer: step 2659, counter 42047\n",
      "Environment 9: Episode 636, Score -102.84334328496332, Avg_Score -103.13439617732736\n",
      "Adding trajectory to replay buffer: step 2660, counter 42096\n",
      "Environment 4: Episode 637, Score -104.1138827701273, Avg_Score -103.13890318831831\n",
      "Adding trajectory to replay buffer: step 2661, counter 42159\n",
      "Environment 2: Episode 638, Score -121.5920924259331, Avg_Score -103.31349218170051\n",
      "Adding trajectory to replay buffer: step 2662, counter 42203\n",
      "Environment 0: Episode 639, Score -103.98358660986213, Avg_Score -103.19108650774886\n",
      "Adding trajectory to replay buffer: step 2664, counter 42255\n",
      "Environment 14: Episode 640, Score -102.67793333436673, Avg_Score -103.19567954910127\n",
      "Adding trajectory to replay buffer: step 2665, counter 42308\n",
      "Environment 3: Episode 641, Score -101.6740792537195, Avg_Score -103.20313803402054\n",
      "Adding trajectory to replay buffer: step 2666, counter 42358\n",
      "Environment 12: Episode 642, Score -103.14491741116966, Avg_Score -103.1667194741308\n",
      "Adding trajectory to replay buffer: step 2668, counter 42409\n",
      "Environment 13: Episode 643, Score -102.45652803518624, Avg_Score -103.17351467642075\n",
      "Adding trajectory to replay buffer: step 2669, counter 42458\n",
      "Environment 7: Episode 644, Score -101.99449251530568, Avg_Score -103.17125859784237\n",
      "Adding trajectory to replay buffer: step 2669, counter 42513\n",
      "Environment 8: Episode 645, Score -108.75270898259741, Avg_Score -103.24748129025858\n",
      "Adding trajectory to replay buffer: step 2681, counter 42567\n",
      "Environment 5: Episode 646, Score -102.12592770240953, Avg_Score -103.24526163225329\n",
      "Adding trajectory to replay buffer: step 2694, counter 42617\n",
      "Environment 6: Episode 647, Score -101.99993784313122, Avg_Score -103.23771654604103\n",
      "Adding trajectory to replay buffer: step 2699, counter 42669\n",
      "Environment 1: Episode 648, Score -102.00030508813573, Avg_Score -103.23010131402762\n",
      "Adding trajectory to replay buffer: step 2699, counter 42722\n",
      "Environment 10: Episode 649, Score -103.36999612715405, Avg_Score -103.23872681548805\n",
      "Adding trajectory to replay buffer: step 2701, counter 42775\n",
      "Environment 15: Episode 650, Score -102.911749701264, Avg_Score -103.23973886597169\n",
      "Adding trajectory to replay buffer: step 2706, counter 42823\n",
      "Environment 11: Episode 651, Score -104.38964502249968, Avg_Score -103.26104148669393\n",
      "Adding trajectory to replay buffer: step 2708, counter 42869\n",
      "Environment 0: Episode 652, Score -103.89067118238098, Avg_Score -103.26243336630077\n",
      "Adding trajectory to replay buffer: step 2710, counter 42919\n",
      "Environment 4: Episode 653, Score -102.9501351018281, Avg_Score -103.27653887729262\n",
      "Adding trajectory to replay buffer: step 2714, counter 42974\n",
      "Environment 9: Episode 654, Score -102.01865349063402, Avg_Score -103.26368924301381\n",
      "Adding trajectory to replay buffer: step 2715, counter 43024\n",
      "Environment 3: Episode 655, Score -102.75518312018364, Avg_Score -103.26701389688577\n",
      "Adding trajectory to replay buffer: step 2716, counter 43071\n",
      "Environment 7: Episode 656, Score -103.47228107534225, Avg_Score -103.26970555845824\n",
      "Adding trajectory to replay buffer: step 2717, counter 43127\n",
      "Environment 2: Episode 657, Score -102.52535463725154, Avg_Score -103.25930220428819\n",
      "Adding trajectory to replay buffer: step 2717, counter 43180\n",
      "Environment 14: Episode 658, Score -102.91310219329782, Avg_Score -103.26409790551556\n",
      "Adding trajectory to replay buffer: step 2718, counter 43229\n",
      "Environment 8: Episode 659, Score -102.32589849045144, Avg_Score -103.25757229724697\n",
      "Adding trajectory to replay buffer: step 2719, counter 43282\n",
      "Environment 12: Episode 660, Score -101.03983129031583, Avg_Score -103.24180586561398\n",
      "Adding trajectory to replay buffer: step 2721, counter 43322\n",
      "Environment 5: Episode 661, Score -104.51388668118169, Avg_Score -103.24703940443956\n",
      "Adding trajectory to replay buffer: step 2724, counter 43378\n",
      "Environment 13: Episode 662, Score -101.26107221892912, Avg_Score -103.21405453982878\n",
      "Adding trajectory to replay buffer: step 2743, counter 43427\n",
      "Environment 6: Episode 663, Score -102.68484456163645, Avg_Score -103.16789525568045\n",
      "Adding trajectory to replay buffer: step 2744, counter 43472\n",
      "Environment 10: Episode 664, Score -104.05743175019137, Avg_Score -103.16986043880033\n",
      "Adding trajectory to replay buffer: step 2749, counter 43522\n",
      "Environment 1: Episode 665, Score -103.97327141398067, Avg_Score -103.17840634949341\n",
      "Adding trajectory to replay buffer: step 2755, counter 43571\n",
      "Environment 11: Episode 666, Score -101.62364814085947, Avg_Score -103.1623915682163\n",
      "Adding trajectory to replay buffer: step 2757, counter 43618\n",
      "Environment 4: Episode 667, Score -103.49450969167054, Avg_Score -103.17432011153024\n",
      "Adding trajectory to replay buffer: step 2760, counter 43663\n",
      "Environment 3: Episode 668, Score -103.75431901206511, Avg_Score -103.15552977353869\n",
      "Adding trajectory to replay buffer: step 2760, counter 43709\n",
      "Environment 9: Episode 669, Score -103.82031886441013, Avg_Score -103.16874048271502\n",
      "Adding trajectory to replay buffer: step 2761, counter 43762\n",
      "Environment 0: Episode 670, Score -102.05722890650294, Avg_Score -103.12052553743948\n",
      "Adding trajectory to replay buffer: step 2764, counter 43809\n",
      "Environment 2: Episode 671, Score -102.52618593937656, Avg_Score -103.12555486421368\n",
      "Adding trajectory to replay buffer: step 2767, counter 43860\n",
      "Environment 7: Episode 672, Score -102.0244251306678, Avg_Score -103.12325603459036\n",
      "Adding trajectory to replay buffer: step 2767, counter 43910\n",
      "Environment 14: Episode 673, Score -102.51292619348503, Avg_Score -103.11945294582661\n",
      "Adding trajectory to replay buffer: step 2772, counter 43963\n",
      "Environment 12: Episode 674, Score -102.27054885569153, Avg_Score -103.11825072586089\n",
      "Adding trajectory to replay buffer: step 2774, counter 44016\n",
      "Environment 5: Episode 675, Score -102.62995337566113, Avg_Score -103.09654927367704\n",
      "Adding trajectory to replay buffer: step 2775, counter 44067\n",
      "Environment 13: Episode 676, Score -101.60880222288736, Avg_Score -103.08587806275087\n",
      "Adding trajectory to replay buffer: step 2786, counter 44135\n",
      "Environment 8: Episode 677, Score -102.91537573686925, Avg_Score -103.10185013117307\n",
      "Adding trajectory to replay buffer: step 2793, counter 44185\n",
      "Environment 6: Episode 678, Score -102.17842770130187, Avg_Score -103.07106174274458\n",
      "Adding trajectory to replay buffer: step 2793, counter 44234\n",
      "Environment 10: Episode 679, Score -103.27794027023452, Avg_Score -103.03547601790653\n",
      "Adding trajectory to replay buffer: step 2798, counter 44283\n",
      "Environment 1: Episode 680, Score -102.64312611741883, Avg_Score -103.03822464425022\n",
      "Adding trajectory to replay buffer: step 2804, counter 44330\n",
      "Environment 4: Episode 681, Score -104.20110720178361, Avg_Score -103.05366802123191\n",
      "Adding trajectory to replay buffer: step 2804, counter 44433\n",
      "Environment 15: Episode 682, Score -105.65883688627369, Avg_Score -103.08084582487722\n",
      "Adding trajectory to replay buffer: step 2805, counter 44478\n",
      "Environment 9: Episode 683, Score -116.38042230058275, Avg_Score -103.19751886433292\n",
      "Adding trajectory to replay buffer: step 2805, counter 44528\n",
      "Environment 11: Episode 684, Score -102.64831731992463, Avg_Score -103.18633590731335\n",
      "Adding trajectory to replay buffer: step 2810, counter 44577\n",
      "Environment 0: Episode 685, Score -102.99047636026951, Avg_Score -103.18341833692551\n",
      "Adding trajectory to replay buffer: step 2810, counter 44627\n",
      "Environment 3: Episode 686, Score -104.5636166966036, Avg_Score -103.21100542816204\n",
      "Adding trajectory to replay buffer: step 2817, counter 44677\n",
      "Environment 7: Episode 687, Score -103.71425969309857, Avg_Score -103.2125313375318\n",
      "Adding trajectory to replay buffer: step 2817, counter 44727\n",
      "Environment 14: Episode 688, Score -102.80063154234303, Avg_Score -103.20211256290811\n",
      "Adding trajectory to replay buffer: step 2818, counter 44773\n",
      "Environment 12: Episode 689, Score -103.59698838342851, Avg_Score -103.21784923094782\n",
      "Adding trajectory to replay buffer: step 2820, counter 44829\n",
      "Environment 2: Episode 690, Score -101.45575326634943, Avg_Score -103.19448294144598\n",
      "Adding trajectory to replay buffer: step 2824, counter 44879\n",
      "Environment 5: Episode 691, Score -103.72659293645496, Avg_Score -103.18918276816534\n",
      "Adding trajectory to replay buffer: step 2832, counter 44925\n",
      "Environment 8: Episode 692, Score -102.58612695226073, Avg_Score -103.18685683239451\n",
      "Adding trajectory to replay buffer: step 2837, counter 44987\n",
      "Environment 13: Episode 693, Score -102.81984726247751, Avg_Score -103.18578622730614\n",
      "Adding trajectory to replay buffer: step 2844, counter 45033\n",
      "Environment 1: Episode 694, Score -102.308932332209, Avg_Score -103.17070976931551\n",
      "Adding trajectory to replay buffer: step 2844, counter 45084\n",
      "Environment 10: Episode 695, Score -102.69745817371086, Avg_Score -103.16667053392918\n",
      "Adding trajectory to replay buffer: step 2846, counter 45137\n",
      "Environment 6: Episode 696, Score -102.59610846902554, Avg_Score -103.17835687904771\n",
      "Adding trajectory to replay buffer: step 2854, counter 45187\n",
      "Environment 4: Episode 697, Score -102.5213818602177, Avg_Score -103.18162940171507\n",
      "Adding trajectory to replay buffer: step 2855, counter 45232\n",
      "Environment 3: Episode 698, Score -103.9579983018407, Avg_Score -103.18752494267552\n",
      "Adding trajectory to replay buffer: step 2855, counter 45283\n",
      "Environment 15: Episode 699, Score -102.53767029176156, Avg_Score -103.17885170064683\n",
      "Adding trajectory to replay buffer: step 2859, counter 45337\n",
      "Environment 9: Episode 700, Score -101.58072471379799, Avg_Score -103.16698126326617\n",
      "Adding trajectory to replay buffer: step 2861, counter 45393\n",
      "Environment 11: Episode 701, Score -100.93051439378038, Avg_Score -103.142438622942\n",
      "Adding trajectory to replay buffer: step 2864, counter 45440\n",
      "Environment 7: Episode 702, Score -102.6982396558222, Avg_Score -103.1419197684841\n",
      "Adding trajectory to replay buffer: step 2867, counter 45497\n",
      "Environment 0: Episode 703, Score -101.27040878450312, Avg_Score -103.13197068091544\n",
      "Adding trajectory to replay buffer: step 2867, counter 45546\n",
      "Environment 12: Episode 704, Score -102.694471475179, Avg_Score -103.1379437958662\n",
      "Adding trajectory to replay buffer: step 2871, counter 45600\n",
      "Environment 14: Episode 705, Score -106.46467126025446, Avg_Score -103.1823693990914\n",
      "Adding trajectory to replay buffer: step 2876, counter 45656\n",
      "Environment 2: Episode 706, Score -101.35785474196263, Avg_Score -103.15188679744564\n",
      "Adding trajectory to replay buffer: step 2881, counter 45713\n",
      "Environment 5: Episode 707, Score -102.38307191517576, Avg_Score -103.15060483924977\n",
      "Adding trajectory to replay buffer: step 2884, counter 45765\n",
      "Environment 8: Episode 708, Score -104.37012828126736, Avg_Score -103.1786010064235\n",
      "Adding trajectory to replay buffer: step 2889, counter 45817\n",
      "Environment 13: Episode 709, Score -101.86372627207078, Avg_Score -103.17777507858327\n",
      "Adding trajectory to replay buffer: step 2892, counter 45865\n",
      "Environment 10: Episode 710, Score -102.93044362579721, Avg_Score -103.17044323068654\n",
      "Adding trajectory to replay buffer: step 2896, counter 45917\n",
      "Environment 1: Episode 711, Score -101.57329563751196, Avg_Score -103.15254942578929\n",
      "Adding trajectory to replay buffer: step 2901, counter 45972\n",
      "Environment 6: Episode 712, Score -102.44884483860604, Avg_Score -103.15874955508095\n",
      "Adding trajectory to replay buffer: step 2902, counter 46019\n",
      "Environment 3: Episode 713, Score -102.94950648667167, Avg_Score -103.16306002727032\n",
      "Adding trajectory to replay buffer: step 2903, counter 46068\n",
      "Environment 4: Episode 714, Score -103.8016650952443, Avg_Score -103.17286791815674\n",
      "Adding trajectory to replay buffer: step 2905, counter 46114\n",
      "Environment 9: Episode 715, Score -104.28089911176824, Avg_Score -103.18793776959191\n",
      "Adding trajectory to replay buffer: step 2905, counter 46164\n",
      "Environment 15: Episode 716, Score -103.99908708840422, Avg_Score -103.2001125449553\n",
      "Adding trajectory to replay buffer: step 2909, counter 46212\n",
      "Environment 11: Episode 717, Score -103.3820824427971, Avg_Score -103.1871363408427\n",
      "Adding trajectory to replay buffer: step 2917, counter 46262\n",
      "Environment 0: Episode 718, Score -102.56585422709398, Avg_Score -103.1932974660217\n",
      "Adding trajectory to replay buffer: step 2918, counter 46316\n",
      "Environment 7: Episode 719, Score -101.32645460610472, Avg_Score -103.19497790513351\n",
      "Adding trajectory to replay buffer: step 2919, counter 46368\n",
      "Environment 12: Episode 720, Score -101.94469469658472, Avg_Score -103.20598489767126\n",
      "Adding trajectory to replay buffer: step 2925, counter 46422\n",
      "Environment 14: Episode 721, Score -101.64255227209317, Avg_Score -103.1758740394242\n",
      "Adding trajectory to replay buffer: step 2928, counter 46474\n",
      "Environment 2: Episode 722, Score -102.25965851362578, Avg_Score -103.1713906868047\n",
      "Adding trajectory to replay buffer: step 2930, counter 46523\n",
      "Environment 5: Episode 723, Score -102.74912957435288, Avg_Score -103.18439597014458\n",
      "Adding trajectory to replay buffer: step 2932, counter 46571\n",
      "Environment 8: Episode 724, Score -102.66878777429399, Avg_Score -103.193151793391\n",
      "Adding trajectory to replay buffer: step 2939, counter 46621\n",
      "Environment 13: Episode 725, Score -103.19958645930575, Avg_Score -103.1860487011676\n",
      "Adding trajectory to replay buffer: step 2944, counter 46669\n",
      "Environment 1: Episode 726, Score -103.46153404536905, Avg_Score -103.19513797642419\n",
      "Adding trajectory to replay buffer: step 2949, counter 46726\n",
      "Environment 10: Episode 727, Score -101.32688422910869, Avg_Score -103.18833120835808\n",
      "Adding trajectory to replay buffer: step 2950, counter 46774\n",
      "Environment 3: Episode 728, Score -103.03925294368885, Avg_Score -103.20963703229175\n",
      "Adding trajectory to replay buffer: step 2954, counter 46825\n",
      "Environment 4: Episode 729, Score -103.02356813180198, Avg_Score -103.21998709301957\n",
      "Adding trajectory to replay buffer: step 2957, counter 46877\n",
      "Environment 9: Episode 730, Score -103.09807743845477, Avg_Score -103.21995291154627\n",
      "Adding trajectory to replay buffer: step 2962, counter 46934\n",
      "Environment 15: Episode 731, Score -101.40325701219216, Avg_Score -103.20022559319744\n",
      "Adding trajectory to replay buffer: step 2963, counter 46988\n",
      "Environment 11: Episode 732, Score -102.80082236352314, Avg_Score -103.19193500180853\n",
      "Adding trajectory to replay buffer: step 2966, counter 47035\n",
      "Environment 12: Episode 733, Score -103.42191580206404, Avg_Score -103.1998850710653\n",
      "Adding trajectory to replay buffer: step 2973, counter 47090\n",
      "Environment 7: Episode 734, Score -101.48307660572914, Avg_Score -103.18168457423482\n",
      "Adding trajectory to replay buffer: step 2975, counter 47164\n",
      "Environment 6: Episode 735, Score -102.30419815940348, Avg_Score -103.18211946380558\n",
      "Adding trajectory to replay buffer: step 2977, counter 47224\n",
      "Environment 0: Episode 736, Score -100.92005234649218, Avg_Score -103.16288655442084\n",
      "Adding trajectory to replay buffer: step 2977, counter 47276\n",
      "Environment 14: Episode 737, Score -102.52499155867659, Avg_Score -103.14699764230632\n",
      "Adding trajectory to replay buffer: step 2979, counter 47327\n",
      "Environment 2: Episode 738, Score -101.97258448320937, Avg_Score -102.9508025628791\n",
      "Adding trajectory to replay buffer: step 2982, counter 47379\n",
      "Environment 5: Episode 739, Score -102.3025017442045, Avg_Score -102.93399171422254\n",
      "Adding trajectory to replay buffer: step 2983, counter 47430\n",
      "Environment 8: Episode 740, Score -101.98731150749761, Avg_Score -102.92708549595382\n",
      "Adding trajectory to replay buffer: step 2996, counter 47487\n",
      "Environment 13: Episode 741, Score -101.60431784289517, Avg_Score -102.92638788184557\n",
      "Adding trajectory to replay buffer: step 2999, counter 47536\n",
      "Environment 3: Episode 742, Score -102.44158493156095, Avg_Score -102.91935455704949\n",
      "Adding trajectory to replay buffer: step 2999, counter 47586\n",
      "Environment 10: Episode 743, Score -101.78234507473962, Avg_Score -102.91261272744504\n",
      "Adding trajectory to replay buffer: step 3003, counter 47635\n",
      "Environment 4: Episode 744, Score -103.76760544749163, Avg_Score -102.93034385676691\n",
      "Adding trajectory to replay buffer: step 3004, counter 47695\n",
      "Environment 1: Episode 745, Score -104.9302221220949, Avg_Score -102.89211898816187\n",
      "Adding trajectory to replay buffer: step 3004, counter 47742\n",
      "Environment 9: Episode 746, Score -102.30114692111748, Avg_Score -102.89387118034894\n",
      "Adding trajectory to replay buffer: step 3013, counter 47793\n",
      "Environment 15: Episode 747, Score -102.75342182259448, Avg_Score -102.90140602014357\n",
      "Adding trajectory to replay buffer: step 3014, counter 47844\n",
      "Environment 11: Episode 748, Score -102.50770885205208, Avg_Score -102.90648005778276\n",
      "Adding trajectory to replay buffer: step 3019, counter 47897\n",
      "Environment 12: Episode 749, Score -102.46127478092288, Avg_Score -102.89739284432042\n",
      "Adding trajectory to replay buffer: step 3023, counter 47947\n",
      "Environment 7: Episode 750, Score -102.20701874214132, Avg_Score -102.89034553472919\n",
      "Adding trajectory to replay buffer: step 3027, counter 47997\n",
      "Environment 0: Episode 751, Score -102.23424476403619, Avg_Score -102.86879153214457\n",
      "Adding trajectory to replay buffer: step 3028, counter 48048\n",
      "Environment 14: Episode 752, Score -101.8390847476516, Avg_Score -102.84827566779727\n",
      "Adding trajectory to replay buffer: step 3030, counter 48103\n",
      "Environment 6: Episode 753, Score -101.8227951280425, Avg_Score -102.83700226805941\n",
      "Adding trajectory to replay buffer: step 3033, counter 48157\n",
      "Environment 2: Episode 754, Score -102.21969875842457, Avg_Score -102.83901272073732\n",
      "Adding trajectory to replay buffer: step 3037, counter 48211\n",
      "Environment 8: Episode 755, Score -101.40880466425295, Avg_Score -102.82554893617802\n",
      "Adding trajectory to replay buffer: step 3038, counter 48267\n",
      "Environment 5: Episode 756, Score -101.26160635982951, Avg_Score -102.80344218902287\n",
      "Adding trajectory to replay buffer: step 3048, counter 48316\n",
      "Environment 3: Episode 757, Score -102.9538493326151, Avg_Score -102.8077271359765\n",
      "Adding trajectory to replay buffer: step 3049, counter 48369\n",
      "Environment 13: Episode 758, Score -101.59246187007675, Avg_Score -102.79452073274432\n",
      "Adding trajectory to replay buffer: step 3050, counter 48420\n",
      "Environment 10: Episode 759, Score -102.08869397307373, Avg_Score -102.7921486875705\n",
      "Adding trajectory to replay buffer: step 3052, counter 48469\n",
      "Environment 4: Episode 760, Score -103.28907747305122, Avg_Score -102.81464114939784\n",
      "Adding trajectory to replay buffer: step 3060, counter 48525\n",
      "Environment 9: Episode 761, Score -101.04992667858365, Avg_Score -102.78000154937186\n",
      "Adding trajectory to replay buffer: step 3061, counter 48573\n",
      "Environment 15: Episode 762, Score -103.73366314234273, Avg_Score -102.804727458606\n",
      "Adding trajectory to replay buffer: step 3063, counter 48632\n",
      "Environment 1: Episode 763, Score -101.74412059336156, Avg_Score -102.79532021892324\n",
      "Adding trajectory to replay buffer: step 3064, counter 48682\n",
      "Environment 11: Episode 764, Score -103.07706937625632, Avg_Score -102.7855165951839\n",
      "Adding trajectory to replay buffer: step 3078, counter 48732\n",
      "Environment 14: Episode 765, Score -103.41240373077119, Avg_Score -102.7799079183518\n",
      "Adding trajectory to replay buffer: step 3079, counter 48792\n",
      "Environment 12: Episode 766, Score -101.29840377085966, Avg_Score -102.7766554746518\n",
      "Adding trajectory to replay buffer: step 3081, counter 48843\n",
      "Environment 6: Episode 767, Score -102.6309613562785, Avg_Score -102.76801999129789\n",
      "Adding trajectory to replay buffer: step 3082, counter 48898\n",
      "Environment 0: Episode 768, Score -102.42495812438676, Avg_Score -102.75472638242113\n",
      "Adding trajectory to replay buffer: step 3089, counter 48964\n",
      "Environment 7: Episode 769, Score -103.53678933695579, Avg_Score -102.75189108714657\n",
      "Adding trajectory to replay buffer: step 3094, counter 49025\n",
      "Environment 2: Episode 770, Score -100.83140691795448, Avg_Score -102.7396328672611\n",
      "Adding trajectory to replay buffer: step 3094, counter 49081\n",
      "Environment 5: Episode 771, Score -101.75284821772948, Avg_Score -102.73189949004461\n",
      "Adding trajectory to replay buffer: step 3100, counter 49129\n",
      "Environment 4: Episode 772, Score -103.0359725717126, Avg_Score -102.74201496445508\n",
      "Adding trajectory to replay buffer: step 3100, counter 49192\n",
      "Environment 8: Episode 773, Score -103.13726189831955, Avg_Score -102.74825832150341\n",
      "Adding trajectory to replay buffer: step 3104, counter 49248\n",
      "Environment 3: Episode 774, Score -100.9337822568727, Avg_Score -102.73489065551523\n",
      "Adding trajectory to replay buffer: step 3104, counter 49303\n",
      "Environment 13: Episode 775, Score -101.47147569460236, Avg_Score -102.72330587870465\n",
      "Adding trajectory to replay buffer: step 3105, counter 49358\n",
      "Environment 10: Episode 776, Score -101.43129954646156, Avg_Score -102.7215308519404\n",
      "Adding trajectory to replay buffer: step 3110, counter 49405\n",
      "Environment 1: Episode 777, Score -103.57053632136123, Avg_Score -102.72808245778533\n",
      "Adding trajectory to replay buffer: step 3116, counter 49460\n",
      "Environment 15: Episode 778, Score -102.04524828133235, Avg_Score -102.72675066358562\n",
      "Adding trajectory to replay buffer: step 3117, counter 49513\n",
      "Environment 11: Episode 779, Score -101.5388598407836, Avg_Score -102.70935985929114\n",
      "Adding trajectory to replay buffer: step 3123, counter 49576\n",
      "Environment 9: Episode 780, Score -103.30127105546433, Avg_Score -102.71594130867157\n",
      "Adding trajectory to replay buffer: step 3128, counter 49625\n",
      "Environment 12: Episode 781, Score -102.97978929081373, Avg_Score -102.7037281295619\n",
      "Adding trajectory to replay buffer: step 3131, counter 49678\n",
      "Environment 14: Episode 782, Score -101.3226791617119, Avg_Score -102.6603665523163\n",
      "Adding trajectory to replay buffer: step 3132, counter 49728\n",
      "Environment 0: Episode 783, Score -102.58209932366634, Avg_Score -102.52238332254714\n",
      "Adding trajectory to replay buffer: step 3133, counter 49780\n",
      "Environment 6: Episode 784, Score -102.8311702692189, Avg_Score -102.52421185204007\n",
      "Adding trajectory to replay buffer: step 3143, counter 49829\n",
      "Environment 5: Episode 785, Score -102.78473904238517, Avg_Score -102.52215447886122\n",
      "Adding trajectory to replay buffer: step 3144, counter 49879\n",
      "Environment 2: Episode 786, Score -103.7816506990244, Avg_Score -102.51433481888543\n",
      "Adding trajectory to replay buffer: step 3145, counter 49935\n",
      "Environment 7: Episode 787, Score -102.60251161321936, Avg_Score -102.50321733808661\n",
      "Adding trajectory to replay buffer: step 3150, counter 49981\n",
      "Environment 3: Episode 788, Score -103.77485134985236, Avg_Score -102.51295953616169\n",
      "Adding trajectory to replay buffer: step 3150, counter 50031\n",
      "Environment 4: Episode 789, Score -103.30644582315348, Avg_Score -102.51005411055895\n",
      "Adding trajectory to replay buffer: step 3150, counter 50081\n",
      "Environment 8: Episode 790, Score -102.64254333523536, Avg_Score -102.52192201124782\n",
      "Adding trajectory to replay buffer: step 3151, counter 50128\n",
      "Environment 13: Episode 791, Score -103.00815876230845, Avg_Score -102.51473766950632\n",
      "Adding trajectory to replay buffer: step 3159, counter 50182\n",
      "Environment 10: Episode 792, Score -102.401735846214, Avg_Score -102.51289375844588\n",
      "Adding trajectory to replay buffer: step 3163, counter 50235\n",
      "Environment 1: Episode 793, Score -101.49129673950871, Avg_Score -102.49960825321618\n",
      "Adding trajectory to replay buffer: step 3168, counter 50286\n",
      "Environment 11: Episode 794, Score -102.85775456666636, Avg_Score -102.50509647556079\n",
      "Adding trajectory to replay buffer: step 3172, counter 50335\n",
      "Environment 9: Episode 795, Score -102.13349196718819, Avg_Score -102.49945681349554\n",
      "Adding trajectory to replay buffer: step 3175, counter 50379\n",
      "Environment 14: Episode 796, Score -103.99085242879143, Avg_Score -102.51340425309317\n",
      "Adding trajectory to replay buffer: step 3181, counter 50432\n",
      "Environment 12: Episode 797, Score -101.58766639794595, Avg_Score -102.50406709847044\n",
      "Adding trajectory to replay buffer: step 3182, counter 50482\n",
      "Environment 0: Episode 798, Score -102.93623576626368, Avg_Score -102.49384947311468\n",
      "Adding trajectory to replay buffer: step 3187, counter 50536\n",
      "Environment 6: Episode 799, Score -101.77419715639824, Avg_Score -102.48621474176103\n",
      "Adding trajectory to replay buffer: step 3188, counter 50608\n",
      "Environment 15: Episode 800, Score -102.52191759955697, Avg_Score -102.4956266706186\n",
      "Adding trajectory to replay buffer: step 3192, counter 50657\n",
      "Environment 5: Episode 801, Score -104.18231626242275, Avg_Score -102.52814468930502\n",
      "Adding trajectory to replay buffer: step 3193, counter 50706\n",
      "Environment 2: Episode 802, Score -102.55512186330185, Avg_Score -102.52671351137982\n",
      "Adding trajectory to replay buffer: step 3193, counter 50754\n",
      "Environment 7: Episode 803, Score -103.61707069431245, Avg_Score -102.5501801304779\n",
      "Adding trajectory to replay buffer: step 3199, counter 50803\n",
      "Environment 4: Episode 804, Score -103.49651000721938, Avg_Score -102.5582005157983\n",
      "Adding trajectory to replay buffer: step 3200, counter 50853\n",
      "Environment 8: Episode 805, Score -102.66366754241272, Avg_Score -102.5201904786199\n",
      "Adding trajectory to replay buffer: step 3201, counter 50904\n",
      "Environment 3: Episode 806, Score -102.1007167695742, Avg_Score -102.527619098896\n",
      "Adding trajectory to replay buffer: step 3204, counter 50957\n",
      "Environment 13: Episode 807, Score -101.97738377796237, Avg_Score -102.52356221752387\n",
      "Adding trajectory to replay buffer: step 3209, counter 51007\n",
      "Environment 10: Episode 808, Score -101.2782339926182, Avg_Score -102.49264327463739\n",
      "Adding trajectory to replay buffer: step 3218, counter 51057\n",
      "Environment 11: Episode 809, Score -102.35867910846757, Avg_Score -102.49759280300135\n",
      "Adding trajectory to replay buffer: step 3224, counter 51109\n",
      "Environment 9: Episode 810, Score -103.98980049346636, Avg_Score -102.50818637167804\n",
      "Adding trajectory to replay buffer: step 3227, counter 51173\n",
      "Environment 1: Episode 811, Score -103.05377182784893, Avg_Score -102.5229911335814\n",
      "Adding trajectory to replay buffer: step 3228, counter 51220\n",
      "Environment 12: Episode 812, Score -103.21981505964882, Avg_Score -102.53070083579185\n",
      "Adding trajectory to replay buffer: step 3232, counter 51265\n",
      "Environment 6: Episode 813, Score -102.86069986485069, Avg_Score -102.52981276957364\n",
      "Adding trajectory to replay buffer: step 3233, counter 51323\n",
      "Environment 14: Episode 814, Score -101.74698777318436, Avg_Score -102.50926599635302\n",
      "Adding trajectory to replay buffer: step 3234, counter 51375\n",
      "Environment 0: Episode 815, Score -101.92613583262761, Avg_Score -102.48571836356163\n",
      "Adding trajectory to replay buffer: step 3235, counter 51422\n",
      "Environment 15: Episode 816, Score -103.4820145963033, Avg_Score -102.4805476386406\n",
      "Adding trajectory to replay buffer: step 3239, counter 51469\n",
      "Environment 5: Episode 817, Score -102.99580102554071, Avg_Score -102.47668482446805\n",
      "Adding trajectory to replay buffer: step 3243, counter 51519\n",
      "Environment 2: Episode 818, Score -102.10754079640967, Avg_Score -102.47210169016121\n",
      "Adding trajectory to replay buffer: step 3243, counter 51561\n",
      "Environment 3: Episode 819, Score -103.69824674575403, Avg_Score -102.49581961155772\n",
      "Adding trajectory to replay buffer: step 3248, counter 51609\n",
      "Environment 8: Episode 820, Score -103.51734344203335, Avg_Score -102.51154609901221\n",
      "Adding trajectory to replay buffer: step 3250, counter 51660\n",
      "Environment 4: Episode 821, Score -102.35569779787834, Avg_Score -102.51867755427008\n",
      "Adding trajectory to replay buffer: step 3252, counter 51719\n",
      "Environment 7: Episode 822, Score -101.18179499583816, Avg_Score -102.5078989190922\n",
      "Adding trajectory to replay buffer: step 3260, counter 51775\n",
      "Environment 13: Episode 823, Score -100.92572635710737, Avg_Score -102.48966488691974\n",
      "Adding trajectory to replay buffer: step 3268, counter 51819\n",
      "Environment 9: Episode 824, Score -103.61228142350477, Avg_Score -102.49909982341185\n",
      "Adding trajectory to replay buffer: step 3269, counter 51870\n",
      "Environment 11: Episode 825, Score -102.1427973573211, Avg_Score -102.488531932392\n",
      "Adding trajectory to replay buffer: step 3280, counter 51923\n",
      "Environment 1: Episode 826, Score -101.1757090954762, Avg_Score -102.46567368289308\n",
      "Adding trajectory to replay buffer: step 3281, counter 51976\n",
      "Environment 12: Episode 827, Score -102.40351933624657, Avg_Score -102.47644003396447\n",
      "Adding trajectory to replay buffer: step 3282, counter 52025\n",
      "Environment 14: Episode 828, Score -102.81504598040382, Avg_Score -102.4741979643316\n",
      "Adding trajectory to replay buffer: step 3283, counter 52073\n",
      "Environment 15: Episode 829, Score -103.45542389664737, Avg_Score -102.47851652198005\n",
      "Adding trajectory to replay buffer: step 3286, counter 52125\n",
      "Environment 0: Episode 830, Score -102.0241988559086, Avg_Score -102.46777773615459\n",
      "Adding trajectory to replay buffer: step 3287, counter 52173\n",
      "Environment 5: Episode 831, Score -103.88510218901622, Avg_Score -102.49259618792284\n",
      "Adding trajectory to replay buffer: step 3288, counter 52218\n",
      "Environment 3: Episode 832, Score -102.79186452464945, Avg_Score -102.49250660953412\n",
      "Adding trajectory to replay buffer: step 3290, counter 52265\n",
      "Environment 2: Episode 833, Score -102.15613895478596, Avg_Score -102.47984884106134\n",
      "Adding trajectory to replay buffer: step 3293, counter 52326\n",
      "Environment 6: Episode 834, Score -103.27616508084473, Avg_Score -102.4977797258125\n",
      "Adding trajectory to replay buffer: step 3295, counter 52371\n",
      "Environment 4: Episode 835, Score -102.97259855365505, Avg_Score -102.50446372975505\n",
      "Adding trajectory to replay buffer: step 3302, counter 52425\n",
      "Environment 8: Episode 836, Score -103.71081110325021, Avg_Score -102.53237131732261\n",
      "Adding trajectory to replay buffer: step 3303, counter 52476\n",
      "Environment 7: Episode 837, Score -102.61522432472184, Avg_Score -102.53327364498305\n",
      "Adding trajectory to replay buffer: step 3309, counter 52525\n",
      "Environment 13: Episode 838, Score -101.90220609895326, Avg_Score -102.53256986114049\n",
      "Adding trajectory to replay buffer: step 3317, counter 52574\n",
      "Environment 9: Episode 839, Score -102.65744604993488, Avg_Score -102.5361193041978\n",
      "Adding trajectory to replay buffer: step 3319, counter 52624\n",
      "Environment 11: Episode 840, Score -103.48482516144588, Avg_Score -102.55109444073727\n",
      "Adding trajectory to replay buffer: step 3324, counter 52666\n",
      "Environment 14: Episode 841, Score -104.23128488724741, Avg_Score -102.5773641111808\n",
      "Adding trajectory to replay buffer: step 3329, counter 52714\n",
      "Environment 12: Episode 842, Score -102.62760356985281, Avg_Score -102.57922429756371\n",
      "Adding trajectory to replay buffer: step 3333, counter 52767\n",
      "Environment 1: Episode 843, Score -103.68982048843925, Avg_Score -102.5982990517007\n",
      "Adding trajectory to replay buffer: step 3335, counter 52816\n",
      "Environment 0: Episode 844, Score -103.21316084509156, Avg_Score -102.5927546056767\n",
      "Adding trajectory to replay buffer: step 3336, counter 52869\n",
      "Environment 15: Episode 845, Score -101.66784255014421, Avg_Score -102.56013080995722\n",
      "Adding trajectory to replay buffer: step 3340, counter 52922\n",
      "Environment 5: Episode 846, Score -101.81026534395292, Avg_Score -102.55522199418556\n",
      "Adding trajectory to replay buffer: step 3341, counter 53054\n",
      "Environment 10: Episode 847, Score -130.49394066327378, Avg_Score -102.83262718259233\n",
      "Adding trajectory to replay buffer: step 3342, counter 53108\n",
      "Environment 3: Episode 848, Score -102.74637619681718, Avg_Score -102.83501385604\n",
      "Adding trajectory to replay buffer: step 3342, counter 53155\n",
      "Environment 4: Episode 849, Score -103.07014426688664, Avg_Score -102.84110255089963\n",
      "Adding trajectory to replay buffer: step 3343, counter 53205\n",
      "Environment 6: Episode 850, Score -102.42160357955844, Avg_Score -102.8432483992738\n",
      "Adding trajectory to replay buffer: step 3352, counter 53254\n",
      "Environment 7: Episode 851, Score -103.96506771165008, Avg_Score -102.86055662874995\n",
      "Adding trajectory to replay buffer: step 3354, counter 53318\n",
      "Environment 2: Episode 852, Score -100.94948223882044, Avg_Score -102.85166060366163\n",
      "Adding trajectory to replay buffer: step 3357, counter 53366\n",
      "Environment 13: Episode 853, Score -103.96174551997831, Avg_Score -102.87305010758097\n",
      "Adding trajectory to replay buffer: step 3358, counter 53422\n",
      "Environment 8: Episode 854, Score -102.37853006377816, Avg_Score -102.87463842063451\n",
      "Adding trajectory to replay buffer: step 3371, counter 53476\n",
      "Environment 9: Episode 855, Score -101.514224346634, Avg_Score -102.87569261745831\n",
      "Adding trajectory to replay buffer: step 3371, counter 53528\n",
      "Environment 11: Episode 856, Score -103.66874027202651, Avg_Score -102.89976395658029\n",
      "Adding trajectory to replay buffer: step 3375, counter 53574\n",
      "Environment 12: Episode 857, Score -104.09439810728779, Avg_Score -102.91116944432703\n",
      "Adding trajectory to replay buffer: step 3377, counter 53627\n",
      "Environment 14: Episode 858, Score -101.90760282654253, Avg_Score -102.91432085389165\n",
      "Adding trajectory to replay buffer: step 3382, counter 53676\n",
      "Environment 1: Episode 859, Score -103.57563342988118, Avg_Score -102.92919024845975\n",
      "Adding trajectory to replay buffer: step 3387, counter 53727\n",
      "Environment 15: Episode 860, Score -101.88955072741925, Avg_Score -102.91519498100342\n",
      "Adding trajectory to replay buffer: step 3390, counter 53782\n",
      "Environment 0: Episode 861, Score -101.32157078899832, Avg_Score -102.91791142210755\n",
      "Adding trajectory to replay buffer: step 3390, counter 53832\n",
      "Environment 5: Episode 862, Score -103.02025752959587, Avg_Score -102.91077736598008\n",
      "Adding trajectory to replay buffer: step 3390, counter 53881\n",
      "Environment 10: Episode 863, Score -102.92435248922308, Avg_Score -102.9225796849387\n",
      "Adding trajectory to replay buffer: step 3392, counter 53931\n",
      "Environment 4: Episode 864, Score -102.44077957338281, Avg_Score -102.91621678690997\n",
      "Adding trajectory to replay buffer: step 3392, counter 53980\n",
      "Environment 6: Episode 865, Score -102.97164386732193, Avg_Score -102.91180918827547\n",
      "Adding trajectory to replay buffer: step 3400, counter 54038\n",
      "Environment 3: Episode 866, Score -105.09951880867034, Avg_Score -102.9498203386536\n",
      "Adding trajectory to replay buffer: step 3405, counter 54085\n",
      "Environment 8: Episode 867, Score -103.33335542335051, Avg_Score -102.9568442793243\n",
      "Adding trajectory to replay buffer: step 3407, counter 54140\n",
      "Environment 7: Episode 868, Score -101.73368184748323, Avg_Score -102.94993151655528\n",
      "Adding trajectory to replay buffer: step 3407, counter 54190\n",
      "Environment 13: Episode 869, Score -102.48538376669337, Avg_Score -102.93941746085265\n",
      "Adding trajectory to replay buffer: step 3421, counter 54240\n",
      "Environment 11: Episode 870, Score -105.04326103345429, Avg_Score -102.98153600200767\n",
      "Adding trajectory to replay buffer: step 3430, counter 54288\n",
      "Environment 1: Episode 871, Score -101.9048542390174, Avg_Score -102.98305606222056\n",
      "Adding trajectory to replay buffer: step 3431, counter 54348\n",
      "Environment 9: Episode 872, Score -102.10966198331056, Avg_Score -102.97379295633651\n",
      "Adding trajectory to replay buffer: step 3437, counter 54395\n",
      "Environment 0: Episode 873, Score -105.00470302220185, Avg_Score -102.99246736757532\n",
      "Adding trajectory to replay buffer: step 3438, counter 54446\n",
      "Environment 15: Episode 874, Score -103.4634259568043, Avg_Score -103.01776380457464\n",
      "Adding trajectory to replay buffer: step 3439, counter 54495\n",
      "Environment 10: Episode 875, Score -102.64205664706293, Avg_Score -103.02946961409926\n",
      "Adding trajectory to replay buffer: step 3441, counter 54544\n",
      "Environment 4: Episode 876, Score -103.3558278101678, Avg_Score -103.04871489673631\n",
      "Adding trajectory to replay buffer: step 3441, counter 54595\n",
      "Environment 5: Episode 877, Score -102.14094425000499, Avg_Score -103.03441897602274\n",
      "Adding trajectory to replay buffer: step 3443, counter 54661\n",
      "Environment 14: Episode 878, Score -101.93954979305032, Avg_Score -103.03336199113991\n",
      "Adding trajectory to replay buffer: step 3444, counter 54713\n",
      "Environment 6: Episode 879, Score -102.05199538628385, Avg_Score -103.03849334659493\n",
      "Adding trajectory to replay buffer: step 3445, counter 54783\n",
      "Environment 12: Episode 880, Score -100.67754928235213, Avg_Score -103.01225612886383\n",
      "Adding trajectory to replay buffer: step 3456, counter 54839\n",
      "Environment 3: Episode 881, Score -106.9766491236637, Avg_Score -103.05222472719234\n",
      "Adding trajectory to replay buffer: step 3459, counter 54891\n",
      "Environment 7: Episode 882, Score -101.2860639827551, Avg_Score -103.05185857540279\n",
      "Adding trajectory to replay buffer: step 3460, counter 54944\n",
      "Environment 13: Episode 883, Score -103.21899570183704, Avg_Score -103.0582275391845\n",
      "Adding trajectory to replay buffer: step 3464, counter 55054\n",
      "Environment 2: Episode 884, Score -132.7023731330354, Avg_Score -103.35693956782265\n",
      "Adding trajectory to replay buffer: step 3465, counter 55114\n",
      "Environment 8: Episode 885, Score -100.57562472944521, Avg_Score -103.33484842469325\n",
      "Adding trajectory to replay buffer: step 3472, counter 55165\n",
      "Environment 11: Episode 886, Score -102.33183515844432, Avg_Score -103.32035026928745\n",
      "Adding trajectory to replay buffer: step 3483, counter 55218\n",
      "Environment 1: Episode 887, Score -101.91776807949071, Avg_Score -103.31350283395015\n",
      "Adding trajectory to replay buffer: step 3483, counter 55270\n",
      "Environment 9: Episode 888, Score -102.065008427497, Avg_Score -103.29640440472659\n",
      "Adding trajectory to replay buffer: step 3486, counter 55319\n",
      "Environment 0: Episode 889, Score -102.90845035954503, Avg_Score -103.29242445009051\n",
      "Adding trajectory to replay buffer: step 3488, counter 55369\n",
      "Environment 15: Episode 890, Score -103.19085687174461, Avg_Score -103.29790758545559\n",
      "Adding trajectory to replay buffer: step 3490, counter 55418\n",
      "Environment 5: Episode 891, Score -102.96547778795784, Avg_Score -103.29748077571212\n",
      "Adding trajectory to replay buffer: step 3491, counter 55468\n",
      "Environment 4: Episode 892, Score -102.6255742003601, Avg_Score -103.29971915925358\n",
      "Adding trajectory to replay buffer: step 3491, counter 55520\n",
      "Environment 10: Episode 893, Score -101.82944146631968, Avg_Score -103.30310060652171\n",
      "Adding trajectory to replay buffer: step 3493, counter 55570\n",
      "Environment 14: Episode 894, Score -102.57908465532958, Avg_Score -103.30031390740834\n",
      "Adding trajectory to replay buffer: step 3495, counter 55620\n",
      "Environment 12: Episode 895, Score -103.27784015036747, Avg_Score -103.31175738924016\n",
      "Adding trajectory to replay buffer: step 3502, counter 55678\n",
      "Environment 6: Episode 896, Score -100.7391617736388, Avg_Score -103.27924048268862\n",
      "Adding trajectory to replay buffer: step 3506, counter 55725\n",
      "Environment 7: Episode 897, Score -103.46691722253897, Avg_Score -103.29803299093457\n",
      "Adding trajectory to replay buffer: step 3507, counter 55776\n",
      "Environment 3: Episode 898, Score -102.85222481907532, Avg_Score -103.29719288146266\n",
      "Adding trajectory to replay buffer: step 3515, counter 55826\n",
      "Environment 8: Episode 899, Score -102.26280051527188, Avg_Score -103.30207891505138\n",
      "Adding trajectory to replay buffer: step 3517, counter 55883\n",
      "Environment 13: Episode 900, Score -105.28755483784713, Avg_Score -103.32973528743429\n",
      "Adding trajectory to replay buffer: step 3521, counter 55940\n",
      "Environment 2: Episode 901, Score -101.50024187770548, Avg_Score -103.30291454358712\n",
      "Adding trajectory to replay buffer: step 3526, counter 55994\n",
      "Environment 11: Episode 902, Score -101.93315084731702, Avg_Score -103.29669483342727\n",
      "Adding trajectory to replay buffer: step 3532, counter 56043\n",
      "Environment 1: Episode 903, Score -102.81845114871922, Avg_Score -103.28870863797134\n",
      "Adding trajectory to replay buffer: step 3536, counter 56096\n",
      "Environment 9: Episode 904, Score -102.77052430261361, Avg_Score -103.28144878092527\n",
      "Adding trajectory to replay buffer: step 3536, counter 56144\n",
      "Environment 15: Episode 905, Score -103.63898007228102, Avg_Score -103.29120190622396\n",
      "Adding trajectory to replay buffer: step 3537, counter 56186\n",
      "Environment 12: Episode 906, Score -104.40667273794922, Avg_Score -103.3142614659077\n",
      "Adding trajectory to replay buffer: step 3539, counter 56235\n",
      "Environment 5: Episode 907, Score -102.38582766708421, Avg_Score -103.31834590479892\n",
      "Adding trajectory to replay buffer: step 3541, counter 56285\n",
      "Environment 10: Episode 908, Score -102.23121435220546, Avg_Score -103.32787570839479\n",
      "Adding trajectory to replay buffer: step 3543, counter 56337\n",
      "Environment 4: Episode 909, Score -102.89723960625194, Avg_Score -103.33326131337262\n",
      "Adding trajectory to replay buffer: step 3544, counter 56395\n",
      "Environment 0: Episode 910, Score -104.61138293209423, Avg_Score -103.33947713775892\n",
      "Adding trajectory to replay buffer: step 3546, counter 56448\n",
      "Environment 14: Episode 911, Score -101.50733158800503, Avg_Score -103.32401273536047\n",
      "Adding trajectory to replay buffer: step 3548, counter 56489\n",
      "Environment 3: Episode 912, Score -103.48836615669231, Avg_Score -103.3266982463309\n",
      "Adding trajectory to replay buffer: step 3558, counter 56545\n",
      "Environment 6: Episode 913, Score -103.23743236365232, Avg_Score -103.33046557131892\n",
      "Adding trajectory to replay buffer: step 3563, counter 56602\n",
      "Environment 7: Episode 914, Score -100.6501423300039, Avg_Score -103.3194971168871\n",
      "Adding trajectory to replay buffer: step 3565, counter 56652\n",
      "Environment 8: Episode 915, Score -102.04264568165193, Avg_Score -103.32066221537734\n",
      "Adding trajectory to replay buffer: step 3574, counter 56705\n",
      "Environment 2: Episode 916, Score -103.95237617981496, Avg_Score -103.32536583121245\n",
      "Adding trajectory to replay buffer: step 3574, counter 56762\n",
      "Environment 13: Episode 917, Score -105.19406800148698, Avg_Score -103.34734850097192\n",
      "Adding trajectory to replay buffer: step 3576, counter 56812\n",
      "Environment 11: Episode 918, Score -102.58607777396278, Avg_Score -103.35213387074745\n",
      "Adding trajectory to replay buffer: step 3582, counter 56862\n",
      "Environment 1: Episode 919, Score -103.71356419407638, Avg_Score -103.3522870452307\n",
      "Adding trajectory to replay buffer: step 3583, counter 56909\n",
      "Environment 15: Episode 920, Score -104.16733364856243, Avg_Score -103.358786947296\n",
      "Adding trajectory to replay buffer: step 3586, counter 56956\n",
      "Environment 5: Episode 921, Score -103.13084320354027, Avg_Score -103.36653840135259\n",
      "Adding trajectory to replay buffer: step 3586, counter 57006\n",
      "Environment 9: Episode 922, Score -101.58585031931537, Avg_Score -103.37057895458739\n",
      "Adding trajectory to replay buffer: step 3592, counter 57061\n",
      "Environment 12: Episode 923, Score -103.48008202795312, Avg_Score -103.39612251129581\n",
      "Adding trajectory to replay buffer: step 3598, counter 57115\n",
      "Environment 0: Episode 924, Score -103.17576208321564, Avg_Score -103.39175731789294\n",
      "Adding trajectory to replay buffer: step 3600, counter 57167\n",
      "Environment 3: Episode 925, Score -101.5962386809362, Avg_Score -103.38629173112909\n",
      "Adding trajectory to replay buffer: step 3602, counter 57226\n",
      "Environment 4: Episode 926, Score -101.93748704375795, Avg_Score -103.39390951061192\n",
      "Adding trajectory to replay buffer: step 3607, counter 57287\n",
      "Environment 14: Episode 927, Score -100.83429006278763, Avg_Score -103.37821721787734\n",
      "Adding trajectory to replay buffer: step 3609, counter 57338\n",
      "Environment 6: Episode 928, Score -102.52901293648532, Avg_Score -103.37535688743816\n",
      "Adding trajectory to replay buffer: step 3613, counter 57388\n",
      "Environment 7: Episode 929, Score -102.95296840007045, Avg_Score -103.37033233247239\n",
      "Adding trajectory to replay buffer: step 3616, counter 57439\n",
      "Environment 8: Episode 930, Score -102.20132745468368, Avg_Score -103.37210361846013\n",
      "Adding trajectory to replay buffer: step 3617, counter 57515\n",
      "Environment 10: Episode 931, Score -102.81435721357292, Avg_Score -103.36139616870568\n",
      "Adding trajectory to replay buffer: step 3621, counter 57562\n",
      "Environment 2: Episode 932, Score -102.16446446954893, Avg_Score -103.35512216815468\n",
      "Adding trajectory to replay buffer: step 3625, counter 57611\n",
      "Environment 11: Episode 933, Score -102.0212384434181, Avg_Score -103.35377316304101\n",
      "Adding trajectory to replay buffer: step 3627, counter 57664\n",
      "Environment 13: Episode 934, Score -101.52535658185246, Avg_Score -103.33626507805107\n",
      "Adding trajectory to replay buffer: step 3634, counter 57715\n",
      "Environment 15: Episode 935, Score -103.06715105991934, Avg_Score -103.33721060311373\n",
      "Adding trajectory to replay buffer: step 3640, counter 57773\n",
      "Environment 1: Episode 936, Score -101.03753464804403, Avg_Score -103.31047783856164\n",
      "Adding trajectory to replay buffer: step 3640, counter 57827\n",
      "Environment 5: Episode 937, Score -102.41885081384362, Avg_Score -103.30851410345285\n",
      "Adding trajectory to replay buffer: step 3640, counter 57881\n",
      "Environment 9: Episode 938, Score -102.89341732826642, Avg_Score -103.31842621574599\n",
      "Adding trajectory to replay buffer: step 3640, counter 57929\n",
      "Environment 12: Episode 939, Score -102.47814486619396, Avg_Score -103.31663320390857\n",
      "Adding trajectory to replay buffer: step 3648, counter 57979\n",
      "Environment 0: Episode 940, Score -102.6775978473549, Avg_Score -103.30856093076765\n",
      "Adding trajectory to replay buffer: step 3651, counter 58030\n",
      "Environment 3: Episode 941, Score -103.15321618370339, Avg_Score -103.29778024373223\n",
      "Adding trajectory to replay buffer: step 3654, counter 58082\n",
      "Environment 4: Episode 942, Score -101.83211793134362, Avg_Score -103.28982538734715\n",
      "Adding trajectory to replay buffer: step 3656, counter 58131\n",
      "Environment 14: Episode 943, Score -102.63902144332292, Avg_Score -103.279317396896\n",
      "Adding trajectory to replay buffer: step 3664, counter 58186\n",
      "Environment 6: Episode 944, Score -101.50272499944643, Avg_Score -103.26221303843955\n",
      "Adding trajectory to replay buffer: step 3666, counter 58239\n",
      "Environment 7: Episode 945, Score -103.39795194216755, Avg_Score -103.2795141323598\n",
      "Adding trajectory to replay buffer: step 3666, counter 58289\n",
      "Environment 8: Episode 946, Score -102.7837060098617, Avg_Score -103.28924853901889\n",
      "Adding trajectory to replay buffer: step 3666, counter 58338\n",
      "Environment 10: Episode 947, Score -102.31393291611225, Avg_Score -103.00744846154728\n",
      "Adding trajectory to replay buffer: step 3670, counter 58387\n",
      "Environment 2: Episode 948, Score -102.96625033810734, Avg_Score -103.00964720296018\n",
      "Adding trajectory to replay buffer: step 3677, counter 58437\n",
      "Environment 13: Episode 949, Score -103.11580971998225, Avg_Score -103.01010385749116\n",
      "Adding trajectory to replay buffer: step 3683, counter 58486\n",
      "Environment 15: Episode 950, Score -102.72144857650747, Avg_Score -103.01310230746063\n",
      "Adding trajectory to replay buffer: step 3689, counter 58535\n",
      "Environment 1: Episode 951, Score -102.35831082022935, Avg_Score -102.99703473854643\n",
      "Adding trajectory to replay buffer: step 3689, counter 58584\n",
      "Environment 12: Episode 952, Score -103.88968020444007, Avg_Score -103.02643671820262\n",
      "Adding trajectory to replay buffer: step 3692, counter 58636\n",
      "Environment 9: Episode 953, Score -103.2907665446295, Avg_Score -103.01972692844913\n",
      "Adding trajectory to replay buffer: step 3693, counter 58689\n",
      "Environment 5: Episode 954, Score -101.61914776238613, Avg_Score -103.01213310543521\n",
      "Adding trajectory to replay buffer: step 3697, counter 58735\n",
      "Environment 3: Episode 955, Score -102.27368852397241, Avg_Score -103.01972774720858\n",
      "Adding trajectory to replay buffer: step 3705, counter 58792\n",
      "Environment 0: Episode 956, Score -102.24973231001695, Avg_Score -103.0055376675885\n",
      "Adding trajectory to replay buffer: step 3709, counter 58847\n",
      "Environment 4: Episode 957, Score -101.26967120204556, Avg_Score -102.97729039853608\n",
      "Adding trajectory to replay buffer: step 3712, counter 58934\n",
      "Environment 11: Episode 958, Score -104.35882028202526, Avg_Score -103.00180257309091\n",
      "Adding trajectory to replay buffer: step 3716, counter 58986\n",
      "Environment 6: Episode 959, Score -102.33074650932103, Avg_Score -102.9893537038853\n",
      "Adding trajectory to replay buffer: step 3718, counter 59048\n",
      "Environment 14: Episode 960, Score -100.9361595891534, Avg_Score -102.97981979250265\n",
      "Adding trajectory to replay buffer: step 3721, counter 59103\n",
      "Environment 7: Episode 961, Score -106.93857139958317, Avg_Score -103.03598979860848\n",
      "Adding trajectory to replay buffer: step 3728, counter 59165\n",
      "Environment 10: Episode 962, Score -102.69662973001412, Avg_Score -103.03275352061266\n",
      "Adding trajectory to replay buffer: step 3732, counter 59220\n",
      "Environment 13: Episode 963, Score -100.9648568809051, Avg_Score -103.01315856452948\n",
      "Adding trajectory to replay buffer: step 3738, counter 59292\n",
      "Environment 8: Episode 964, Score -105.06834538826223, Avg_Score -103.03943422267825\n",
      "Adding trajectory to replay buffer: step 3740, counter 59349\n",
      "Environment 15: Episode 965, Score -101.48702520579845, Avg_Score -103.02458803606304\n",
      "Adding trajectory to replay buffer: step 3742, counter 59421\n",
      "Environment 2: Episode 966, Score -103.47074091807318, Avg_Score -103.00830025715706\n",
      "Adding trajectory to replay buffer: step 3742, counter 59470\n",
      "Environment 5: Episode 967, Score -102.96957239505835, Avg_Score -103.00466242687415\n",
      "Adding trajectory to replay buffer: step 3743, counter 59521\n",
      "Environment 9: Episode 968, Score -102.96027332088165, Avg_Score -103.0169283416081\n",
      "Adding trajectory to replay buffer: step 3743, counter 59575\n",
      "Environment 12: Episode 969, Score -101.61966090067166, Avg_Score -103.00827111294788\n",
      "Adding trajectory to replay buffer: step 3745, counter 59631\n",
      "Environment 1: Episode 970, Score -102.05832030077744, Avg_Score -102.97842170562114\n",
      "Adding trajectory to replay buffer: step 3748, counter 59682\n",
      "Environment 3: Episode 971, Score -102.24584470851285, Avg_Score -102.98183161031609\n",
      "Adding trajectory to replay buffer: step 3755, counter 59732\n",
      "Environment 0: Episode 972, Score -102.59605345111216, Avg_Score -102.9866955249941\n",
      "Adding trajectory to replay buffer: step 3755, counter 59778\n",
      "Environment 4: Episode 973, Score -103.9936469096591, Avg_Score -102.97658496386867\n",
      "Adding trajectory to replay buffer: step 3761, counter 59827\n",
      "Environment 11: Episode 974, Score -103.21578918314229, Avg_Score -102.97410859613207\n",
      "Adding trajectory to replay buffer: step 3769, counter 59878\n",
      "Environment 14: Episode 975, Score -102.80629326335962, Avg_Score -102.97575096229501\n",
      "Adding trajectory to replay buffer: step 3780, counter 59926\n",
      "Environment 13: Episode 976, Score -102.63850813618427, Avg_Score -102.96857776555518\n",
      "Adding trajectory to replay buffer: step 3781, counter 59986\n",
      "Environment 7: Episode 977, Score -102.38276792424979, Avg_Score -102.97099600229764\n",
      "Adding trajectory to replay buffer: step 3782, counter 60040\n",
      "Environment 10: Episode 978, Score -102.02805255795829, Avg_Score -102.97188102994673\n",
      "Adding trajectory to replay buffer: step 3783, counter 60083\n",
      "Environment 15: Episode 979, Score -104.54518389663596, Avg_Score -102.99681291505026\n",
      "Adding trajectory to replay buffer: step 3785, counter 60152\n",
      "Environment 6: Episode 980, Score -102.62305434677377, Avg_Score -103.01626796569448\n",
      "Adding trajectory to replay buffer: step 3790, counter 60204\n",
      "Environment 8: Episode 981, Score -103.10016041638703, Avg_Score -102.97750307862168\n",
      "Adding trajectory to replay buffer: step 3791, counter 60250\n",
      "Environment 1: Episode 982, Score -103.78602771403082, Avg_Score -103.00250271593445\n",
      "Adding trajectory to replay buffer: step 3791, counter 60298\n",
      "Environment 9: Episode 983, Score -101.92975514690205, Avg_Score -102.98961031038512\n",
      "Adding trajectory to replay buffer: step 3798, counter 60354\n",
      "Environment 2: Episode 984, Score -106.39942708245354, Avg_Score -102.7265808498793\n",
      "Adding trajectory to replay buffer: step 3803, counter 60415\n",
      "Environment 5: Episode 985, Score -102.26583329479148, Avg_Score -102.74348293553277\n",
      "Adding trajectory to replay buffer: step 3806, counter 60473\n",
      "Environment 3: Episode 986, Score -101.11407181557392, Avg_Score -102.73130530210405\n",
      "Adding trajectory to replay buffer: step 3808, counter 60526\n",
      "Environment 4: Episode 987, Score -106.12841275431403, Avg_Score -102.77341174885228\n",
      "Adding trajectory to replay buffer: step 3812, counter 60583\n",
      "Environment 0: Episode 988, Score -105.01533678239832, Avg_Score -102.8029150324013\n",
      "Adding trajectory to replay buffer: step 3814, counter 60636\n",
      "Environment 11: Episode 989, Score -101.7624212697198, Avg_Score -102.79145474150305\n",
      "Adding trajectory to replay buffer: step 3815, counter 60682\n",
      "Environment 14: Episode 990, Score -103.97086620978142, Avg_Score -102.79925483488341\n",
      "Adding trajectory to replay buffer: step 3821, counter 60760\n",
      "Environment 12: Episode 991, Score -102.34935990568437, Avg_Score -102.79309365606068\n",
      "Adding trajectory to replay buffer: step 3824, counter 60803\n",
      "Environment 7: Episode 992, Score -104.51352932514375, Avg_Score -102.8119732073085\n",
      "Adding trajectory to replay buffer: step 3834, counter 60852\n",
      "Environment 6: Episode 993, Score -102.67247610684981, Avg_Score -102.8204035537138\n",
      "Adding trajectory to replay buffer: step 3835, counter 60905\n",
      "Environment 10: Episode 994, Score -101.19165403115564, Avg_Score -102.80652924747206\n",
      "Adding trajectory to replay buffer: step 3835, counter 60960\n",
      "Environment 13: Episode 995, Score -104.02463754418493, Avg_Score -102.81399722141025\n",
      "Adding trajectory to replay buffer: step 3835, counter 61012\n",
      "Environment 15: Episode 996, Score -104.71025520127142, Avg_Score -102.85370815568656\n",
      "Adding trajectory to replay buffer: step 3844, counter 61065\n",
      "Environment 9: Episode 997, Score -101.98287354666616, Avg_Score -102.83886771892783\n",
      "Adding trajectory to replay buffer: step 3850, counter 61117\n",
      "Environment 2: Episode 998, Score -101.79565578055382, Avg_Score -102.82830202854261\n",
      "Adding trajectory to replay buffer: step 3852, counter 61178\n",
      "Environment 1: Episode 999, Score -101.83286746604182, Avg_Score -102.8240026980503\n",
      "Adding trajectory to replay buffer: step 3859, counter 61231\n",
      "Environment 3: Episode 1000, Score -102.73388874273002, Avg_Score -102.79846603709913\n",
      "Adding trajectory to replay buffer: step 3861, counter 61289\n",
      "Environment 5: Episode 1001, Score -102.05899892477629, Avg_Score -102.80405360756984\n",
      "Adding trajectory to replay buffer: step 3862, counter 61361\n",
      "Environment 8: Episode 1002, Score -105.5151194466607, Avg_Score -102.83987329356327\n",
      "Adding trajectory to replay buffer: step 3863, counter 61410\n",
      "Environment 11: Episode 1003, Score -102.44704548684074, Avg_Score -102.83615923694445\n",
      "Adding trajectory to replay buffer: step 3864, counter 61459\n",
      "Environment 14: Episode 1004, Score -102.79472410831701, Avg_Score -102.8364012350015\n",
      "Adding trajectory to replay buffer: step 3865, counter 61512\n",
      "Environment 0: Episode 1005, Score -102.57469258590167, Avg_Score -102.8257583601377\n",
      "Adding trajectory to replay buffer: step 3866, counter 61557\n",
      "Environment 12: Episode 1006, Score -103.25240566617995, Avg_Score -102.81421568942\n",
      "Adding trajectory to replay buffer: step 3893, counter 61615\n",
      "Environment 15: Episode 1007, Score -111.97138537460317, Avg_Score -102.9100712664952\n",
      "Adding trajectory to replay buffer: step 3898, counter 61679\n",
      "Environment 6: Episode 1008, Score -115.28821459874261, Avg_Score -103.04064126896056\n",
      "Adding trajectory to replay buffer: step 3902, counter 61731\n",
      "Environment 2: Episode 1009, Score -121.87661416934742, Avg_Score -103.2304350145915\n",
      "Adding trajectory to replay buffer: step 3906, counter 61829\n",
      "Environment 4: Episode 1010, Score -117.54356791560042, Avg_Score -103.35975686442656\n",
      "Adding trajectory to replay buffer: step 3906, counter 61911\n",
      "Environment 7: Episode 1011, Score -116.81387936155375, Avg_Score -103.51282234216207\n",
      "Adding trajectory to replay buffer: step 3906, counter 61982\n",
      "Environment 13: Episode 1012, Score -115.36834511279625, Avg_Score -103.6316221317231\n",
      "Adding trajectory to replay buffer: step 3907, counter 62054\n",
      "Environment 10: Episode 1013, Score -115.9967813194065, Avg_Score -103.75921562128067\n",
      "Adding trajectory to replay buffer: step 3911, counter 62113\n",
      "Environment 1: Episode 1014, Score -118.99335940317437, Avg_Score -103.94264779201235\n",
      "Adding trajectory to replay buffer: step 3912, counter 62164\n",
      "Environment 5: Episode 1015, Score -125.10154048505302, Avg_Score -104.17323674004638\n",
      "Adding trajectory to replay buffer: step 3914, counter 62234\n",
      "Environment 9: Episode 1016, Score -110.36641391751418, Avg_Score -104.23737711742335\n",
      "Adding trajectory to replay buffer: step 3925, counter 62293\n",
      "Environment 12: Episode 1017, Score -119.66920905999467, Avg_Score -104.38212852800845\n",
      "Adding trajectory to replay buffer: step 3927, counter 62356\n",
      "Environment 14: Episode 1018, Score -118.91347100681067, Avg_Score -104.54540246033693\n",
      "Adding trajectory to replay buffer: step 3942, counter 62405\n",
      "Environment 15: Episode 1019, Score -126.04018140361707, Avg_Score -104.76866863243232\n",
      "Adding trajectory to replay buffer: step 3946, counter 62453\n",
      "Environment 6: Episode 1020, Score -124.81643175370432, Avg_Score -104.97515961348375\n",
      "Adding trajectory to replay buffer: step 3949, counter 62543\n",
      "Environment 3: Episode 1021, Score -112.93260095160952, Avg_Score -105.07317719096443\n",
      "Adding trajectory to replay buffer: step 3953, counter 62594\n",
      "Environment 2: Episode 1022, Score -124.0650786945317, Avg_Score -105.29796947471661\n",
      "Adding trajectory to replay buffer: step 3956, counter 62644\n",
      "Environment 4: Episode 1023, Score -128.09653949868493, Avg_Score -105.54413404942393\n",
      "Adding trajectory to replay buffer: step 3957, counter 62695\n",
      "Environment 7: Episode 1024, Score -126.39650068197088, Avg_Score -105.77634143541151\n",
      "Adding trajectory to replay buffer: step 3958, counter 62746\n",
      "Environment 10: Episode 1025, Score -127.05537213869331, Avg_Score -106.03093276998905\n",
      "Adding trajectory to replay buffer: step 3958, counter 62798\n",
      "Environment 13: Episode 1026, Score -127.15191084351888, Avg_Score -106.28307700798666\n",
      "Adding trajectory to replay buffer: step 3960, counter 62846\n",
      "Environment 5: Episode 1027, Score -124.28207862658303, Avg_Score -106.51755489362462\n",
      "Adding trajectory to replay buffer: step 3962, counter 62894\n",
      "Environment 9: Episode 1028, Score -126.34973381563834, Avg_Score -106.75576210241616\n",
      "Adding trajectory to replay buffer: step 3966, counter 62998\n",
      "Environment 8: Episode 1029, Score -117.53909840832588, Avg_Score -106.90162340249871\n",
      "Adding trajectory to replay buffer: step 3972, counter 63107\n",
      "Environment 11: Episode 1030, Score -120.81583900289547, Avg_Score -107.08776851798083\n",
      "Adding trajectory to replay buffer: step 3974, counter 63156\n",
      "Environment 12: Episode 1031, Score -127.04063353734713, Avg_Score -107.33003128121857\n",
      "Adding trajectory to replay buffer: step 3984, counter 63213\n",
      "Environment 14: Episode 1032, Score -119.29755075337489, Avg_Score -107.50136214405684\n",
      "Adding trajectory to replay buffer: step 3999, counter 63266\n",
      "Environment 6: Episode 1033, Score -120.32935414864434, Avg_Score -107.68444330110907\n",
      "Adding trajectory to replay buffer: step 4010, counter 63314\n",
      "Environment 9: Episode 1034, Score -127.48857640909466, Avg_Score -107.94407549938151\n",
      "Adding trajectory to replay buffer: step 4014, counter 63379\n",
      "Environment 3: Episode 1035, Score -118.95109617504664, Avg_Score -108.10291495053278\n",
      "Adding trajectory to replay buffer: step 4017, counter 63436\n",
      "Environment 5: Episode 1036, Score -119.01103711781465, Avg_Score -108.28264997523048\n",
      "Adding trajectory to replay buffer: step 4020, counter 63484\n",
      "Environment 11: Episode 1037, Score -127.62631436075458, Avg_Score -108.5347246106996\n",
      "Adding trajectory to replay buffer: step 4022, counter 63532\n",
      "Environment 12: Episode 1038, Score -125.3833123762291, Avg_Score -108.75962356117921\n",
      "Adding trajectory to replay buffer: step 4034, counter 63582\n",
      "Environment 14: Episode 1039, Score -124.4327971425044, Avg_Score -108.97917008394232\n",
      "Adding trajectory to replay buffer: step 4050, counter 63633\n",
      "Environment 6: Episode 1040, Score -127.38689962215582, Avg_Score -109.22626310169034\n",
      "Adding trajectory to replay buffer: step 4058, counter 63681\n",
      "Environment 9: Episode 1041, Score -124.62573723744212, Avg_Score -109.44098831222773\n",
      "Adding trajectory to replay buffer: step 4062, counter 63729\n",
      "Environment 3: Episode 1042, Score -126.11931416540624, Avg_Score -109.68386027456835\n",
      "Adding trajectory to replay buffer: step 4066, counter 63778\n",
      "Environment 5: Episode 1043, Score -126.53956872242628, Avg_Score -109.92286574735937\n",
      "Adding trajectory to replay buffer: step 4067, counter 63825\n",
      "Environment 11: Episode 1044, Score -124.7830410234183, Avg_Score -110.15566890759911\n",
      "Adding trajectory to replay buffer: step 4074, counter 63877\n",
      "Environment 12: Episode 1045, Score -126.9448015389486, Avg_Score -110.3911374035669\n",
      "Adding trajectory to replay buffer: step 4081, counter 63924\n",
      "Environment 14: Episode 1046, Score -127.12406603051537, Avg_Score -110.63454100377345\n",
      "Adding trajectory to replay buffer: step 4104, counter 63978\n",
      "Environment 6: Episode 1047, Score -122.24101737694753, Avg_Score -110.8338118483818\n",
      "Adding trajectory to replay buffer: step 4105, counter 64130\n",
      "Environment 2: Episode 1048, Score -118.84478567019416, Avg_Score -110.99259720170267\n",
      "Adding trajectory to replay buffer: step 4108, counter 64180\n",
      "Environment 9: Episode 1049, Score -128.43429227861327, Avg_Score -111.24578202728897\n",
      "Adding trajectory to replay buffer: step 4113, counter 64231\n",
      "Environment 3: Episode 1050, Score -120.28943800028787, Avg_Score -111.42146192152677\n",
      "Adding trajectory to replay buffer: step 4114, counter 64279\n",
      "Environment 5: Episode 1051, Score -126.00472918450211, Avg_Score -111.6579261051695\n",
      "Adding trajectory to replay buffer: step 4115, counter 64327\n",
      "Environment 11: Episode 1052, Score -126.66182889245698, Avg_Score -111.88564759204965\n",
      "Adding trajectory to replay buffer: step 4128, counter 64381\n",
      "Environment 12: Episode 1053, Score -122.03687755858029, Avg_Score -112.07310870218916\n",
      "Adding trajectory to replay buffer: step 4132, counter 64432\n",
      "Environment 14: Episode 1054, Score -125.91632340777045, Avg_Score -112.31608045864301\n",
      "Adding trajectory to replay buffer: step 4159, counter 64486\n",
      "Environment 2: Episode 1055, Score -119.44796819624554, Avg_Score -112.48782325536575\n",
      "Adding trajectory to replay buffer: step 4163, counter 64545\n",
      "Environment 6: Episode 1056, Score -119.52922197916173, Avg_Score -112.6606181520572\n",
      "Adding trajectory to replay buffer: step 4165, counter 64595\n",
      "Environment 11: Episode 1057, Score -124.361014154315, Avg_Score -112.8915315815799\n",
      "Adding trajectory to replay buffer: step 4166, counter 64653\n",
      "Environment 9: Episode 1058, Score -124.1235652067146, Avg_Score -113.0891790308268\n",
      "Adding trajectory to replay buffer: step 4178, counter 64703\n",
      "Environment 12: Episode 1059, Score -120.36776946168455, Avg_Score -113.26954926035044\n",
      "Adding trajectory to replay buffer: step 4181, counter 64752\n",
      "Environment 14: Episode 1060, Score -120.76492579021243, Avg_Score -113.46783692236104\n",
      "Adding trajectory to replay buffer: step 4210, counter 64803\n",
      "Environment 2: Episode 1061, Score -117.20936777722649, Avg_Score -113.57054488613747\n",
      "Adding trajectory to replay buffer: step 4212, counter 64902\n",
      "Environment 3: Episode 1062, Score -115.62174217009803, Avg_Score -113.69979601053832\n",
      "Adding trajectory to replay buffer: step 4216, counter 64955\n",
      "Environment 6: Episode 1063, Score -127.60043229429425, Avg_Score -113.96615176467222\n",
      "Adding trajectory to replay buffer: step 4221, counter 65011\n",
      "Environment 11: Episode 1064, Score -120.94071019189289, Avg_Score -114.12487541270852\n",
      "Adding trajectory to replay buffer: step 4232, counter 65062\n",
      "Environment 14: Episode 1065, Score -121.32147852527598, Avg_Score -114.32321994590329\n",
      "Adding trajectory to replay buffer: step 4257, counter 65109\n",
      "Environment 2: Episode 1066, Score -125.53468203369341, Avg_Score -114.5438593570595\n",
      "Adding trajectory to replay buffer: step 4259, counter 65156\n",
      "Environment 3: Episode 1067, Score -128.63249871894718, Avg_Score -114.80048862029838\n",
      "Adding trajectory to replay buffer: step 4264, counter 65204\n",
      "Environment 6: Episode 1068, Score -126.4505490680641, Avg_Score -115.03539137777021\n",
      "Adding trajectory to replay buffer: step 4269, counter 65252\n",
      "Environment 11: Episode 1069, Score -127.4354001493243, Avg_Score -115.29354877025675\n",
      "Adding trajectory to replay buffer: step 4281, counter 65301\n",
      "Environment 14: Episode 1070, Score -123.44949425333304, Avg_Score -115.50746050978229\n",
      "Adding trajectory to replay buffer: step 4298, counter 65485\n",
      "Environment 5: Episode 1071, Score -122.83415584219495, Avg_Score -115.71334362111911\n",
      "Adding trajectory to replay buffer: step 4309, counter 65537\n",
      "Environment 2: Episode 1072, Score -125.14891602388211, Avg_Score -115.9388722468468\n",
      "Adding trajectory to replay buffer: step 4316, counter 65589\n",
      "Environment 6: Episode 1073, Score -126.41224332620266, Avg_Score -116.16305821101223\n",
      "Adding trajectory to replay buffer: step 4325, counter 65645\n",
      "Environment 11: Episode 1074, Score -122.48204211554429, Avg_Score -116.35572074033625\n",
      "Adding trajectory to replay buffer: step 4328, counter 66007\n",
      "Environment 8: Episode 1075, Score -141.2160615034761, Avg_Score -116.73981842273741\n",
      "Adding trajectory to replay buffer: step 4347, counter 66056\n",
      "Environment 5: Episode 1076, Score -125.7479502761339, Avg_Score -116.9709128441369\n",
      "Adding trajectory to replay buffer: step 4360, counter 66107\n",
      "Environment 2: Episode 1077, Score -116.86338011080834, Avg_Score -117.11571896600249\n",
      "Adding trajectory to replay buffer: step 4360, counter 66186\n",
      "Environment 14: Episode 1078, Score -114.15729220684804, Avg_Score -117.23701136249142\n",
      "Adding trajectory to replay buffer: step 4396, counter 66235\n",
      "Environment 5: Episode 1079, Score -125.103878645751, Avg_Score -117.44259830998253\n",
      "Adding trajectory to replay buffer: step 4398, counter 66455\n",
      "Environment 12: Episode 1080, Score -127.74334900395944, Avg_Score -117.69380125655442\n",
      "Adding trajectory to replay buffer: step 4412, counter 66507\n",
      "Environment 2: Episode 1081, Score -124.43877026644783, Avg_Score -117.90718735505502\n",
      "Adding trajectory to replay buffer: step 4419, counter 66566\n",
      "Environment 14: Episode 1082, Score -119.021827417378, Avg_Score -118.05954535208849\n",
      "Adding trajectory to replay buffer: step 4446, counter 66616\n",
      "Environment 5: Episode 1083, Score -127.075191130807, Avg_Score -118.31099971192756\n",
      "Adding trajectory to replay buffer: step 4463, counter 66751\n",
      "Environment 8: Episode 1084, Score -126.40954793430771, Avg_Score -118.5111009204461\n",
      "Adding trajectory to replay buffer: step 4480, counter 66812\n",
      "Environment 14: Episode 1085, Score -125.18915961876635, Avg_Score -118.74033418368585\n",
      "Adding trajectory to replay buffer: step 4495, counter 66861\n",
      "Environment 5: Episode 1086, Score -128.3218182558731, Avg_Score -119.01241164808884\n",
      "Adding trajectory to replay buffer: step 4510, counter 66908\n",
      "Environment 8: Episode 1087, Score -125.73717902180243, Avg_Score -119.20849931076373\n",
      "Adding trajectory to replay buffer: step 4511, counter 67007\n",
      "Environment 2: Episode 1088, Score -114.24434216791454, Avg_Score -119.30078936461888\n",
      "Adding trajectory to replay buffer: step 4529, counter 67056\n",
      "Environment 14: Episode 1089, Score -127.1172476514969, Avg_Score -119.55433762843664\n",
      "Adding trajectory to replay buffer: step 4559, counter 67105\n",
      "Environment 8: Episode 1090, Score -117.86795313850729, Avg_Score -119.69330849772392\n",
      "Adding trajectory to replay buffer: step 4609, counter 67155\n",
      "Environment 8: Episode 1091, Score -128.64756723954466, Avg_Score -119.95629057106251\n",
      "Adding trajectory to replay buffer: step 4659, counter 67205\n",
      "Environment 8: Episode 1092, Score -120.4882512437279, Avg_Score -120.11603779024834\n",
      "Adding trajectory to replay buffer: step 4707, counter 67253\n",
      "Environment 8: Episode 1093, Score -128.16058459799862, Avg_Score -120.37091887515982\n",
      "Adding trajectory to replay buffer: step 4754, counter 67300\n",
      "Environment 8: Episode 1094, Score -125.17074031526161, Avg_Score -120.61070973800088\n",
      "Adding trajectory to replay buffer: step 5466, counter 68901\n",
      "Environment 0: Episode 1095, Score -157.6646920878185, Avg_Score -121.14711028343723\n",
      "Adding trajectory to replay buffer: step 5512, counter 70502\n",
      "Environment 1: Episode 1096, Score -146.1491585593275, Avg_Score -121.56149931701776\n",
      "Adding trajectory to replay buffer: step 5522, counter 70558\n",
      "Environment 0: Episode 1097, Score -119.60293028437346, Avg_Score -121.73769988439484\n",
      "Adding trajectory to replay buffer: step 5543, counter 72159\n",
      "Environment 15: Episode 1098, Score -154.56240164550186, Avg_Score -122.2653673430443\n",
      "Adding trajectory to replay buffer: step 5557, counter 73760\n",
      "Environment 4: Episode 1099, Score -154.78810104894956, Avg_Score -122.7949196788734\n",
      "Adding trajectory to replay buffer: step 5558, counter 75361\n",
      "Environment 7: Episode 1100, Score -161.604501380998, Avg_Score -123.38362580525607\n",
      "Adding trajectory to replay buffer: step 5559, counter 76962\n",
      "Environment 10: Episode 1101, Score -157.98165774252337, Avg_Score -123.94285239343353\n",
      "Adding trajectory to replay buffer: step 5559, counter 78563\n",
      "Environment 13: Episode 1102, Score -166.35328124418672, Avg_Score -124.55123401140878\n",
      "Adding trajectory to replay buffer: step 5562, counter 78613\n",
      "Environment 1: Episode 1103, Score -121.88782707193431, Avg_Score -124.74564182725972\n",
      "Adding trajectory to replay buffer: step 5597, counter 78667\n",
      "Environment 15: Episode 1104, Score -120.87958967833842, Avg_Score -124.92649048295995\n",
      "Adding trajectory to replay buffer: step 5604, counter 78749\n",
      "Environment 0: Episode 1105, Score -112.38021798531338, Avg_Score -125.02454573695408\n",
      "Adding trajectory to replay buffer: step 5605, counter 78796\n",
      "Environment 7: Episode 1106, Score -118.24468217480306, Avg_Score -125.17446850204033\n",
      "Adding trajectory to replay buffer: step 5606, counter 78845\n",
      "Environment 4: Episode 1107, Score -123.31000738919899, Avg_Score -125.28785472218628\n",
      "Adding trajectory to replay buffer: step 5609, counter 78895\n",
      "Environment 10: Episode 1108, Score -128.74295039360598, Avg_Score -125.42240208013493\n",
      "Adding trajectory to replay buffer: step 5610, counter 78946\n",
      "Environment 13: Episode 1109, Score -123.75934035203295, Avg_Score -125.44122934196177\n",
      "Adding trajectory to replay buffer: step 5612, counter 78996\n",
      "Environment 1: Episode 1110, Score -124.78677393672119, Avg_Score -125.51366140217299\n",
      "Adding trajectory to replay buffer: step 5649, counter 79048\n",
      "Environment 15: Episode 1111, Score -125.91968240788579, Avg_Score -125.6047194326363\n",
      "Adding trajectory to replay buffer: step 5653, counter 79097\n",
      "Environment 0: Episode 1112, Score -120.49607732282206, Avg_Score -125.65599675473655\n",
      "Adding trajectory to replay buffer: step 5654, counter 79145\n",
      "Environment 4: Episode 1113, Score -124.65949526623575, Avg_Score -125.74262389420483\n",
      "Adding trajectory to replay buffer: step 5659, counter 79194\n",
      "Environment 13: Episode 1114, Score -126.08969090662089, Avg_Score -125.81358720923932\n",
      "Adding trajectory to replay buffer: step 5665, counter 79247\n",
      "Environment 1: Episode 1115, Score -120.31824906349368, Avg_Score -125.76575429502371\n",
      "Adding trajectory to replay buffer: step 5699, counter 79297\n",
      "Environment 15: Episode 1116, Score -124.54209616115565, Avg_Score -125.9075111174601\n",
      "Adding trajectory to replay buffer: step 5746, counter 79344\n",
      "Environment 15: Episode 1117, Score -126.27441127119586, Avg_Score -125.97356313957215\n",
      "Adding trajectory to replay buffer: step 5763, counter 79442\n",
      "Environment 1: Episode 1118, Score -120.14851367583064, Avg_Score -125.98591356626235\n",
      "Adding trajectory to replay buffer: step 5767, counter 81043\n",
      "Environment 9: Episode 1119, Score -149.82263356969227, Avg_Score -126.22373808792308\n",
      "Adding trajectory to replay buffer: step 5796, counter 81093\n",
      "Environment 15: Episode 1120, Score -127.35549859272254, Avg_Score -126.24912875631325\n",
      "Adding trajectory to replay buffer: step 5813, counter 81143\n",
      "Environment 1: Episode 1121, Score -120.72210161493966, Avg_Score -126.32702376294655\n",
      "Adding trajectory to replay buffer: step 5816, counter 81192\n",
      "Environment 9: Episode 1122, Score -127.8422160393913, Avg_Score -126.36479513639514\n",
      "Adding trajectory to replay buffer: step 5860, counter 82793\n",
      "Environment 3: Episode 1123, Score -155.06325007822215, Avg_Score -126.63446224219052\n",
      "Adding trajectory to replay buffer: step 5868, counter 82845\n",
      "Environment 9: Episode 1124, Score -120.9576816183937, Avg_Score -126.58007405155476\n",
      "Adding trajectory to replay buffer: step 5875, counter 82907\n",
      "Environment 1: Episode 1125, Score -120.20707225873383, Avg_Score -126.51159105275518\n",
      "Adding trajectory to replay buffer: step 5908, counter 82955\n",
      "Environment 3: Episode 1126, Score -119.09440245291714, Avg_Score -126.43101596884914\n",
      "Adding trajectory to replay buffer: step 5916, counter 83003\n",
      "Environment 9: Episode 1127, Score -125.13430624807563, Avg_Score -126.43953824506409\n",
      "Adding trajectory to replay buffer: step 5917, counter 84604\n",
      "Environment 6: Episode 1128, Score -151.655649958045, Avg_Score -126.69259740648813\n",
      "Adding trajectory to replay buffer: step 5922, counter 84651\n",
      "Environment 1: Episode 1129, Score -126.05158733689288, Avg_Score -126.77772229577381\n",
      "Adding trajectory to replay buffer: step 5926, counter 86252\n",
      "Environment 11: Episode 1130, Score -157.15301086145925, Avg_Score -127.14109401435942\n",
      "Adding trajectory to replay buffer: step 5966, counter 86302\n",
      "Environment 9: Episode 1131, Score -125.64591870768616, Avg_Score -127.12714686606283\n",
      "Adding trajectory to replay buffer: step 5970, counter 86350\n",
      "Environment 1: Episode 1132, Score -125.66233383625033, Avg_Score -127.19079469689159\n",
      "Adding trajectory to replay buffer: step 5975, counter 86408\n",
      "Environment 6: Episode 1133, Score -120.0921892581011, Avg_Score -127.18842304798615\n",
      "Adding trajectory to replay buffer: step 5975, counter 86457\n",
      "Environment 11: Episode 1134, Score -127.71705946603356, Avg_Score -127.19070787855553\n",
      "Adding trajectory to replay buffer: step 5991, counter 86652\n",
      "Environment 15: Episode 1135, Score -124.68716738776861, Avg_Score -127.24806859068276\n",
      "Adding trajectory to replay buffer: step 5999, counter 88253\n",
      "Environment 12: Episode 1136, Score -153.45002044554133, Avg_Score -127.59245842396001\n",
      "Adding trajectory to replay buffer: step 6025, counter 88308\n",
      "Environment 1: Episode 1137, Score -120.99058991420392, Avg_Score -127.5261011794945\n",
      "Adding trajectory to replay buffer: step 6036, counter 88369\n",
      "Environment 11: Episode 1138, Score -119.29351506889549, Avg_Score -127.4652032064212\n",
      "Adding trajectory to replay buffer: step 6041, counter 88419\n",
      "Environment 15: Episode 1139, Score -118.93103952724114, Avg_Score -127.41018563026856\n",
      "Adding trajectory to replay buffer: step 6053, counter 88473\n",
      "Environment 12: Episode 1140, Score -121.98886525902711, Avg_Score -127.35620528663725\n",
      "Adding trajectory to replay buffer: step 6076, counter 88524\n",
      "Environment 1: Episode 1141, Score -121.54897721473561, Avg_Score -127.32543768641018\n",
      "Adding trajectory to replay buffer: step 6089, counter 88577\n",
      "Environment 11: Episode 1142, Score -125.97948701176605, Avg_Score -127.32403941487377\n",
      "Adding trajectory to replay buffer: step 6096, counter 90178\n",
      "Environment 5: Episode 1143, Score -154.25879422421536, Avg_Score -127.60123166989166\n",
      "Adding trajectory to replay buffer: step 6102, counter 90227\n",
      "Environment 12: Episode 1144, Score -125.42167714367744, Avg_Score -127.60761803109426\n",
      "Adding trajectory to replay buffer: step 6112, counter 91828\n",
      "Environment 2: Episode 1145, Score -149.9856044487349, Avg_Score -127.83802606019215\n",
      "Adding trajectory to replay buffer: step 6123, counter 91875\n",
      "Environment 1: Episode 1146, Score -126.14439358781341, Avg_Score -127.8282293357651\n",
      "Adding trajectory to replay buffer: step 6125, counter 91959\n",
      "Environment 15: Episode 1147, Score -114.63031144529705, Avg_Score -127.7521222764486\n",
      "Adding trajectory to replay buffer: step 6130, counter 93560\n",
      "Environment 14: Episode 1148, Score -155.51630273625463, Avg_Score -128.11883744710923\n",
      "Adding trajectory to replay buffer: step 6136, counter 93607\n",
      "Environment 11: Episode 1149, Score -118.10029862537452, Avg_Score -128.0154975105768\n",
      "Adding trajectory to replay buffer: step 6158, counter 93663\n",
      "Environment 12: Episode 1150, Score -119.11103686473953, Avg_Score -128.00371349922133\n",
      "Adding trajectory to replay buffer: step 6164, counter 93715\n",
      "Environment 2: Episode 1151, Score -125.2390468889686, Avg_Score -127.996056676266\n",
      "Adding trajectory to replay buffer: step 6176, counter 93766\n",
      "Environment 15: Episode 1152, Score -125.28183406189704, Avg_Score -127.98225672796039\n",
      "Adding trajectory to replay buffer: step 6178, counter 93821\n",
      "Environment 1: Episode 1153, Score -114.33062202704744, Avg_Score -127.90519417264505\n",
      "Adding trajectory to replay buffer: step 6179, counter 93870\n",
      "Environment 14: Episode 1154, Score -118.59897320808656, Avg_Score -127.83202067064825\n",
      "Adding trajectory to replay buffer: step 6186, counter 93920\n",
      "Environment 11: Episode 1155, Score -123.18330522564798, Avg_Score -127.86937404094229\n",
      "Adding trajectory to replay buffer: step 6191, counter 94015\n",
      "Environment 5: Episode 1156, Score -118.5481365151927, Avg_Score -127.85956318630258\n",
      "Adding trajectory to replay buffer: step 6212, counter 94069\n",
      "Environment 12: Episode 1157, Score -120.10827435729591, Avg_Score -127.81703578833239\n",
      "Adding trajectory to replay buffer: step 6227, counter 94118\n",
      "Environment 1: Episode 1158, Score -126.0563173867672, Avg_Score -127.83636331013291\n",
      "Adding trajectory to replay buffer: step 6227, counter 94166\n",
      "Environment 14: Episode 1159, Score -123.83636930620608, Avg_Score -127.87104930857814\n",
      "Adding trajectory to replay buffer: step 6228, counter 94218\n",
      "Environment 15: Episode 1160, Score -122.02759345369972, Avg_Score -127.88367598521299\n",
      "Adding trajectory to replay buffer: step 6237, counter 94269\n",
      "Environment 11: Episode 1161, Score -123.0532492541143, Avg_Score -127.94211479998187\n",
      "Adding trajectory to replay buffer: step 6239, counter 94317\n",
      "Environment 5: Episode 1162, Score -125.59441394752078, Avg_Score -128.0418415177561\n",
      "Adding trajectory to replay buffer: step 6276, counter 94366\n",
      "Environment 1: Episode 1163, Score -124.24660081060976, Avg_Score -128.00830320291928\n",
      "Adding trajectory to replay buffer: step 6278, counter 94416\n",
      "Environment 15: Episode 1164, Score -127.15291646383889, Avg_Score -128.07042526563873\n",
      "Adding trajectory to replay buffer: step 6279, counter 94468\n",
      "Environment 14: Episode 1165, Score -124.84186476767933, Avg_Score -128.10562912806273\n",
      "Adding trajectory to replay buffer: step 6285, counter 94516\n",
      "Environment 11: Episode 1166, Score -123.91811435103075, Avg_Score -128.08946345123613\n",
      "Adding trajectory to replay buffer: step 6291, counter 94568\n",
      "Environment 5: Episode 1167, Score -116.30867457611673, Avg_Score -127.96622520980782\n",
      "Adding trajectory to replay buffer: step 6325, counter 94617\n",
      "Environment 1: Episode 1168, Score -123.63783585870702, Avg_Score -127.93809807771426\n",
      "Adding trajectory to replay buffer: step 6326, counter 94664\n",
      "Environment 14: Episode 1169, Score -123.78417516972871, Avg_Score -127.90158582791831\n",
      "Adding trajectory to replay buffer: step 6332, counter 94711\n",
      "Environment 11: Episode 1170, Score -128.86832575657655, Avg_Score -127.95577414295076\n",
      "Adding trajectory to replay buffer: step 6342, counter 94762\n",
      "Environment 5: Episode 1171, Score -127.30279306210517, Avg_Score -128.00046051514985\n",
      "Adding trajectory to replay buffer: step 6344, counter 94894\n",
      "Environment 12: Episode 1172, Score -114.75494149431151, Avg_Score -127.89652076985413\n",
      "Adding trajectory to replay buffer: step 6351, counter 95081\n",
      "Environment 2: Episode 1173, Score -122.71223179987693, Avg_Score -127.85952065459088\n",
      "Adding trajectory to replay buffer: step 6355, counter 96682\n",
      "Environment 8: Episode 1174, Score -153.0237763084067, Avg_Score -128.1649379965195\n",
      "Adding trajectory to replay buffer: step 6385, counter 96735\n",
      "Environment 11: Episode 1175, Score -123.22000132942883, Avg_Score -127.98497739477905\n",
      "Adding trajectory to replay buffer: step 6386, counter 96796\n",
      "Environment 1: Episode 1176, Score -122.53537414397796, Avg_Score -127.9528516334575\n",
      "Adding trajectory to replay buffer: step 6393, counter 96847\n",
      "Environment 5: Episode 1177, Score -126.28076651501097, Avg_Score -128.0470254974995\n",
      "Adding trajectory to replay buffer: step 6399, counter 96895\n",
      "Environment 2: Episode 1178, Score -126.85184449255466, Avg_Score -128.17397102035656\n",
      "Adding trajectory to replay buffer: step 6404, counter 96973\n",
      "Environment 14: Episode 1179, Score -113.7056890637651, Avg_Score -128.0599891245367\n",
      "Adding trajectory to replay buffer: step 6406, counter 97024\n",
      "Environment 8: Episode 1180, Score -127.77054621531629, Avg_Score -128.0602610966503\n",
      "Adding trajectory to replay buffer: step 6434, counter 97072\n",
      "Environment 1: Episode 1181, Score -128.10584755689962, Avg_Score -128.09693186955482\n",
      "Adding trajectory to replay buffer: step 6435, counter 97122\n",
      "Environment 11: Episode 1182, Score -129.55214385875004, Avg_Score -128.20223503396852\n",
      "Adding trajectory to replay buffer: step 6442, counter 97171\n",
      "Environment 5: Episode 1183, Score -124.033312306044, Avg_Score -128.17181624572092\n",
      "Adding trajectory to replay buffer: step 6451, counter 97223\n",
      "Environment 2: Episode 1184, Score -126.853854018153, Avg_Score -128.17625930655936\n",
      "Adding trajectory to replay buffer: step 6455, counter 97274\n",
      "Environment 14: Episode 1185, Score -126.725532810842, Avg_Score -128.19162303848012\n",
      "Adding trajectory to replay buffer: step 6456, counter 97324\n",
      "Environment 8: Episode 1186, Score -125.70152179430984, Avg_Score -128.16542007386445\n",
      "Adding trajectory to replay buffer: step 6493, counter 97383\n",
      "Environment 1: Episode 1187, Score -118.19479425236645, Avg_Score -128.0899962261701\n",
      "Adding trajectory to replay buffer: step 6500, counter 97432\n",
      "Environment 2: Episode 1188, Score -126.55346191618032, Avg_Score -128.21308742365278\n",
      "Adding trajectory to replay buffer: step 6510, counter 97486\n",
      "Environment 8: Episode 1189, Score -117.35889036164184, Avg_Score -128.11550385075424\n",
      "Adding trajectory to replay buffer: step 6522, counter 97573\n",
      "Environment 11: Episode 1190, Score -114.3626526332243, Avg_Score -128.0804508457014\n",
      "Adding trajectory to replay buffer: step 6549, counter 97629\n",
      "Environment 1: Episode 1191, Score -122.98956313634974, Avg_Score -128.02387080466943\n",
      "Adding trajectory to replay buffer: step 6551, counter 97680\n",
      "Environment 2: Episode 1192, Score -121.69705092952151, Avg_Score -128.03595880152739\n",
      "Adding trajectory to replay buffer: step 6559, counter 97729\n",
      "Environment 8: Episode 1193, Score -126.23188704298312, Avg_Score -128.0166718259772\n",
      "Adding trajectory to replay buffer: step 6575, counter 97782\n",
      "Environment 11: Episode 1194, Score -123.74376246695097, Avg_Score -128.00240204749412\n",
      "Adding trajectory to replay buffer: step 6599, counter 97832\n",
      "Environment 1: Episode 1195, Score -126.89304183683358, Avg_Score -127.69468554498425\n",
      "Adding trajectory to replay buffer: step 6603, counter 97884\n",
      "Environment 2: Episode 1196, Score -125.70200873717666, Avg_Score -127.49021404676276\n",
      "Adding trajectory to replay buffer: step 6615, counter 98044\n",
      "Environment 14: Episode 1197, Score -121.05116661697565, Avg_Score -127.50469641008877\n",
      "Adding trajectory to replay buffer: step 6616, counter 98101\n",
      "Environment 8: Episode 1198, Score -118.6440865745973, Avg_Score -127.1455132593797\n",
      "Adding trajectory to replay buffer: step 6625, counter 98151\n",
      "Environment 11: Episode 1199, Score -123.97726645134608, Avg_Score -126.83740491340366\n",
      "Adding trajectory to replay buffer: step 6647, counter 98199\n",
      "Environment 1: Episode 1200, Score -127.07461454828393, Avg_Score -126.49210604507653\n",
      "Adding trajectory to replay buffer: step 6654, counter 98250\n",
      "Environment 2: Episode 1201, Score -120.40618510289552, Avg_Score -126.11635131868026\n",
      "Adding trajectory to replay buffer: step 6667, counter 98301\n",
      "Environment 8: Episode 1202, Score -125.31346918194555, Avg_Score -125.70595319805783\n",
      "Adding trajectory to replay buffer: step 6676, counter 98352\n",
      "Environment 11: Episode 1203, Score -125.95245849025996, Avg_Score -125.74659951224106\n",
      "Adding trajectory to replay buffer: step 6705, counter 98410\n",
      "Environment 1: Episode 1204, Score -126.20480038915017, Avg_Score -125.7998516193492\n",
      "Adding trajectory to replay buffer: step 6705, counter 98461\n",
      "Environment 2: Episode 1205, Score -123.39164985802087, Avg_Score -125.90996593807625\n",
      "Adding trajectory to replay buffer: step 6732, counter 98517\n",
      "Environment 11: Episode 1206, Score -122.04301109664826, Avg_Score -125.94794922729471\n",
      "Adding trajectory to replay buffer: step 6754, counter 98566\n",
      "Environment 2: Episode 1207, Score -125.78133563555156, Avg_Score -125.97266250975825\n",
      "Adding trajectory to replay buffer: step 6760, counter 98621\n",
      "Environment 1: Episode 1208, Score -124.46067727674543, Avg_Score -125.92983977858965\n",
      "Adding trajectory to replay buffer: step 6801, counter 98668\n",
      "Environment 2: Episode 1209, Score -117.00112861063083, Avg_Score -125.86225766117563\n",
      "Adding trajectory to replay buffer: step 6809, counter 98717\n",
      "Environment 1: Episode 1210, Score -126.00419010248284, Avg_Score -125.87443182283324\n",
      "Adding trajectory to replay buffer: step 6849, counter 98765\n",
      "Environment 2: Episode 1211, Score -120.13607146048483, Avg_Score -125.81659571335923\n",
      "Adding trajectory to replay buffer: step 6896, counter 98812\n",
      "Environment 2: Episode 1212, Score -124.34756603252515, Avg_Score -125.85511060045629\n",
      "Adding trajectory to replay buffer: step 7206, counter 100413\n",
      "Environment 7: Episode 1213, Score -161.51697375254307, Avg_Score -126.22368538531937\n",
      "Adding trajectory to replay buffer: step 7210, counter 102014\n",
      "Environment 10: Episode 1214, Score -158.78475955323975, Avg_Score -126.55063607178556\n",
      "Adding trajectory to replay buffer: step 7254, counter 103615\n",
      "Environment 0: Episode 1215, Score -152.9342229906658, Avg_Score -126.87679581105728\n",
      "Adding trajectory to replay buffer: step 7255, counter 105216\n",
      "Environment 4: Episode 1216, Score -149.59507913240543, Avg_Score -127.12732564076975\n",
      "Adding trajectory to replay buffer: step 7255, counter 105265\n",
      "Environment 7: Episode 1217, Score -122.19541426422944, Avg_Score -127.0865356707001\n",
      "Adding trajectory to replay buffer: step 7258, counter 105313\n",
      "Environment 10: Episode 1218, Score -127.07948959619739, Avg_Score -127.15584542990378\n",
      "Adding trajectory to replay buffer: step 7260, counter 106914\n",
      "Environment 13: Episode 1219, Score -164.59469892426975, Avg_Score -127.30356608344955\n",
      "Adding trajectory to replay buffer: step 7305, counter 106965\n",
      "Environment 0: Episode 1220, Score -120.99977193099014, Avg_Score -127.24000881683222\n",
      "Adding trajectory to replay buffer: step 7312, counter 107019\n",
      "Environment 10: Episode 1221, Score -125.16050124486628, Avg_Score -127.28439281313149\n",
      "Adding trajectory to replay buffer: step 7313, counter 107072\n",
      "Environment 13: Episode 1222, Score -126.58310057654916, Avg_Score -127.27180165850305\n",
      "Adding trajectory to replay buffer: step 7314, counter 107131\n",
      "Environment 4: Episode 1223, Score -120.33334207305748, Avg_Score -126.92450257845142\n",
      "Adding trajectory to replay buffer: step 7354, counter 107180\n",
      "Environment 0: Episode 1224, Score -129.06235102803632, Avg_Score -127.00554927254785\n",
      "Adding trajectory to replay buffer: step 7366, counter 107234\n",
      "Environment 10: Episode 1225, Score -119.0594591669881, Avg_Score -126.9940731416304\n",
      "Adding trajectory to replay buffer: step 7403, counter 107283\n",
      "Environment 0: Episode 1226, Score -124.75380075407215, Avg_Score -127.05066712464195\n",
      "Adding trajectory to replay buffer: step 7433, counter 107350\n",
      "Environment 10: Episode 1227, Score -119.65397499063302, Avg_Score -126.99586381206751\n",
      "Adding trajectory to replay buffer: step 7456, counter 107403\n",
      "Environment 0: Episode 1228, Score -124.91830440686519, Avg_Score -126.72849035655572\n",
      "Adding trajectory to replay buffer: step 7482, counter 107452\n",
      "Environment 10: Episode 1229, Score -117.9565391146342, Avg_Score -126.64753987433312\n",
      "Adding trajectory to replay buffer: step 7502, counter 107498\n",
      "Environment 0: Episode 1230, Score -126.2492089940173, Avg_Score -126.33850185565872\n",
      "Adding trajectory to replay buffer: step 7509, counter 109099\n",
      "Environment 3: Episode 1231, Score -152.4320541363723, Avg_Score -126.60636320994557\n",
      "Adding trajectory to replay buffer: step 7530, counter 109147\n",
      "Environment 10: Episode 1232, Score -125.34469420049278, Avg_Score -126.60318681358802\n",
      "Adding trajectory to replay buffer: step 7556, counter 109201\n",
      "Environment 0: Episode 1233, Score -123.83921853229155, Avg_Score -126.64065710632994\n",
      "Adding trajectory to replay buffer: step 7567, counter 110802\n",
      "Environment 9: Episode 1234, Score -156.22102667714182, Avg_Score -126.92569677844102\n",
      "Adding trajectory to replay buffer: step 7576, counter 112403\n",
      "Environment 6: Episode 1235, Score -157.88641246946642, Avg_Score -127.25768922925799\n",
      "Adding trajectory to replay buffer: step 7585, counter 112458\n",
      "Environment 10: Episode 1236, Score -121.39496490142913, Avg_Score -126.93713867381685\n",
      "Adding trajectory to replay buffer: step 7611, counter 112513\n",
      "Environment 0: Episode 1237, Score -125.53535713860269, Avg_Score -126.98258634606083\n",
      "Adding trajectory to replay buffer: step 7619, counter 112565\n",
      "Environment 9: Episode 1238, Score -126.70443585572703, Avg_Score -127.05669555392916\n",
      "Adding trajectory to replay buffer: step 7627, counter 112616\n",
      "Environment 6: Episode 1239, Score -124.38533513115036, Avg_Score -127.11123850996826\n",
      "Adding trajectory to replay buffer: step 7635, counter 112666\n",
      "Environment 10: Episode 1240, Score -127.23821330046529, Avg_Score -127.16373199038264\n",
      "Adding trajectory to replay buffer: step 7659, counter 112714\n",
      "Environment 0: Episode 1241, Score -125.42830349558218, Avg_Score -127.20252525319108\n",
      "Adding trajectory to replay buffer: step 7667, counter 112762\n",
      "Environment 9: Episode 1242, Score -122.6209541243012, Avg_Score -127.16893992431643\n",
      "Adding trajectory to replay buffer: step 7678, counter 112813\n",
      "Environment 6: Episode 1243, Score -121.56264125784176, Avg_Score -126.84197839465273\n",
      "Adding trajectory to replay buffer: step 7686, counter 112864\n",
      "Environment 10: Episode 1244, Score -123.9920709791008, Avg_Score -126.82768233300692\n",
      "Adding trajectory to replay buffer: step 7711, counter 112916\n",
      "Environment 0: Episode 1245, Score -123.19503531930596, Avg_Score -126.55977664171263\n",
      "Adding trajectory to replay buffer: step 7715, counter 112964\n",
      "Environment 9: Episode 1246, Score -123.54821577403818, Avg_Score -126.5338148635749\n",
      "Adding trajectory to replay buffer: step 7732, counter 113018\n",
      "Environment 6: Episode 1247, Score -123.31805245626283, Avg_Score -126.62069227368457\n",
      "Adding trajectory to replay buffer: step 7734, counter 113066\n",
      "Environment 10: Episode 1248, Score -127.31711770464605, Avg_Score -126.33870042336845\n",
      "Adding trajectory to replay buffer: step 7766, counter 113117\n",
      "Environment 9: Episode 1249, Score -125.28662568064779, Avg_Score -126.41056369392119\n",
      "Adding trajectory to replay buffer: step 7785, counter 113168\n",
      "Environment 10: Episode 1250, Score -123.39995206265337, Avg_Score -126.45345284590034\n",
      "Adding trajectory to replay buffer: step 7786, counter 113222\n",
      "Environment 6: Episode 1251, Score -123.90704334428597, Avg_Score -126.44013281045352\n",
      "Adding trajectory to replay buffer: step 7833, counter 113270\n",
      "Environment 10: Episode 1252, Score -124.39330468198658, Avg_Score -126.43124751665444\n",
      "Adding trajectory to replay buffer: step 7834, counter 113318\n",
      "Environment 6: Episode 1253, Score -124.83493655986402, Avg_Score -126.53629066198259\n",
      "Adding trajectory to replay buffer: step 7879, counter 114919\n",
      "Environment 15: Episode 1254, Score -147.13047144056006, Avg_Score -126.82160564430733\n",
      "Adding trajectory to replay buffer: step 7884, counter 114969\n",
      "Environment 6: Episode 1255, Score -123.5332792097951, Avg_Score -126.8251053841488\n",
      "Adding trajectory to replay buffer: step 7934, counter 115019\n",
      "Environment 6: Episode 1256, Score -126.91579424952467, Avg_Score -126.9087819614921\n",
      "Adding trajectory to replay buffer: step 7945, counter 116620\n",
      "Environment 12: Episode 1257, Score -152.49680944279535, Avg_Score -127.23266731234709\n",
      "Adding trajectory to replay buffer: step 7966, counter 116707\n",
      "Environment 15: Episode 1258, Score -113.49881404834613, Avg_Score -127.10709227896288\n",
      "Adding trajectory to replay buffer: step 7983, counter 116756\n",
      "Environment 6: Episode 1259, Score -119.1390590622785, Avg_Score -127.06011917652359\n",
      "Adding trajectory to replay buffer: step 8000, counter 116811\n",
      "Environment 12: Episode 1260, Score -118.12162586212281, Avg_Score -127.02105950060782\n",
      "Adding trajectory to replay buffer: step 8017, counter 116862\n",
      "Environment 15: Episode 1261, Score -127.37220011467704, Avg_Score -127.06424900921344\n",
      "Adding trajectory to replay buffer: step 8031, counter 116910\n",
      "Environment 6: Episode 1262, Score -127.36270205427148, Avg_Score -127.08193189028094\n",
      "Adding trajectory to replay buffer: step 8043, counter 118511\n",
      "Environment 5: Episode 1263, Score -155.77582942935248, Avg_Score -127.39722417646838\n",
      "Adding trajectory to replay buffer: step 8051, counter 118562\n",
      "Environment 12: Episode 1264, Score -118.64073433176087, Avg_Score -127.3121023551476\n",
      "Adding trajectory to replay buffer: step 8065, counter 118610\n",
      "Environment 15: Episode 1265, Score -127.9449754580073, Avg_Score -127.34313346205089\n",
      "Adding trajectory to replay buffer: step 8087, counter 118666\n",
      "Environment 6: Episode 1266, Score -121.45866008586313, Avg_Score -127.31853891939922\n",
      "Adding trajectory to replay buffer: step 8098, counter 118713\n",
      "Environment 12: Episode 1267, Score -126.73654064908563, Avg_Score -127.4228175801289\n",
      "Adding trajectory to replay buffer: step 8099, counter 118769\n",
      "Environment 5: Episode 1268, Score -124.31674229165664, Avg_Score -127.4296066444584\n",
      "Adding trajectory to replay buffer: step 8113, counter 118817\n",
      "Environment 15: Episode 1269, Score -126.2568861629137, Avg_Score -127.45433375439025\n",
      "Adding trajectory to replay buffer: step 8140, counter 118870\n",
      "Environment 6: Episode 1270, Score -118.32571969451507, Avg_Score -127.34890769376962\n",
      "Adding trajectory to replay buffer: step 8149, counter 118920\n",
      "Environment 5: Episode 1271, Score -126.15517890767637, Avg_Score -127.33743155222534\n",
      "Adding trajectory to replay buffer: step 8149, counter 118971\n",
      "Environment 12: Episode 1272, Score -126.56824532544427, Avg_Score -127.45556459053667\n",
      "Adding trajectory to replay buffer: step 8154, counter 119359\n",
      "Environment 9: Episode 1273, Score -143.86328489731034, Avg_Score -127.667075121511\n",
      "Adding trajectory to replay buffer: step 8192, counter 119411\n",
      "Environment 6: Episode 1274, Score -125.8215461884352, Avg_Score -127.3950528203113\n",
      "Adding trajectory to replay buffer: step 8199, counter 119461\n",
      "Environment 12: Episode 1275, Score -122.3248361866828, Avg_Score -127.38610116888384\n",
      "Adding trajectory to replay buffer: step 8201, counter 119513\n",
      "Environment 5: Episode 1276, Score -124.5814195869236, Avg_Score -127.4065616233133\n",
      "Adding trajectory to replay buffer: step 8203, counter 119603\n",
      "Environment 15: Episode 1277, Score -114.11047107583099, Avg_Score -127.28485866892152\n",
      "Adding trajectory to replay buffer: step 8207, counter 119656\n",
      "Environment 9: Episode 1278, Score -120.9221483172914, Avg_Score -127.2255617071689\n",
      "Adding trajectory to replay buffer: step 8216, counter 121257\n",
      "Environment 14: Episode 1279, Score -147.0087930811037, Avg_Score -127.5585927473423\n",
      "Adding trajectory to replay buffer: step 8242, counter 121307\n",
      "Environment 6: Episode 1280, Score -125.75522507584157, Avg_Score -127.53843953594755\n",
      "Adding trajectory to replay buffer: step 8249, counter 121357\n",
      "Environment 12: Episode 1281, Score -126.20229757959396, Avg_Score -127.51940403617448\n",
      "Adding trajectory to replay buffer: step 8257, counter 121407\n",
      "Environment 9: Episode 1282, Score -120.72656317364984, Avg_Score -127.43114822932347\n",
      "Adding trajectory to replay buffer: step 8261, counter 121467\n",
      "Environment 5: Episode 1283, Score -118.44007387680995, Avg_Score -127.37521584503112\n",
      "Adding trajectory to replay buffer: step 8268, counter 123068\n",
      "Environment 8: Episode 1284, Score -149.53424575890537, Avg_Score -127.60201976243863\n",
      "Adding trajectory to replay buffer: step 8271, counter 123136\n",
      "Environment 15: Episode 1285, Score -119.78430981920785, Avg_Score -127.5326075325223\n",
      "Adding trajectory to replay buffer: step 8277, counter 123197\n",
      "Environment 14: Episode 1286, Score -124.67472563756614, Avg_Score -127.52233957095484\n",
      "Adding trajectory to replay buffer: step 8289, counter 123244\n",
      "Environment 6: Episode 1287, Score -125.43405805650727, Avg_Score -127.59473220899626\n",
      "Adding trajectory to replay buffer: step 8296, counter 123291\n",
      "Environment 12: Episode 1288, Score -122.67176663631015, Avg_Score -127.55591525619755\n",
      "Adding trajectory to replay buffer: step 8314, counter 123344\n",
      "Environment 5: Episode 1289, Score -119.8624802395093, Avg_Score -127.58095115497625\n",
      "Adding trajectory to replay buffer: step 8316, counter 123392\n",
      "Environment 8: Episode 1290, Score -129.11895121958727, Avg_Score -127.72851414083985\n",
      "Adding trajectory to replay buffer: step 8320, counter 123455\n",
      "Environment 9: Episode 1291, Score -123.4001100894548, Avg_Score -127.73261961037089\n",
      "Adding trajectory to replay buffer: step 8322, counter 123500\n",
      "Environment 14: Episode 1292, Score -116.73388085929801, Avg_Score -127.68298790966867\n",
      "Adding trajectory to replay buffer: step 8323, counter 123552\n",
      "Environment 15: Episode 1293, Score -118.14038608930011, Avg_Score -127.60207290013186\n",
      "Adding trajectory to replay buffer: step 8333, counter 125153\n",
      "Environment 11: Episode 1294, Score -158.75167537539429, Avg_Score -127.95215202921628\n",
      "Adding trajectory to replay buffer: step 8335, counter 125199\n",
      "Environment 6: Episode 1295, Score -122.67834080671457, Avg_Score -127.91000501891511\n",
      "Adding trajectory to replay buffer: step 8341, counter 125244\n",
      "Environment 12: Episode 1296, Score -122.67547809431454, Avg_Score -127.87973971248647\n",
      "Adding trajectory to replay buffer: step 8361, counter 125289\n",
      "Environment 8: Episode 1297, Score -121.56589084761154, Avg_Score -127.88488695479283\n",
      "Adding trajectory to replay buffer: step 8365, counter 125334\n",
      "Environment 9: Episode 1298, Score -122.62791582592018, Avg_Score -127.92472524730607\n",
      "Adding trajectory to replay buffer: step 8367, counter 125387\n",
      "Environment 5: Episode 1299, Score -119.20605203187094, Avg_Score -127.8770131031113\n",
      "Adding trajectory to replay buffer: step 8367, counter 125432\n",
      "Environment 14: Episode 1300, Score -122.42753913467317, Avg_Score -127.83054234897517\n",
      "Adding trajectory to replay buffer: step 8379, counter 126100\n",
      "Environment 0: Episode 1301, Score -177.172875998262, Avg_Score -128.39820925792884\n",
      "Adding trajectory to replay buffer: step 8379, counter 126144\n",
      "Environment 6: Episode 1302, Score -121.38574604747382, Avg_Score -128.35893202658414\n",
      "Adding trajectory to replay buffer: step 8380, counter 126201\n",
      "Environment 15: Episode 1303, Score -115.1977847310106, Avg_Score -128.25138528899163\n",
      "Adding trajectory to replay buffer: step 8382, counter 127774\n",
      "Environment 1: Episode 1304, Score -263.82793116530445, Avg_Score -129.62761659675317\n",
      "Adding trajectory to replay buffer: step 8384, counter 129262\n",
      "Environment 2: Episode 1305, Score -254.18143443296623, Avg_Score -130.9355144425026\n",
      "Adding trajectory to replay buffer: step 8385, counter 130333\n",
      "Environment 4: Episode 1306, Score -214.74588516405487, Avg_Score -131.86254318317668\n",
      "Adding trajectory to replay buffer: step 8386, counter 130378\n",
      "Environment 12: Episode 1307, Score -120.91297693797387, Avg_Score -131.8138595962009\n",
      "Adding trajectory to replay buffer: step 8387, counter 131510\n",
      "Environment 7: Episode 1308, Score -213.80885296359943, Avg_Score -132.70734135306947\n",
      "Adding trajectory to replay buffer: step 8391, counter 132068\n",
      "Environment 10: Episode 1309, Score -167.89149021025068, Avg_Score -133.21624496906566\n",
      "Adding trajectory to replay buffer: step 8392, counter 133147\n",
      "Environment 13: Episode 1310, Score -212.1121833950526, Avg_Score -134.07732490199135\n",
      "Adding trajectory to replay buffer: step 8395, counter 133209\n",
      "Environment 11: Episode 1311, Score -114.6197705601876, Avg_Score -134.0221618929884\n",
      "Adding trajectory to replay buffer: step 8398, counter 134098\n",
      "Environment 3: Episode 1312, Score -199.56811924959743, Avg_Score -134.7743674251591\n",
      "Adding trajectory to replay buffer: step 8404, counter 134141\n",
      "Environment 8: Episode 1313, Score -121.80428227295727, Avg_Score -134.37724051036324\n",
      "Adding trajectory to replay buffer: step 8409, counter 134185\n",
      "Environment 9: Episode 1314, Score -122.32434471562257, Avg_Score -134.01263636198706\n",
      "Adding trajectory to replay buffer: step 8409, counter 134227\n",
      "Environment 14: Episode 1315, Score -117.97248236800917, Avg_Score -133.66301895576046\n",
      "Adding trajectory to replay buffer: step 8413, counter 134273\n",
      "Environment 5: Episode 1316, Score -119.54897107828222, Avg_Score -133.36255787521927\n",
      "Adding trajectory to replay buffer: step 8422, counter 134316\n",
      "Environment 0: Episode 1317, Score -121.40441544600452, Avg_Score -133.354647887037\n",
      "Adding trajectory to replay buffer: step 8423, counter 134360\n",
      "Environment 6: Episode 1318, Score -121.37619644885697, Avg_Score -133.2976149555636\n",
      "Adding trajectory to replay buffer: step 8426, counter 134404\n",
      "Environment 1: Episode 1319, Score -122.49599649135955, Avg_Score -132.8766279312345\n",
      "Adding trajectory to replay buffer: step 8431, counter 134448\n",
      "Environment 7: Episode 1320, Score -116.71837887790426, Avg_Score -132.83381400070363\n",
      "Adding trajectory to replay buffer: step 8432, counter 134494\n",
      "Environment 12: Episode 1321, Score -121.72268928335794, Avg_Score -132.79943588108858\n",
      "Adding trajectory to replay buffer: step 8435, counter 134538\n",
      "Environment 10: Episode 1322, Score -122.90643221015328, Avg_Score -132.76266919742463\n",
      "Adding trajectory to replay buffer: step 8436, counter 134582\n",
      "Environment 13: Episode 1323, Score -122.16363349509301, Avg_Score -132.78097211164496\n",
      "Adding trajectory to replay buffer: step 8440, counter 134627\n",
      "Environment 11: Episode 1324, Score -120.17356673898547, Avg_Score -132.69208426875448\n",
      "Adding trajectory to replay buffer: step 8441, counter 134670\n",
      "Environment 3: Episode 1325, Score -120.0969141476086, Avg_Score -132.70245881856067\n",
      "Adding trajectory to replay buffer: step 8447, counter 134713\n",
      "Environment 8: Episode 1326, Score -115.30879250258657, Avg_Score -132.60800873604583\n",
      "Adding trajectory to replay buffer: step 8453, counter 134757\n",
      "Environment 9: Episode 1327, Score -116.4098867156698, Avg_Score -132.5755678532962\n",
      "Adding trajectory to replay buffer: step 8453, counter 134801\n",
      "Environment 14: Episode 1328, Score -115.67326908773401, Avg_Score -132.4831175001049\n",
      "Adding trajectory to replay buffer: step 8460, counter 134877\n",
      "Environment 2: Episode 1329, Score -115.91919673381993, Avg_Score -132.46274407629676\n",
      "Adding trajectory to replay buffer: step 8471, counter 134935\n",
      "Environment 5: Episode 1330, Score -113.50271118493751, Avg_Score -132.33527909820594\n",
      "Adding trajectory to replay buffer: step 8474, counter 134983\n",
      "Environment 1: Episode 1331, Score -122.81031975055362, Avg_Score -132.03906175434776\n",
      "Adding trajectory to replay buffer: step 8475, counter 135026\n",
      "Environment 12: Episode 1332, Score -122.70283855905073, Avg_Score -132.01264319793336\n",
      "Adding trajectory to replay buffer: step 8478, counter 135069\n",
      "Environment 10: Episode 1333, Score -118.21725894648706, Avg_Score -131.9564236020753\n",
      "Adding trajectory to replay buffer: step 8485, counter 135131\n",
      "Environment 6: Episode 1334, Score -116.88042058526771, Avg_Score -131.56301754115657\n",
      "Adding trajectory to replay buffer: step 8486, counter 135177\n",
      "Environment 11: Episode 1335, Score -122.15080792718257, Avg_Score -131.20566149573375\n",
      "Adding trajectory to replay buffer: step 8486, counter 135283\n",
      "Environment 15: Episode 1336, Score -116.01546725550915, Avg_Score -131.15186651927456\n",
      "Adding trajectory to replay buffer: step 8487, counter 135329\n",
      "Environment 3: Episode 1337, Score -121.63627315428046, Avg_Score -131.11287567943134\n",
      "Adding trajectory to replay buffer: step 8494, counter 135401\n",
      "Environment 0: Episode 1338, Score -115.98524929014158, Avg_Score -131.00568381377548\n",
      "Adding trajectory to replay buffer: step 8494, counter 135448\n",
      "Environment 8: Episode 1339, Score -122.61192202063651, Avg_Score -130.98794968267035\n",
      "Adding trajectory to replay buffer: step 8496, counter 135491\n",
      "Environment 9: Episode 1340, Score -123.1254921109751, Avg_Score -130.94682247077546\n",
      "Adding trajectory to replay buffer: step 8504, counter 135535\n",
      "Environment 2: Episode 1341, Score -120.96131138205931, Avg_Score -130.90215254964022\n",
      "Adding trajectory to replay buffer: step 8516, counter 135580\n",
      "Environment 5: Episode 1342, Score -122.11642708933788, Avg_Score -130.8971072792906\n",
      "Adding trajectory to replay buffer: step 8518, counter 135624\n",
      "Environment 1: Episode 1343, Score -120.51825228510735, Avg_Score -130.88666338956324\n",
      "Adding trajectory to replay buffer: step 8532, counter 135669\n",
      "Environment 3: Episode 1344, Score -116.72900848709916, Avg_Score -130.81403276464323\n",
      "Adding trajectory to replay buffer: step 8536, counter 135720\n",
      "Environment 6: Episode 1345, Score -117.91232019684402, Avg_Score -130.76120561341858\n",
      "Adding trajectory to replay buffer: step 8537, counter 135763\n",
      "Environment 0: Episode 1346, Score -120.08068092402878, Avg_Score -130.72653026491852\n",
      "Adding trajectory to replay buffer: step 8538, counter 135807\n",
      "Environment 8: Episode 1347, Score -119.69963657188106, Avg_Score -130.69034610607468\n",
      "Adding trajectory to replay buffer: step 8538, counter 135870\n",
      "Environment 12: Episode 1348, Score -117.16183483722496, Avg_Score -130.5887932774005\n",
      "Adding trajectory to replay buffer: step 8540, counter 135932\n",
      "Environment 10: Episode 1349, Score -116.68246302435175, Avg_Score -130.5027516508375\n",
      "Adding trajectory to replay buffer: step 8543, counter 135979\n",
      "Environment 9: Episode 1350, Score -120.17961261121991, Avg_Score -130.47054825632318\n",
      "Adding trajectory to replay buffer: step 8561, counter 136024\n",
      "Environment 5: Episode 1351, Score -122.47271951465744, Avg_Score -130.45620501802688\n",
      "Adding trajectory to replay buffer: step 8563, counter 136069\n",
      "Environment 1: Episode 1352, Score -122.71691534851492, Avg_Score -130.43944112469217\n",
      "Adding trajectory to replay buffer: step 8569, counter 136152\n",
      "Environment 11: Episode 1353, Score -112.80905065924613, Avg_Score -130.319182265686\n",
      "Adding trajectory to replay buffer: step 8576, counter 136196\n",
      "Environment 3: Episode 1354, Score -123.38976027395329, Avg_Score -130.08177515401994\n",
      "Adding trajectory to replay buffer: step 8578, counter 136389\n",
      "Environment 4: Episode 1355, Score -119.14277266788048, Avg_Score -130.03787008860078\n",
      "Adding trajectory to replay buffer: step 8579, counter 136431\n",
      "Environment 0: Episode 1356, Score -123.43413164180394, Avg_Score -130.00305346252355\n",
      "Adding trajectory to replay buffer: step 8579, counter 136474\n",
      "Environment 6: Episode 1357, Score -123.14385526093282, Avg_Score -129.70952392070495\n",
      "Adding trajectory to replay buffer: step 8581, counter 136517\n",
      "Environment 8: Episode 1358, Score -123.61909582080507, Avg_Score -129.81072673842954\n",
      "Adding trajectory to replay buffer: step 8581, counter 136560\n",
      "Environment 12: Episode 1359, Score -123.46222624558273, Avg_Score -129.85395841026258\n",
      "Adding trajectory to replay buffer: step 8584, counter 136604\n",
      "Environment 10: Episode 1360, Score -121.74174366451474, Avg_Score -129.8901595882865\n",
      "Adding trajectory to replay buffer: step 8588, counter 136649\n",
      "Environment 9: Episode 1361, Score -123.18278847838752, Avg_Score -129.8482654719236\n",
      "Adding trajectory to replay buffer: step 8588, counter 136784\n",
      "Environment 14: Episode 1362, Score -113.93941917679956, Avg_Score -129.7140326431489\n",
      "Adding trajectory to replay buffer: step 8606, counter 136829\n",
      "Environment 5: Episode 1363, Score -123.31883903997826, Avg_Score -129.38946273925515\n",
      "Adding trajectory to replay buffer: step 8607, counter 136873\n",
      "Environment 1: Episode 1364, Score -122.28508847107055, Avg_Score -129.42590628064823\n",
      "Adding trajectory to replay buffer: step 8607, counter 136976\n",
      "Environment 2: Episode 1365, Score -113.71683779507659, Avg_Score -129.2836249040189\n",
      "Adding trajectory to replay buffer: step 8614, counter 137021\n",
      "Environment 11: Episode 1366, Score -117.52808262837306, Avg_Score -129.24431912944402\n",
      "Adding trajectory to replay buffer: step 8615, counter 137200\n",
      "Environment 13: Episode 1367, Score -120.5112803479489, Avg_Score -129.18206652643264\n",
      "Adding trajectory to replay buffer: step 8618, counter 137387\n",
      "Environment 7: Episode 1368, Score -120.56050934884016, Avg_Score -129.1445041970045\n",
      "Adding trajectory to replay buffer: step 8621, counter 137429\n",
      "Environment 0: Episode 1369, Score -123.0062978394578, Avg_Score -129.11199831376996\n",
      "Adding trajectory to replay buffer: step 8621, counter 137471\n",
      "Environment 6: Episode 1370, Score -122.92239395790298, Avg_Score -129.15796505640384\n",
      "Adding trajectory to replay buffer: step 8623, counter 137518\n",
      "Environment 3: Episode 1371, Score -119.04230384808841, Avg_Score -129.0868363058079\n",
      "Adding trajectory to replay buffer: step 8624, counter 137561\n",
      "Environment 8: Episode 1372, Score -123.08832005309375, Avg_Score -129.05203705308443\n",
      "Adding trajectory to replay buffer: step 8624, counter 137604\n",
      "Environment 12: Episode 1373, Score -123.17215613719696, Avg_Score -128.8451257654833\n",
      "Adding trajectory to replay buffer: step 8625, counter 137651\n",
      "Environment 4: Episode 1374, Score -118.07858974566233, Avg_Score -128.76769620105557\n",
      "Adding trajectory to replay buffer: step 8628, counter 137695\n",
      "Environment 10: Episode 1375, Score -121.98918364616608, Avg_Score -128.7643396756504\n",
      "Adding trajectory to replay buffer: step 8628, counter 137837\n",
      "Environment 15: Episode 1376, Score -116.00798246331684, Avg_Score -128.67860530441433\n",
      "Adding trajectory to replay buffer: step 8634, counter 137883\n",
      "Environment 9: Episode 1377, Score -119.91207028246981, Avg_Score -128.73662129648073\n",
      "Adding trajectory to replay buffer: step 8650, counter 137927\n",
      "Environment 5: Episode 1378, Score -120.81554941937203, Avg_Score -128.73555530750153\n",
      "Adding trajectory to replay buffer: step 8652, counter 137972\n",
      "Environment 1: Episode 1379, Score -120.3193142793694, Avg_Score -128.4686605194842\n",
      "Adding trajectory to replay buffer: step 8654, counter 138019\n",
      "Environment 2: Episode 1380, Score -119.86703160848903, Avg_Score -128.40977858481065\n",
      "Adding trajectory to replay buffer: step 8657, counter 138088\n",
      "Environment 14: Episode 1381, Score -114.00567127619497, Avg_Score -128.28781232177664\n",
      "Adding trajectory to replay buffer: step 8661, counter 138135\n",
      "Environment 11: Episode 1382, Score -116.67494650990889, Avg_Score -128.24729615513925\n",
      "Adding trajectory to replay buffer: step 8665, counter 138179\n",
      "Environment 6: Episode 1383, Score -122.71866615792115, Avg_Score -128.2900820779504\n",
      "Adding trajectory to replay buffer: step 8667, counter 138222\n",
      "Environment 8: Episode 1384, Score -122.82915096365846, Avg_Score -128.0230311299979\n",
      "Adding trajectory to replay buffer: step 8667, counter 138265\n",
      "Environment 12: Episode 1385, Score -122.48713036513328, Avg_Score -128.05005933545715\n",
      "Adding trajectory to replay buffer: step 8667, counter 138317\n",
      "Environment 13: Episode 1386, Score -117.15543722643456, Avg_Score -127.97486645134582\n",
      "Adding trajectory to replay buffer: step 8668, counter 138364\n",
      "Environment 0: Episode 1387, Score -122.41012695069487, Avg_Score -127.94462714028771\n",
      "Adding trajectory to replay buffer: step 8669, counter 138408\n",
      "Environment 4: Episode 1388, Score -123.54691591299635, Avg_Score -127.95337863305457\n",
      "Adding trajectory to replay buffer: step 8671, counter 138451\n",
      "Environment 10: Episode 1389, Score -123.04145388384909, Avg_Score -127.98516836949797\n",
      "Adding trajectory to replay buffer: step 8671, counter 138494\n",
      "Environment 15: Episode 1390, Score -123.89878109079972, Avg_Score -127.93296666821009\n",
      "Adding trajectory to replay buffer: step 8676, counter 138552\n",
      "Environment 7: Episode 1391, Score -116.19757521818144, Avg_Score -127.86094131949734\n",
      "Adding trajectory to replay buffer: step 8676, counter 138594\n",
      "Environment 9: Episode 1392, Score -122.60352163467556, Avg_Score -127.91963772725111\n",
      "Adding trajectory to replay buffer: step 8679, counter 138650\n",
      "Environment 3: Episode 1393, Score -115.50896207918599, Avg_Score -127.89332348714998\n",
      "Adding trajectory to replay buffer: step 8694, counter 138694\n",
      "Environment 5: Episode 1394, Score -121.65623735337084, Avg_Score -127.52236910692976\n",
      "Adding trajectory to replay buffer: step 8696, counter 138738\n",
      "Environment 1: Episode 1395, Score -123.27103554307732, Avg_Score -127.5282960542934\n",
      "Adding trajectory to replay buffer: step 8699, counter 138783\n",
      "Environment 2: Episode 1396, Score -122.73888044782542, Avg_Score -127.5289300778285\n",
      "Adding trajectory to replay buffer: step 8708, counter 138830\n",
      "Environment 11: Episode 1397, Score -121.95828085353598, Avg_Score -127.53285397788774\n",
      "Adding trajectory to replay buffer: step 8709, counter 138872\n",
      "Environment 8: Episode 1398, Score -115.1351133077635, Avg_Score -127.45792595270618\n",
      "Adding trajectory to replay buffer: step 8713, counter 138917\n",
      "Environment 0: Episode 1399, Score -119.61085705586211, Avg_Score -127.46197400294609\n",
      "Adding trajectory to replay buffer: step 8713, counter 138965\n",
      "Environment 6: Episode 1400, Score -119.72663305215352, Avg_Score -127.43496494212091\n",
      "Adding trajectory to replay buffer: step 8713, counter 139011\n",
      "Environment 13: Episode 1401, Score -119.81227656710831, Avg_Score -126.86135894780936\n",
      "Adding trajectory to replay buffer: step 8714, counter 139058\n",
      "Environment 12: Episode 1402, Score -120.69339988136664, Avg_Score -126.8544354861483\n",
      "Adding trajectory to replay buffer: step 8718, counter 139107\n",
      "Environment 4: Episode 1403, Score -117.9242002940476, Avg_Score -126.88169964177868\n",
      "Adding trajectory to replay buffer: step 8723, counter 139154\n",
      "Environment 7: Episode 1404, Score -120.36876595166325, Avg_Score -125.44710798964228\n",
      "Adding trajectory to replay buffer: step 8724, counter 139202\n",
      "Environment 9: Episode 1405, Score -120.63965984320268, Avg_Score -124.11169024374468\n",
      "Adding trajectory to replay buffer: step 8726, counter 139249\n",
      "Environment 3: Episode 1406, Score -120.34100977550447, Avg_Score -123.16764148985916\n",
      "Adding trajectory to replay buffer: step 8737, counter 139292\n",
      "Environment 5: Episode 1407, Score -122.59649702034146, Avg_Score -123.18447669068284\n",
      "Adding trajectory to replay buffer: step 8739, counter 139335\n",
      "Environment 1: Episode 1408, Score -122.27385446537411, Avg_Score -122.26912670570059\n",
      "Adding trajectory to replay buffer: step 8740, counter 139404\n",
      "Environment 15: Episode 1409, Score -111.94843622566573, Avg_Score -121.70969616585472\n",
      "Adding trajectory to replay buffer: step 8743, counter 139448\n",
      "Environment 2: Episode 1410, Score -121.05271711464103, Avg_Score -120.79910150305061\n",
      "Adding trajectory to replay buffer: step 8750, counter 139490\n",
      "Environment 11: Episode 1411, Score -123.06159101495332, Avg_Score -120.88351970759827\n",
      "Adding trajectory to replay buffer: step 8752, counter 139533\n",
      "Environment 8: Episode 1412, Score -123.00124903503806, Avg_Score -120.11785100545266\n",
      "Adding trajectory to replay buffer: step 8756, counter 139575\n",
      "Environment 12: Episode 1413, Score -122.74047627600966, Avg_Score -120.1272129454832\n",
      "Adding trajectory to replay buffer: step 8757, counter 139619\n",
      "Environment 0: Episode 1414, Score -123.26326787410304, Avg_Score -120.13660217706799\n",
      "Adding trajectory to replay buffer: step 8757, counter 139663\n",
      "Environment 6: Episode 1415, Score -123.2439318294966, Avg_Score -120.18931667168286\n",
      "Adding trajectory to replay buffer: step 8757, counter 139707\n",
      "Environment 13: Episode 1416, Score -123.33006255497628, Avg_Score -120.22712758644981\n",
      "Adding trajectory to replay buffer: step 8760, counter 139749\n",
      "Environment 4: Episode 1417, Score -122.24855579379131, Avg_Score -120.23556898992767\n",
      "Adding trajectory to replay buffer: step 8767, counter 139793\n",
      "Environment 7: Episode 1418, Score -121.94336671176906, Avg_Score -120.24124069255679\n",
      "Adding trajectory to replay buffer: step 8768, counter 139890\n",
      "Environment 10: Episode 1419, Score -123.15526769710208, Avg_Score -120.2478334046142\n",
      "Adding trajectory to replay buffer: step 8770, counter 139934\n",
      "Environment 3: Episode 1420, Score -121.98038048173612, Avg_Score -120.30045342065252\n",
      "Adding trajectory to replay buffer: step 8770, counter 139980\n",
      "Environment 9: Episode 1421, Score -118.52151896520517, Avg_Score -120.26844171747099\n",
      "Adding trajectory to replay buffer: step 8791, counter 140034\n",
      "Environment 5: Episode 1422, Score -117.82357053708533, Avg_Score -120.21761310074032\n",
      "Adding trajectory to replay buffer: step 8795, counter 140090\n",
      "Environment 1: Episode 1423, Score -119.1308639412392, Avg_Score -120.1872854052018\n",
      "Adding trajectory to replay buffer: step 8799, counter 140146\n",
      "Environment 2: Episode 1424, Score -118.05881755148434, Avg_Score -120.1661379133268\n",
      "Adding trajectory to replay buffer: step 8802, counter 140192\n",
      "Environment 12: Episode 1425, Score -117.11868250329033, Avg_Score -120.13635559688362\n",
      "Adding trajectory to replay buffer: step 8803, counter 140238\n",
      "Environment 0: Episode 1426, Score -122.9851447700352, Avg_Score -120.2131191195581\n",
      "Adding trajectory to replay buffer: step 8803, counter 140284\n",
      "Environment 13: Episode 1427, Score -119.54830665694364, Avg_Score -120.24450331897084\n",
      "Adding trajectory to replay buffer: step 8804, counter 140331\n",
      "Environment 6: Episode 1428, Score -119.80378543796019, Avg_Score -120.28580848247307\n",
      "Adding trajectory to replay buffer: step 8806, counter 140377\n",
      "Environment 4: Episode 1429, Score -120.48885587504196, Avg_Score -120.3315050738853\n",
      "Adding trajectory to replay buffer: step 8806, counter 140431\n",
      "Environment 8: Episode 1430, Score -113.3207091050769, Avg_Score -120.3296850530867\n",
      "Adding trajectory to replay buffer: step 8814, counter 140475\n",
      "Environment 9: Episode 1431, Score -121.70309108816771, Avg_Score -120.3186127664628\n",
      "Adding trajectory to replay buffer: step 8815, counter 140520\n",
      "Environment 3: Episode 1432, Score -122.59731757468049, Avg_Score -120.31755755661911\n",
      "Adding trajectory to replay buffer: step 8836, counter 140589\n",
      "Environment 7: Episode 1433, Score -113.89591401455614, Avg_Score -120.2743441072998\n",
      "Adding trajectory to replay buffer: step 8836, counter 140657\n",
      "Environment 10: Episode 1434, Score -116.23622383124226, Avg_Score -120.26790213975951\n",
      "Adding trajectory to replay buffer: step 8841, counter 140703\n",
      "Environment 1: Episode 1435, Score -121.37644308163722, Avg_Score -120.26015849130407\n",
      "Adding trajectory to replay buffer: step 8842, counter 140746\n",
      "Environment 2: Episode 1436, Score -121.39729095488912, Avg_Score -120.31397672829786\n",
      "Adding trajectory to replay buffer: step 8846, counter 140790\n",
      "Environment 12: Episode 1437, Score -122.01810973442004, Avg_Score -120.31779509409925\n",
      "Adding trajectory to replay buffer: step 8848, counter 140835\n",
      "Environment 0: Episode 1438, Score -121.29900980904823, Avg_Score -120.37093269928832\n",
      "Adding trajectory to replay buffer: step 8849, counter 140881\n",
      "Environment 13: Episode 1439, Score -120.00770040560948, Avg_Score -120.34489048313804\n",
      "Adding trajectory to replay buffer: step 8850, counter 140925\n",
      "Environment 4: Episode 1440, Score -115.97411984596773, Avg_Score -120.27337676048795\n",
      "Adding trajectory to replay buffer: step 8853, counter 141028\n",
      "Environment 11: Episode 1441, Score -124.20796877755791, Avg_Score -120.30584333444294\n",
      "Adding trajectory to replay buffer: step 8860, counter 141082\n",
      "Environment 8: Episode 1442, Score -111.73731208247195, Avg_Score -120.2020521843743\n",
      "Adding trajectory to replay buffer: step 8862, counter 141129\n",
      "Environment 3: Episode 1443, Score -123.22952913298147, Avg_Score -120.22916495285303\n",
      "Adding trajectory to replay buffer: step 8876, counter 141169\n",
      "Environment 7: Episode 1444, Score -116.39732839833945, Avg_Score -120.22584815196544\n",
      "Adding trajectory to replay buffer: step 8878, counter 141211\n",
      "Environment 10: Episode 1445, Score -116.03579629795811, Avg_Score -120.20708291297659\n",
      "Adding trajectory to replay buffer: step 8883, counter 141253\n",
      "Environment 1: Episode 1446, Score -116.5443194799417, Avg_Score -120.1717192985357\n",
      "Adding trajectory to replay buffer: step 8887, counter 141400\n",
      "Environment 15: Episode 1447, Score -116.6647304217412, Avg_Score -120.14137023703428\n",
      "Adding trajectory to replay buffer: step 8889, counter 141447\n",
      "Environment 2: Episode 1448, Score -119.80020513413328, Avg_Score -120.16775394000337\n",
      "Adding trajectory to replay buffer: step 8893, counter 141683\n",
      "Environment 14: Episode 1449, Score -123.97970437609156, Avg_Score -120.24072635352078\n",
      "Adding trajectory to replay buffer: step 8899, counter 141791\n",
      "Environment 5: Episode 1450, Score -117.98772217151088, Avg_Score -120.21880744912369\n",
      "Adding trajectory to replay buffer: step 8900, counter 141877\n",
      "Environment 9: Episode 1451, Score -111.33760373475216, Avg_Score -120.10745629132464\n",
      "Adding trajectory to replay buffer: step 8900, counter 141924\n",
      "Environment 11: Episode 1452, Score -118.77692973863458, Avg_Score -120.06805643522584\n",
      "Adding trajectory to replay buffer: step 8903, counter 142023\n",
      "Environment 6: Episode 1453, Score -123.46394315160936, Avg_Score -120.17460536014947\n",
      "Adding trajectory to replay buffer: step 8904, counter 142067\n",
      "Environment 8: Episode 1454, Score -123.20354282685928, Avg_Score -120.17274318567854\n",
      "Adding trajectory to replay buffer: step 8907, counter 142126\n",
      "Environment 0: Episode 1455, Score -113.18444603265698, Avg_Score -120.11315991932629\n",
      "Adding trajectory to replay buffer: step 8920, counter 142170\n",
      "Environment 7: Episode 1456, Score -123.1780509509885, Avg_Score -120.11059911241814\n",
      "Adding trajectory to replay buffer: step 8921, counter 142213\n",
      "Environment 10: Episode 1457, Score -123.43419802503165, Avg_Score -120.11350254005913\n",
      "Adding trajectory to replay buffer: step 8926, counter 142290\n",
      "Environment 13: Episode 1458, Score -116.2713334479065, Avg_Score -120.04002491633015\n",
      "Adding trajectory to replay buffer: step 8929, counter 142336\n",
      "Environment 1: Episode 1459, Score -118.75931251299816, Avg_Score -119.99299577900432\n",
      "Adding trajectory to replay buffer: step 8933, counter 142382\n",
      "Environment 15: Episode 1460, Score -121.54968392306193, Avg_Score -119.99107518158976\n",
      "Adding trajectory to replay buffer: step 8936, counter 142429\n",
      "Environment 2: Episode 1461, Score -121.79639801483167, Avg_Score -119.97721127695422\n",
      "Adding trajectory to replay buffer: step 8941, counter 142471\n",
      "Environment 5: Episode 1462, Score -123.10791323809202, Avg_Score -120.06889621756716\n",
      "Adding trajectory to replay buffer: step 8942, counter 142513\n",
      "Environment 9: Episode 1463, Score -123.05562635181224, Avg_Score -120.0662640906855\n",
      "Adding trajectory to replay buffer: step 8942, counter 142555\n",
      "Environment 11: Episode 1464, Score -123.01157606233843, Avg_Score -120.07352896659818\n",
      "Adding trajectory to replay buffer: step 8942, counter 142604\n",
      "Environment 14: Episode 1465, Score -122.97433783192498, Avg_Score -120.16610396696667\n",
      "Adding trajectory to replay buffer: step 8947, counter 142648\n",
      "Environment 6: Episode 1466, Score -121.89033171455189, Avg_Score -120.20972645782845\n",
      "Adding trajectory to replay buffer: step 8947, counter 142691\n",
      "Environment 8: Episode 1467, Score -122.25019336628728, Avg_Score -120.22711558801186\n",
      "Adding trajectory to replay buffer: step 8950, counter 142734\n",
      "Environment 0: Episode 1468, Score -122.34144937608578, Avg_Score -120.24492498828432\n",
      "Adding trajectory to replay buffer: step 8953, counter 142841\n",
      "Environment 12: Episode 1469, Score -115.819582667976, Avg_Score -120.17305783656951\n",
      "Adding trajectory to replay buffer: step 8964, counter 142943\n",
      "Environment 3: Episode 1470, Score -122.18411936780927, Avg_Score -120.16567509066859\n",
      "Adding trajectory to replay buffer: step 8970, counter 142992\n",
      "Environment 10: Episode 1471, Score -116.47933757703007, Avg_Score -120.14004542795801\n",
      "Adding trajectory to replay buffer: step 8973, counter 143036\n",
      "Environment 1: Episode 1472, Score -123.26918885037054, Avg_Score -120.14185411593078\n",
      "Adding trajectory to replay buffer: step 8976, counter 143079\n",
      "Environment 15: Episode 1473, Score -122.55409993624066, Avg_Score -120.1356735539212\n",
      "Adding trajectory to replay buffer: step 8979, counter 143122\n",
      "Environment 2: Episode 1474, Score -123.04761071601169, Avg_Score -120.18536376362471\n",
      "Adding trajectory to replay buffer: step 8983, counter 143164\n",
      "Environment 5: Episode 1475, Score -123.21579821398731, Avg_Score -120.1976299093029\n",
      "Adding trajectory to replay buffer: step 8985, counter 143207\n",
      "Environment 9: Episode 1476, Score -123.23443703308578, Avg_Score -120.26989445500061\n",
      "Adding trajectory to replay buffer: step 8985, counter 143250\n",
      "Environment 11: Episode 1477, Score -123.56906783039364, Avg_Score -120.30646443047985\n",
      "Adding trajectory to replay buffer: step 8985, counter 143309\n",
      "Environment 13: Episode 1478, Score -115.98314804933717, Avg_Score -120.25814041677951\n",
      "Adding trajectory to replay buffer: step 8985, counter 143352\n",
      "Environment 14: Episode 1479, Score -123.28754334489506, Avg_Score -120.28782270743477\n",
      "Adding trajectory to replay buffer: step 8991, counter 143396\n",
      "Environment 6: Episode 1480, Score -122.99298803425891, Avg_Score -120.31908227169248\n",
      "Adding trajectory to replay buffer: step 8991, counter 143440\n",
      "Environment 8: Episode 1481, Score -123.15273503192824, Avg_Score -120.41055290924983\n",
      "Adding trajectory to replay buffer: step 8996, counter 143486\n",
      "Environment 0: Episode 1482, Score -121.47167933135418, Avg_Score -120.45852023746427\n",
      "Adding trajectory to replay buffer: step 8996, counter 143529\n",
      "Environment 12: Episode 1483, Score -123.34730457170556, Avg_Score -120.46480662160212\n",
      "Adding trajectory to replay buffer: step 9004, counter 143683\n",
      "Environment 4: Episode 1484, Score -130.90629873477482, Avg_Score -120.5455780993133\n",
      "Adding trajectory to replay buffer: step 9012, counter 143725\n",
      "Environment 10: Episode 1485, Score -120.41804177446974, Avg_Score -120.52488721340666\n",
      "Adding trajectory to replay buffer: step 9015, counter 143776\n",
      "Environment 3: Episode 1486, Score -115.17376509164895, Avg_Score -120.5050704920588\n",
      "Adding trajectory to replay buffer: step 9018, counter 143821\n",
      "Environment 1: Episode 1487, Score -122.98880776364543, Avg_Score -120.5108573001883\n",
      "Adding trajectory to replay buffer: step 9021, counter 143866\n",
      "Environment 15: Episode 1488, Score -121.66298232387255, Avg_Score -120.49201796429708\n",
      "Adding trajectory to replay buffer: step 9023, counter 143910\n",
      "Environment 2: Episode 1489, Score -122.24164645988309, Avg_Score -120.48401989005743\n",
      "Adding trajectory to replay buffer: step 9027, counter 143952\n",
      "Environment 11: Episode 1490, Score -122.57978979409796, Avg_Score -120.4708299770904\n",
      "Adding trajectory to replay buffer: step 9028, counter 143995\n",
      "Environment 9: Episode 1491, Score -123.16110037638683, Avg_Score -120.54046522867243\n",
      "Adding trajectory to replay buffer: step 9028, counter 144038\n",
      "Environment 13: Episode 1492, Score -123.26704184270898, Avg_Score -120.54710043075278\n",
      "Adding trajectory to replay buffer: step 9028, counter 144081\n",
      "Environment 14: Episode 1493, Score -123.13572151681223, Avg_Score -120.62336802512904\n",
      "Adding trajectory to replay buffer: step 9035, counter 144125\n",
      "Environment 8: Episode 1494, Score -122.90963593583616, Avg_Score -120.63590201095369\n",
      "Adding trajectory to replay buffer: step 9037, counter 144171\n",
      "Environment 6: Episode 1495, Score -120.55652199446472, Avg_Score -120.60875687546758\n",
      "Adding trajectory to replay buffer: step 9040, counter 144215\n",
      "Environment 12: Episode 1496, Score -116.56516877628118, Avg_Score -120.54701975875213\n",
      "Adding trajectory to replay buffer: step 9041, counter 144260\n",
      "Environment 0: Episode 1497, Score -117.50511861083595, Avg_Score -120.50248813632514\n",
      "Adding trajectory to replay buffer: step 9045, counter 144301\n",
      "Environment 4: Episode 1498, Score -116.57570249895504, Avg_Score -120.51689402823703\n",
      "Adding trajectory to replay buffer: step 9045, counter 144426\n",
      "Environment 7: Episode 1499, Score -127.82969358172393, Avg_Score -120.59908239349562\n",
      "Adding trajectory to replay buffer: step 9061, counter 144469\n",
      "Environment 1: Episode 1500, Score -120.04346048541493, Avg_Score -120.60225066782823\n",
      "Adding trajectory to replay buffer: step 9067, counter 144515\n",
      "Environment 15: Episode 1501, Score -121.14811923365792, Avg_Score -120.6156090944937\n",
      "Adding trajectory to replay buffer: step 9069, counter 144561\n",
      "Environment 2: Episode 1502, Score -120.63002367138365, Avg_Score -120.61497533239388\n",
      "Adding trajectory to replay buffer: step 9069, counter 144618\n",
      "Environment 10: Episode 1503, Score -115.15531619868304, Avg_Score -120.58728649144027\n",
      "Adding trajectory to replay buffer: step 9070, counter 144660\n",
      "Environment 13: Episode 1504, Score -123.47241243294627, Avg_Score -120.61832295625308\n",
      "Adding trajectory to replay buffer: step 9072, counter 144705\n",
      "Environment 11: Episode 1505, Score -121.42773720305786, Avg_Score -120.62620372985162\n",
      "Adding trajectory to replay buffer: step 9072, counter 144749\n",
      "Environment 14: Episode 1506, Score -122.54904416442973, Avg_Score -120.64828407374088\n",
      "Adding trajectory to replay buffer: step 9081, counter 144815\n",
      "Environment 3: Episode 1507, Score -117.08290445313912, Avg_Score -120.59314814806885\n",
      "Adding trajectory to replay buffer: step 9082, counter 144860\n",
      "Environment 6: Episode 1508, Score -122.26788383844183, Avg_Score -120.5930884417995\n",
      "Adding trajectory to replay buffer: step 9086, counter 144905\n",
      "Environment 0: Episode 1509, Score -121.70095485228529, Avg_Score -120.69061362806569\n",
      "Adding trajectory to replay buffer: step 9099, counter 144964\n",
      "Environment 12: Episode 1510, Score -121.39739108109474, Avg_Score -120.69406036773023\n",
      "Adding trajectory to replay buffer: step 9105, counter 145008\n",
      "Environment 1: Episode 1511, Score -122.87681005830504, Avg_Score -120.69221255816376\n",
      "Adding trajectory to replay buffer: step 9112, counter 145051\n",
      "Environment 2: Episode 1512, Score -122.28627398957684, Avg_Score -120.68506280770916\n",
      "Adding trajectory to replay buffer: step 9112, counter 145094\n",
      "Environment 10: Episode 1513, Score -122.33099702190546, Avg_Score -120.6809680151681\n",
      "Adding trajectory to replay buffer: step 9114, counter 145141\n",
      "Environment 15: Episode 1514, Score -118.68979045689417, Avg_Score -120.63523324099602\n",
      "Adding trajectory to replay buffer: step 9116, counter 145185\n",
      "Environment 11: Episode 1515, Score -121.89681954364727, Avg_Score -120.62176211813753\n",
      "Adding trajectory to replay buffer: step 9116, counter 145229\n",
      "Environment 14: Episode 1516, Score -121.98700250465484, Avg_Score -120.60833151763431\n",
      "Adding trajectory to replay buffer: step 9118, counter 145319\n",
      "Environment 9: Episode 1517, Score -114.22925383756248, Avg_Score -120.52813849807202\n",
      "Adding trajectory to replay buffer: step 9123, counter 145459\n",
      "Environment 5: Episode 1518, Score -119.26609981391763, Avg_Score -120.5013658290935\n",
      "Adding trajectory to replay buffer: step 9124, counter 145502\n",
      "Environment 3: Episode 1519, Score -118.51394632927888, Avg_Score -120.45495261541524\n",
      "Adding trajectory to replay buffer: step 9126, counter 145546\n",
      "Environment 6: Episode 1520, Score -118.55254281205747, Avg_Score -120.42067423871848\n",
      "Adding trajectory to replay buffer: step 9129, counter 145589\n",
      "Environment 0: Episode 1521, Score -123.67764813875034, Avg_Score -120.47223553045393\n",
      "Adding trajectory to replay buffer: step 9134, counter 145688\n",
      "Environment 8: Episode 1522, Score -126.63615789766796, Avg_Score -120.56036140405976\n",
      "Adding trajectory to replay buffer: step 9144, counter 145733\n",
      "Environment 12: Episode 1523, Score -122.99002202753537, Avg_Score -120.5989529849227\n",
      "Adding trajectory to replay buffer: step 9156, counter 145777\n",
      "Environment 2: Episode 1524, Score -122.51585058350426, Avg_Score -120.64352331524292\n",
      "Adding trajectory to replay buffer: step 9156, counter 145821\n",
      "Environment 10: Episode 1525, Score -122.46607612938621, Avg_Score -120.69699725150387\n",
      "Adding trajectory to replay buffer: step 9158, counter 145865\n",
      "Environment 15: Episode 1526, Score -122.8184231041409, Avg_Score -120.69533003484494\n",
      "Adding trajectory to replay buffer: step 9159, counter 145908\n",
      "Environment 11: Episode 1527, Score -122.8332062675804, Avg_Score -120.72817903095128\n",
      "Adding trajectory to replay buffer: step 9159, counter 145951\n",
      "Environment 14: Episode 1528, Score -122.67863548542746, Avg_Score -120.75692753142596\n",
      "Adding trajectory to replay buffer: step 9161, counter 145994\n",
      "Environment 9: Episode 1529, Score -122.34695327477468, Avg_Score -120.77550850542328\n",
      "Adding trajectory to replay buffer: step 9164, counter 146034\n",
      "Environment 3: Episode 1530, Score -117.26075864391277, Avg_Score -120.81490900081164\n",
      "Adding trajectory to replay buffer: step 9166, counter 146077\n",
      "Environment 5: Episode 1531, Score -122.903570553964, Avg_Score -120.82691379546961\n",
      "Adding trajectory to replay buffer: step 9173, counter 146121\n",
      "Environment 0: Episode 1532, Score -122.65140715669499, Avg_Score -120.82745469128977\n",
      "Adding trajectory to replay buffer: step 9177, counter 146228\n",
      "Environment 13: Episode 1533, Score -113.71220818879411, Avg_Score -120.82561763303214\n",
      "Adding trajectory to replay buffer: step 9178, counter 146272\n",
      "Environment 8: Episode 1534, Score -122.58222069434822, Avg_Score -120.88907760166322\n",
      "Adding trajectory to replay buffer: step 9187, counter 146315\n",
      "Environment 12: Episode 1535, Score -122.7694597720839, Avg_Score -120.90300776856768\n",
      "Adding trajectory to replay buffer: step 9200, counter 146359\n",
      "Environment 2: Episode 1536, Score -120.47697717047296, Avg_Score -120.89380463072352\n",
      "Adding trajectory to replay buffer: step 9200, counter 146403\n",
      "Environment 10: Episode 1537, Score -120.54979959809478, Avg_Score -120.87912152936025\n",
      "Adding trajectory to replay buffer: step 9201, counter 146446\n",
      "Environment 15: Episode 1538, Score -122.3980072678874, Avg_Score -120.89011150394865\n",
      "Adding trajectory to replay buffer: step 9203, counter 146490\n",
      "Environment 11: Episode 1539, Score -122.63199689626445, Avg_Score -120.9163544688552\n",
      "Adding trajectory to replay buffer: step 9205, counter 146536\n",
      "Environment 14: Episode 1540, Score -120.16060093893235, Avg_Score -120.95821927978484\n",
      "Adding trajectory to replay buffer: step 9207, counter 146582\n",
      "Environment 9: Episode 1541, Score -120.96528353889349, Avg_Score -120.9257924273982\n",
      "Adding trajectory to replay buffer: step 9212, counter 146749\n",
      "Environment 7: Episode 1542, Score -119.19011331625501, Avg_Score -121.00032043973604\n",
      "Adding trajectory to replay buffer: step 9223, counter 146806\n",
      "Environment 5: Episode 1543, Score -119.88859628385926, Avg_Score -120.96691111124483\n",
      "Adding trajectory to replay buffer: step 9232, counter 146851\n",
      "Environment 12: Episode 1544, Score -118.70830545950619, Avg_Score -120.99002088185647\n",
      "Adding trajectory to replay buffer: step 9237, counter 146924\n",
      "Environment 3: Episode 1545, Score -111.37942658781323, Avg_Score -120.94345718475505\n",
      "Adding trajectory to replay buffer: step 9243, counter 146967\n",
      "Environment 2: Episode 1546, Score -123.13633421825432, Avg_Score -121.00937733213816\n",
      "Adding trajectory to replay buffer: step 9246, counter 147013\n",
      "Environment 10: Episode 1547, Score -120.64589842159674, Avg_Score -121.04918901213671\n",
      "Adding trajectory to replay buffer: step 9246, counter 147056\n",
      "Environment 11: Episode 1548, Score -118.25273145050306, Avg_Score -121.03371427530041\n",
      "Adding trajectory to replay buffer: step 9246, counter 147101\n",
      "Environment 15: Episode 1549, Score -122.33252879313193, Avg_Score -121.01724251947078\n",
      "Adding trajectory to replay buffer: step 9248, counter 147172\n",
      "Environment 13: Episode 1550, Score -118.72172902043661, Avg_Score -121.02458258796005\n",
      "Adding trajectory to replay buffer: step 9250, counter 147217\n",
      "Environment 14: Episode 1551, Score -120.17548563589466, Avg_Score -121.11296140697146\n",
      "Adding trajectory to replay buffer: step 9257, counter 147262\n",
      "Environment 7: Episode 1552, Score -121.27048030724004, Avg_Score -121.13789691265751\n",
      "Adding trajectory to replay buffer: step 9258, counter 147342\n",
      "Environment 8: Episode 1553, Score -122.52140878368355, Avg_Score -121.12847156897826\n",
      "Adding trajectory to replay buffer: step 9258, counter 147393\n",
      "Environment 9: Episode 1554, Score -116.76034990604222, Avg_Score -121.06403963977007\n",
      "Adding trajectory to replay buffer: step 9261, counter 147609\n",
      "Environment 4: Episode 1555, Score -122.81920381798594, Avg_Score -121.16038721762335\n",
      "Adding trajectory to replay buffer: step 9265, counter 147651\n",
      "Environment 5: Episode 1556, Score -122.59017887317202, Avg_Score -121.1545084968452\n",
      "Adding trajectory to replay buffer: step 9281, counter 147695\n",
      "Environment 3: Episode 1557, Score -122.50902888924877, Avg_Score -121.14525680548736\n",
      "Adding trajectory to replay buffer: step 9291, counter 147743\n",
      "Environment 2: Episode 1558, Score -117.45844897667195, Avg_Score -121.15712796077501\n",
      "Adding trajectory to replay buffer: step 9291, counter 147788\n",
      "Environment 11: Episode 1559, Score -121.81325117361607, Avg_Score -121.18766734738121\n",
      "Adding trajectory to replay buffer: step 9292, counter 147834\n",
      "Environment 10: Episode 1560, Score -121.49023452427983, Avg_Score -121.18707285339339\n",
      "Adding trajectory to replay buffer: step 9292, counter 147880\n",
      "Environment 15: Episode 1561, Score -120.250198836059, Avg_Score -121.17161086160564\n",
      "Adding trajectory to replay buffer: step 9294, counter 147924\n",
      "Environment 14: Episode 1562, Score -122.87989434375739, Avg_Score -121.16933067266231\n",
      "Adding trajectory to replay buffer: step 9296, counter 147988\n",
      "Environment 12: Episode 1563, Score -115.89905646520667, Avg_Score -121.09776497379625\n",
      "Adding trajectory to replay buffer: step 9304, counter 148035\n",
      "Environment 7: Episode 1564, Score -118.990714556812, Avg_Score -121.05755635874097\n",
      "Adding trajectory to replay buffer: step 9305, counter 148214\n",
      "Environment 6: Episode 1565, Score -122.973779733321, Avg_Score -121.05755077775495\n",
      "Adding trajectory to replay buffer: step 9307, counter 148260\n",
      "Environment 4: Episode 1566, Score -120.71658493022807, Avg_Score -121.04581330991168\n",
      "Adding trajectory to replay buffer: step 9310, counter 148305\n",
      "Environment 5: Episode 1567, Score -121.81828900953136, Avg_Score -121.04149426634416\n",
      "Adding trajectory to replay buffer: step 9313, counter 148370\n",
      "Environment 13: Episode 1568, Score -117.95533323759834, Avg_Score -120.99763310495929\n",
      "Adding trajectory to replay buffer: step 9326, counter 148415\n",
      "Environment 3: Episode 1569, Score -122.36819873618336, Avg_Score -121.06311926564136\n",
      "Adding trajectory to replay buffer: step 9328, counter 148485\n",
      "Environment 8: Episode 1570, Score -113.42612934026401, Avg_Score -120.97553936536589\n",
      "Adding trajectory to replay buffer: step 9328, counter 148555\n",
      "Environment 9: Episode 1571, Score -117.67143375635769, Avg_Score -120.98746032715917\n",
      "Adding trajectory to replay buffer: step 9334, counter 148598\n",
      "Environment 2: Episode 1572, Score -122.08913289587758, Avg_Score -120.97565976761423\n",
      "Adding trajectory to replay buffer: step 9334, counter 148641\n",
      "Environment 11: Episode 1573, Score -121.74897178804378, Avg_Score -120.96760848613226\n",
      "Adding trajectory to replay buffer: step 9335, counter 148684\n",
      "Environment 15: Episode 1574, Score -123.49941409004356, Avg_Score -120.97212651987256\n",
      "Adding trajectory to replay buffer: step 9336, counter 148728\n",
      "Environment 10: Episode 1575, Score -122.43714049007247, Avg_Score -120.96433994263342\n",
      "Adding trajectory to replay buffer: step 9342, counter 148774\n",
      "Environment 12: Episode 1576, Score -120.73263589420915, Avg_Score -120.93932193124466\n",
      "Adding trajectory to replay buffer: step 9350, counter 148819\n",
      "Environment 6: Episode 1577, Score -122.22193926678412, Avg_Score -120.92585064560858\n",
      "Adding trajectory to replay buffer: step 9355, counter 148864\n",
      "Environment 5: Episode 1578, Score -123.49187147229415, Avg_Score -121.00093787983813\n",
      "Adding trajectory to replay buffer: step 9359, counter 148910\n",
      "Environment 13: Episode 1579, Score -120.25588221681552, Avg_Score -120.97062126855734\n",
      "Adding trajectory to replay buffer: step 9367, counter 149172\n",
      "Environment 1: Episode 1580, Score -126.4032316005478, Avg_Score -121.00472370422025\n",
      "Adding trajectory to replay buffer: step 9369, counter 149215\n",
      "Environment 3: Episode 1581, Score -122.0224803185407, Avg_Score -120.99342115708637\n",
      "Adding trajectory to replay buffer: step 9371, counter 149258\n",
      "Environment 8: Episode 1582, Score -122.48405416903583, Avg_Score -121.0035449054632\n",
      "Adding trajectory to replay buffer: step 9371, counter 149301\n",
      "Environment 9: Episode 1583, Score -122.49901476582761, Avg_Score -120.99506200740441\n",
      "Adding trajectory to replay buffer: step 9377, counter 149344\n",
      "Environment 11: Episode 1584, Score -120.2202912851122, Avg_Score -120.88820193290779\n",
      "Adding trajectory to replay buffer: step 9378, counter 149549\n",
      "Environment 0: Episode 1585, Score -120.76022422110289, Avg_Score -120.89162375737413\n",
      "Adding trajectory to replay buffer: step 9378, counter 149592\n",
      "Environment 15: Episode 1586, Score -120.00791899568898, Avg_Score -120.93996529641454\n",
      "Adding trajectory to replay buffer: step 9379, counter 149635\n",
      "Environment 10: Episode 1587, Score -121.65706331967563, Avg_Score -120.92664785197483\n",
      "Adding trajectory to replay buffer: step 9383, counter 149714\n",
      "Environment 7: Episode 1588, Score -116.5140495693249, Avg_Score -120.87515852442937\n",
      "Adding trajectory to replay buffer: step 9386, counter 149758\n",
      "Environment 12: Episode 1589, Score -122.74202572776626, Avg_Score -120.8801623171082\n",
      "Adding trajectory to replay buffer: step 9393, counter 149801\n",
      "Environment 6: Episode 1590, Score -122.9015606901956, Avg_Score -120.88338002606919\n",
      "Adding trajectory to replay buffer: step 9397, counter 149904\n",
      "Environment 14: Episode 1591, Score -120.73439989260086, Avg_Score -120.85911302123131\n",
      "Adding trajectory to replay buffer: step 9398, counter 149947\n",
      "Environment 5: Episode 1592, Score -122.22431454554263, Avg_Score -120.84868574825967\n",
      "Adding trajectory to replay buffer: step 9410, counter 150050\n",
      "Environment 4: Episode 1593, Score -113.88902930043265, Avg_Score -120.7562188260959\n",
      "Adding trajectory to replay buffer: step 9413, counter 150096\n",
      "Environment 1: Episode 1594, Score -115.74967467071923, Avg_Score -120.6846192134447\n",
      "Adding trajectory to replay buffer: step 9426, counter 150139\n",
      "Environment 7: Episode 1595, Score -122.51388548945324, Avg_Score -120.70419284839461\n",
      "Adding trajectory to replay buffer: step 9435, counter 150205\n",
      "Environment 3: Episode 1596, Score -113.70306471880899, Avg_Score -120.67557180781989\n",
      "Adding trajectory to replay buffer: step 9437, counter 150264\n",
      "Environment 0: Episode 1597, Score -115.97417326998183, Avg_Score -120.66026235441136\n",
      "Adding trajectory to replay buffer: step 9440, counter 150333\n",
      "Environment 9: Episode 1598, Score -119.20643702290208, Avg_Score -120.68656969965083\n",
      "Adding trajectory to replay buffer: step 9441, counter 150395\n",
      "Environment 10: Episode 1599, Score -116.66428664040194, Avg_Score -120.5749156302376\n",
      "Adding trajectory to replay buffer: step 9442, counter 150440\n",
      "Environment 14: Episode 1600, Score -120.35638665731345, Avg_Score -120.57804489195658\n",
      "Adding trajectory to replay buffer: step 9443, counter 150497\n",
      "Environment 12: Episode 1601, Score -113.53376986520459, Avg_Score -120.50190139827204\n",
      "Adding trajectory to replay buffer: step 9447, counter 150546\n",
      "Environment 5: Episode 1602, Score -120.32984769066859, Avg_Score -120.4988996384649\n",
      "Adding trajectory to replay buffer: step 9452, counter 150605\n",
      "Environment 6: Episode 1603, Score -114.96474466520549, Avg_Score -120.49699392313012\n",
      "Adding trajectory to replay buffer: step 9455, counter 150647\n",
      "Environment 1: Episode 1604, Score -116.23301673260767, Avg_Score -120.42459996612673\n",
      "Adding trajectory to replay buffer: step 9470, counter 150691\n",
      "Environment 7: Episode 1605, Score -122.91428620649906, Avg_Score -120.43946545616114\n",
      "Adding trajectory to replay buffer: step 9479, counter 150811\n",
      "Environment 13: Episode 1606, Score -126.82310579178979, Avg_Score -120.48220607243475\n",
      "Adding trajectory to replay buffer: step 9483, counter 150857\n",
      "Environment 0: Episode 1607, Score -123.46751883222225, Avg_Score -120.54605221622556\n",
      "Adding trajectory to replay buffer: step 9484, counter 150900\n",
      "Environment 10: Episode 1608, Score -123.15573723519842, Avg_Score -120.55493075019312\n",
      "Adding trajectory to replay buffer: step 9484, counter 150942\n",
      "Environment 14: Episode 1609, Score -117.25729725246566, Avg_Score -120.51049417419492\n",
      "Adding trajectory to replay buffer: step 9488, counter 150995\n",
      "Environment 3: Episode 1610, Score -117.81931637209405, Avg_Score -120.4747134271049\n",
      "Adding trajectory to replay buffer: step 9488, counter 151043\n",
      "Environment 9: Episode 1611, Score -122.11093969978765, Avg_Score -120.46705472351972\n",
      "Adding trajectory to replay buffer: step 9489, counter 151089\n",
      "Environment 12: Episode 1612, Score -119.66450585633147, Avg_Score -120.44083704218728\n",
      "Adding trajectory to replay buffer: step 9489, counter 151200\n",
      "Environment 15: Episode 1613, Score -113.78104763060497, Avg_Score -120.35533754827428\n",
      "Adding trajectory to replay buffer: step 9490, counter 151243\n",
      "Environment 5: Episode 1614, Score -123.4908807590995, Avg_Score -120.40334845129635\n",
      "Adding trajectory to replay buffer: step 9495, counter 151286\n",
      "Environment 6: Episode 1615, Score -119.6245434537648, Avg_Score -120.38062569039752\n",
      "Adding trajectory to replay buffer: step 9501, counter 151332\n",
      "Environment 1: Episode 1616, Score -120.15060089375451, Avg_Score -120.3622616742885\n",
      "Adding trajectory to replay buffer: step 9513, counter 151375\n",
      "Environment 7: Episode 1617, Score -121.68972670595409, Avg_Score -120.43686640297243\n",
      "Adding trajectory to replay buffer: step 9529, counter 151570\n",
      "Environment 2: Episode 1618, Score -121.44334257168167, Avg_Score -120.45863883055009\n",
      "Adding trajectory to replay buffer: step 9530, counter 151616\n",
      "Environment 14: Episode 1619, Score -123.16994678580792, Avg_Score -120.50519883511535\n",
      "Adding trajectory to replay buffer: step 9532, counter 151665\n",
      "Environment 0: Episode 1620, Score -122.86610618773926, Avg_Score -120.54833446887216\n",
      "Adding trajectory to replay buffer: step 9532, counter 151709\n",
      "Environment 3: Episode 1621, Score -117.39747476036302, Avg_Score -120.48553273508828\n",
      "Adding trajectory to replay buffer: step 9533, counter 151754\n",
      "Environment 9: Episode 1622, Score -122.34092365266632, Avg_Score -120.44258039263826\n",
      "Adding trajectory to replay buffer: step 9535, counter 151800\n",
      "Environment 12: Episode 1623, Score -121.99956397382108, Avg_Score -120.43267581210114\n",
      "Adding trajectory to replay buffer: step 9538, counter 151843\n",
      "Environment 6: Episode 1624, Score -122.03218239623496, Avg_Score -120.42783913022842\n",
      "Adding trajectory to replay buffer: step 9543, counter 151976\n",
      "Environment 4: Episode 1625, Score -114.91821633973221, Avg_Score -120.35236053233187\n",
      "Adding trajectory to replay buffer: step 9546, counter 152021\n",
      "Environment 1: Episode 1626, Score -121.70089037442705, Avg_Score -120.34118520503475\n",
      "Adding trajectory to replay buffer: step 9546, counter 152190\n",
      "Environment 11: Episode 1627, Score -119.96364192093455, Avg_Score -120.31248956156827\n",
      "Adding trajectory to replay buffer: step 9547, counter 152258\n",
      "Environment 13: Episode 1628, Score -117.85973552662756, Avg_Score -120.26430056198028\n",
      "Adding trajectory to replay buffer: step 9555, counter 152324\n",
      "Environment 15: Episode 1629, Score -116.85112615984058, Avg_Score -120.20934229083096\n",
      "Adding trajectory to replay buffer: step 9558, counter 152369\n",
      "Environment 7: Episode 1630, Score -121.64480496088663, Avg_Score -120.25318275400068\n",
      "Adding trajectory to replay buffer: step 9559, counter 152444\n",
      "Environment 10: Episode 1631, Score -118.76423098579073, Avg_Score -120.21178935831895\n",
      "Adding trajectory to replay buffer: step 9568, counter 152522\n",
      "Environment 5: Episode 1632, Score -119.43503093290826, Avg_Score -120.17962559608108\n",
      "Adding trajectory to replay buffer: step 9575, counter 152567\n",
      "Environment 14: Episode 1633, Score -121.19581540618759, Avg_Score -120.25446166825499\n",
      "Adding trajectory to replay buffer: step 9576, counter 152614\n",
      "Environment 2: Episode 1634, Score -118.07764375039935, Avg_Score -120.20941589881551\n",
      "Adding trajectory to replay buffer: step 9577, counter 152659\n",
      "Environment 0: Episode 1635, Score -121.89223942381143, Avg_Score -120.20064369533276\n",
      "Adding trajectory to replay buffer: step 9577, counter 152704\n",
      "Environment 3: Episode 1636, Score -122.26881359205085, Avg_Score -120.21856205954855\n",
      "Adding trajectory to replay buffer: step 9577, counter 152748\n",
      "Environment 9: Episode 1637, Score -122.02409489087574, Avg_Score -120.23330501247638\n",
      "Adding trajectory to replay buffer: step 9586, counter 152796\n",
      "Environment 6: Episode 1638, Score -122.29761487336705, Avg_Score -120.23230108853117\n",
      "Adding trajectory to replay buffer: step 9592, counter 152842\n",
      "Environment 11: Episode 1639, Score -116.90895208587288, Avg_Score -120.17507064042728\n",
      "Adding trajectory to replay buffer: step 9594, counter 153065\n",
      "Environment 8: Episode 1640, Score -123.2536402575299, Avg_Score -120.20600103361325\n",
      "Adding trajectory to replay buffer: step 9601, counter 153120\n",
      "Environment 1: Episode 1641, Score -111.35397949601648, Avg_Score -120.10988799318447\n",
      "Adding trajectory to replay buffer: step 9603, counter 153165\n",
      "Environment 7: Episode 1642, Score -120.75666677302185, Avg_Score -120.12555352775216\n",
      "Adding trajectory to replay buffer: step 9603, counter 153209\n",
      "Environment 10: Episode 1643, Score -123.35596315330577, Avg_Score -120.16022719644663\n",
      "Adding trajectory to replay buffer: step 9613, counter 153267\n",
      "Environment 15: Episode 1644, Score -115.47109479445406, Avg_Score -120.12785508979609\n",
      "Adding trajectory to replay buffer: step 9616, counter 153308\n",
      "Environment 14: Episode 1645, Score -116.75810417298725, Avg_Score -120.18164186564782\n",
      "Adding trajectory to replay buffer: step 9619, counter 153359\n",
      "Environment 5: Episode 1646, Score -119.97464127155393, Avg_Score -120.15002493618081\n",
      "Adding trajectory to replay buffer: step 9631, counter 153404\n",
      "Environment 6: Episode 1647, Score -122.37900945873311, Avg_Score -120.16735604655217\n",
      "Adding trajectory to replay buffer: step 9634, counter 153461\n",
      "Environment 0: Episode 1648, Score -114.8549125229319, Avg_Score -120.13337785727646\n",
      "Adding trajectory to replay buffer: step 9635, counter 153519\n",
      "Environment 3: Episode 1649, Score -115.13249110453017, Avg_Score -120.06137748039046\n",
      "Adding trajectory to replay buffer: step 9636, counter 153563\n",
      "Environment 11: Episode 1650, Score -119.78646113899785, Avg_Score -120.07202480157606\n",
      "Adding trajectory to replay buffer: step 9639, counter 153608\n",
      "Environment 8: Episode 1651, Score -120.15904855680031, Avg_Score -120.07186043078511\n",
      "Adding trajectory to replay buffer: step 9647, counter 153652\n",
      "Environment 7: Episode 1652, Score -121.73698620443047, Avg_Score -120.076525489757\n",
      "Adding trajectory to replay buffer: step 9647, counter 153696\n",
      "Environment 10: Episode 1653, Score -121.84347357715603, Avg_Score -120.06974613769177\n",
      "Adding trajectory to replay buffer: step 9655, counter 153750\n",
      "Environment 1: Episode 1654, Score -112.62196221941026, Avg_Score -120.02836226082542\n",
      "Adding trajectory to replay buffer: step 9658, counter 153831\n",
      "Environment 9: Episode 1655, Score -121.89090854215713, Avg_Score -120.01907930806712\n",
      "Adding trajectory to replay buffer: step 9663, counter 153878\n",
      "Environment 14: Episode 1656, Score -122.15613482734251, Avg_Score -120.01473886760881\n",
      "Adding trajectory to replay buffer: step 9665, counter 153924\n",
      "Environment 5: Episode 1657, Score -120.41790543705784, Avg_Score -119.99382763308691\n",
      "Adding trajectory to replay buffer: step 9676, counter 154057\n",
      "Environment 4: Episode 1658, Score -126.9456867090563, Avg_Score -120.08870001041073\n",
      "Adding trajectory to replay buffer: step 9680, counter 154103\n",
      "Environment 0: Episode 1659, Score -118.47945481303769, Avg_Score -120.05536204680497\n",
      "Adding trajectory to replay buffer: step 9681, counter 154149\n",
      "Environment 3: Episode 1660, Score -122.5324206626111, Avg_Score -120.0657839081883\n",
      "Adding trajectory to replay buffer: step 9684, counter 154197\n",
      "Environment 11: Episode 1661, Score -122.19540339973817, Avg_Score -120.0852359538251\n",
      "Adding trajectory to replay buffer: step 9691, counter 154241\n",
      "Environment 10: Episode 1662, Score -122.5785844295745, Avg_Score -120.08222285468327\n",
      "Adding trajectory to replay buffer: step 9692, counter 154286\n",
      "Environment 7: Episode 1663, Score -121.54393387847333, Avg_Score -120.13867162881591\n",
      "Adding trajectory to replay buffer: step 9694, counter 154349\n",
      "Environment 6: Episode 1664, Score -116.75793318649258, Avg_Score -120.11634381511276\n",
      "Adding trajectory to replay buffer: step 9699, counter 154393\n",
      "Environment 1: Episode 1665, Score -118.62005593967439, Avg_Score -120.0728065771763\n",
      "Adding trajectory to replay buffer: step 9701, counter 154436\n",
      "Environment 9: Episode 1666, Score -122.57700197894933, Avg_Score -120.0914107476635\n",
      "Adding trajectory to replay buffer: step 9707, counter 154480\n",
      "Environment 14: Episode 1667, Score -123.22256456467075, Avg_Score -120.1054535032149\n",
      "Adding trajectory to replay buffer: step 9723, counter 154522\n",
      "Environment 3: Episode 1668, Score -123.29718941840592, Avg_Score -120.15887206502298\n",
      "Adding trajectory to replay buffer: step 9726, counter 154564\n",
      "Environment 11: Episode 1669, Score -122.88956781775815, Avg_Score -120.16408575583873\n",
      "Adding trajectory to replay buffer: step 9726, counter 154743\n",
      "Environment 13: Episode 1670, Score -118.06955137261167, Avg_Score -120.21051997616219\n",
      "Adding trajectory to replay buffer: step 9732, counter 154899\n",
      "Environment 2: Episode 1671, Score -119.25912745855811, Avg_Score -120.2263969131842\n",
      "Adding trajectory to replay buffer: step 9734, counter 154942\n",
      "Environment 10: Episode 1672, Score -123.04213422974573, Avg_Score -120.23592692652286\n",
      "Adding trajectory to replay buffer: step 9735, counter 154985\n",
      "Environment 7: Episode 1673, Score -122.39626555288521, Avg_Score -120.24239986417126\n",
      "Adding trajectory to replay buffer: step 9738, counter 155029\n",
      "Environment 6: Episode 1674, Score -122.11755223397478, Avg_Score -120.22858124561057\n",
      "Adding trajectory to replay buffer: step 9739, counter 155088\n",
      "Environment 0: Episode 1675, Score -116.63735118082165, Avg_Score -120.17058335251807\n",
      "Adding trajectory to replay buffer: step 9739, counter 155292\n",
      "Environment 12: Episode 1676, Score -123.77795514164492, Avg_Score -120.20103654499242\n",
      "Adding trajectory to replay buffer: step 9745, counter 155398\n",
      "Environment 8: Episode 1677, Score -117.59848878764609, Avg_Score -120.15480204020105\n",
      "Adding trajectory to replay buffer: step 9746, counter 155445\n",
      "Environment 1: Episode 1678, Score -120.05621352943032, Avg_Score -120.12044546077239\n",
      "Adding trajectory to replay buffer: step 9747, counter 155491\n",
      "Environment 9: Episode 1679, Score -123.14668456697433, Avg_Score -120.149353484274\n",
      "Adding trajectory to replay buffer: step 9747, counter 155625\n",
      "Environment 15: Episode 1680, Score -115.2947902154817, Avg_Score -120.03826907042333\n",
      "Adding trajectory to replay buffer: step 9749, counter 155698\n",
      "Environment 4: Episode 1681, Score -115.62662471425347, Avg_Score -119.97431051438046\n",
      "Adding trajectory to replay buffer: step 9751, counter 155742\n",
      "Environment 14: Episode 1682, Score -120.13517114621493, Avg_Score -119.95082168415227\n",
      "Adding trajectory to replay buffer: step 9752, counter 155829\n",
      "Environment 5: Episode 1683, Score -115.4711571628773, Avg_Score -119.88054310812277\n",
      "Adding trajectory to replay buffer: step 9771, counter 155874\n",
      "Environment 13: Episode 1684, Score -123.48482492718534, Avg_Score -119.91318844454351\n",
      "Adding trajectory to replay buffer: step 9772, counter 155920\n",
      "Environment 11: Episode 1685, Score -120.4173102870745, Avg_Score -119.90975930520322\n",
      "Adding trajectory to replay buffer: step 9778, counter 155964\n",
      "Environment 10: Episode 1686, Score -123.61501399158375, Avg_Score -119.94583025516216\n",
      "Adding trajectory to replay buffer: step 9779, counter 156011\n",
      "Environment 2: Episode 1687, Score -121.25908779979869, Avg_Score -119.94185049996341\n",
      "Adding trajectory to replay buffer: step 9782, counter 156070\n",
      "Environment 3: Episode 1688, Score -115.95467470563514, Avg_Score -119.93625675132652\n",
      "Adding trajectory to replay buffer: step 9782, counter 156117\n",
      "Environment 7: Episode 1689, Score -119.56497981799332, Avg_Score -119.90448629222881\n",
      "Adding trajectory to replay buffer: step 9788, counter 156166\n",
      "Environment 0: Episode 1690, Score -112.36082502958863, Avg_Score -119.79907893562275\n",
      "Adding trajectory to replay buffer: step 9788, counter 156209\n",
      "Environment 8: Episode 1691, Score -123.32039491787863, Avg_Score -119.82493888587553\n",
      "Adding trajectory to replay buffer: step 9788, counter 156258\n",
      "Environment 12: Episode 1692, Score -121.59424441665908, Avg_Score -119.81863818458667\n",
      "Adding trajectory to replay buffer: step 9793, counter 156304\n",
      "Environment 9: Episode 1693, Score -120.22863678614671, Avg_Score -119.88203425944383\n",
      "Adding trajectory to replay buffer: step 9794, counter 156349\n",
      "Environment 4: Episode 1694, Score -122.11325747409153, Avg_Score -119.94567008747755\n",
      "Adding trajectory to replay buffer: step 9797, counter 156400\n",
      "Environment 1: Episode 1695, Score -116.53536270792534, Avg_Score -119.88588485966227\n",
      "Adding trajectory to replay buffer: step 9797, counter 156445\n",
      "Environment 5: Episode 1696, Score -120.1726782895755, Avg_Score -119.95058099536993\n",
      "Adding trajectory to replay buffer: step 9810, counter 156508\n",
      "Environment 15: Episode 1697, Score -115.8394331147143, Avg_Score -119.94923359381724\n",
      "Adding trajectory to replay buffer: step 9814, counter 156550\n",
      "Environment 11: Episode 1698, Score -122.9573817508705, Avg_Score -119.98674304109693\n",
      "Adding trajectory to replay buffer: step 9814, counter 156593\n",
      "Environment 13: Episode 1699, Score -122.32325280894277, Avg_Score -120.04333270278234\n",
      "Adding trajectory to replay buffer: step 9815, counter 156657\n",
      "Environment 14: Episode 1700, Score -112.50198229328542, Avg_Score -119.96478865914203\n",
      "Adding trajectory to replay buffer: step 9822, counter 156700\n",
      "Environment 2: Episode 1701, Score -122.72797323387861, Avg_Score -120.0567306928288\n",
      "Adding trajectory to replay buffer: step 9824, counter 156746\n",
      "Environment 10: Episode 1702, Score -120.64504768252931, Avg_Score -120.05988269274738\n",
      "Adding trajectory to replay buffer: step 9832, counter 156790\n",
      "Environment 8: Episode 1703, Score -123.30871927394904, Avg_Score -120.14332243883484\n",
      "Adding trajectory to replay buffer: step 9834, counter 156836\n",
      "Environment 0: Episode 1704, Score -123.06573751897488, Avg_Score -120.21164964669852\n",
      "Adding trajectory to replay buffer: step 9837, counter 156879\n",
      "Environment 4: Episode 1705, Score -121.77045585770657, Avg_Score -120.2002113432106\n",
      "Adding trajectory to replay buffer: step 9837, counter 156923\n",
      "Environment 9: Episode 1706, Score -121.7473520041133, Avg_Score -120.14945380533383\n",
      "Adding trajectory to replay buffer: step 9842, counter 156968\n",
      "Environment 1: Episode 1707, Score -123.10004440191513, Avg_Score -120.14577906103075\n",
      "Adding trajectory to replay buffer: step 9842, counter 157013\n",
      "Environment 5: Episode 1708, Score -122.49875108336595, Avg_Score -120.13920919951244\n",
      "Adding trajectory to replay buffer: step 9853, counter 157056\n",
      "Environment 15: Episode 1709, Score -123.05826179284352, Avg_Score -120.19721884491621\n",
      "Adding trajectory to replay buffer: step 9854, counter 157122\n",
      "Environment 12: Episode 1710, Score -118.77060866168824, Avg_Score -120.20673176781216\n",
      "Adding trajectory to replay buffer: step 9857, counter 157165\n",
      "Environment 11: Episode 1711, Score -120.46859635438831, Avg_Score -120.19030833435818\n",
      "Adding trajectory to replay buffer: step 9857, counter 157208\n",
      "Environment 13: Episode 1712, Score -120.38088636498154, Avg_Score -120.19747213944468\n",
      "Adding trajectory to replay buffer: step 9858, counter 157251\n",
      "Environment 14: Episode 1713, Score -121.41248077007694, Avg_Score -120.27378647083941\n",
      "Adding trajectory to replay buffer: step 9869, counter 157296\n",
      "Environment 10: Episode 1714, Score -119.93915361317683, Avg_Score -120.23826919938016\n",
      "Adding trajectory to replay buffer: step 9881, counter 157340\n",
      "Environment 4: Episode 1715, Score -123.0420261441103, Avg_Score -120.27244402628362\n",
      "Adding trajectory to replay buffer: step 9881, counter 157384\n",
      "Environment 9: Episode 1716, Score -121.72212736861904, Avg_Score -120.28815929103229\n",
      "Adding trajectory to replay buffer: step 9886, counter 157428\n",
      "Environment 5: Episode 1717, Score -121.93345321750346, Avg_Score -120.29059655614778\n",
      "Adding trajectory to replay buffer: step 9896, counter 157490\n",
      "Environment 0: Episode 1718, Score -114.77162036202228, Avg_Score -120.22387933405116\n",
      "Adding trajectory to replay buffer: step 9898, counter 157535\n",
      "Environment 15: Episode 1719, Score -121.91598103748758, Avg_Score -120.21133967656797\n",
      "Adding trajectory to replay buffer: step 9902, counter 157583\n",
      "Environment 12: Episode 1720, Score -120.11556199155439, Avg_Score -120.18383423460611\n",
      "Adding trajectory to replay buffer: step 9902, counter 157627\n",
      "Environment 14: Episode 1721, Score -116.39951521479338, Avg_Score -120.17385463915039\n",
      "Adding trajectory to replay buffer: step 9904, counter 157674\n",
      "Environment 11: Episode 1722, Score -116.84319910239739, Avg_Score -120.1188773936477\n",
      "Adding trajectory to replay buffer: step 9904, counter 157721\n",
      "Environment 13: Episode 1723, Score -119.6814277554682, Avg_Score -120.09569603146417\n",
      "Adding trajectory to replay buffer: step 9914, counter 157853\n",
      "Environment 3: Episode 1724, Score -123.3234622846016, Avg_Score -120.10860883034782\n",
      "Adding trajectory to replay buffer: step 9915, counter 157899\n",
      "Environment 10: Episode 1725, Score -121.62833637277657, Avg_Score -120.17571003067827\n",
      "Adding trajectory to replay buffer: step 9922, counter 157940\n",
      "Environment 9: Episode 1726, Score -116.75279464223422, Avg_Score -120.12622907335634\n",
      "Adding trajectory to replay buffer: step 9929, counter 157983\n",
      "Environment 5: Episode 1727, Score -116.34778217482132, Avg_Score -120.09007047589525\n",
      "Adding trajectory to replay buffer: step 9937, counter 158098\n",
      "Environment 2: Episode 1728, Score -118.29585289497207, Avg_Score -120.09443164957867\n",
      "Adding trajectory to replay buffer: step 9937, counter 158154\n",
      "Environment 4: Episode 1729, Score -115.82065299305941, Avg_Score -120.08412691791087\n",
      "Adding trajectory to replay buffer: step 9943, counter 158201\n",
      "Environment 0: Episode 1730, Score -120.04190932178932, Avg_Score -120.06809796151988\n",
      "Adding trajectory to replay buffer: step 9948, counter 158247\n",
      "Environment 12: Episode 1731, Score -123.16548367407793, Avg_Score -120.11211048840276\n",
      "Adding trajectory to replay buffer: step 9949, counter 158294\n",
      "Environment 14: Episode 1732, Score -115.82164071749771, Avg_Score -120.07597658624863\n",
      "Adding trajectory to replay buffer: step 9954, counter 158350\n",
      "Environment 15: Episode 1733, Score -114.19771010615553, Avg_Score -120.00599553324831\n",
      "Adding trajectory to replay buffer: step 9959, counter 158405\n",
      "Environment 13: Episode 1734, Score -118.13151486603606, Avg_Score -120.00653424440469\n",
      "Adding trajectory to replay buffer: step 9961, counter 158452\n",
      "Environment 3: Episode 1735, Score -121.80448383143047, Avg_Score -120.00565668848093\n",
      "Adding trajectory to replay buffer: step 9962, counter 158492\n",
      "Environment 9: Episode 1736, Score -117.77011118298397, Avg_Score -119.96066966439027\n",
      "Adding trajectory to replay buffer: step 9970, counter 158547\n",
      "Environment 10: Episode 1737, Score -115.1110776243272, Avg_Score -119.89153949172479\n",
      "Adding trajectory to replay buffer: step 9971, counter 158736\n",
      "Environment 7: Episode 1738, Score -121.45908917116809, Avg_Score -119.88315423470277\n",
      "Adding trajectory to replay buffer: step 9975, counter 158782\n",
      "Environment 5: Episode 1739, Score -121.36982937781389, Avg_Score -119.9277630076222\n",
      "Adding trajectory to replay buffer: step 9982, counter 158827\n",
      "Environment 2: Episode 1740, Score -122.77826596039162, Avg_Score -119.92300926465083\n",
      "Adding trajectory to replay buffer: step 9983, counter 158968\n",
      "Environment 1: Episode 1741, Score -114.42117927496027, Avg_Score -119.95368126244028\n",
      "Adding trajectory to replay buffer: step 9986, counter 159011\n",
      "Environment 0: Episode 1742, Score -122.91911820437883, Avg_Score -119.97530577675383\n",
      "Adding trajectory to replay buffer: step 9986, counter 159060\n",
      "Environment 4: Episode 1743, Score -121.3167024622069, Avg_Score -119.95491316984284\n",
      "Adding trajectory to replay buffer: step 9992, counter 159104\n",
      "Environment 12: Episode 1744, Score -122.10762890222483, Avg_Score -120.02127851092057\n",
      "Adding trajectory to replay buffer: step 9993, counter 159265\n",
      "Environment 8: Episode 1745, Score -117.20497040956901, Avg_Score -120.02574717328636\n",
      "Adding trajectory to replay buffer: step 9993, counter 159309\n",
      "Environment 14: Episode 1746, Score -121.96408201256953, Avg_Score -120.04564158069653\n",
      "Adding trajectory to replay buffer: step 9998, counter 159569\n",
      "Environment 6: Episode 1747, Score -126.98770728936535, Avg_Score -120.09172855900286\n",
      "Adding trajectory to replay buffer: step 9998, counter 159613\n",
      "Environment 15: Episode 1748, Score -122.51813407647984, Avg_Score -120.16836077453833\n",
      "Adding trajectory to replay buffer: step 10001, counter 159710\n",
      "Environment 11: Episode 1749, Score -115.42944321053352, Avg_Score -120.17133029559837\n",
      "Adding trajectory to replay buffer: step 10007, counter 159758\n",
      "Environment 13: Episode 1750, Score -117.1203205475323, Avg_Score -120.14466888968373\n",
      "Adding trajectory to replay buffer: step 10010, counter 159806\n",
      "Environment 9: Episode 1751, Score -120.96017737286166, Avg_Score -120.15268017784429\n",
      "Adding trajectory to replay buffer: step 10011, counter 159856\n",
      "Environment 3: Episode 1752, Score -118.75139182771868, Avg_Score -120.1228242340772\n",
      "Adding trajectory to replay buffer: step 10015, counter 159900\n",
      "Environment 7: Episode 1753, Score -116.41427960243449, Avg_Score -120.06853229433\n",
      "Adding trajectory to replay buffer: step 10017, counter 159942\n",
      "Environment 5: Episode 1754, Score -123.10422132081848, Avg_Score -120.17335488534408\n",
      "Adding trajectory to replay buffer: step 10027, counter 159987\n",
      "Environment 2: Episode 1755, Score -122.40748634065687, Avg_Score -120.1785206633291\n",
      "Adding trajectory to replay buffer: step 10030, counter 160031\n",
      "Environment 0: Episode 1756, Score -119.41407001478598, Avg_Score -120.15110001520353\n",
      "Adding trajectory to replay buffer: step 10030, counter 160078\n",
      "Environment 1: Episode 1757, Score -121.25897372670472, Avg_Score -120.15951069809998\n",
      "Adding trajectory to replay buffer: step 10032, counter 160124\n",
      "Environment 4: Episode 1758, Score -119.55253112706107, Avg_Score -120.08557914228003\n",
      "Adding trajectory to replay buffer: step 10034, counter 160166\n",
      "Environment 12: Episode 1759, Score -122.4283928651972, Avg_Score -120.12506852280161\n",
      "Adding trajectory to replay buffer: step 10037, counter 160210\n",
      "Environment 14: Episode 1760, Score -123.1873179447738, Avg_Score -120.13161749562323\n",
      "Adding trajectory to replay buffer: step 10043, counter 160255\n",
      "Environment 6: Episode 1761, Score -121.51237766114684, Avg_Score -120.12478723823732\n",
      "Adding trajectory to replay buffer: step 10045, counter 160302\n",
      "Environment 15: Episode 1762, Score -119.51518896982695, Avg_Score -120.09415328363984\n",
      "Adding trajectory to replay buffer: step 10050, counter 160359\n",
      "Environment 8: Episode 1763, Score -115.2953807821603, Avg_Score -120.0316677526767\n",
      "Adding trajectory to replay buffer: step 10057, counter 160409\n",
      "Environment 13: Episode 1764, Score -116.97908183502281, Avg_Score -120.03387923916202\n",
      "Adding trajectory to replay buffer: step 10062, counter 160461\n",
      "Environment 9: Episode 1765, Score -116.32388099632149, Avg_Score -120.01091748972848\n",
      "Adding trajectory to replay buffer: step 10071, counter 160505\n",
      "Environment 2: Episode 1766, Score -121.13949432098121, Avg_Score -119.99654241314882\n",
      "Adding trajectory to replay buffer: step 10073, counter 160548\n",
      "Environment 0: Episode 1767, Score -120.47283598475593, Avg_Score -119.96904512734967\n",
      "Adding trajectory to replay buffer: step 10073, counter 160591\n",
      "Environment 1: Episode 1768, Score -122.16053452901791, Avg_Score -119.95767857845576\n",
      "Adding trajectory to replay buffer: step 10075, counter 160634\n",
      "Environment 4: Episode 1769, Score -122.72300431399893, Avg_Score -119.95601294341817\n",
      "Adding trajectory to replay buffer: step 10075, counter 160694\n",
      "Environment 7: Episode 1770, Score -116.51709533013155, Avg_Score -119.94048838299335\n",
      "Adding trajectory to replay buffer: step 10077, counter 160737\n",
      "Environment 12: Episode 1771, Score -121.95947963889192, Avg_Score -119.9674919047967\n",
      "Adding trajectory to replay buffer: step 10082, counter 160782\n",
      "Environment 14: Episode 1772, Score -122.13925858733741, Avg_Score -119.95846314837263\n",
      "Adding trajectory to replay buffer: step 10085, counter 160866\n",
      "Environment 11: Episode 1773, Score -111.98495021838447, Avg_Score -119.8543499950276\n",
      "Adding trajectory to replay buffer: step 10089, counter 160912\n",
      "Environment 6: Episode 1774, Score -121.58648250621683, Avg_Score -119.84903929775001\n",
      "Adding trajectory to replay buffer: step 10090, counter 160957\n",
      "Environment 15: Episode 1775, Score -117.68856255264953, Avg_Score -119.85955141146829\n",
      "Adding trajectory to replay buffer: step 10093, counter 161000\n",
      "Environment 8: Episode 1776, Score -122.793464866442, Avg_Score -119.84970650871627\n",
      "Adding trajectory to replay buffer: step 10102, counter 161085\n",
      "Environment 5: Episode 1777, Score -122.90896586217359, Avg_Score -119.90281127946153\n",
      "Adding trajectory to replay buffer: step 10103, counter 161131\n",
      "Environment 13: Episode 1778, Score -118.550151597213, Avg_Score -119.88775066013937\n",
      "Adding trajectory to replay buffer: step 10114, counter 161174\n",
      "Environment 2: Episode 1779, Score -123.54897992789, Avg_Score -119.89177361374851\n",
      "Adding trajectory to replay buffer: step 10115, counter 161216\n",
      "Environment 1: Episode 1780, Score -119.09038534322568, Avg_Score -119.92972956502598\n",
      "Adding trajectory to replay buffer: step 10117, counter 161258\n",
      "Environment 4: Episode 1781, Score -115.938495654298, Avg_Score -119.93284827442642\n",
      "Adding trajectory to replay buffer: step 10127, counter 161303\n",
      "Environment 14: Episode 1782, Score -120.84638191382028, Avg_Score -119.93996038210246\n",
      "Adding trajectory to replay buffer: step 10129, counter 161347\n",
      "Environment 11: Episode 1783, Score -122.9814763364202, Avg_Score -120.0150635738379\n",
      "Adding trajectory to replay buffer: step 10134, counter 161419\n",
      "Environment 9: Episode 1784, Score -118.1406092513601, Avg_Score -119.96162141707966\n",
      "Adding trajectory to replay buffer: step 10136, counter 161465\n",
      "Environment 15: Episode 1785, Score -121.13907185906669, Avg_Score -119.9688390327996\n",
      "Adding trajectory to replay buffer: step 10139, counter 161511\n",
      "Environment 8: Episode 1786, Score -121.0827976559264, Avg_Score -119.94351686944302\n",
      "Adding trajectory to replay buffer: step 10149, counter 161558\n",
      "Environment 5: Episode 1787, Score -119.94620491295308, Avg_Score -119.93038804057453\n",
      "Adding trajectory to replay buffer: step 10161, counter 161602\n",
      "Environment 4: Episode 1788, Score -122.31655489014213, Avg_Score -119.99400684241961\n",
      "Adding trajectory to replay buffer: step 10163, counter 161651\n",
      "Environment 2: Episode 1789, Score -119.81800639717528, Avg_Score -119.99653710821141\n",
      "Adding trajectory to replay buffer: step 10163, counter 161803\n",
      "Environment 3: Episode 1790, Score -122.86466496952997, Avg_Score -120.10157550761083\n",
      "Adding trajectory to replay buffer: step 10163, counter 161996\n",
      "Environment 10: Episode 1791, Score -121.83164962086144, Avg_Score -120.08668805464066\n",
      "Adding trajectory to replay buffer: step 10166, counter 162047\n",
      "Environment 1: Episode 1792, Score -114.65088868805518, Avg_Score -120.01725449735463\n",
      "Adding trajectory to replay buffer: step 10180, counter 162091\n",
      "Environment 15: Episode 1793, Score -121.88673933372151, Avg_Score -120.0338355228304\n",
      "Adding trajectory to replay buffer: step 10181, counter 162199\n",
      "Environment 0: Episode 1794, Score -126.71729901310368, Avg_Score -120.07987593822052\n",
      "Adding trajectory to replay buffer: step 10182, counter 162252\n",
      "Environment 11: Episode 1795, Score -119.57625841671538, Avg_Score -120.11028489530844\n",
      "Adding trajectory to replay buffer: step 10185, counter 162298\n",
      "Environment 8: Episode 1796, Score -120.24554584399237, Avg_Score -120.1110135708526\n",
      "Adding trajectory to replay buffer: step 10188, counter 162359\n",
      "Environment 14: Episode 1797, Score -115.25444573240665, Avg_Score -120.10516369702951\n",
      "Adding trajectory to replay buffer: step 10194, counter 162404\n",
      "Environment 5: Episode 1798, Score -123.0434780687727, Avg_Score -120.10602466020855\n",
      "Adding trajectory to replay buffer: step 10196, counter 162523\n",
      "Environment 12: Episode 1799, Score -117.65723948736675, Avg_Score -120.0593645269928\n",
      "Adding trajectory to replay buffer: step 10207, counter 162567\n",
      "Environment 10: Episode 1800, Score -117.15438812511167, Avg_Score -120.10588858531106\n",
      "Adding trajectory to replay buffer: step 10208, counter 162612\n",
      "Environment 3: Episode 1801, Score -119.51950000885812, Avg_Score -120.07380385306085\n",
      "Adding trajectory to replay buffer: step 10210, counter 162656\n",
      "Environment 1: Episode 1802, Score -121.5928892149236, Avg_Score -120.08328226838479\n",
      "Adding trajectory to replay buffer: step 10214, counter 162795\n",
      "Environment 7: Episode 1803, Score -114.87714202813588, Avg_Score -119.99896649592667\n",
      "Adding trajectory to replay buffer: step 10220, counter 162854\n",
      "Environment 4: Episode 1804, Score -111.60209501079595, Avg_Score -119.88433007084487\n",
      "Adding trajectory to replay buffer: step 10225, counter 162897\n",
      "Environment 11: Episode 1805, Score -118.2745932941207, Avg_Score -119.84937144520902\n",
      "Adding trajectory to replay buffer: step 10226, counter 163034\n",
      "Environment 6: Episode 1806, Score -127.36921053751993, Avg_Score -119.90559003054308\n",
      "Adding trajectory to replay buffer: step 10228, counter 163081\n",
      "Environment 0: Episode 1807, Score -117.22386279410807, Avg_Score -119.846828214465\n",
      "Adding trajectory to replay buffer: step 10231, counter 163127\n",
      "Environment 8: Episode 1808, Score -122.70080164662127, Avg_Score -119.84884872009754\n",
      "Adding trajectory to replay buffer: step 10238, counter 163171\n",
      "Environment 5: Episode 1809, Score -121.81327598518692, Avg_Score -119.83639886202096\n",
      "Adding trajectory to replay buffer: step 10239, counter 163230\n",
      "Environment 15: Episode 1810, Score -116.7785995367964, Avg_Score -119.81647877077206\n",
      "Adding trajectory to replay buffer: step 10248, counter 163282\n",
      "Environment 12: Episode 1811, Score -119.69874356644198, Avg_Score -119.8087802428926\n",
      "Adding trajectory to replay buffer: step 10251, counter 163325\n",
      "Environment 3: Episode 1812, Score -123.0591454227815, Avg_Score -119.83556283347058\n",
      "Adding trajectory to replay buffer: step 10253, counter 163371\n",
      "Environment 10: Episode 1813, Score -122.40284443334178, Avg_Score -119.84546647010323\n",
      "Adding trajectory to replay buffer: step 10255, counter 163416\n",
      "Environment 1: Episode 1814, Score -122.04980586605768, Avg_Score -119.86657299263204\n",
      "Adding trajectory to replay buffer: step 10255, counter 163457\n",
      "Environment 7: Episode 1815, Score -115.58622815674171, Avg_Score -119.79201501275837\n",
      "Adding trajectory to replay buffer: step 10259, counter 163528\n",
      "Environment 14: Episode 1816, Score -119.46716795844523, Avg_Score -119.76946541865662\n",
      "Adding trajectory to replay buffer: step 10264, counter 163572\n",
      "Environment 4: Episode 1817, Score -122.80948100437783, Avg_Score -119.77822569652535\n",
      "Adding trajectory to replay buffer: step 10270, counter 163616\n",
      "Environment 6: Episode 1818, Score -122.54619956608738, Avg_Score -119.85597148856603\n",
      "Adding trajectory to replay buffer: step 10272, counter 163660\n",
      "Environment 0: Episode 1819, Score -123.0327874415399, Avg_Score -119.86713955260655\n",
      "Adding trajectory to replay buffer: step 10275, counter 163704\n",
      "Environment 8: Episode 1820, Score -122.0186824782944, Avg_Score -119.88617075747396\n",
      "Adding trajectory to replay buffer: step 10280, counter 163746\n",
      "Environment 5: Episode 1821, Score -120.1344408939965, Avg_Score -119.92352001426597\n",
      "Adding trajectory to replay buffer: step 10286, counter 163793\n",
      "Environment 15: Episode 1822, Score -120.41989709872566, Avg_Score -119.95928699422926\n",
      "Adding trajectory to replay buffer: step 10288, counter 163978\n",
      "Environment 13: Episode 1823, Score -123.02247617957428, Avg_Score -119.99269747847033\n",
      "Adding trajectory to replay buffer: step 10299, counter 164022\n",
      "Environment 1: Episode 1824, Score -120.25656401559462, Avg_Score -119.96202849578027\n",
      "Adding trajectory to replay buffer: step 10299, counter 164068\n",
      "Environment 10: Episode 1825, Score -119.07678476232539, Avg_Score -119.93651297967574\n",
      "Adding trajectory to replay buffer: step 10300, counter 164113\n",
      "Environment 7: Episode 1826, Score -121.27926595416541, Avg_Score -119.98177769279506\n",
      "Adding trajectory to replay buffer: step 10303, counter 164157\n",
      "Environment 14: Episode 1827, Score -122.50119697505235, Avg_Score -120.04331184079734\n",
      "Adding trajectory to replay buffer: step 10306, counter 164199\n",
      "Environment 4: Episode 1828, Score -124.15863421736472, Avg_Score -120.10193965402128\n",
      "Adding trajectory to replay buffer: step 10310, counter 164375\n",
      "Environment 9: Episode 1829, Score -116.69455302785461, Avg_Score -120.11067865436924\n",
      "Adding trajectory to replay buffer: step 10312, counter 164417\n",
      "Environment 6: Episode 1830, Score -119.94705433034152, Avg_Score -120.10973010445475\n",
      "Adding trajectory to replay buffer: step 10318, counter 164463\n",
      "Environment 0: Episode 1831, Score -121.91468217226986, Avg_Score -120.09722208943667\n",
      "Adding trajectory to replay buffer: step 10319, counter 164507\n",
      "Environment 8: Episode 1832, Score -121.63231770118139, Avg_Score -120.15532885927352\n",
      "Adding trajectory to replay buffer: step 10322, counter 164578\n",
      "Environment 3: Episode 1833, Score -117.76342042049828, Avg_Score -120.19098596241693\n",
      "Adding trajectory to replay buffer: step 10333, counter 164631\n",
      "Environment 5: Episode 1834, Score -115.2526380074552, Avg_Score -120.16219719383115\n",
      "Adding trajectory to replay buffer: step 10340, counter 164746\n",
      "Environment 11: Episode 1835, Score -116.28508179830139, Avg_Score -120.10700317349985\n",
      "Adding trajectory to replay buffer: step 10341, counter 164839\n",
      "Environment 12: Episode 1836, Score -120.5384963238157, Avg_Score -120.13468702490816\n",
      "Adding trajectory to replay buffer: step 10343, counter 164883\n",
      "Environment 1: Episode 1837, Score -121.66819186122592, Avg_Score -120.20025816727716\n",
      "Adding trajectory to replay buffer: step 10344, counter 164927\n",
      "Environment 7: Episode 1838, Score -121.23942550806237, Avg_Score -120.1980615306461\n",
      "Adding trajectory to replay buffer: step 10344, counter 164972\n",
      "Environment 10: Episode 1839, Score -121.19237146042039, Avg_Score -120.19628695147216\n",
      "Adding trajectory to replay buffer: step 10346, counter 165015\n",
      "Environment 14: Episode 1840, Score -123.46576003664856, Avg_Score -120.20316189223472\n",
      "Adding trajectory to replay buffer: step 10347, counter 165076\n",
      "Environment 15: Episode 1841, Score -116.53819928056886, Avg_Score -120.22433209229081\n",
      "Adding trajectory to replay buffer: step 10350, counter 165120\n",
      "Environment 4: Episode 1842, Score -122.72146953467093, Avg_Score -120.22235560559373\n",
      "Adding trajectory to replay buffer: step 10353, counter 165163\n",
      "Environment 9: Episode 1843, Score -118.3009391377053, Avg_Score -120.19219797234874\n",
      "Adding trajectory to replay buffer: step 10365, counter 165209\n",
      "Environment 8: Episode 1844, Score -123.10064304550923, Avg_Score -120.20212811378157\n",
      "Adding trajectory to replay buffer: step 10373, counter 165419\n",
      "Environment 2: Episode 1845, Score -120.43153712949218, Avg_Score -120.2343937809808\n",
      "Adding trajectory to replay buffer: step 10375, counter 165482\n",
      "Environment 6: Episode 1846, Score -117.3164764071827, Avg_Score -120.18791772492692\n",
      "Adding trajectory to replay buffer: step 10380, counter 165529\n",
      "Environment 5: Episode 1847, Score -117.54572079503411, Avg_Score -120.09349785998361\n",
      "Adding trajectory to replay buffer: step 10384, counter 165570\n",
      "Environment 1: Episode 1848, Score -118.64843903019633, Avg_Score -120.05480090952076\n",
      "Adding trajectory to replay buffer: step 10384, counter 165614\n",
      "Environment 11: Episode 1849, Score -122.62905062253526, Avg_Score -120.1267969836408\n",
      "Adding trajectory to replay buffer: step 10386, counter 165659\n",
      "Environment 12: Episode 1850, Score -122.37672937140366, Avg_Score -120.17936107187953\n",
      "Adding trajectory to replay buffer: step 10388, counter 165703\n",
      "Environment 10: Episode 1851, Score -122.08026851927303, Avg_Score -120.19056198334366\n",
      "Adding trajectory to replay buffer: step 10390, counter 165775\n",
      "Environment 0: Episode 1852, Score -117.16598237954639, Avg_Score -120.1747078888619\n",
      "Adding trajectory to replay buffer: step 10390, counter 165819\n",
      "Environment 14: Episode 1853, Score -120.6729262918968, Avg_Score -120.21729435575654\n",
      "Adding trajectory to replay buffer: step 10394, counter 165891\n",
      "Environment 3: Episode 1854, Score -117.96920505083585, Avg_Score -120.16594419305672\n",
      "Adding trajectory to replay buffer: step 10394, counter 165938\n",
      "Environment 15: Episode 1855, Score -122.66197277557912, Avg_Score -120.16848905740594\n",
      "Adding trajectory to replay buffer: step 10408, counter 165981\n",
      "Environment 8: Episode 1856, Score -123.43810517591052, Avg_Score -120.20872940901718\n",
      "Adding trajectory to replay buffer: step 10418, counter 166026\n",
      "Environment 2: Episode 1857, Score -122.20759089369389, Avg_Score -120.21821558068704\n",
      "Adding trajectory to replay buffer: step 10420, counter 166071\n",
      "Environment 6: Episode 1858, Score -117.16737917238598, Avg_Score -120.19436406114032\n",
      "Adding trajectory to replay buffer: step 10425, counter 166116\n",
      "Environment 5: Episode 1859, Score -121.86192613924668, Avg_Score -120.1886993938808\n",
      "Adding trajectory to replay buffer: step 10428, counter 166160\n",
      "Environment 1: Episode 1860, Score -115.58323831891082, Avg_Score -120.11265859762216\n",
      "Adding trajectory to replay buffer: step 10431, counter 166205\n",
      "Environment 12: Episode 1861, Score -122.72964381629974, Avg_Score -120.12483125917367\n",
      "Adding trajectory to replay buffer: step 10433, counter 166250\n",
      "Environment 10: Episode 1862, Score -122.12576871640485, Avg_Score -120.15093705663946\n",
      "Adding trajectory to replay buffer: step 10434, counter 166294\n",
      "Environment 0: Episode 1863, Score -122.28960744180654, Avg_Score -120.22087932323592\n",
      "Adding trajectory to replay buffer: step 10434, counter 166338\n",
      "Environment 14: Episode 1864, Score -121.71344434848552, Avg_Score -120.26822294837054\n",
      "Adding trajectory to replay buffer: step 10435, counter 166485\n",
      "Environment 13: Episode 1865, Score -129.4768103309401, Avg_Score -120.39975224171673\n",
      "Adding trajectory to replay buffer: step 10438, counter 166529\n",
      "Environment 3: Episode 1866, Score -120.2505664384657, Avg_Score -120.39086296289157\n",
      "Adding trajectory to replay buffer: step 10440, counter 166575\n",
      "Environment 15: Episode 1867, Score -120.24458962957081, Avg_Score -120.38858049933974\n",
      "Adding trajectory to replay buffer: step 10446, counter 166671\n",
      "Environment 4: Episode 1868, Score -113.71628623055791, Avg_Score -120.3041380163551\n",
      "Adding trajectory to replay buffer: step 10450, counter 166768\n",
      "Environment 9: Episode 1869, Score -125.9407729183541, Avg_Score -120.33631570239868\n",
      "Adding trajectory to replay buffer: step 10460, counter 166810\n",
      "Environment 2: Episode 1870, Score -123.21854626308132, Avg_Score -120.40333021172819\n",
      "Adding trajectory to replay buffer: step 10461, counter 166927\n",
      "Environment 7: Episode 1871, Score -114.08168116368353, Avg_Score -120.32455222697608\n",
      "Adding trajectory to replay buffer: step 10466, counter 166973\n",
      "Environment 6: Episode 1872, Score -120.62586891069014, Avg_Score -120.30941833020962\n",
      "Adding trajectory to replay buffer: step 10467, counter 167032\n",
      "Environment 8: Episode 1873, Score -116.1100509110503, Avg_Score -120.35066933713628\n",
      "Adding trajectory to replay buffer: step 10474, counter 167075\n",
      "Environment 12: Episode 1874, Score -123.87526356107867, Avg_Score -120.37355714768489\n",
      "Adding trajectory to replay buffer: step 10477, counter 167118\n",
      "Environment 0: Episode 1875, Score -123.47133750133031, Avg_Score -120.4313848971717\n",
      "Adding trajectory to replay buffer: step 10477, counter 167161\n",
      "Environment 14: Episode 1876, Score -123.45935544493298, Avg_Score -120.43804380295661\n",
      "Adding trajectory to replay buffer: step 10478, counter 167204\n",
      "Environment 13: Episode 1877, Score -123.53723216275996, Avg_Score -120.44432646596248\n",
      "Adding trajectory to replay buffer: step 10480, counter 167246\n",
      "Environment 3: Episode 1878, Score -122.95255528507569, Avg_Score -120.48835050284112\n",
      "Adding trajectory to replay buffer: step 10483, counter 167289\n",
      "Environment 15: Episode 1879, Score -123.0680010004559, Avg_Score -120.48354071356677\n",
      "Adding trajectory to replay buffer: step 10489, counter 167332\n",
      "Environment 4: Episode 1880, Score -122.43307922232151, Avg_Score -120.51696765235772\n",
      "Adding trajectory to replay buffer: step 10495, counter 167377\n",
      "Environment 9: Episode 1881, Score -121.91258909481391, Avg_Score -120.57670858676285\n",
      "Adding trajectory to replay buffer: step 10504, counter 167421\n",
      "Environment 2: Episode 1882, Score -122.91935269905503, Avg_Score -120.5974382946152\n",
      "Adding trajectory to replay buffer: step 10507, counter 167503\n",
      "Environment 5: Episode 1883, Score -115.47491954061574, Avg_Score -120.52237272665714\n",
      "Adding trajectory to replay buffer: step 10509, counter 167546\n",
      "Environment 6: Episode 1884, Score -122.3270092319722, Avg_Score -120.56423672646328\n",
      "Adding trajectory to replay buffer: step 10513, counter 167631\n",
      "Environment 1: Episode 1885, Score -121.8667398620105, Avg_Score -120.57151340649273\n",
      "Adding trajectory to replay buffer: step 10513, counter 167677\n",
      "Environment 8: Episode 1886, Score -121.58986368429039, Avg_Score -120.57658406677636\n",
      "Adding trajectory to replay buffer: step 10520, counter 167723\n",
      "Environment 12: Episode 1887, Score -120.92836814080304, Avg_Score -120.58640569905486\n",
      "Adding trajectory to replay buffer: step 10521, counter 167767\n",
      "Environment 0: Episode 1888, Score -118.67173896474205, Avg_Score -120.54995753980086\n",
      "Adding trajectory to replay buffer: step 10521, counter 167811\n",
      "Environment 14: Episode 1889, Score -121.53837712465786, Avg_Score -120.56716124707569\n",
      "Adding trajectory to replay buffer: step 10524, counter 167857\n",
      "Environment 13: Episode 1890, Score -119.4367090876593, Avg_Score -120.53288168825696\n",
      "Adding trajectory to replay buffer: step 10525, counter 167921\n",
      "Environment 7: Episode 1891, Score -115.18393694589473, Avg_Score -120.46640456150732\n",
      "Adding trajectory to replay buffer: step 10527, counter 167968\n",
      "Environment 3: Episode 1892, Score -121.68140229119733, Avg_Score -120.53670969753875\n",
      "Adding trajectory to replay buffer: step 10528, counter 168013\n",
      "Environment 15: Episode 1893, Score -120.54375021674127, Avg_Score -120.52327980636893\n",
      "Adding trajectory to replay buffer: step 10540, counter 168058\n",
      "Environment 9: Episode 1894, Score -123.25201556427467, Avg_Score -120.48862697188063\n",
      "Adding trajectory to replay buffer: step 10548, counter 168102\n",
      "Environment 2: Episode 1895, Score -122.00737945031872, Avg_Score -120.5129381822167\n",
      "Adding trajectory to replay buffer: step 10551, counter 168146\n",
      "Environment 5: Episode 1896, Score -121.8701694944268, Avg_Score -120.52918441872103\n",
      "Adding trajectory to replay buffer: step 10556, counter 168189\n",
      "Environment 1: Episode 1897, Score -122.74915949318621, Avg_Score -120.60413155632882\n",
      "Adding trajectory to replay buffer: step 10556, counter 168232\n",
      "Environment 8: Episode 1898, Score -122.72232171295894, Avg_Score -120.60091999277068\n",
      "Adding trajectory to replay buffer: step 10562, counter 168305\n",
      "Environment 4: Episode 1899, Score -113.31859828596686, Avg_Score -120.5575335807567\n",
      "Adding trajectory to replay buffer: step 10564, counter 168349\n",
      "Environment 12: Episode 1900, Score -121.83259611533646, Avg_Score -120.60431566065894\n",
      "Adding trajectory to replay buffer: step 10565, counter 168393\n",
      "Environment 14: Episode 1901, Score -120.71861835646257, Avg_Score -120.61630684413498\n",
      "Adding trajectory to replay buffer: step 10567, counter 168439\n",
      "Environment 0: Episode 1902, Score -121.53244221353376, Avg_Score -120.6157023741211\n",
      "Adding trajectory to replay buffer: step 10569, counter 168484\n",
      "Environment 13: Episode 1903, Score -119.09769285766225, Avg_Score -120.65790788241637\n",
      "Adding trajectory to replay buffer: step 10570, counter 168545\n",
      "Environment 6: Episode 1904, Score -115.96932743998927, Avg_Score -120.70158020670827\n",
      "Adding trajectory to replay buffer: step 10571, counter 168591\n",
      "Environment 7: Episode 1905, Score -122.0316992010586, Avg_Score -120.73915126577765\n",
      "Adding trajectory to replay buffer: step 10572, counter 168636\n",
      "Environment 3: Episode 1906, Score -123.16434981831846, Avg_Score -120.69710265858565\n",
      "Adding trajectory to replay buffer: step 10574, counter 168777\n",
      "Environment 10: Episode 1907, Score -119.30256540611998, Avg_Score -120.71788968470577\n",
      "Adding trajectory to replay buffer: step 10587, counter 168824\n",
      "Environment 9: Episode 1908, Score -120.37582588744753, Avg_Score -120.69463992711404\n",
      "Adding trajectory to replay buffer: step 10591, counter 168867\n",
      "Environment 2: Episode 1909, Score -118.28579184550156, Avg_Score -120.65936508571718\n",
      "Adding trajectory to replay buffer: step 10594, counter 168910\n",
      "Environment 5: Episode 1910, Score -116.95428242697008, Avg_Score -120.66112191461893\n",
      "Adding trajectory to replay buffer: step 10601, counter 168955\n",
      "Environment 8: Episode 1911, Score -122.59483989096434, Avg_Score -120.69008287786413\n",
      "Adding trajectory to replay buffer: step 10603, counter 169002\n",
      "Environment 1: Episode 1912, Score -118.26854808848677, Avg_Score -120.64217690452121\n",
      "Adding trajectory to replay buffer: step 10606, counter 169046\n",
      "Environment 4: Episode 1913, Score -121.61420306807011, Avg_Score -120.63429049086847\n",
      "Adding trajectory to replay buffer: step 10606, counter 169088\n",
      "Environment 12: Episode 1914, Score -123.05108273711987, Avg_Score -120.64430325957909\n",
      "Adding trajectory to replay buffer: step 10607, counter 169130\n",
      "Environment 14: Episode 1915, Score -122.87155922652595, Avg_Score -120.71715657027691\n",
      "Adding trajectory to replay buffer: step 10610, counter 169173\n",
      "Environment 0: Episode 1916, Score -123.49782978244747, Avg_Score -120.75746318851692\n",
      "Adding trajectory to replay buffer: step 10612, counter 169215\n",
      "Environment 6: Episode 1917, Score -122.76891305793946, Avg_Score -120.75705750905256\n",
      "Adding trajectory to replay buffer: step 10614, counter 169260\n",
      "Environment 13: Episode 1918, Score -122.04051852659074, Avg_Score -120.7520006986576\n",
      "Adding trajectory to replay buffer: step 10616, counter 169305\n",
      "Environment 7: Episode 1919, Score -121.79133394050288, Avg_Score -120.7395861636472\n",
      "Adding trajectory to replay buffer: step 10617, counter 169350\n",
      "Environment 3: Episode 1920, Score -122.02546593131126, Avg_Score -120.73965399817739\n",
      "Adding trajectory to replay buffer: step 10634, counter 169393\n",
      "Environment 2: Episode 1921, Score -118.74788838339038, Avg_Score -120.72578847307132\n",
      "Adding trajectory to replay buffer: step 10639, counter 169438\n",
      "Environment 5: Episode 1922, Score -121.98564482899508, Avg_Score -120.74144595037401\n",
      "Adding trajectory to replay buffer: step 10643, counter 169480\n",
      "Environment 8: Episode 1923, Score -123.48131396984371, Avg_Score -120.74603432827672\n",
      "Adding trajectory to replay buffer: step 10646, counter 169523\n",
      "Environment 1: Episode 1924, Score -122.07500284157072, Avg_Score -120.7642187165365\n",
      "Adding trajectory to replay buffer: step 10649, counter 169566\n",
      "Environment 4: Episode 1925, Score -122.89694868018975, Avg_Score -120.80242035571514\n",
      "Adding trajectory to replay buffer: step 10652, counter 169612\n",
      "Environment 12: Episode 1926, Score -122.12884117882328, Avg_Score -120.81091610796173\n",
      "Adding trajectory to replay buffer: step 10652, counter 169657\n",
      "Environment 14: Episode 1927, Score -121.29476086091125, Avg_Score -120.79885174682033\n",
      "Adding trajectory to replay buffer: step 10655, counter 169702\n",
      "Environment 0: Episode 1928, Score -120.6950160624298, Avg_Score -120.76421556527099\n",
      "Adding trajectory to replay buffer: step 10655, counter 169745\n",
      "Environment 6: Episode 1929, Score -122.66899029646007, Avg_Score -120.82395993795704\n",
      "Adding trajectory to replay buffer: step 10660, counter 169791\n",
      "Environment 13: Episode 1930, Score -122.72917522627301, Avg_Score -120.85178114691635\n",
      "Adding trajectory to replay buffer: step 10661, counter 169836\n",
      "Environment 7: Episode 1931, Score -120.96788569038237, Avg_Score -120.84231318209748\n",
      "Adding trajectory to replay buffer: step 10662, counter 169881\n",
      "Environment 3: Episode 1932, Score -121.8954371830852, Avg_Score -120.84494437691652\n",
      "Adding trajectory to replay buffer: step 10667, counter 169961\n",
      "Environment 9: Episode 1933, Score -119.6913700468211, Avg_Score -120.86422387317977\n",
      "Adding trajectory to replay buffer: step 10677, counter 170004\n",
      "Environment 2: Episode 1934, Score -123.47387657549356, Avg_Score -120.94643625886015\n",
      "Adding trajectory to replay buffer: step 10680, counter 170110\n",
      "Environment 10: Episode 1935, Score -126.05281033779525, Avg_Score -121.04411354425507\n",
      "Adding trajectory to replay buffer: step 10687, counter 170154\n",
      "Environment 8: Episode 1936, Score -117.15971288820667, Avg_Score -121.01032570989896\n",
      "Adding trajectory to replay buffer: step 10691, counter 170199\n",
      "Environment 1: Episode 1937, Score -121.50704230143751, Avg_Score -121.0087142143011\n",
      "Adding trajectory to replay buffer: step 10694, counter 170244\n",
      "Environment 4: Episode 1938, Score -121.26070612577132, Avg_Score -121.00892702047818\n",
      "Adding trajectory to replay buffer: step 10697, counter 170289\n",
      "Environment 12: Episode 1939, Score -120.3595338892682, Avg_Score -121.00059864476665\n",
      "Adding trajectory to replay buffer: step 10697, counter 170334\n",
      "Environment 14: Episode 1940, Score -120.6957133560013, Avg_Score -120.97289817796018\n",
      "Adding trajectory to replay buffer: step 10699, counter 170378\n",
      "Environment 0: Episode 1941, Score -122.07453347929878, Avg_Score -121.02826151994746\n",
      "Adding trajectory to replay buffer: step 10700, counter 170694\n",
      "Environment 11: Episode 1942, Score -133.99580154014146, Avg_Score -121.14100484000217\n",
      "Adding trajectory to replay buffer: step 10701, counter 170740\n",
      "Environment 6: Episode 1943, Score -118.15096045419077, Avg_Score -121.13950505316706\n",
      "Adding trajectory to replay buffer: step 10710, counter 170783\n",
      "Environment 9: Episode 1944, Score -117.91359492869489, Avg_Score -121.08763457199892\n",
      "Adding trajectory to replay buffer: step 10714, counter 170858\n",
      "Environment 5: Episode 1945, Score -116.23002263609258, Avg_Score -121.04561942706492\n",
      "Adding trajectory to replay buffer: step 10718, counter 171048\n",
      "Environment 15: Episode 1946, Score -124.68063207822357, Avg_Score -121.1192609837753\n",
      "Adding trajectory to replay buffer: step 10724, counter 171095\n",
      "Environment 2: Episode 1947, Score -120.95857703762502, Avg_Score -121.15338954620124\n",
      "Adding trajectory to replay buffer: step 10724, counter 171158\n",
      "Environment 7: Episode 1948, Score -112.23654836094008, Avg_Score -121.08927063950867\n",
      "Adding trajectory to replay buffer: step 10724, counter 171202\n",
      "Environment 10: Episode 1949, Score -122.32611188184144, Avg_Score -121.08624125210171\n",
      "Adding trajectory to replay buffer: step 10727, counter 171267\n",
      "Environment 3: Episode 1950, Score -116.84853262720567, Avg_Score -121.03095928465974\n",
      "Adding trajectory to replay buffer: step 10731, counter 171311\n",
      "Environment 8: Episode 1951, Score -122.3537755133342, Avg_Score -121.03369435460036\n",
      "Adding trajectory to replay buffer: step 10736, counter 171356\n",
      "Environment 1: Episode 1952, Score -118.99615805752825, Avg_Score -121.05199611138019\n",
      "Adding trajectory to replay buffer: step 10741, counter 171400\n",
      "Environment 14: Episode 1953, Score -117.49742231320404, Avg_Score -121.02024107159328\n",
      "Adding trajectory to replay buffer: step 10742, counter 171445\n",
      "Environment 12: Episode 1954, Score -118.19602031963878, Avg_Score -121.02250922428131\n",
      "Adding trajectory to replay buffer: step 10746, counter 171497\n",
      "Environment 4: Episode 1955, Score -116.3701602757896, Avg_Score -120.9595910992834\n",
      "Adding trajectory to replay buffer: step 10749, counter 171545\n",
      "Environment 6: Episode 1956, Score -121.75137528383857, Avg_Score -120.94272380036267\n",
      "Adding trajectory to replay buffer: step 10755, counter 171590\n",
      "Environment 9: Episode 1957, Score -123.75610390821907, Avg_Score -120.95820893050794\n",
      "Adding trajectory to replay buffer: step 10760, counter 171632\n",
      "Environment 15: Episode 1958, Score -123.00770507366397, Avg_Score -121.01661218952073\n",
      "Adding trajectory to replay buffer: step 10762, counter 171680\n",
      "Environment 5: Episode 1959, Score -120.95216756877676, Avg_Score -121.00751460381603\n",
      "Adding trajectory to replay buffer: step 10767, counter 171723\n",
      "Environment 7: Episode 1960, Score -123.02884991678471, Avg_Score -121.08197071979477\n",
      "Adding trajectory to replay buffer: step 10777, counter 171769\n",
      "Environment 8: Episode 1961, Score -120.57956354870632, Avg_Score -121.06046991711884\n",
      "Adding trajectory to replay buffer: step 10781, counter 171814\n",
      "Environment 1: Episode 1962, Score -122.1488648981744, Avg_Score -121.06070087893653\n",
      "Adding trajectory to replay buffer: step 10785, counter 171858\n",
      "Environment 14: Episode 1963, Score -122.79643253684728, Avg_Score -121.06576912988692\n",
      "Adding trajectory to replay buffer: step 10787, counter 171903\n",
      "Environment 12: Episode 1964, Score -121.85442089438749, Avg_Score -121.06717889534593\n",
      "Adding trajectory to replay buffer: step 10788, counter 171991\n",
      "Environment 11: Episode 1965, Score -122.84985156348782, Avg_Score -121.00090930767139\n",
      "Adding trajectory to replay buffer: step 10790, counter 172054\n",
      "Environment 3: Episode 1966, Score -118.00377498629442, Avg_Score -120.9784413931497\n",
      "Adding trajectory to replay buffer: step 10790, counter 172098\n",
      "Environment 4: Episode 1967, Score -118.96660727739396, Avg_Score -120.96566156962791\n",
      "Adding trajectory to replay buffer: step 10791, counter 172165\n",
      "Environment 2: Episode 1968, Score -113.4904923535039, Avg_Score -120.96340363085739\n",
      "Adding trajectory to replay buffer: step 10791, counter 172296\n",
      "Environment 13: Episode 1969, Score -116.69684445702097, Avg_Score -120.87096434624408\n",
      "Adding trajectory to replay buffer: step 10794, counter 172341\n",
      "Environment 6: Episode 1970, Score -121.14276779304942, Avg_Score -120.85020656154376\n",
      "Adding trajectory to replay buffer: step 10799, counter 172416\n",
      "Environment 10: Episode 1971, Score -113.22851242917466, Avg_Score -120.84167487419867\n",
      "Adding trajectory to replay buffer: step 10802, counter 172458\n",
      "Environment 15: Episode 1972, Score -123.02295819555098, Avg_Score -120.86564576704728\n",
      "Adding trajectory to replay buffer: step 10805, counter 172501\n",
      "Environment 5: Episode 1973, Score -123.37993758645965, Avg_Score -120.93834463380136\n",
      "Adding trajectory to replay buffer: step 10811, counter 172545\n",
      "Environment 7: Episode 1974, Score -121.18004514635479, Avg_Score -120.91139244965414\n",
      "Adding trajectory to replay buffer: step 10813, counter 172659\n",
      "Environment 0: Episode 1975, Score -116.95268182991073, Avg_Score -120.84620589293995\n",
      "Adding trajectory to replay buffer: step 10820, counter 172702\n",
      "Environment 8: Episode 1976, Score -121.76043275698646, Avg_Score -120.82921666606047\n",
      "Adding trajectory to replay buffer: step 10824, counter 172771\n",
      "Environment 9: Episode 1977, Score -115.93584945964317, Avg_Score -120.75320283902929\n",
      "Adding trajectory to replay buffer: step 10827, counter 172817\n",
      "Environment 1: Episode 1978, Score -121.76875614458073, Avg_Score -120.74136484762434\n",
      "Adding trajectory to replay buffer: step 10828, counter 172860\n",
      "Environment 14: Episode 1979, Score -121.64062703104193, Avg_Score -120.72709110793019\n",
      "Adding trajectory to replay buffer: step 10832, counter 172901\n",
      "Environment 2: Episode 1980, Score -115.00176405227309, Avg_Score -120.65277795622971\n",
      "Adding trajectory to replay buffer: step 10833, counter 172947\n",
      "Environment 12: Episode 1981, Score -120.68072348839542, Avg_Score -120.64045930016553\n",
      "Adding trajectory to replay buffer: step 10833, counter 172989\n",
      "Environment 13: Episode 1982, Score -115.09613465268785, Avg_Score -120.56222711970187\n",
      "Adding trajectory to replay buffer: step 10834, counter 173035\n",
      "Environment 11: Episode 1983, Score -122.50736983823465, Avg_Score -120.63255162267808\n",
      "Adding trajectory to replay buffer: step 10836, counter 173081\n",
      "Environment 3: Episode 1984, Score -118.58821058156342, Avg_Score -120.59516363617395\n",
      "Adding trajectory to replay buffer: step 10839, counter 173126\n",
      "Environment 6: Episode 1985, Score -123.625947789353, Avg_Score -120.61275571544738\n",
      "Adding trajectory to replay buffer: step 10846, counter 173173\n",
      "Environment 10: Episode 1986, Score -122.10439181442248, Avg_Score -120.61790099674867\n",
      "Adding trajectory to replay buffer: step 10847, counter 173230\n",
      "Environment 4: Episode 1987, Score -112.31783683604934, Avg_Score -120.53179568370115\n",
      "Adding trajectory to replay buffer: step 10848, counter 173276\n",
      "Environment 15: Episode 1988, Score -119.63722567642635, Avg_Score -120.54145055081798\n",
      "Adding trajectory to replay buffer: step 10849, counter 173320\n",
      "Environment 5: Episode 1989, Score -122.72775143133103, Avg_Score -120.5533442938847\n",
      "Adding trajectory to replay buffer: step 10867, counter 173376\n",
      "Environment 7: Episode 1990, Score -114.47328833037243, Avg_Score -120.50371008631186\n",
      "Adding trajectory to replay buffer: step 10867, counter 173423\n",
      "Environment 8: Episode 1991, Score -120.40709798215889, Avg_Score -120.55594169667451\n",
      "Adding trajectory to replay buffer: step 10870, counter 173466\n",
      "Environment 1: Episode 1992, Score -123.57787123282627, Avg_Score -120.5749063860908\n",
      "Adding trajectory to replay buffer: step 10873, counter 173511\n",
      "Environment 14: Episode 1993, Score -120.85760584017697, Avg_Score -120.57804494232514\n",
      "Adding trajectory to replay buffer: step 10876, counter 173555\n",
      "Environment 2: Episode 1994, Score -122.38292194744758, Avg_Score -120.56935400615689\n",
      "Adding trajectory to replay buffer: step 10876, counter 173598\n",
      "Environment 12: Episode 1995, Score -122.08365594903381, Avg_Score -120.57011677114404\n",
      "Adding trajectory to replay buffer: step 10876, counter 173641\n",
      "Environment 13: Episode 1996, Score -122.09517259531654, Avg_Score -120.57236680215291\n",
      "Adding trajectory to replay buffer: step 10877, counter 173705\n",
      "Environment 0: Episode 1997, Score -114.35354142761292, Avg_Score -120.48841062149718\n",
      "Adding trajectory to replay buffer: step 10877, counter 173748\n",
      "Environment 11: Episode 1998, Score -122.45682079373611, Avg_Score -120.48575561230497\n",
      "Adding trajectory to replay buffer: step 10881, counter 173793\n",
      "Environment 3: Episode 1999, Score -117.92579121442822, Avg_Score -120.53182754158955\n",
      "Adding trajectory to replay buffer: step 10883, counter 173837\n",
      "Environment 6: Episode 2000, Score -120.1040457695853, Avg_Score -120.51454203813206\n",
      "Adding trajectory to replay buffer: step 10888, counter 173879\n",
      "Environment 10: Episode 2001, Score -122.04839996827766, Avg_Score -120.52783985425022\n",
      "Adding trajectory to replay buffer: step 10891, counter 173923\n",
      "Environment 4: Episode 2002, Score -119.30431707384002, Avg_Score -120.50555860285327\n",
      "Adding trajectory to replay buffer: step 10892, counter 173966\n",
      "Environment 5: Episode 2003, Score -122.27883494970233, Avg_Score -120.53737002377369\n",
      "Adding trajectory to replay buffer: step 10894, counter 174012\n",
      "Environment 15: Episode 2004, Score -120.92550919024677, Avg_Score -120.58693184127625\n",
      "Adding trajectory to replay buffer: step 10912, counter 174057\n",
      "Environment 7: Episode 2005, Score -120.51230353331628, Avg_Score -120.57173788459882\n",
      "Adding trajectory to replay buffer: step 10912, counter 174102\n",
      "Environment 8: Episode 2006, Score -120.24445103547225, Avg_Score -120.54253889677037\n",
      "Adding trajectory to replay buffer: step 10919, counter 174145\n",
      "Environment 13: Episode 2007, Score -115.27106616405584, Avg_Score -120.50222390434972\n",
      "Adding trajectory to replay buffer: step 10921, counter 174193\n",
      "Environment 14: Episode 2008, Score -120.4378659838109, Avg_Score -120.50284430531336\n",
      "Adding trajectory to replay buffer: step 10923, counter 174240\n",
      "Environment 12: Episode 2009, Score -120.0295292089538, Avg_Score -120.52028167894788\n",
      "Adding trajectory to replay buffer: step 10924, counter 174288\n",
      "Environment 2: Episode 2010, Score -121.329306506671, Avg_Score -120.5640319197449\n",
      "Adding trajectory to replay buffer: step 10931, counter 174336\n",
      "Environment 6: Episode 2011, Score -121.021230574868, Avg_Score -120.54829582658398\n",
      "Adding trajectory to replay buffer: step 10934, counter 174389\n",
      "Environment 3: Episode 2012, Score -119.85192030895564, Avg_Score -120.56412954878869\n",
      "Adding trajectory to replay buffer: step 10936, counter 174434\n",
      "Environment 4: Episode 2013, Score -122.1541837873403, Avg_Score -120.56952935598137\n",
      "Adding trajectory to replay buffer: step 10937, counter 174494\n",
      "Environment 0: Episode 2014, Score -115.65158653046191, Avg_Score -120.49553439391481\n",
      "Adding trajectory to replay buffer: step 10937, counter 174539\n",
      "Environment 5: Episode 2015, Score -123.80271460258588, Avg_Score -120.50484594767539\n",
      "Adding trajectory to replay buffer: step 10941, counter 174603\n",
      "Environment 11: Episode 2016, Score -115.06050386143157, Avg_Score -120.42047268846524\n",
      "Adding trajectory to replay buffer: step 10943, counter 174652\n",
      "Environment 15: Episode 2017, Score -112.48768631998574, Avg_Score -120.3176604210857\n",
      "Adding trajectory to replay buffer: step 10951, counter 174715\n",
      "Environment 10: Episode 2018, Score -114.29050663578076, Avg_Score -120.24016030217763\n",
      "Adding trajectory to replay buffer: step 10956, counter 174759\n",
      "Environment 7: Episode 2019, Score -120.55966991533774, Avg_Score -120.22784366192596\n",
      "Adding trajectory to replay buffer: step 10957, counter 174804\n",
      "Environment 8: Episode 2020, Score -119.80417862884451, Avg_Score -120.2056307889013\n",
      "Adding trajectory to replay buffer: step 10965, counter 174850\n",
      "Environment 13: Episode 2021, Score -122.1924010172443, Avg_Score -120.24007591523983\n",
      "Adding trajectory to replay buffer: step 10965, counter 174894\n",
      "Environment 14: Episode 2022, Score -122.14690517096781, Avg_Score -120.24168851865956\n",
      "Adding trajectory to replay buffer: step 10967, counter 174937\n",
      "Environment 2: Episode 2023, Score -122.9640563167495, Avg_Score -120.23651594212859\n",
      "Adding trajectory to replay buffer: step 10970, counter 175083\n",
      "Environment 9: Episode 2024, Score -116.07549063798598, Avg_Score -120.17652082009276\n",
      "Adding trajectory to replay buffer: step 10970, counter 175130\n",
      "Environment 12: Episode 2025, Score -122.96610430147375, Avg_Score -120.17721237630558\n",
      "Adding trajectory to replay buffer: step 10977, counter 175237\n",
      "Environment 1: Episode 2026, Score -125.0566542006793, Avg_Score -120.20649050652415\n",
      "Adding trajectory to replay buffer: step 10983, counter 175289\n",
      "Environment 6: Episode 2027, Score -120.25866575092792, Avg_Score -120.1961295554243\n",
      "Adding trajectory to replay buffer: step 10986, counter 175334\n",
      "Environment 11: Episode 2028, Score -121.33400175928821, Avg_Score -120.2025194123929\n",
      "Adding trajectory to replay buffer: step 10986, counter 175377\n",
      "Environment 15: Episode 2029, Score -120.65152712636441, Avg_Score -120.18234478069195\n",
      "Adding trajectory to replay buffer: step 10994, counter 175434\n",
      "Environment 5: Episode 2030, Score -115.51316211393538, Avg_Score -120.11018464956858\n",
      "Adding trajectory to replay buffer: step 10995, counter 175478\n",
      "Environment 10: Episode 2031, Score -120.54500112266652, Avg_Score -120.10595580389143\n",
      "Adding trajectory to replay buffer: step 10999, counter 175543\n",
      "Environment 3: Episode 2032, Score -116.2706713902404, Avg_Score -120.04970814596298\n",
      "Adding trajectory to replay buffer: step 10999, counter 175586\n",
      "Environment 7: Episode 2033, Score -120.4988361927825, Avg_Score -120.05778280742257\n",
      "Adding trajectory to replay buffer: step 11001, counter 175630\n",
      "Environment 8: Episode 2034, Score -121.84201859205342, Avg_Score -120.04146422758818\n",
      "Adding trajectory to replay buffer: step 11011, counter 175674\n",
      "Environment 2: Episode 2035, Score -122.70216910552266, Avg_Score -120.00795781526546\n",
      "Adding trajectory to replay buffer: step 11012, counter 175721\n",
      "Environment 13: Episode 2036, Score -121.71452946932925, Avg_Score -120.05350598107668\n",
      "Adding trajectory to replay buffer: step 11013, counter 175798\n",
      "Environment 4: Episode 2037, Score -121.4908847836939, Avg_Score -120.05334440589924\n",
      "Adding trajectory to replay buffer: step 11013, counter 175841\n",
      "Environment 9: Episode 2038, Score -122.55576762822892, Avg_Score -120.06629502092385\n",
      "Adding trajectory to replay buffer: step 11013, counter 175884\n",
      "Environment 12: Episode 2039, Score -122.48996491383265, Avg_Score -120.08759933116947\n",
      "Adding trajectory to replay buffer: step 11020, counter 175927\n",
      "Environment 1: Episode 2040, Score -122.74697303253413, Avg_Score -120.10811192793479\n",
      "Adding trajectory to replay buffer: step 11024, counter 175986\n",
      "Environment 14: Episode 2041, Score -115.94283041413625, Avg_Score -120.04679489728318\n",
      "Adding trajectory to replay buffer: step 11034, counter 176034\n",
      "Environment 15: Episode 2042, Score -122.83224546686188, Avg_Score -119.93515933655037\n",
      "Adding trajectory to replay buffer: step 11039, counter 176079\n",
      "Environment 5: Episode 2043, Score -122.7109257734927, Avg_Score -119.98075898974339\n",
      "Adding trajectory to replay buffer: step 11041, counter 176125\n",
      "Environment 10: Episode 2044, Score -120.55352514526746, Avg_Score -120.0071582919091\n",
      "Adding trajectory to replay buffer: step 11045, counter 176171\n",
      "Environment 7: Episode 2045, Score -120.81622463726998, Avg_Score -120.05302031192086\n",
      "Adding trajectory to replay buffer: step 11045, counter 176215\n",
      "Environment 8: Episode 2046, Score -122.34687170327393, Avg_Score -120.02968270817136\n",
      "Adding trajectory to replay buffer: step 11052, counter 176281\n",
      "Environment 11: Episode 2047, Score -119.00424272395671, Avg_Score -120.01013936503469\n",
      "Adding trajectory to replay buffer: step 11055, counter 176325\n",
      "Environment 2: Episode 2048, Score -121.90406656013181, Avg_Score -120.10681454702659\n",
      "Adding trajectory to replay buffer: step 11055, counter 176368\n",
      "Environment 13: Episode 2049, Score -121.89756100884452, Avg_Score -120.10252903829661\n",
      "Adding trajectory to replay buffer: step 11056, counter 176411\n",
      "Environment 4: Episode 2050, Score -121.607158880104, Avg_Score -120.1501153008256\n",
      "Adding trajectory to replay buffer: step 11056, counter 176454\n",
      "Environment 9: Episode 2051, Score -122.1882208612673, Avg_Score -120.14845975430494\n",
      "Adding trajectory to replay buffer: step 11056, counter 176497\n",
      "Environment 12: Episode 2052, Score -122.19411964772455, Avg_Score -120.18043937020688\n",
      "Adding trajectory to replay buffer: step 11068, counter 176582\n",
      "Environment 6: Episode 2053, Score -124.47575747648007, Avg_Score -120.25022272183965\n",
      "Adding trajectory to replay buffer: step 11076, counter 176634\n",
      "Environment 14: Episode 2054, Score -112.30591719524872, Avg_Score -120.19132169059574\n",
      "Adding trajectory to replay buffer: step 11078, counter 176678\n",
      "Environment 15: Episode 2055, Score -122.14670120130852, Avg_Score -120.24908709985095\n",
      "Adding trajectory to replay buffer: step 11080, counter 176738\n",
      "Environment 1: Episode 2056, Score -116.37633473175578, Avg_Score -120.19533669433012\n",
      "Adding trajectory to replay buffer: step 11082, counter 176781\n",
      "Environment 5: Episode 2057, Score -122.84874602029348, Avg_Score -120.18626311545087\n",
      "Adding trajectory to replay buffer: step 11094, counter 176938\n",
      "Environment 0: Episode 2058, Score -121.82220372493938, Avg_Score -120.17440810196364\n",
      "Adding trajectory to replay buffer: step 11094, counter 176987\n",
      "Environment 7: Episode 2059, Score -120.39514816165777, Avg_Score -120.16883790789247\n",
      "Adding trajectory to replay buffer: step 11095, counter 177030\n",
      "Environment 11: Episode 2060, Score -118.12766389097771, Avg_Score -120.11982604763438\n",
      "Adding trajectory to replay buffer: step 11102, counter 177076\n",
      "Environment 4: Episode 2061, Score -117.05008037672378, Avg_Score -120.08453121591458\n",
      "Adding trajectory to replay buffer: step 11105, counter 177126\n",
      "Environment 13: Episode 2062, Score -119.22747326437447, Avg_Score -120.05531729957656\n",
      "Adding trajectory to replay buffer: step 11106, counter 177176\n",
      "Environment 12: Episode 2063, Score -117.26243452354149, Avg_Score -119.99997731944352\n",
      "Adding trajectory to replay buffer: step 11113, counter 177221\n",
      "Environment 6: Episode 2064, Score -122.47720266325274, Avg_Score -120.00620513713214\n",
      "Adding trajectory to replay buffer: step 11120, counter 177265\n",
      "Environment 14: Episode 2065, Score -122.94703869374592, Avg_Score -120.00717700843475\n",
      "Adding trajectory to replay buffer: step 11121, counter 177308\n",
      "Environment 15: Episode 2066, Score -122.84775945873993, Avg_Score -120.0556168531592\n",
      "Adding trajectory to replay buffer: step 11124, counter 177350\n",
      "Environment 5: Episode 2067, Score -123.11295816810679, Avg_Score -120.09708036206632\n",
      "Adding trajectory to replay buffer: step 11125, counter 177395\n",
      "Environment 1: Episode 2068, Score -122.46402551533345, Avg_Score -120.18681569368462\n",
      "Adding trajectory to replay buffer: step 11127, counter 177523\n",
      "Environment 3: Episode 2069, Score -114.91049936710112, Avg_Score -120.16895224278542\n",
      "Adding trajectory to replay buffer: step 11129, counter 177611\n",
      "Environment 10: Episode 2070, Score -110.78700038471197, Avg_Score -120.06539456870203\n",
      "Adding trajectory to replay buffer: step 11137, counter 177654\n",
      "Environment 0: Episode 2071, Score -122.93632302690857, Avg_Score -120.16247267467938\n",
      "Adding trajectory to replay buffer: step 11137, counter 177697\n",
      "Environment 7: Episode 2072, Score -122.93898624866331, Avg_Score -120.16163295521048\n",
      "Adding trajectory to replay buffer: step 11139, counter 177741\n",
      "Environment 11: Episode 2073, Score -123.01030658136867, Avg_Score -120.1579366451596\n",
      "Adding trajectory to replay buffer: step 11148, counter 177787\n",
      "Environment 4: Episode 2074, Score -121.13830369719676, Avg_Score -120.15751923066799\n",
      "Adding trajectory to replay buffer: step 11149, counter 177830\n",
      "Environment 12: Episode 2075, Score -123.24112119782825, Avg_Score -120.22040362434718\n",
      "Adding trajectory to replay buffer: step 11152, counter 177937\n",
      "Environment 8: Episode 2076, Score -124.82305932273593, Avg_Score -120.25102989000469\n",
      "Adding trajectory to replay buffer: step 11155, counter 177979\n",
      "Environment 6: Episode 2077, Score -122.81812716943026, Avg_Score -120.31985266710257\n",
      "Adding trajectory to replay buffer: step 11158, counter 178081\n",
      "Environment 9: Episode 2078, Score -113.98926554286108, Avg_Score -120.24205776108536\n",
      "Adding trajectory to replay buffer: step 11164, counter 178124\n",
      "Environment 15: Episode 2079, Score -123.28270230589621, Avg_Score -120.25847851383392\n",
      "Adding trajectory to replay buffer: step 11165, counter 178169\n",
      "Environment 14: Episode 2080, Score -122.87690024176614, Avg_Score -120.33722987572884\n",
      "Adding trajectory to replay buffer: step 11167, counter 178212\n",
      "Environment 5: Episode 2081, Score -123.18750550662726, Avg_Score -120.36229769591118\n",
      "Adding trajectory to replay buffer: step 11168, counter 178255\n",
      "Environment 1: Episode 2082, Score -122.82330996872547, Avg_Score -120.43956944907156\n",
      "Adding trajectory to replay buffer: step 11170, counter 178298\n",
      "Environment 3: Episode 2083, Score -122.46452366206795, Avg_Score -120.43914098730986\n",
      "Adding trajectory to replay buffer: step 11183, counter 178344\n",
      "Environment 0: Episode 2084, Score -121.68484008158869, Avg_Score -120.47010728231012\n",
      "Adding trajectory to replay buffer: step 11183, counter 178390\n",
      "Environment 7: Episode 2085, Score -121.96920804899, Avg_Score -120.45353988490648\n",
      "Adding trajectory to replay buffer: step 11184, counter 178435\n",
      "Environment 11: Episode 2086, Score -120.1531034334513, Avg_Score -120.43402700109678\n",
      "Adding trajectory to replay buffer: step 11189, counter 178569\n",
      "Environment 2: Episode 2087, Score -127.39164047159254, Avg_Score -120.58476503745219\n",
      "Adding trajectory to replay buffer: step 11192, counter 178613\n",
      "Environment 4: Episode 2088, Score -121.97216729573533, Avg_Score -120.60811445364527\n",
      "Adding trajectory to replay buffer: step 11196, counter 178660\n",
      "Environment 12: Episode 2089, Score -120.24317330857987, Avg_Score -120.58326867241774\n",
      "Adding trajectory to replay buffer: step 11197, counter 178705\n",
      "Environment 8: Episode 2090, Score -122.98702791583196, Avg_Score -120.66840606827232\n",
      "Adding trajectory to replay buffer: step 11198, counter 178798\n",
      "Environment 13: Episode 2091, Score -113.213683712362, Avg_Score -120.59647192557435\n",
      "Adding trajectory to replay buffer: step 11199, counter 178842\n",
      "Environment 6: Episode 2092, Score -121.78518950762351, Avg_Score -120.5785451083223\n",
      "Adding trajectory to replay buffer: step 11201, counter 178885\n",
      "Environment 9: Episode 2093, Score -122.0604678016901, Avg_Score -120.59057372793747\n",
      "Adding trajectory to replay buffer: step 11206, counter 178962\n",
      "Environment 10: Episode 2094, Score -112.934117373541, Avg_Score -120.49608568219837\n",
      "Adding trajectory to replay buffer: step 11213, counter 179005\n",
      "Environment 3: Episode 2095, Score -120.28086456436105, Avg_Score -120.47805776835168\n",
      "Adding trajectory to replay buffer: step 11213, counter 179051\n",
      "Environment 5: Episode 2096, Score -121.289645975686, Avg_Score -120.47000250215535\n",
      "Adding trajectory to replay buffer: step 11223, counter 179109\n",
      "Environment 14: Episode 2097, Score -115.97341861172765, Avg_Score -120.4862012739965\n",
      "Adding trajectory to replay buffer: step 11226, counter 179152\n",
      "Environment 7: Episode 2098, Score -117.5461317570582, Avg_Score -120.43709438362971\n",
      "Adding trajectory to replay buffer: step 11228, counter 179196\n",
      "Environment 11: Episode 2099, Score -122.39870752285235, Avg_Score -120.48182354671397\n",
      "Adding trajectory to replay buffer: step 11229, counter 179257\n",
      "Environment 1: Episode 2100, Score -116.49928328202789, Avg_Score -120.44577592183839\n",
      "Adding trajectory to replay buffer: step 11232, counter 179325\n",
      "Environment 15: Episode 2101, Score -118.03735141023, Avg_Score -120.40566543625793\n",
      "Adding trajectory to replay buffer: step 11236, counter 179372\n",
      "Environment 2: Episode 2102, Score -118.7270134139719, Avg_Score -120.39989239965924\n",
      "Adding trajectory to replay buffer: step 11239, counter 179414\n",
      "Environment 8: Episode 2103, Score -123.18136482675746, Avg_Score -120.40891769842979\n",
      "Adding trajectory to replay buffer: step 11240, counter 179458\n",
      "Environment 12: Episode 2104, Score -122.86794437992634, Avg_Score -120.42834205032658\n",
      "Adding trajectory to replay buffer: step 11241, counter 179501\n",
      "Environment 13: Episode 2105, Score -123.16514069223776, Avg_Score -120.4548704219158\n",
      "Adding trajectory to replay buffer: step 11242, counter 179544\n",
      "Environment 6: Episode 2106, Score -121.99446748234456, Avg_Score -120.47237058638451\n",
      "Adding trajectory to replay buffer: step 11246, counter 179589\n",
      "Environment 9: Episode 2107, Score -121.10500437245598, Avg_Score -120.53070996846849\n",
      "Adding trajectory to replay buffer: step 11253, counter 179636\n",
      "Environment 10: Episode 2108, Score -120.4902832911412, Avg_Score -120.5312341415418\n",
      "Adding trajectory to replay buffer: step 11256, counter 179679\n",
      "Environment 5: Episode 2109, Score -123.9318574198025, Avg_Score -120.57025742365029\n",
      "Adding trajectory to replay buffer: step 11258, counter 179724\n",
      "Environment 3: Episode 2110, Score -121.63544667904483, Avg_Score -120.57331882537403\n",
      "Adding trajectory to replay buffer: step 11267, counter 179768\n",
      "Environment 14: Episode 2111, Score -122.13095637486627, Avg_Score -120.584416083374\n",
      "Adding trajectory to replay buffer: step 11268, counter 179853\n",
      "Environment 0: Episode 2112, Score -112.7963776924902, Avg_Score -120.51386065720934\n",
      "Adding trajectory to replay buffer: step 11270, counter 179894\n",
      "Environment 1: Episode 2113, Score -116.43726119970779, Avg_Score -120.45669143133301\n",
      "Adding trajectory to replay buffer: step 11271, counter 179939\n",
      "Environment 7: Episode 2114, Score -121.45730416822558, Avg_Score -120.51474860771064\n",
      "Adding trajectory to replay buffer: step 11272, counter 179983\n",
      "Environment 11: Episode 2115, Score -123.15095901869238, Avg_Score -120.50823105187172\n",
      "Adding trajectory to replay buffer: step 11274, counter 180025\n",
      "Environment 15: Episode 2116, Score -114.91416780826326, Avg_Score -120.50676769134004\n",
      "Adding trajectory to replay buffer: step 11281, counter 180070\n",
      "Environment 2: Episode 2117, Score -122.01761692375317, Avg_Score -120.60206699737772\n",
      "Adding trajectory to replay buffer: step 11285, counter 180163\n",
      "Environment 4: Episode 2118, Score -112.16865112670884, Avg_Score -120.580848442287\n",
      "Adding trajectory to replay buffer: step 11288, counter 180209\n",
      "Environment 6: Episode 2119, Score -119.48125666724891, Avg_Score -120.57006430980611\n",
      "Adding trajectory to replay buffer: step 11289, counter 180252\n",
      "Environment 9: Episode 2120, Score -121.75202044610865, Avg_Score -120.58954272797877\n",
      "Adding trajectory to replay buffer: step 11296, counter 180295\n",
      "Environment 10: Episode 2121, Score -122.77830057483477, Avg_Score -120.59540172355467\n",
      "Adding trajectory to replay buffer: step 11298, counter 180352\n",
      "Environment 13: Episode 2122, Score -115.53011710402556, Avg_Score -120.52923384288528\n",
      "Adding trajectory to replay buffer: step 11299, counter 180395\n",
      "Environment 5: Episode 2123, Score -122.70945573969853, Avg_Score -120.52668783711476\n",
      "Adding trajectory to replay buffer: step 11301, counter 180456\n",
      "Environment 12: Episode 2124, Score -113.8686110110879, Avg_Score -120.5046190408458\n",
      "Adding trajectory to replay buffer: step 11305, counter 180503\n",
      "Environment 3: Episode 2125, Score -120.88572201409812, Avg_Score -120.48381521797204\n",
      "Adding trajectory to replay buffer: step 11310, counter 180546\n",
      "Environment 14: Episode 2126, Score -117.3062415417321, Avg_Score -120.40631109138256\n",
      "Adding trajectory to replay buffer: step 11318, counter 180593\n",
      "Environment 7: Episode 2127, Score -119.46785924499792, Avg_Score -120.39840302632325\n",
      "Adding trajectory to replay buffer: step 11318, counter 180637\n",
      "Environment 15: Episode 2128, Score -121.42038867946529, Avg_Score -120.39926689552502\n",
      "Adding trajectory to replay buffer: step 11323, counter 180688\n",
      "Environment 11: Episode 2129, Score -120.62182039958797, Avg_Score -120.39896982825725\n",
      "Adding trajectory to replay buffer: step 11325, counter 180732\n",
      "Environment 2: Episode 2130, Score -120.61938452172342, Avg_Score -120.45003205233513\n",
      "Adding trajectory to replay buffer: step 11326, counter 180819\n",
      "Environment 8: Episode 2131, Score -114.89586396748635, Avg_Score -120.39354068078332\n",
      "Adding trajectory to replay buffer: step 11329, counter 180863\n",
      "Environment 4: Episode 2132, Score -121.36313089072519, Avg_Score -120.44446527578818\n",
      "Adding trajectory to replay buffer: step 11338, counter 180931\n",
      "Environment 1: Episode 2133, Score -114.86913057928844, Avg_Score -120.38816821965324\n",
      "Adding trajectory to replay buffer: step 11338, counter 180980\n",
      "Environment 9: Episode 2134, Score -120.41154121088174, Avg_Score -120.37386344584151\n",
      "Adding trajectory to replay buffer: step 11340, counter 181032\n",
      "Environment 6: Episode 2135, Score -118.81510040092779, Avg_Score -120.33499275879554\n",
      "Adding trajectory to replay buffer: step 11346, counter 181079\n",
      "Environment 5: Episode 2136, Score -123.29174904728433, Avg_Score -120.3507649545751\n",
      "Adding trajectory to replay buffer: step 11351, counter 181125\n",
      "Environment 3: Episode 2137, Score -122.92572562104712, Avg_Score -120.36511336294865\n",
      "Adding trajectory to replay buffer: step 11352, counter 181179\n",
      "Environment 13: Episode 2138, Score -118.02273887038045, Avg_Score -120.31978307537015\n",
      "Adding trajectory to replay buffer: step 11353, counter 181222\n",
      "Environment 14: Episode 2139, Score -123.06156282336947, Avg_Score -120.32549905446554\n",
      "Adding trajectory to replay buffer: step 11355, counter 181281\n",
      "Environment 10: Episode 2140, Score -115.64957472528455, Avg_Score -120.25452507139308\n",
      "Adding trajectory to replay buffer: step 11361, counter 181324\n",
      "Environment 15: Episode 2141, Score -119.37326555056622, Avg_Score -120.28882942275737\n",
      "Adding trajectory to replay buffer: step 11362, counter 181368\n",
      "Environment 7: Episode 2142, Score -120.72195661090501, Avg_Score -120.26772653419779\n",
      "Adding trajectory to replay buffer: step 11370, counter 181415\n",
      "Environment 11: Episode 2143, Score -120.11333202166347, Avg_Score -120.24175059667951\n",
      "Adding trajectory to replay buffer: step 11377, counter 181463\n",
      "Environment 4: Episode 2144, Score -120.38166021882307, Avg_Score -120.24003194741506\n",
      "Adding trajectory to replay buffer: step 11377, counter 181514\n",
      "Environment 8: Episode 2145, Score -120.28963654659876, Avg_Score -120.23476606650834\n",
      "Adding trajectory to replay buffer: step 11381, counter 181557\n",
      "Environment 1: Episode 2146, Score -117.6529795897901, Avg_Score -120.1878271453735\n",
      "Adding trajectory to replay buffer: step 11381, counter 181600\n",
      "Environment 9: Episode 2147, Score -119.1660620741658, Avg_Score -120.1894453388756\n",
      "Adding trajectory to replay buffer: step 11395, counter 181643\n",
      "Environment 13: Episode 2148, Score -117.08066341385866, Avg_Score -120.14121130741287\n",
      "Adding trajectory to replay buffer: step 11396, counter 181688\n",
      "Environment 3: Episode 2149, Score -122.54575895581456, Avg_Score -120.14769328688257\n",
      "Adding trajectory to replay buffer: step 11396, counter 181731\n",
      "Environment 14: Episode 2150, Score -117.85206955017335, Avg_Score -120.11014239358329\n",
      "Adding trajectory to replay buffer: step 11405, counter 181774\n",
      "Environment 7: Episode 2151, Score -122.94698201239234, Avg_Score -120.11773000509451\n",
      "Adding trajectory to replay buffer: step 11405, counter 181818\n",
      "Environment 15: Episode 2152, Score -122.0580333198725, Avg_Score -120.116369141816\n",
      "Adding trajectory to replay buffer: step 11408, counter 181901\n",
      "Environment 2: Episode 2153, Score -122.4221215105698, Avg_Score -120.09583278215689\n",
      "Adding trajectory to replay buffer: step 11411, counter 181957\n",
      "Environment 10: Episode 2154, Score -115.11017271139907, Avg_Score -120.12387533731838\n",
      "Adding trajectory to replay buffer: step 11415, counter 182002\n",
      "Environment 11: Episode 2155, Score -121.51185226792718, Avg_Score -120.11752684798458\n",
      "Adding trajectory to replay buffer: step 11417, counter 182079\n",
      "Environment 6: Episode 2156, Score -117.13354699912605, Avg_Score -120.12509897065829\n",
      "Adding trajectory to replay buffer: step 11421, counter 182123\n",
      "Environment 4: Episode 2157, Score -120.39623237048463, Avg_Score -120.10057383416019\n",
      "Adding trajectory to replay buffer: step 11421, counter 182167\n",
      "Environment 8: Episode 2158, Score -120.81916297917999, Avg_Score -120.09054342670262\n",
      "Adding trajectory to replay buffer: step 11422, counter 182288\n",
      "Environment 12: Episode 2159, Score -113.0437667037553, Avg_Score -120.01702961212361\n",
      "Adding trajectory to replay buffer: step 11424, counter 182331\n",
      "Environment 1: Episode 2160, Score -119.53461806421541, Avg_Score -120.03109915385596\n",
      "Adding trajectory to replay buffer: step 11425, counter 182375\n",
      "Environment 9: Episode 2161, Score -121.27758558573078, Avg_Score -120.07337420594602\n",
      "Adding trajectory to replay buffer: step 11427, counter 182534\n",
      "Environment 0: Episode 2162, Score -116.9890778737211, Avg_Score -120.05099025203948\n",
      "Adding trajectory to replay buffer: step 11440, counter 182578\n",
      "Environment 3: Episode 2163, Score -122.15240714938194, Avg_Score -120.09988997829788\n",
      "Adding trajectory to replay buffer: step 11440, counter 182623\n",
      "Environment 13: Episode 2164, Score -122.99076004780984, Avg_Score -120.10502555214345\n",
      "Adding trajectory to replay buffer: step 11440, counter 182667\n",
      "Environment 14: Episode 2165, Score -120.47828967109571, Avg_Score -120.08033806191693\n",
      "Adding trajectory to replay buffer: step 11453, counter 182715\n",
      "Environment 15: Episode 2166, Score -122.80974485737954, Avg_Score -120.07995791590331\n",
      "Adding trajectory to replay buffer: step 11455, counter 182824\n",
      "Environment 5: Episode 2167, Score -124.97835685633247, Avg_Score -120.09861190278558\n",
      "Adding trajectory to replay buffer: step 11458, counter 182867\n",
      "Environment 11: Episode 2168, Score -122.5485121325478, Avg_Score -120.0994567689577\n",
      "Adding trajectory to replay buffer: step 11462, counter 182912\n",
      "Environment 6: Episode 2169, Score -122.25762686771216, Avg_Score -120.17292804396381\n",
      "Adding trajectory to replay buffer: step 11466, counter 182957\n",
      "Environment 4: Episode 2170, Score -123.86673143332266, Avg_Score -120.30372535444991\n",
      "Adding trajectory to replay buffer: step 11467, counter 183002\n",
      "Environment 12: Episode 2171, Score -123.31744258963627, Avg_Score -120.30753655007722\n",
      "Adding trajectory to replay buffer: step 11468, counter 183049\n",
      "Environment 8: Episode 2172, Score -116.98939297632687, Avg_Score -120.24804061735385\n",
      "Adding trajectory to replay buffer: step 11471, counter 183095\n",
      "Environment 9: Episode 2173, Score -117.02360406899662, Avg_Score -120.1881735922301\n",
      "Adding trajectory to replay buffer: step 11472, counter 183162\n",
      "Environment 7: Episode 2174, Score -113.98836370572448, Avg_Score -120.11667419231539\n",
      "Adding trajectory to replay buffer: step 11475, counter 183229\n",
      "Environment 2: Episode 2175, Score -112.43277541541121, Avg_Score -120.00859073449121\n",
      "Adding trajectory to replay buffer: step 11484, counter 183273\n",
      "Environment 3: Episode 2176, Score -121.53708558390197, Avg_Score -119.97573099710289\n",
      "Adding trajectory to replay buffer: step 11484, counter 183317\n",
      "Environment 13: Episode 2177, Score -121.6879350710592, Avg_Score -119.96442907611917\n",
      "Adding trajectory to replay buffer: step 11484, counter 183361\n",
      "Environment 14: Episode 2178, Score -122.39143014721449, Avg_Score -120.04845072216273\n",
      "Adding trajectory to replay buffer: step 11487, counter 183421\n",
      "Environment 0: Episode 2179, Score -115.80284696456914, Avg_Score -119.97365216874945\n",
      "Adding trajectory to replay buffer: step 11495, counter 183492\n",
      "Environment 1: Episode 2180, Score -118.84609424427276, Avg_Score -119.9333441087745\n",
      "Adding trajectory to replay buffer: step 11500, counter 183537\n",
      "Environment 5: Episode 2181, Score -121.77394161158986, Avg_Score -119.91920846982413\n",
      "Adding trajectory to replay buffer: step 11501, counter 183580\n",
      "Environment 11: Episode 2182, Score -120.83447864794793, Avg_Score -119.89932015661635\n",
      "Adding trajectory to replay buffer: step 11508, counter 183626\n",
      "Environment 6: Episode 2183, Score -122.54593303384942, Avg_Score -119.90013425033418\n",
      "Adding trajectory to replay buffer: step 11509, counter 183669\n",
      "Environment 4: Episode 2184, Score -122.85603604455231, Avg_Score -119.91184620996378\n",
      "Adding trajectory to replay buffer: step 11510, counter 183712\n",
      "Environment 12: Episode 2185, Score -123.16286200333884, Avg_Score -119.9237827495073\n",
      "Adding trajectory to replay buffer: step 11512, counter 183756\n",
      "Environment 8: Episode 2186, Score -122.9171797165107, Avg_Score -119.9514235123379\n",
      "Adding trajectory to replay buffer: step 11513, counter 183798\n",
      "Environment 9: Episode 2187, Score -123.23799512209744, Avg_Score -119.90988705884293\n",
      "Adding trajectory to replay buffer: step 11519, counter 183842\n",
      "Environment 2: Episode 2188, Score -121.67280039629588, Avg_Score -119.90689338984852\n",
      "Adding trajectory to replay buffer: step 11526, counter 183884\n",
      "Environment 3: Episode 2189, Score -122.79903036277307, Avg_Score -119.93245196039048\n",
      "Adding trajectory to replay buffer: step 11526, counter 183926\n",
      "Environment 14: Episode 2190, Score -122.74200555317476, Avg_Score -119.9300017367639\n",
      "Adding trajectory to replay buffer: step 11527, counter 183969\n",
      "Environment 13: Episode 2191, Score -123.48979922684345, Avg_Score -120.03276289190872\n",
      "Adding trajectory to replay buffer: step 11543, counter 184012\n",
      "Environment 5: Episode 2192, Score -120.89256992545228, Avg_Score -120.023836696087\n",
      "Adding trajectory to replay buffer: step 11544, counter 184055\n",
      "Environment 11: Episode 2193, Score -119.01894523556437, Avg_Score -119.99342147042574\n",
      "Adding trajectory to replay buffer: step 11545, counter 184113\n",
      "Environment 0: Episode 2194, Score -113.16435793277373, Avg_Score -119.99572387601808\n",
      "Adding trajectory to replay buffer: step 11545, counter 184247\n",
      "Environment 10: Episode 2195, Score -115.03479992005074, Avg_Score -119.94326322957498\n",
      "Adding trajectory to replay buffer: step 11552, counter 184327\n",
      "Environment 7: Episode 2196, Score -112.9529534824205, Avg_Score -119.85989630464233\n",
      "Adding trajectory to replay buffer: step 11552, counter 184426\n",
      "Environment 15: Episode 2197, Score -115.14747535759398, Avg_Score -119.85163687210101\n",
      "Adding trajectory to replay buffer: step 11554, counter 184471\n",
      "Environment 4: Episode 2198, Score -120.20793652044671, Avg_Score -119.87825491973489\n",
      "Adding trajectory to replay buffer: step 11555, counter 184516\n",
      "Environment 12: Episode 2199, Score -121.53075465048725, Avg_Score -119.86957539101124\n",
      "Adding trajectory to replay buffer: step 11558, counter 184562\n",
      "Environment 8: Episode 2200, Score -121.43628909376885, Avg_Score -119.91894544912864\n",
      "Adding trajectory to replay buffer: step 11562, counter 184629\n",
      "Environment 1: Episode 2201, Score -115.65739671546594, Avg_Score -119.89514590218097\n",
      "Adding trajectory to replay buffer: step 11571, counter 184674\n",
      "Environment 3: Episode 2202, Score -121.86166038286129, Avg_Score -119.92649237186987\n",
      "Adding trajectory to replay buffer: step 11572, counter 184719\n",
      "Environment 13: Episode 2203, Score -123.2148240283467, Avg_Score -119.92682696388576\n",
      "Adding trajectory to replay buffer: step 11572, counter 184765\n",
      "Environment 14: Episode 2204, Score -123.78923934826938, Avg_Score -119.93603991356919\n",
      "Adding trajectory to replay buffer: step 11578, counter 184835\n",
      "Environment 6: Episode 2205, Score -118.98945633724021, Avg_Score -119.89428307001923\n",
      "Adding trajectory to replay buffer: step 11586, counter 184878\n",
      "Environment 5: Episode 2206, Score -122.20712046635089, Avg_Score -119.89640959985928\n",
      "Adding trajectory to replay buffer: step 11588, counter 184921\n",
      "Environment 0: Episode 2207, Score -122.48109249713521, Avg_Score -119.91017048110606\n",
      "Adding trajectory to replay buffer: step 11588, counter 184964\n",
      "Environment 10: Episode 2208, Score -118.77527217995748, Avg_Score -119.89302036999425\n",
      "Adding trajectory to replay buffer: step 11596, counter 185008\n",
      "Environment 15: Episode 2209, Score -123.41745540536567, Avg_Score -119.88787634984986\n",
      "Adding trajectory to replay buffer: step 11597, counter 185053\n",
      "Environment 7: Episode 2210, Score -120.70458199793286, Avg_Score -119.87856770303875\n",
      "Adding trajectory to replay buffer: step 11599, counter 185098\n",
      "Environment 4: Episode 2211, Score -122.39688351138682, Avg_Score -119.88122697440393\n",
      "Adding trajectory to replay buffer: step 11604, counter 185144\n",
      "Environment 8: Episode 2212, Score -121.66024209257962, Avg_Score -119.96986561840484\n",
      "Adding trajectory to replay buffer: step 11610, counter 185192\n",
      "Environment 1: Episode 2213, Score -121.05317161892145, Avg_Score -120.01602472259695\n",
      "Adding trajectory to replay buffer: step 11614, counter 185234\n",
      "Environment 14: Episode 2214, Score -117.22836269606181, Avg_Score -119.97373530787534\n",
      "Adding trajectory to replay buffer: step 11617, counter 185279\n",
      "Environment 13: Episode 2215, Score -122.0369678176567, Avg_Score -119.96259539586498\n",
      "Adding trajectory to replay buffer: step 11618, counter 185326\n",
      "Environment 3: Episode 2216, Score -117.36964868730493, Avg_Score -119.98715020465542\n",
      "Adding trajectory to replay buffer: step 11619, counter 185426\n",
      "Environment 2: Episode 2217, Score -113.18880021934832, Avg_Score -119.89886203761138\n",
      "Adding trajectory to replay buffer: step 11624, counter 185472\n",
      "Environment 6: Episode 2218, Score -119.31583174595423, Avg_Score -119.97033384380381\n",
      "Adding trajectory to replay buffer: step 11628, counter 185556\n",
      "Environment 11: Episode 2219, Score -115.24899122459442, Avg_Score -119.92801118937729\n",
      "Adding trajectory to replay buffer: step 11632, counter 185602\n",
      "Environment 5: Episode 2220, Score -121.82970378840342, Avg_Score -119.92878802280022\n",
      "Adding trajectory to replay buffer: step 11632, counter 185721\n",
      "Environment 9: Episode 2221, Score -119.3767375052219, Avg_Score -119.89477239210412\n",
      "Adding trajectory to replay buffer: step 11639, counter 185761\n",
      "Environment 4: Episode 2222, Score -117.64830251398558, Avg_Score -119.91595424620371\n",
      "Adding trajectory to replay buffer: step 11640, counter 185804\n",
      "Environment 7: Episode 2223, Score -123.50275123966733, Avg_Score -119.92388720120336\n",
      "Adding trajectory to replay buffer: step 11648, counter 185848\n",
      "Environment 8: Episode 2224, Score -122.46891794969157, Avg_Score -120.00989027058941\n",
      "Adding trajectory to replay buffer: step 11652, counter 185904\n",
      "Environment 15: Episode 2225, Score -115.8326219674355, Avg_Score -119.95935927012277\n",
      "Adding trajectory to replay buffer: step 11655, counter 185949\n",
      "Environment 1: Episode 2226, Score -121.07424510886497, Avg_Score -119.99703930579412\n",
      "Adding trajectory to replay buffer: step 11656, counter 186050\n",
      "Environment 12: Episode 2227, Score -112.4790873476568, Avg_Score -119.92715158682073\n",
      "Adding trajectory to replay buffer: step 11664, counter 186095\n",
      "Environment 2: Episode 2228, Score -119.84760101617749, Avg_Score -119.91142371018785\n",
      "Adding trajectory to replay buffer: step 11668, counter 186149\n",
      "Environment 14: Episode 2229, Score -114.75271605189641, Avg_Score -119.85273266671092\n",
      "Adding trajectory to replay buffer: step 11671, counter 186192\n",
      "Environment 11: Episode 2230, Score -122.58638039822395, Avg_Score -119.87240262547593\n",
      "Adding trajectory to replay buffer: step 11674, counter 186249\n",
      "Environment 13: Episode 2231, Score -118.27746625529913, Avg_Score -119.90621864835408\n",
      "Adding trajectory to replay buffer: step 11675, counter 186292\n",
      "Environment 5: Episode 2232, Score -122.69333422063539, Avg_Score -119.91952068165318\n",
      "Adding trajectory to replay buffer: step 11675, counter 186335\n",
      "Environment 9: Episode 2233, Score -122.74984561788973, Avg_Score -119.9983278320392\n",
      "Adding trajectory to replay buffer: step 11682, counter 186378\n",
      "Environment 4: Episode 2234, Score -122.1105665455802, Avg_Score -120.01531808538618\n",
      "Adding trajectory to replay buffer: step 11686, counter 186424\n",
      "Environment 7: Episode 2235, Score -121.08457916617643, Avg_Score -120.03801287303868\n",
      "Adding trajectory to replay buffer: step 11698, counter 186467\n",
      "Environment 1: Episode 2236, Score -122.84990246027958, Avg_Score -120.03359440716862\n",
      "Adding trajectory to replay buffer: step 11698, counter 186509\n",
      "Environment 12: Episode 2237, Score -122.9528457664475, Avg_Score -120.03386560862262\n",
      "Adding trajectory to replay buffer: step 11708, counter 186593\n",
      "Environment 6: Episode 2238, Score -117.23977604618545, Avg_Score -120.02603598038067\n",
      "Adding trajectory to replay buffer: step 11713, counter 186642\n",
      "Environment 2: Episode 2239, Score -116.9329778754227, Avg_Score -119.9647501309012\n",
      "Adding trajectory to replay buffer: step 11714, counter 186704\n",
      "Environment 15: Episode 2240, Score -113.88625018554988, Avg_Score -119.94711688550387\n",
      "Adding trajectory to replay buffer: step 11717, counter 186747\n",
      "Environment 13: Episode 2241, Score -122.15149687756598, Avg_Score -119.97489919877387\n",
      "Adding trajectory to replay buffer: step 11719, counter 186795\n",
      "Environment 11: Episode 2242, Score -117.30532562053142, Avg_Score -119.94073288887013\n",
      "Adding trajectory to replay buffer: step 11723, counter 186870\n",
      "Environment 8: Episode 2243, Score -111.54910200468885, Avg_Score -119.8550905887004\n",
      "Adding trajectory to replay buffer: step 11724, counter 187006\n",
      "Environment 10: Episode 2244, Score -128.08513577784132, Avg_Score -119.93212534429058\n",
      "Adding trajectory to replay buffer: step 11726, counter 187050\n",
      "Environment 4: Episode 2245, Score -123.5078222760788, Avg_Score -119.9643072015854\n",
      "Adding trajectory to replay buffer: step 11728, counter 187160\n",
      "Environment 3: Episode 2246, Score -127.23765487265865, Avg_Score -120.06015395441403\n",
      "Adding trajectory to replay buffer: step 11729, counter 187203\n",
      "Environment 7: Episode 2247, Score -122.80416363819192, Avg_Score -120.09653497005431\n",
      "Adding trajectory to replay buffer: step 11738, counter 187273\n",
      "Environment 14: Episode 2248, Score -115.34422563713268, Avg_Score -120.07917059228703\n",
      "Adding trajectory to replay buffer: step 11755, counter 187330\n",
      "Environment 1: Episode 2249, Score -119.21903560547096, Avg_Score -120.0459033587836\n",
      "Adding trajectory to replay buffer: step 11759, counter 187501\n",
      "Environment 0: Episode 2250, Score -118.66712169904727, Avg_Score -120.0540538802723\n",
      "Adding trajectory to replay buffer: step 11759, counter 187547\n",
      "Environment 2: Episode 2251, Score -119.62401655746251, Avg_Score -120.020824225723\n",
      "Adding trajectory to replay buffer: step 11759, counter 187592\n",
      "Environment 15: Episode 2252, Score -121.73220034555966, Avg_Score -120.0175658959799\n",
      "Adding trajectory to replay buffer: step 11760, counter 187635\n",
      "Environment 13: Episode 2253, Score -122.16285833276933, Avg_Score -120.01497326420187\n",
      "Adding trajectory to replay buffer: step 11771, counter 187680\n",
      "Environment 4: Episode 2254, Score -122.39480048978888, Avg_Score -120.08781954198575\n",
      "Adding trajectory to replay buffer: step 11772, counter 187723\n",
      "Environment 7: Episode 2255, Score -122.31752635621962, Avg_Score -120.09587628286869\n",
      "Adding trajectory to replay buffer: step 11776, counter 187824\n",
      "Environment 9: Episode 2256, Score -122.23734903101189, Avg_Score -120.14691430318754\n",
      "Adding trajectory to replay buffer: step 11776, counter 187881\n",
      "Environment 11: Episode 2257, Score -114.96216777314433, Avg_Score -120.09257365721413\n",
      "Adding trajectory to replay buffer: step 11777, counter 187935\n",
      "Environment 8: Episode 2258, Score -119.30222577275833, Avg_Score -120.07740428514991\n",
      "Adding trajectory to replay buffer: step 11781, counter 187978\n",
      "Environment 14: Episode 2259, Score -122.72882236531501, Avg_Score -120.17425484176552\n",
      "Adding trajectory to replay buffer: step 11783, counter 188086\n",
      "Environment 5: Episode 2260, Score -124.42597993210889, Avg_Score -120.22316846044446\n",
      "Adding trajectory to replay buffer: step 11796, counter 188154\n",
      "Environment 3: Episode 2261, Score -114.35761843806257, Avg_Score -120.15396878896777\n",
      "Adding trajectory to replay buffer: step 11799, counter 188198\n",
      "Environment 1: Episode 2262, Score -122.82358807358145, Avg_Score -120.21231389096634\n",
      "Adding trajectory to replay buffer: step 11805, counter 188244\n",
      "Environment 0: Episode 2263, Score -117.4587625238678, Avg_Score -120.16537744471121\n",
      "Adding trajectory to replay buffer: step 11808, counter 188293\n",
      "Environment 2: Episode 2264, Score -118.16842908494236, Avg_Score -120.11715413508256\n",
      "Adding trajectory to replay buffer: step 11816, counter 188338\n",
      "Environment 4: Episode 2265, Score -122.17985356510927, Avg_Score -120.13416977402268\n",
      "Adding trajectory to replay buffer: step 11816, counter 188394\n",
      "Environment 13: Episode 2266, Score -114.2910425636503, Avg_Score -120.0489827510854\n",
      "Adding trajectory to replay buffer: step 11817, counter 188439\n",
      "Environment 7: Episode 2267, Score -122.68039643358316, Avg_Score -120.02600314685792\n",
      "Adding trajectory to replay buffer: step 11817, counter 188558\n",
      "Environment 12: Episode 2268, Score -123.23124875513402, Avg_Score -120.03283051308377\n",
      "Adding trajectory to replay buffer: step 11820, counter 188602\n",
      "Environment 9: Episode 2269, Score -122.42150718707032, Avg_Score -120.03446931627735\n",
      "Adding trajectory to replay buffer: step 11820, counter 188646\n",
      "Environment 11: Episode 2270, Score -122.32539091663062, Avg_Score -120.01905591111041\n",
      "Adding trajectory to replay buffer: step 11821, counter 188690\n",
      "Environment 8: Episode 2271, Score -121.57076314880916, Avg_Score -120.00158911670216\n",
      "Adding trajectory to replay buffer: step 11826, counter 188792\n",
      "Environment 10: Episode 2272, Score -115.2010725700818, Avg_Score -119.9837059126397\n",
      "Adding trajectory to replay buffer: step 11826, counter 188837\n",
      "Environment 14: Episode 2273, Score -122.59267417280438, Avg_Score -120.03939661367778\n",
      "Adding trajectory to replay buffer: step 11827, counter 188881\n",
      "Environment 5: Episode 2274, Score -122.03708220323176, Avg_Score -120.11988379865285\n",
      "Adding trajectory to replay buffer: step 11827, counter 188949\n",
      "Environment 15: Episode 2275, Score -113.96059452269847, Avg_Score -120.13516198972572\n",
      "Adding trajectory to replay buffer: step 11839, counter 189080\n",
      "Environment 6: Episode 2276, Score -119.42967670150877, Avg_Score -120.11408790090178\n",
      "Adding trajectory to replay buffer: step 11843, counter 189127\n",
      "Environment 3: Episode 2277, Score -120.20343231470137, Avg_Score -120.09924287333821\n",
      "Adding trajectory to replay buffer: step 11859, counter 189170\n",
      "Environment 4: Episode 2278, Score -122.2947798327816, Avg_Score -120.09827637019389\n",
      "Adding trajectory to replay buffer: step 11859, counter 189213\n",
      "Environment 13: Episode 2279, Score -122.16765663784369, Avg_Score -120.16192446692664\n",
      "Adding trajectory to replay buffer: step 11861, counter 189257\n",
      "Environment 7: Episode 2280, Score -121.87002552956093, Avg_Score -120.19216377977953\n",
      "Adding trajectory to replay buffer: step 11861, counter 189301\n",
      "Environment 12: Episode 2281, Score -122.1139938381842, Avg_Score -120.19556430204548\n",
      "Adding trajectory to replay buffer: step 11863, counter 189359\n",
      "Environment 0: Episode 2282, Score -112.64213583967339, Avg_Score -120.11364087396272\n",
      "Adding trajectory to replay buffer: step 11863, counter 189402\n",
      "Environment 11: Episode 2283, Score -121.35701251551446, Avg_Score -120.10175166877939\n",
      "Adding trajectory to replay buffer: step 11865, counter 189446\n",
      "Environment 8: Episode 2284, Score -122.28784112588068, Avg_Score -120.09606971959266\n",
      "Adding trajectory to replay buffer: step 11867, counter 189505\n",
      "Environment 2: Episode 2285, Score -116.31795271601527, Avg_Score -120.02762062671944\n",
      "Adding trajectory to replay buffer: step 11867, counter 189552\n",
      "Environment 9: Episode 2286, Score -119.75055389091932, Avg_Score -119.99595436846353\n",
      "Adding trajectory to replay buffer: step 11869, counter 189595\n",
      "Environment 14: Episode 2287, Score -117.37461805757694, Avg_Score -119.93732059781833\n",
      "Adding trajectory to replay buffer: step 11871, counter 189640\n",
      "Environment 10: Episode 2288, Score -120.65869447619468, Avg_Score -119.92717953861731\n",
      "Adding trajectory to replay buffer: step 11885, counter 189682\n",
      "Environment 3: Episode 2289, Score -123.61784360148137, Avg_Score -119.93536767100439\n",
      "Adding trajectory to replay buffer: step 11885, counter 189740\n",
      "Environment 15: Episode 2290, Score -114.45495938281032, Avg_Score -119.85249720930075\n",
      "Adding trajectory to replay buffer: step 11887, counter 189788\n",
      "Environment 6: Episode 2291, Score -117.47558897590079, Avg_Score -119.79235510679133\n",
      "Adding trajectory to replay buffer: step 11900, counter 189861\n",
      "Environment 5: Episode 2292, Score -119.50079477167439, Avg_Score -119.77843735525353\n",
      "Adding trajectory to replay buffer: step 11904, counter 189906\n",
      "Environment 4: Episode 2293, Score -119.644060526552, Avg_Score -119.78468850816344\n",
      "Adding trajectory to replay buffer: step 11906, counter 189951\n",
      "Environment 12: Episode 2294, Score -122.52974791051571, Avg_Score -119.87834240794088\n",
      "Adding trajectory to replay buffer: step 11906, counter 189998\n",
      "Environment 13: Episode 2295, Score -122.33791203782987, Avg_Score -119.95137352911864\n",
      "Adding trajectory to replay buffer: step 11907, counter 190044\n",
      "Environment 7: Episode 2296, Score -122.46896108160976, Avg_Score -120.04653360511053\n",
      "Adding trajectory to replay buffer: step 11908, counter 190089\n",
      "Environment 11: Episode 2297, Score -117.85263064968959, Avg_Score -120.07358515803149\n",
      "Adding trajectory to replay buffer: step 11910, counter 190132\n",
      "Environment 2: Episode 2298, Score -121.90367223201693, Avg_Score -120.09054251514718\n",
      "Adding trajectory to replay buffer: step 11910, counter 190175\n",
      "Environment 9: Episode 2299, Score -122.0516848656262, Avg_Score -120.09575181729859\n",
      "Adding trajectory to replay buffer: step 11913, counter 190217\n",
      "Environment 10: Episode 2300, Score -122.96357311914365, Avg_Score -120.11102465755232\n",
      "Adding trajectory to replay buffer: step 11913, counter 190261\n",
      "Environment 14: Episode 2301, Score -122.5528901658052, Avg_Score -120.17997959205572\n",
      "Adding trajectory to replay buffer: step 11921, counter 190317\n",
      "Environment 8: Episode 2302, Score -115.99616835820551, Avg_Score -120.12132467180918\n",
      "Adding trajectory to replay buffer: step 11929, counter 190447\n",
      "Environment 1: Episode 2303, Score -128.89150168666538, Avg_Score -120.17809144839237\n",
      "Adding trajectory to replay buffer: step 11931, counter 190491\n",
      "Environment 6: Episode 2304, Score -118.78196316670254, Avg_Score -120.12801868657671\n",
      "Adding trajectory to replay buffer: step 11943, counter 190534\n",
      "Environment 5: Episode 2305, Score -121.66129432557025, Avg_Score -120.15473706646\n",
      "Adding trajectory to replay buffer: step 11946, counter 190595\n",
      "Environment 3: Episode 2306, Score -111.60037536588248, Avg_Score -120.04866961545531\n",
      "Adding trajectory to replay buffer: step 11948, counter 190637\n",
      "Environment 12: Episode 2307, Score -122.96386718716658, Avg_Score -120.05349736235563\n",
      "Adding trajectory to replay buffer: step 11949, counter 190682\n",
      "Environment 4: Episode 2308, Score -122.42392446266177, Avg_Score -120.08998388518266\n",
      "Adding trajectory to replay buffer: step 11950, counter 190726\n",
      "Environment 13: Episode 2309, Score -121.52454157936387, Avg_Score -120.07105474692266\n",
      "Adding trajectory to replay buffer: step 11955, counter 190771\n",
      "Environment 2: Episode 2310, Score -116.37811514107139, Avg_Score -120.02779007835402\n",
      "Adding trajectory to replay buffer: step 11955, counter 190816\n",
      "Environment 9: Episode 2311, Score -116.1981736273182, Avg_Score -119.96580297951334\n",
      "Adding trajectory to replay buffer: step 11956, counter 190859\n",
      "Environment 14: Episode 2312, Score -122.2296223688523, Avg_Score -119.97149678227608\n",
      "Adding trajectory to replay buffer: step 11957, counter 190903\n",
      "Environment 10: Episode 2313, Score -120.46729224889104, Avg_Score -119.96563798857578\n",
      "Adding trajectory to replay buffer: step 11966, counter 191006\n",
      "Environment 0: Episode 2314, Score -118.24843366169247, Avg_Score -119.97583869823207\n",
      "Adding trajectory to replay buffer: step 11970, counter 191068\n",
      "Environment 11: Episode 2315, Score -116.2139058786792, Avg_Score -119.91760807884233\n",
      "Adding trajectory to replay buffer: step 11971, counter 191154\n",
      "Environment 15: Episode 2316, Score -112.45015584227939, Avg_Score -119.86841315039204\n",
      "Adding trajectory to replay buffer: step 11977, counter 191202\n",
      "Environment 1: Episode 2317, Score -120.00637128401175, Avg_Score -119.93658886103871\n",
      "Adding trajectory to replay buffer: step 11979, counter 191260\n",
      "Environment 8: Episode 2318, Score -115.06132735090702, Avg_Score -119.89404381708825\n",
      "Adding trajectory to replay buffer: step 11988, counter 191305\n",
      "Environment 5: Episode 2319, Score -120.09706868936495, Avg_Score -119.94252459173595\n",
      "Adding trajectory to replay buffer: step 11992, counter 191351\n",
      "Environment 3: Episode 2320, Score -122.3921744909746, Avg_Score -119.94814929876165\n",
      "Adding trajectory to replay buffer: step 11992, counter 191395\n",
      "Environment 12: Episode 2321, Score -123.35724969890651, Avg_Score -119.9879544206985\n",
      "Adding trajectory to replay buffer: step 11996, counter 191441\n",
      "Environment 13: Episode 2322, Score -119.02547346231155, Avg_Score -120.00172613018177\n",
      "Adding trajectory to replay buffer: step 11998, counter 191484\n",
      "Environment 9: Episode 2323, Score -122.53146966323257, Avg_Score -119.99201331441742\n",
      "Adding trajectory to replay buffer: step 11999, counter 191528\n",
      "Environment 2: Episode 2324, Score -121.80962963961127, Avg_Score -119.9854204313166\n",
      "Adding trajectory to replay buffer: step 12001, counter 191572\n",
      "Environment 10: Episode 2325, Score -119.86794699227127, Avg_Score -120.02577368156496\n",
      "Adding trajectory to replay buffer: step 12014, counter 191620\n",
      "Environment 0: Episode 2326, Score -121.30106056896659, Avg_Score -120.02804183616598\n",
      "Adding trajectory to replay buffer: step 12015, counter 191664\n",
      "Environment 15: Episode 2327, Score -121.23370672294311, Avg_Score -120.11558802991884\n",
      "Adding trajectory to replay buffer: step 12017, counter 191711\n",
      "Environment 11: Episode 2328, Score -121.78634709946651, Avg_Score -120.13497549075173\n",
      "Adding trajectory to replay buffer: step 12031, counter 191811\n",
      "Environment 6: Episode 2329, Score -121.61458683597235, Avg_Score -120.2035941985925\n",
      "Adding trajectory to replay buffer: step 12036, counter 191859\n",
      "Environment 5: Episode 2330, Score -119.06852395802488, Avg_Score -120.16841563419051\n",
      "Adding trajectory to replay buffer: step 12043, counter 191904\n",
      "Environment 9: Episode 2331, Score -122.45062209357788, Avg_Score -120.21014719257329\n",
      "Adding trajectory to replay buffer: step 12044, counter 191947\n",
      "Environment 10: Episode 2332, Score -122.5171486378964, Avg_Score -120.20838533674592\n",
      "Adding trajectory to replay buffer: step 12045, counter 191993\n",
      "Environment 2: Episode 2333, Score -121.89654441171263, Avg_Score -120.19985232468414\n",
      "Adding trajectory to replay buffer: step 12049, counter 192050\n",
      "Environment 3: Episode 2334, Score -115.01048546778597, Avg_Score -120.1288515139062\n",
      "Adding trajectory to replay buffer: step 12055, counter 192156\n",
      "Environment 4: Episode 2335, Score -123.75277682730369, Avg_Score -120.15553349051748\n",
      "Adding trajectory to replay buffer: step 12060, counter 192202\n",
      "Environment 0: Episode 2336, Score -116.46618249747902, Avg_Score -120.09169629088947\n",
      "Adding trajectory to replay buffer: step 12061, counter 192246\n",
      "Environment 11: Episode 2337, Score -116.31458688636062, Avg_Score -120.02531370208862\n",
      "Adding trajectory to replay buffer: step 12066, counter 192405\n",
      "Environment 7: Episode 2338, Score -117.57137923097548, Avg_Score -120.0286297339365\n",
      "Adding trajectory to replay buffer: step 12068, counter 192481\n",
      "Environment 12: Episode 2339, Score -122.47523652456762, Avg_Score -120.08405232042794\n",
      "Adding trajectory to replay buffer: step 12069, counter 192573\n",
      "Environment 1: Episode 2340, Score -112.62034101769825, Avg_Score -120.07139322874944\n",
      "Adding trajectory to replay buffer: step 12073, counter 192615\n",
      "Environment 6: Episode 2341, Score -123.12070853090223, Avg_Score -120.08108534528279\n",
      "Adding trajectory to replay buffer: step 12073, counter 192732\n",
      "Environment 14: Episode 2342, Score -115.89423781285745, Avg_Score -120.06697446720605\n",
      "Adding trajectory to replay buffer: step 12079, counter 192775\n",
      "Environment 5: Episode 2343, Score -119.60649164004873, Avg_Score -120.14754836355968\n",
      "Adding trajectory to replay buffer: step 12084, counter 192880\n",
      "Environment 8: Episode 2344, Score -124.68640683982646, Avg_Score -120.11356107417953\n",
      "Adding trajectory to replay buffer: step 12086, counter 192923\n",
      "Environment 9: Episode 2345, Score -115.5248838261813, Avg_Score -120.03373168968054\n",
      "Adding trajectory to replay buffer: step 12087, counter 192965\n",
      "Environment 2: Episode 2346, Score -118.97056355588634, Avg_Score -119.9510607765128\n",
      "Adding trajectory to replay buffer: step 12087, counter 193056\n",
      "Environment 13: Episode 2347, Score -114.17902206275116, Avg_Score -119.86480936075839\n",
      "Adding trajectory to replay buffer: step 12090, counter 193131\n",
      "Environment 15: Episode 2348, Score -121.82809014646337, Avg_Score -119.9296480058517\n",
      "Adding trajectory to replay buffer: step 12092, counter 193174\n",
      "Environment 3: Episode 2349, Score -121.86888408948667, Avg_Score -119.95614649069186\n",
      "Adding trajectory to replay buffer: step 12102, counter 193232\n",
      "Environment 10: Episode 2350, Score -115.59399249878774, Avg_Score -119.92541519868927\n",
      "Adding trajectory to replay buffer: step 12103, counter 193275\n",
      "Environment 0: Episode 2351, Score -122.92704203398526, Avg_Score -119.95844545345447\n",
      "Adding trajectory to replay buffer: step 12105, counter 193319\n",
      "Environment 11: Episode 2352, Score -122.00983416426305, Avg_Score -119.9612217916415\n",
      "Adding trajectory to replay buffer: step 12111, counter 193364\n",
      "Environment 7: Episode 2353, Score -121.72643806698731, Avg_Score -119.9568575889837\n",
      "Adding trajectory to replay buffer: step 12113, counter 193408\n",
      "Environment 1: Episode 2354, Score -122.2435081310328, Avg_Score -119.95534466539613\n",
      "Adding trajectory to replay buffer: step 12113, counter 193453\n",
      "Environment 12: Episode 2355, Score -121.17858910905942, Avg_Score -119.94395529292451\n",
      "Adding trajectory to replay buffer: step 12128, counter 193497\n",
      "Environment 8: Episode 2356, Score -120.00176167747068, Avg_Score -119.9215994193891\n",
      "Adding trajectory to replay buffer: step 12133, counter 193543\n",
      "Environment 2: Episode 2357, Score -122.57569568648996, Avg_Score -119.99773469852255\n",
      "Adding trajectory to replay buffer: step 12134, counter 193591\n",
      "Environment 9: Episode 2358, Score -117.8353014199296, Avg_Score -119.98306545499426\n",
      "Adding trajectory to replay buffer: step 12137, counter 193641\n",
      "Environment 13: Episode 2359, Score -117.03008743675673, Avg_Score -119.9260781057087\n",
      "Adding trajectory to replay buffer: step 12139, counter 193690\n",
      "Environment 15: Episode 2360, Score -117.75994372949005, Avg_Score -119.85941774368249\n",
      "Adding trajectory to replay buffer: step 12146, counter 193734\n",
      "Environment 10: Episode 2361, Score -117.62044956852318, Avg_Score -119.89204605498709\n",
      "Adding trajectory to replay buffer: step 12148, counter 193779\n",
      "Environment 0: Episode 2362, Score -121.52582650700211, Avg_Score -119.87906843932132\n",
      "Adding trajectory to replay buffer: step 12154, counter 193822\n",
      "Environment 7: Episode 2363, Score -123.26503973339948, Avg_Score -119.93713121141661\n",
      "Adding trajectory to replay buffer: step 12155, counter 193864\n",
      "Environment 12: Episode 2364, Score -122.68039954798421, Avg_Score -119.98225091604705\n",
      "Adding trajectory to replay buffer: step 12159, counter 193910\n",
      "Environment 1: Episode 2365, Score -120.65900858610061, Avg_Score -119.96704246625696\n",
      "Adding trajectory to replay buffer: step 12176, counter 193958\n",
      "Environment 8: Episode 2366, Score -119.36021516277393, Avg_Score -120.01773419224818\n",
      "Adding trajectory to replay buffer: step 12177, counter 194062\n",
      "Environment 14: Episode 2367, Score -118.89560228571482, Avg_Score -119.97988625076952\n",
      "Adding trajectory to replay buffer: step 12178, counter 194106\n",
      "Environment 9: Episode 2368, Score -116.01882077289248, Avg_Score -119.90776197094712\n",
      "Adding trajectory to replay buffer: step 12184, counter 194211\n",
      "Environment 5: Episode 2369, Score -124.4767887063343, Avg_Score -119.92831478613975\n",
      "Adding trajectory to replay buffer: step 12186, counter 194292\n",
      "Environment 11: Episode 2370, Score -119.74362861897487, Avg_Score -119.90249716316318\n",
      "Adding trajectory to replay buffer: step 12194, counter 194338\n",
      "Environment 0: Episode 2371, Score -122.88113423958235, Avg_Score -119.91560087407092\n",
      "Adding trajectory to replay buffer: step 12198, counter 194381\n",
      "Environment 12: Episode 2372, Score -115.48024759465642, Avg_Score -119.91839262431665\n",
      "Adding trajectory to replay buffer: step 12200, counter 194444\n",
      "Environment 13: Episode 2373, Score -116.13517523529566, Avg_Score -119.85381763494156\n",
      "Adding trajectory to replay buffer: step 12201, counter 194491\n",
      "Environment 7: Episode 2374, Score -121.84514305597732, Avg_Score -119.85189824346904\n",
      "Adding trajectory to replay buffer: step 12202, counter 194534\n",
      "Environment 1: Episode 2375, Score -122.95441520653976, Avg_Score -119.94183645030745\n",
      "Adding trajectory to replay buffer: step 12212, counter 194600\n",
      "Environment 10: Episode 2376, Score -115.03376570239726, Avg_Score -119.89787734031634\n",
      "Adding trajectory to replay buffer: step 12226, counter 194642\n",
      "Environment 5: Episode 2377, Score -123.09184497987354, Avg_Score -119.92676146696806\n",
      "Adding trajectory to replay buffer: step 12229, counter 194685\n",
      "Environment 11: Episode 2378, Score -122.61613372422367, Avg_Score -119.92997500588248\n",
      "Adding trajectory to replay buffer: step 12233, counter 194779\n",
      "Environment 15: Episode 2379, Score -115.72681202918974, Avg_Score -119.86556655979594\n",
      "Adding trajectory to replay buffer: step 12236, counter 194960\n",
      "Environment 4: Episode 2380, Score -118.81920465762161, Avg_Score -119.83505835107655\n",
      "Adding trajectory to replay buffer: step 12236, counter 195123\n",
      "Environment 6: Episode 2381, Score -117.29139227974927, Avg_Score -119.78683233549219\n",
      "Adding trajectory to replay buffer: step 12237, counter 195166\n",
      "Environment 0: Episode 2382, Score -118.9509294586039, Avg_Score -119.84992027168151\n",
      "Adding trajectory to replay buffer: step 12244, counter 195212\n",
      "Environment 12: Episode 2383, Score -122.46197085366398, Avg_Score -119.86096985506302\n",
      "Adding trajectory to replay buffer: step 12245, counter 195279\n",
      "Environment 9: Episode 2384, Score -115.0992733179244, Avg_Score -119.78908417698345\n",
      "Adding trajectory to replay buffer: step 12246, counter 195392\n",
      "Environment 2: Episode 2385, Score -116.72242623457545, Avg_Score -119.79312891216905\n",
      "Adding trajectory to replay buffer: step 12246, counter 195438\n",
      "Environment 13: Episode 2386, Score -122.96687220590624, Avg_Score -119.82529209531894\n",
      "Adding trajectory to replay buffer: step 12248, counter 195485\n",
      "Environment 7: Episode 2387, Score -121.0316664460373, Avg_Score -119.86186257920353\n",
      "Adding trajectory to replay buffer: step 12249, counter 195532\n",
      "Environment 1: Episode 2388, Score -119.03587851217824, Avg_Score -119.8456344195634\n",
      "Adding trajectory to replay buffer: step 12261, counter 195616\n",
      "Environment 14: Episode 2389, Score -120.09776773027579, Avg_Score -119.81043366085134\n",
      "Adding trajectory to replay buffer: step 12273, counter 195713\n",
      "Environment 8: Episode 2390, Score -119.93786410060774, Avg_Score -119.86526270802932\n",
      "Adding trajectory to replay buffer: step 12273, counter 195757\n",
      "Environment 11: Episode 2391, Score -121.65177087816534, Avg_Score -119.90702452705195\n",
      "Adding trajectory to replay buffer: step 12274, counter 195798\n",
      "Environment 15: Episode 2392, Score -117.46675349633892, Avg_Score -119.88668411429859\n",
      "Adding trajectory to replay buffer: step 12280, counter 195986\n",
      "Environment 3: Episode 2393, Score -119.8140456716741, Avg_Score -119.88838396574982\n",
      "Adding trajectory to replay buffer: step 12287, counter 196047\n",
      "Environment 5: Episode 2394, Score -115.80516511743267, Avg_Score -119.82113813781898\n",
      "Adding trajectory to replay buffer: step 12287, counter 196089\n",
      "Environment 9: Episode 2395, Score -114.71309337366498, Avg_Score -119.74488995117731\n",
      "Adding trajectory to replay buffer: step 12290, counter 196135\n",
      "Environment 12: Episode 2396, Score -110.82832680064192, Avg_Score -119.62848360836762\n",
      "Adding trajectory to replay buffer: step 12290, counter 196179\n",
      "Environment 13: Episode 2397, Score -123.80068574403165, Avg_Score -119.68796415931104\n",
      "Adding trajectory to replay buffer: step 12291, counter 196233\n",
      "Environment 0: Episode 2398, Score -114.98276812028078, Avg_Score -119.61875511819368\n",
      "Adding trajectory to replay buffer: step 12292, counter 196279\n",
      "Environment 2: Episode 2399, Score -118.37946606374004, Avg_Score -119.58203293017479\n",
      "Adding trajectory to replay buffer: step 12293, counter 196324\n",
      "Environment 7: Episode 2400, Score -121.7856498578433, Avg_Score -119.5702536975618\n",
      "Adding trajectory to replay buffer: step 12294, counter 196369\n",
      "Environment 1: Episode 2401, Score -121.78931896367916, Avg_Score -119.56261798554053\n",
      "Adding trajectory to replay buffer: step 12308, counter 196416\n",
      "Environment 14: Episode 2402, Score -122.3931212878724, Avg_Score -119.62658751483721\n",
      "Adding trajectory to replay buffer: step 12317, counter 196460\n",
      "Environment 8: Episode 2403, Score -122.81511451746648, Avg_Score -119.56582364314525\n",
      "Adding trajectory to replay buffer: step 12317, counter 196504\n",
      "Environment 11: Episode 2404, Score -122.72587420130195, Avg_Score -119.60526275349125\n",
      "Adding trajectory to replay buffer: step 12325, counter 196549\n",
      "Environment 3: Episode 2405, Score -121.48345983271054, Avg_Score -119.60348440856264\n",
      "Adding trajectory to replay buffer: step 12333, counter 196592\n",
      "Environment 13: Episode 2406, Score -122.66973416348671, Avg_Score -119.71417799653868\n",
      "Adding trajectory to replay buffer: step 12334, counter 196635\n",
      "Environment 0: Episode 2407, Score -122.74351606083911, Avg_Score -119.71197448527539\n",
      "Adding trajectory to replay buffer: step 12335, counter 196680\n",
      "Environment 12: Episode 2408, Score -121.46672360911654, Avg_Score -119.70240247673995\n",
      "Adding trajectory to replay buffer: step 12336, counter 196724\n",
      "Environment 2: Episode 2409, Score -122.17170792153416, Avg_Score -119.70887414016165\n",
      "Adding trajectory to replay buffer: step 12338, counter 196769\n",
      "Environment 7: Episode 2410, Score -121.39339012318354, Avg_Score -119.75902688998276\n",
      "Adding trajectory to replay buffer: step 12340, counter 196815\n",
      "Environment 1: Episode 2411, Score -121.69145880427398, Avg_Score -119.81395974175233\n",
      "Adding trajectory to replay buffer: step 12340, counter 196868\n",
      "Environment 5: Episode 2412, Score -118.61923941428836, Avg_Score -119.77785591220669\n",
      "Adding trajectory to replay buffer: step 12345, counter 196977\n",
      "Environment 6: Episode 2413, Score -124.37407661190505, Avg_Score -119.81692375583684\n",
      "Adding trajectory to replay buffer: step 12346, counter 197087\n",
      "Environment 4: Episode 2414, Score -116.7549765113468, Avg_Score -119.80198918433338\n",
      "Adding trajectory to replay buffer: step 12351, counter 197130\n",
      "Environment 14: Episode 2415, Score -121.33963049970382, Avg_Score -119.8532464305436\n",
      "Adding trajectory to replay buffer: step 12357, counter 197200\n",
      "Environment 9: Episode 2416, Score -119.2691629324872, Avg_Score -119.9214365014457\n",
      "Adding trajectory to replay buffer: step 12357, counter 197283\n",
      "Environment 15: Episode 2417, Score -115.1875125036426, Avg_Score -119.87324791364199\n",
      "Adding trajectory to replay buffer: step 12362, counter 197328\n",
      "Environment 11: Episode 2418, Score -121.61617240181072, Avg_Score -119.93879636415105\n",
      "Adding trajectory to replay buffer: step 12368, counter 197484\n",
      "Environment 10: Episode 2419, Score -119.86650070634361, Avg_Score -119.9364906843208\n",
      "Adding trajectory to replay buffer: step 12382, counter 197549\n",
      "Environment 8: Episode 2420, Score -115.91966833235799, Avg_Score -119.8717656227346\n",
      "Adding trajectory to replay buffer: step 12383, counter 197596\n",
      "Environment 2: Episode 2421, Score -120.85506001609689, Avg_Score -119.8467437259065\n",
      "Adding trajectory to replay buffer: step 12384, counter 197655\n",
      "Environment 3: Episode 2422, Score -110.92135789570399, Avg_Score -119.76570257024045\n",
      "Adding trajectory to replay buffer: step 12385, counter 197700\n",
      "Environment 5: Episode 2423, Score -123.52325585949794, Avg_Score -119.77562043220308\n",
      "Adding trajectory to replay buffer: step 12388, counter 197753\n",
      "Environment 12: Episode 2424, Score -118.1799184303029, Avg_Score -119.73932332011003\n",
      "Adding trajectory to replay buffer: step 12393, counter 197800\n",
      "Environment 4: Episode 2425, Score -123.62329467471751, Avg_Score -119.77687679693449\n",
      "Adding trajectory to replay buffer: step 12394, counter 197856\n",
      "Environment 7: Episode 2426, Score -115.74910988799684, Avg_Score -119.72135729012479\n",
      "Adding trajectory to replay buffer: step 12395, counter 197900\n",
      "Environment 14: Episode 2427, Score -122.8174122904384, Avg_Score -119.73719434579976\n",
      "Adding trajectory to replay buffer: step 12406, counter 197949\n",
      "Environment 9: Episode 2428, Score -121.20203044795471, Avg_Score -119.73135117928462\n",
      "Adding trajectory to replay buffer: step 12412, counter 197993\n",
      "Environment 10: Episode 2429, Score -122.46209217664413, Avg_Score -119.73982623269134\n",
      "Adding trajectory to replay buffer: step 12416, counter 198076\n",
      "Environment 13: Episode 2430, Score -121.66193251149232, Avg_Score -119.76576031822601\n",
      "Adding trajectory to replay buffer: step 12418, counter 198154\n",
      "Environment 1: Episode 2431, Score -112.615195475795, Avg_Score -119.66740605204818\n",
      "Adding trajectory to replay buffer: step 12422, counter 198214\n",
      "Environment 11: Episode 2432, Score -115.99872176891131, Avg_Score -119.60222178335833\n",
      "Adding trajectory to replay buffer: step 12425, counter 198254\n",
      "Environment 5: Episode 2433, Score -117.6840242253542, Avg_Score -119.56009658149475\n",
      "Adding trajectory to replay buffer: step 12425, counter 198297\n",
      "Environment 8: Episode 2434, Score -123.26124428560523, Avg_Score -119.64260416967294\n",
      "Adding trajectory to replay buffer: step 12426, counter 198340\n",
      "Environment 2: Episode 2435, Score -123.16322999209663, Avg_Score -119.63670870132084\n",
      "Adding trajectory to replay buffer: step 12426, counter 198382\n",
      "Environment 3: Episode 2436, Score -122.91742751075141, Avg_Score -119.7012211514536\n",
      "Adding trajectory to replay buffer: step 12432, counter 198426\n",
      "Environment 12: Episode 2437, Score -121.88128902914872, Avg_Score -119.75688817288146\n",
      "Adding trajectory to replay buffer: step 12437, counter 198470\n",
      "Environment 4: Episode 2438, Score -122.37984748552894, Avg_Score -119.80497285542698\n",
      "Adding trajectory to replay buffer: step 12440, counter 198516\n",
      "Environment 7: Episode 2439, Score -120.70376207123884, Avg_Score -119.7872581108937\n",
      "Adding trajectory to replay buffer: step 12454, counter 198564\n",
      "Environment 9: Episode 2440, Score -116.41765222986197, Avg_Score -119.82523122301532\n",
      "Adding trajectory to replay buffer: step 12462, counter 198608\n",
      "Environment 1: Episode 2441, Score -121.98358969646816, Avg_Score -119.81386003467098\n",
      "Adding trajectory to replay buffer: step 12462, counter 198658\n",
      "Environment 10: Episode 2442, Score -117.68717458829097, Avg_Score -119.83178940242533\n",
      "Adding trajectory to replay buffer: step 12465, counter 198701\n",
      "Environment 11: Episode 2443, Score -122.08099980261424, Avg_Score -119.85653448405097\n",
      "Adding trajectory to replay buffer: step 12469, counter 198744\n",
      "Environment 2: Episode 2444, Score -123.44848454008624, Avg_Score -119.84415526105359\n",
      "Adding trajectory to replay buffer: step 12469, counter 198787\n",
      "Environment 3: Episode 2445, Score -122.6715499073981, Avg_Score -119.91562192186574\n",
      "Adding trajectory to replay buffer: step 12469, counter 198831\n",
      "Environment 5: Episode 2446, Score -122.02674898578101, Avg_Score -119.94618377616469\n",
      "Adding trajectory to replay buffer: step 12470, counter 198876\n",
      "Environment 8: Episode 2447, Score -121.99305802314231, Avg_Score -120.02432413576861\n",
      "Adding trajectory to replay buffer: step 12475, counter 198919\n",
      "Environment 12: Episode 2448, Score -122.3198668187745, Avg_Score -120.0292419024917\n",
      "Adding trajectory to replay buffer: step 12478, counter 199063\n",
      "Environment 0: Episode 2449, Score -122.78424896824669, Avg_Score -120.0383955512793\n",
      "Adding trajectory to replay buffer: step 12483, counter 199109\n",
      "Environment 4: Episode 2450, Score -121.69620814462068, Avg_Score -120.09941770773763\n",
      "Adding trajectory to replay buffer: step 12483, counter 199176\n",
      "Environment 13: Episode 2451, Score -115.22893059495185, Avg_Score -120.02243659334734\n",
      "Adding trajectory to replay buffer: step 12484, counter 199220\n",
      "Environment 7: Episode 2452, Score -121.02946887729505, Avg_Score -120.01263294047763\n",
      "Adding trajectory to replay buffer: step 12499, counter 199265\n",
      "Environment 9: Episode 2453, Score -122.50098075652932, Avg_Score -120.02037836737308\n",
      "Adding trajectory to replay buffer: step 12499, counter 199407\n",
      "Environment 15: Episode 2454, Score -115.66364922770649, Avg_Score -119.95457977833983\n",
      "Adding trajectory to replay buffer: step 12505, counter 199450\n",
      "Environment 10: Episode 2455, Score -122.00415258538786, Avg_Score -119.9628354131031\n",
      "Adding trajectory to replay buffer: step 12506, counter 199494\n",
      "Environment 1: Episode 2456, Score -117.99527123777445, Avg_Score -119.94277050870616\n",
      "Adding trajectory to replay buffer: step 12512, counter 199537\n",
      "Environment 2: Episode 2457, Score -123.05702363887306, Avg_Score -119.94758378823\n",
      "Adding trajectory to replay buffer: step 12512, counter 199580\n",
      "Environment 3: Episode 2458, Score -123.20213798835812, Avg_Score -120.00125215391428\n",
      "Adding trajectory to replay buffer: step 12512, counter 199623\n",
      "Environment 5: Episode 2459, Score -123.18062694980205, Avg_Score -120.06275754904475\n",
      "Adding trajectory to replay buffer: step 12527, counter 199667\n",
      "Environment 4: Episode 2460, Score -120.04171478743045, Avg_Score -120.08557525962416\n",
      "Adding trajectory to replay buffer: step 12527, counter 199710\n",
      "Environment 7: Episode 2461, Score -121.19176616762518, Avg_Score -120.12128842561519\n",
      "Adding trajectory to replay buffer: step 12529, counter 199769\n",
      "Environment 8: Episode 2462, Score -116.61344131391061, Avg_Score -120.07216457368423\n",
      "Adding trajectory to replay buffer: step 12529, counter 199815\n",
      "Environment 13: Episode 2463, Score -121.54212873944516, Avg_Score -120.05493546374473\n",
      "Adding trajectory to replay buffer: step 12532, counter 199882\n",
      "Environment 11: Episode 2464, Score -114.1087325184693, Avg_Score -119.96921879344956\n",
      "Adding trajectory to replay buffer: step 12534, counter 200021\n",
      "Environment 14: Episode 2465, Score -116.72532108623908, Avg_Score -119.92988191845096\n",
      "Adding trajectory to replay buffer: step 12544, counter 200066\n",
      "Environment 15: Episode 2466, Score -121.08649004033829, Avg_Score -119.94714466722658\n",
      "Adding trajectory to replay buffer: step 12550, counter 200110\n",
      "Environment 1: Episode 2467, Score -122.56616913101325, Avg_Score -119.98385033567958\n",
      "Adding trajectory to replay buffer: step 12550, counter 200155\n",
      "Environment 10: Episode 2468, Score -121.33711462252028, Avg_Score -120.03703327417584\n",
      "Adding trajectory to replay buffer: step 12556, counter 200199\n",
      "Environment 3: Episode 2469, Score -121.45418087457183, Avg_Score -120.00680719585826\n",
      "Adding trajectory to replay buffer: step 12556, counter 200243\n",
      "Environment 5: Episode 2470, Score -121.42028631607567, Avg_Score -120.02357377282924\n",
      "Adding trajectory to replay buffer: step 12558, counter 200289\n",
      "Environment 2: Episode 2471, Score -121.60037800576787, Avg_Score -120.0107662104911\n",
      "Adding trajectory to replay buffer: step 12566, counter 200380\n",
      "Environment 12: Episode 2472, Score -117.04498299716103, Avg_Score -120.02641356451613\n",
      "Adding trajectory to replay buffer: step 12568, counter 200449\n",
      "Environment 9: Episode 2473, Score -117.75808003016748, Avg_Score -120.04264261246485\n",
      "Adding trajectory to replay buffer: step 12574, counter 200496\n",
      "Environment 7: Episode 2474, Score -117.98182129343226, Avg_Score -120.0040093948394\n",
      "Adding trajectory to replay buffer: step 12576, counter 200543\n",
      "Environment 13: Episode 2475, Score -120.0364449469739, Avg_Score -119.97482969224374\n",
      "Adding trajectory to replay buffer: step 12577, counter 200588\n",
      "Environment 11: Episode 2476, Score -121.46356657413952, Avg_Score -120.03912770096117\n",
      "Adding trajectory to replay buffer: step 12582, counter 200643\n",
      "Environment 4: Episode 2477, Score -116.11590255027109, Avg_Score -119.96936827666515\n",
      "Adding trajectory to replay buffer: step 12585, counter 200694\n",
      "Environment 14: Episode 2478, Score -121.32584045408915, Avg_Score -119.95646534396377\n",
      "Adding trajectory to replay buffer: step 12595, counter 200739\n",
      "Environment 10: Episode 2479, Score -121.72908717743184, Avg_Score -120.0164880954462\n",
      "Adding trajectory to replay buffer: step 12598, counter 200781\n",
      "Environment 3: Episode 2480, Score -122.85815221718637, Avg_Score -120.05687757104184\n",
      "Adding trajectory to replay buffer: step 12598, counter 200823\n",
      "Environment 5: Episode 2481, Score -122.84341110293256, Avg_Score -120.11239775927365\n",
      "Adding trajectory to replay buffer: step 12602, counter 200867\n",
      "Environment 2: Episode 2482, Score -121.26240578141498, Avg_Score -120.13551252250177\n",
      "Adding trajectory to replay buffer: step 12608, counter 200997\n",
      "Environment 0: Episode 2483, Score -128.1808186029441, Avg_Score -120.1927009999946\n",
      "Adding trajectory to replay buffer: step 12610, counter 201057\n",
      "Environment 1: Episode 2484, Score -116.1106462972456, Avg_Score -120.20281472978779\n",
      "Adding trajectory to replay buffer: step 12614, counter 201103\n",
      "Environment 9: Episode 2485, Score -117.27666569730826, Avg_Score -120.2083571244151\n",
      "Adding trajectory to replay buffer: step 12614, counter 201173\n",
      "Environment 15: Episode 2486, Score -118.09619246141489, Avg_Score -120.15965032697018\n",
      "Adding trajectory to replay buffer: step 12617, counter 201216\n",
      "Environment 7: Episode 2487, Score -117.57451724338036, Avg_Score -120.12507883494362\n",
      "Adding trajectory to replay buffer: step 12620, counter 201259\n",
      "Environment 11: Episode 2488, Score -122.38847151197928, Avg_Score -120.15860476494161\n",
      "Adding trajectory to replay buffer: step 12620, counter 201303\n",
      "Environment 13: Episode 2489, Score -123.01317112809978, Avg_Score -120.18775879891986\n",
      "Adding trajectory to replay buffer: step 12621, counter 201579\n",
      "Environment 6: Episode 2490, Score -130.41363238669047, Avg_Score -120.29251648178068\n",
      "Adding trajectory to replay buffer: step 12624, counter 201621\n",
      "Environment 4: Episode 2491, Score -116.40167525045511, Avg_Score -120.24001552550357\n",
      "Adding trajectory to replay buffer: step 12628, counter 201664\n",
      "Environment 14: Episode 2492, Score -121.14177465645285, Avg_Score -120.27676573710471\n",
      "Adding trajectory to replay buffer: step 12636, counter 201734\n",
      "Environment 12: Episode 2493, Score -117.41766050537194, Avg_Score -120.2528018854417\n",
      "Adding trajectory to replay buffer: step 12639, counter 201778\n",
      "Environment 10: Episode 2494, Score -122.51285028519172, Avg_Score -120.31987873711927\n",
      "Adding trajectory to replay buffer: step 12642, counter 201822\n",
      "Environment 3: Episode 2495, Score -122.99891319406467, Avg_Score -120.40273693532326\n",
      "Adding trajectory to replay buffer: step 12642, counter 201866\n",
      "Environment 5: Episode 2496, Score -123.01373253600796, Avg_Score -120.52459099267693\n",
      "Adding trajectory to replay buffer: step 12646, counter 201910\n",
      "Environment 2: Episode 2497, Score -121.19567315562753, Avg_Score -120.4985408667929\n",
      "Adding trajectory to replay buffer: step 12652, counter 201954\n",
      "Environment 0: Episode 2498, Score -121.61010340775984, Avg_Score -120.5648142196677\n",
      "Adding trajectory to replay buffer: step 12661, counter 202001\n",
      "Environment 15: Episode 2499, Score -121.79715293764883, Avg_Score -120.59899108840678\n",
      "Adding trajectory to replay buffer: step 12663, counter 202044\n",
      "Environment 11: Episode 2500, Score -123.17689518046193, Avg_Score -120.612903541633\n",
      "Adding trajectory to replay buffer: step 12664, counter 202087\n",
      "Environment 6: Episode 2501, Score -122.75755965277304, Avg_Score -120.62258594852392\n",
      "Adding trajectory to replay buffer: step 12667, counter 202130\n",
      "Environment 4: Episode 2502, Score -122.29662015092062, Avg_Score -120.6216209371544\n",
      "Adding trajectory to replay buffer: step 12673, counter 202175\n",
      "Environment 14: Episode 2503, Score -122.61909324881745, Avg_Score -120.61966072446793\n",
      "Adding trajectory to replay buffer: step 12679, counter 202218\n",
      "Environment 12: Episode 2504, Score -122.79409811759493, Avg_Score -120.62034296363085\n",
      "Adding trajectory to replay buffer: step 12683, counter 202262\n",
      "Environment 10: Episode 2505, Score -121.87668310760148, Avg_Score -120.62427519637976\n",
      "Adding trajectory to replay buffer: step 12685, counter 202305\n",
      "Environment 3: Episode 2506, Score -123.41487383421541, Avg_Score -120.63172659308704\n",
      "Adding trajectory to replay buffer: step 12688, counter 202351\n",
      "Environment 5: Episode 2507, Score -122.04255876729886, Avg_Score -120.62471702015164\n",
      "Adding trajectory to replay buffer: step 12691, counter 202396\n",
      "Environment 2: Episode 2508, Score -122.49876776059904, Avg_Score -120.63503746166646\n",
      "Adding trajectory to replay buffer: step 12696, counter 202440\n",
      "Environment 0: Episode 2509, Score -122.57379382440573, Avg_Score -120.63905832069518\n",
      "Adding trajectory to replay buffer: step 12704, counter 202483\n",
      "Environment 15: Episode 2510, Score -119.12427743068586, Avg_Score -120.6163671937702\n",
      "Adding trajectory to replay buffer: step 12708, counter 202528\n",
      "Environment 11: Episode 2511, Score -121.72276831115721, Avg_Score -120.61668028883906\n",
      "Adding trajectory to replay buffer: step 12712, counter 202576\n",
      "Environment 6: Episode 2512, Score -122.04200302172266, Avg_Score -120.65090792491338\n",
      "Adding trajectory to replay buffer: step 12715, counter 202618\n",
      "Environment 14: Episode 2513, Score -122.766859751358, Avg_Score -120.63483575630794\n",
      "Adding trajectory to replay buffer: step 12725, counter 202664\n",
      "Environment 12: Episode 2514, Score -121.62944319884254, Avg_Score -120.68358042318289\n",
      "Adding trajectory to replay buffer: step 12728, counter 202709\n",
      "Environment 10: Episode 2515, Score -121.78696893352021, Avg_Score -120.68805380752103\n",
      "Adding trajectory to replay buffer: step 12730, counter 202829\n",
      "Environment 1: Episode 2516, Score -127.51124204502193, Avg_Score -120.77047459864637\n",
      "Adding trajectory to replay buffer: step 12732, counter 202876\n",
      "Environment 3: Episode 2517, Score -120.01498568384226, Avg_Score -120.81874933044837\n",
      "Adding trajectory to replay buffer: step 12733, counter 202921\n",
      "Environment 5: Episode 2518, Score -121.16609767603005, Avg_Score -120.81424858319056\n",
      "Adding trajectory to replay buffer: step 12736, counter 202966\n",
      "Environment 2: Episode 2519, Score -114.1803074322405, Avg_Score -120.75738665044955\n",
      "Adding trajectory to replay buffer: step 12741, counter 203178\n",
      "Environment 8: Episode 2520, Score -126.74801737362208, Avg_Score -120.8656701408622\n",
      "Adding trajectory to replay buffer: step 12744, counter 203308\n",
      "Environment 9: Episode 2521, Score -119.0246718807506, Avg_Score -120.84736625950875\n",
      "Adding trajectory to replay buffer: step 12750, counter 203354\n",
      "Environment 15: Episode 2522, Score -119.00522523501205, Avg_Score -120.92820493290182\n",
      "Adding trajectory to replay buffer: step 12751, counter 203488\n",
      "Environment 7: Episode 2523, Score -124.9965237094344, Avg_Score -120.94293761140119\n",
      "Adding trajectory to replay buffer: step 12751, counter 203531\n",
      "Environment 11: Episode 2524, Score -116.07708779749274, Avg_Score -120.92190930507307\n",
      "Adding trajectory to replay buffer: step 12768, counter 203574\n",
      "Environment 12: Episode 2525, Score -121.57321797654902, Avg_Score -120.90140853809139\n",
      "Adding trajectory to replay buffer: step 12771, counter 203617\n",
      "Environment 10: Episode 2526, Score -122.04748073505486, Avg_Score -120.96439224656197\n",
      "Adding trajectory to replay buffer: step 12775, counter 203662\n",
      "Environment 1: Episode 2527, Score -120.67381768843345, Avg_Score -120.94295630054194\n",
      "Adding trajectory to replay buffer: step 12776, counter 203705\n",
      "Environment 5: Episode 2528, Score -122.90783273321514, Avg_Score -120.96001432339453\n",
      "Adding trajectory to replay buffer: step 12780, counter 203749\n",
      "Environment 2: Episode 2529, Score -121.31513457680805, Avg_Score -120.94854474739617\n",
      "Adding trajectory to replay buffer: step 12782, counter 203799\n",
      "Environment 3: Episode 2530, Score -118.91060424741359, Avg_Score -120.9210314647554\n",
      "Adding trajectory to replay buffer: step 12786, counter 203889\n",
      "Environment 0: Episode 2531, Score -117.77002683715585, Avg_Score -120.972579778369\n",
      "Adding trajectory to replay buffer: step 12787, counter 203961\n",
      "Environment 14: Episode 2532, Score -116.03472124676841, Avg_Score -120.97293977314757\n",
      "Adding trajectory to replay buffer: step 12794, counter 204014\n",
      "Environment 8: Episode 2533, Score -117.88067985312269, Avg_Score -120.97490632942525\n",
      "Adding trajectory to replay buffer: step 12800, counter 204064\n",
      "Environment 15: Episode 2534, Score -117.68648056066968, Avg_Score -120.91915869217588\n",
      "Adding trajectory to replay buffer: step 12803, counter 204200\n",
      "Environment 4: Episode 2535, Score -113.35752279768884, Avg_Score -120.82110162023181\n",
      "Adding trajectory to replay buffer: step 12803, counter 204259\n",
      "Environment 9: Episode 2536, Score -119.50364102139325, Avg_Score -120.78696375533823\n",
      "Adding trajectory to replay buffer: step 12810, counter 204449\n",
      "Environment 13: Episode 2537, Score -120.22908496826857, Avg_Score -120.77044171472944\n",
      "Adding trajectory to replay buffer: step 12816, counter 204497\n",
      "Environment 12: Episode 2538, Score -121.25143617377864, Avg_Score -120.75915760161193\n",
      "Adding trajectory to replay buffer: step 12820, counter 204541\n",
      "Environment 5: Episode 2539, Score -121.92971581263095, Avg_Score -120.77141713902586\n",
      "Adding trajectory to replay buffer: step 12822, counter 204588\n",
      "Environment 1: Episode 2540, Score -116.8466163691959, Avg_Score -120.7757067804192\n",
      "Adding trajectory to replay buffer: step 12823, counter 204631\n",
      "Environment 2: Episode 2541, Score -120.3458458969804, Avg_Score -120.75932934242434\n",
      "Adding trajectory to replay buffer: step 12826, counter 204675\n",
      "Environment 3: Episode 2542, Score -122.06376959489845, Avg_Score -120.8030952924904\n",
      "Adding trajectory to replay buffer: step 12831, counter 204720\n",
      "Environment 0: Episode 2543, Score -122.89360370570297, Avg_Score -120.81122133152127\n",
      "Adding trajectory to replay buffer: step 12831, counter 204764\n",
      "Environment 14: Episode 2544, Score -122.84003947715041, Avg_Score -120.8051368808919\n",
      "Adding trajectory to replay buffer: step 12838, counter 204808\n",
      "Environment 8: Episode 2545, Score -121.20617991566783, Avg_Score -120.79048318097459\n",
      "Adding trajectory to replay buffer: step 12843, counter 204880\n",
      "Environment 10: Episode 2546, Score -117.33107259239503, Avg_Score -120.74352641704073\n",
      "Adding trajectory to replay buffer: step 12845, counter 204922\n",
      "Environment 4: Episode 2547, Score -122.49639169811189, Avg_Score -120.74855975379043\n",
      "Adding trajectory to replay buffer: step 12845, counter 204964\n",
      "Environment 9: Episode 2548, Score -122.51918656958702, Avg_Score -120.75055295129856\n",
      "Adding trajectory to replay buffer: step 12845, counter 205009\n",
      "Environment 15: Episode 2549, Score -123.22246286277101, Avg_Score -120.75493509024382\n",
      "Adding trajectory to replay buffer: step 12864, counter 205057\n",
      "Environment 12: Episode 2550, Score -121.3227049744477, Avg_Score -120.75120005854207\n",
      "Adding trajectory to replay buffer: step 12866, counter 205101\n",
      "Environment 1: Episode 2551, Score -121.2807114813483, Avg_Score -120.81171786740605\n",
      "Adding trajectory to replay buffer: step 12872, counter 205150\n",
      "Environment 2: Episode 2552, Score -122.6425465200972, Avg_Score -120.82784864383407\n",
      "Adding trajectory to replay buffer: step 12874, counter 205193\n",
      "Environment 0: Episode 2553, Score -122.93095358525093, Avg_Score -120.8321483721213\n",
      "Adding trajectory to replay buffer: step 12876, counter 205238\n",
      "Environment 14: Episode 2554, Score -118.18709835087819, Avg_Score -120.857382863353\n",
      "Adding trajectory to replay buffer: step 12885, counter 205285\n",
      "Environment 8: Episode 2555, Score -122.09447771589024, Avg_Score -120.85828611465803\n",
      "Adding trajectory to replay buffer: step 12885, counter 205360\n",
      "Environment 13: Episode 2556, Score -110.86899505099095, Avg_Score -120.7870233527902\n",
      "Adding trajectory to replay buffer: step 12888, counter 205405\n",
      "Environment 10: Episode 2557, Score -122.96121787600158, Avg_Score -120.78606529516148\n",
      "Adding trajectory to replay buffer: step 12890, counter 205469\n",
      "Environment 3: Episode 2558, Score -117.24859851671508, Avg_Score -120.72652990044506\n",
      "Adding trajectory to replay buffer: step 12890, counter 205514\n",
      "Environment 15: Episode 2559, Score -121.88504088140839, Avg_Score -120.71357403976111\n",
      "Adding trajectory to replay buffer: step 12891, counter 205560\n",
      "Environment 9: Episode 2560, Score -122.22866483928946, Avg_Score -120.7354435402797\n",
      "Adding trajectory to replay buffer: step 12892, counter 205607\n",
      "Environment 4: Episode 2561, Score -121.48842440210345, Avg_Score -120.73841012262449\n",
      "Adding trajectory to replay buffer: step 12893, counter 205788\n",
      "Environment 6: Episode 2562, Score -122.49553652095827, Avg_Score -120.79723107469498\n",
      "Adding trajectory to replay buffer: step 12893, counter 205930\n",
      "Environment 11: Episode 2563, Score -114.88752378976109, Avg_Score -120.73068502519814\n",
      "Adding trajectory to replay buffer: step 12904, counter 205970\n",
      "Environment 12: Episode 2564, Score -116.52242341564099, Avg_Score -120.75482193416985\n",
      "Adding trajectory to replay buffer: step 12910, counter 206014\n",
      "Environment 1: Episode 2565, Score -122.8469351939174, Avg_Score -120.81603807524664\n",
      "Adding trajectory to replay buffer: step 12918, counter 206058\n",
      "Environment 0: Episode 2566, Score -118.16198885826114, Avg_Score -120.78679306342586\n",
      "Adding trajectory to replay buffer: step 12929, counter 206115\n",
      "Environment 2: Episode 2567, Score -115.12984377457606, Avg_Score -120.71242980986148\n",
      "Adding trajectory to replay buffer: step 12931, counter 206161\n",
      "Environment 13: Episode 2568, Score -121.20587338261802, Avg_Score -120.71111739746246\n",
      "Adding trajectory to replay buffer: step 12932, counter 206208\n",
      "Environment 8: Episode 2569, Score -122.10399871111599, Avg_Score -120.7176155758279\n",
      "Adding trajectory to replay buffer: step 12934, counter 206391\n",
      "Environment 7: Episode 2570, Score -133.50793515556896, Avg_Score -120.83849206422285\n",
      "Adding trajectory to replay buffer: step 12937, counter 206435\n",
      "Environment 6: Episode 2571, Score -122.92660339176965, Avg_Score -120.85175431808285\n",
      "Adding trajectory to replay buffer: step 12939, counter 206554\n",
      "Environment 5: Episode 2572, Score -127.41372473559653, Avg_Score -120.95544173546723\n",
      "Adding trajectory to replay buffer: step 12939, counter 206600\n",
      "Environment 11: Episode 2573, Score -122.88927373925223, Avg_Score -121.00675367255808\n",
      "Adding trajectory to replay buffer: step 12947, counter 206643\n",
      "Environment 12: Episode 2574, Score -122.87276110267422, Avg_Score -121.05566307065048\n",
      "Adding trajectory to replay buffer: step 12949, counter 206701\n",
      "Environment 9: Episode 2575, Score -116.46936822596193, Avg_Score -121.01999230344038\n",
      "Adding trajectory to replay buffer: step 12954, counter 206745\n",
      "Environment 1: Episode 2576, Score -121.21014754897791, Avg_Score -121.01745811318874\n",
      "Adding trajectory to replay buffer: step 12961, counter 206788\n",
      "Environment 0: Episode 2577, Score -123.05696606535776, Avg_Score -121.08686874833964\n",
      "Adding trajectory to replay buffer: step 12970, counter 206829\n",
      "Environment 2: Episode 2578, Score -117.79844651521245, Avg_Score -121.05159480895087\n",
      "Adding trajectory to replay buffer: step 12973, counter 206910\n",
      "Environment 4: Episode 2579, Score -116.75779512122212, Avg_Score -121.00188188838878\n",
      "Adding trajectory to replay buffer: step 12978, counter 206957\n",
      "Environment 13: Episode 2580, Score -122.19295204802168, Avg_Score -120.99522988669713\n",
      "Adding trajectory to replay buffer: step 12979, counter 207002\n",
      "Environment 7: Episode 2581, Score -122.23391292897053, Avg_Score -120.9891349049575\n",
      "Adding trajectory to replay buffer: step 12982, counter 207045\n",
      "Environment 11: Episode 2582, Score -120.46459788035881, Avg_Score -120.98115682594694\n",
      "Adding trajectory to replay buffer: step 12983, counter 207138\n",
      "Environment 15: Episode 2583, Score -124.2440646539194, Avg_Score -120.9417892864567\n",
      "Adding trajectory to replay buffer: step 12985, counter 207184\n",
      "Environment 5: Episode 2584, Score -121.47628060883905, Avg_Score -120.99544562957263\n",
      "Adding trajectory to replay buffer: step 12987, counter 207281\n",
      "Environment 3: Episode 2585, Score -116.18566807982822, Avg_Score -120.98453565339781\n",
      "Adding trajectory to replay buffer: step 12990, counter 207324\n",
      "Environment 12: Episode 2586, Score -116.00884600476238, Avg_Score -120.9636621888313\n",
      "Adding trajectory to replay buffer: step 12993, counter 207368\n",
      "Environment 9: Episode 2587, Score -122.05209250680979, Avg_Score -121.00843794146559\n",
      "Adding trajectory to replay buffer: step 13000, counter 207431\n",
      "Environment 6: Episode 2588, Score -116.74638873255547, Avg_Score -120.95201711367136\n",
      "Adding trajectory to replay buffer: step 13001, counter 207478\n",
      "Environment 1: Episode 2589, Score -116.63935081006719, Avg_Score -120.88827891049101\n",
      "Adding trajectory to replay buffer: step 13002, counter 207604\n",
      "Environment 14: Episode 2590, Score -116.0995211963801, Avg_Score -120.74513779858789\n",
      "Adding trajectory to replay buffer: step 13007, counter 207650\n",
      "Environment 0: Episode 2591, Score -118.47893337684371, Avg_Score -120.76591037985179\n",
      "Adding trajectory to replay buffer: step 13022, counter 207694\n",
      "Environment 13: Episode 2592, Score -116.50856705777285, Avg_Score -120.719578303865\n",
      "Adding trajectory to replay buffer: step 13029, counter 207741\n",
      "Environment 11: Episode 2593, Score -122.58152843659681, Avg_Score -120.77121698317724\n",
      "Adding trajectory to replay buffer: step 13030, counter 207786\n",
      "Environment 5: Episode 2594, Score -117.57123740323944, Avg_Score -120.72180085435771\n",
      "Adding trajectory to replay buffer: step 13033, counter 207836\n",
      "Environment 15: Episode 2595, Score -117.85574139546246, Avg_Score -120.67036913637168\n",
      "Adding trajectory to replay buffer: step 13034, counter 207880\n",
      "Environment 12: Episode 2596, Score -120.76269228341492, Avg_Score -120.64785873384574\n",
      "Adding trajectory to replay buffer: step 13036, counter 207929\n",
      "Environment 3: Episode 2597, Score -121.08761379978743, Avg_Score -120.64677814028735\n",
      "Adding trajectory to replay buffer: step 13037, counter 207973\n",
      "Environment 9: Episode 2598, Score -122.68765931846016, Avg_Score -120.65755369939436\n",
      "Adding trajectory to replay buffer: step 13043, counter 208015\n",
      "Environment 1: Episode 2599, Score -114.75604082830934, Avg_Score -120.58714257830097\n",
      "Adding trajectory to replay buffer: step 13043, counter 208126\n",
      "Environment 8: Episode 2600, Score -115.55690001019028, Avg_Score -120.51094262659826\n",
      "Adding trajectory to replay buffer: step 13045, counter 208192\n",
      "Environment 7: Episode 2601, Score -118.53615978356886, Avg_Score -120.4687286279062\n",
      "Adding trajectory to replay buffer: step 13053, counter 208238\n",
      "Environment 0: Episode 2602, Score -120.11483586641214, Avg_Score -120.44691078506114\n",
      "Adding trajectory to replay buffer: step 13054, counter 208319\n",
      "Environment 4: Episode 2603, Score -115.88288744928688, Avg_Score -120.37954872706581\n",
      "Adding trajectory to replay buffer: step 13054, counter 208485\n",
      "Environment 10: Episode 2604, Score -120.69997678387226, Avg_Score -120.35860751372859\n",
      "Adding trajectory to replay buffer: step 13068, counter 208531\n",
      "Environment 13: Episode 2605, Score -117.89827592389037, Avg_Score -120.31882344189147\n",
      "Adding trajectory to replay buffer: step 13076, counter 208573\n",
      "Environment 12: Episode 2606, Score -123.2271706161656, Avg_Score -120.316946409711\n",
      "Adding trajectory to replay buffer: step 13077, counter 208620\n",
      "Environment 5: Episode 2607, Score -120.52545497021018, Avg_Score -120.30177537174009\n",
      "Adding trajectory to replay buffer: step 13077, counter 208668\n",
      "Environment 11: Episode 2608, Score -121.76447169071344, Avg_Score -120.29443241104123\n",
      "Adding trajectory to replay buffer: step 13080, counter 208748\n",
      "Environment 6: Episode 2609, Score -118.25744755678748, Avg_Score -120.25126894836505\n",
      "Adding trajectory to replay buffer: step 13080, counter 208791\n",
      "Environment 9: Episode 2610, Score -122.56581051367894, Avg_Score -120.285684279195\n",
      "Adding trajectory to replay buffer: step 13086, counter 208834\n",
      "Environment 1: Episode 2611, Score -121.63730222359797, Avg_Score -120.28482961831938\n",
      "Adding trajectory to replay buffer: step 13087, counter 208878\n",
      "Environment 8: Episode 2612, Score -122.21263407580742, Avg_Score -120.28653592886025\n",
      "Adding trajectory to replay buffer: step 13091, counter 208967\n",
      "Environment 14: Episode 2613, Score -111.863058539267, Avg_Score -120.17749791673934\n",
      "Adding trajectory to replay buffer: step 13094, counter 209016\n",
      "Environment 7: Episode 2614, Score -117.98397998244565, Avg_Score -120.14104328457537\n",
      "Adding trajectory to replay buffer: step 13097, counter 209059\n",
      "Environment 4: Episode 2615, Score -117.58931687140465, Avg_Score -120.09906676395421\n",
      "Adding trajectory to replay buffer: step 13100, counter 209105\n",
      "Environment 10: Episode 2616, Score -119.84422342504561, Avg_Score -120.02239657775446\n",
      "Adding trajectory to replay buffer: step 13101, counter 209236\n",
      "Environment 2: Episode 2617, Score -115.70848168367263, Avg_Score -119.97933153775276\n",
      "Adding trajectory to replay buffer: step 13105, counter 209308\n",
      "Environment 15: Episode 2618, Score -117.81051384897219, Avg_Score -119.94577569948214\n",
      "Adding trajectory to replay buffer: step 13112, counter 209352\n",
      "Environment 13: Episode 2619, Score -122.4450743430114, Avg_Score -120.02842336858988\n",
      "Adding trajectory to replay buffer: step 13121, counter 209397\n",
      "Environment 12: Episode 2620, Score -121.23457094830212, Avg_Score -119.97328890433668\n",
      "Adding trajectory to replay buffer: step 13122, counter 209442\n",
      "Environment 11: Episode 2621, Score -123.01774343363631, Avg_Score -120.01321961986554\n",
      "Adding trajectory to replay buffer: step 13125, counter 209490\n",
      "Environment 5: Episode 2622, Score -121.84990062625397, Avg_Score -120.04166637377793\n",
      "Adding trajectory to replay buffer: step 13132, counter 209536\n",
      "Environment 1: Episode 2623, Score -119.16029870553687, Avg_Score -119.98330412373898\n",
      "Adding trajectory to replay buffer: step 13133, counter 209589\n",
      "Environment 6: Episode 2624, Score -112.72745866975251, Avg_Score -119.94980783246157\n",
      "Adding trajectory to replay buffer: step 13133, counter 209635\n",
      "Environment 8: Episode 2625, Score -120.33524589541477, Avg_Score -119.93742811165025\n",
      "Adding trajectory to replay buffer: step 13139, counter 209677\n",
      "Environment 4: Episode 2626, Score -122.48194017523588, Avg_Score -119.94177270605205\n",
      "Adding trajectory to replay buffer: step 13145, counter 209721\n",
      "Environment 2: Episode 2627, Score -119.008266904726, Avg_Score -119.92511719821499\n",
      "Adding trajectory to replay buffer: step 13147, counter 209768\n",
      "Environment 10: Episode 2628, Score -120.16393827444493, Avg_Score -119.89767825362728\n",
      "Adding trajectory to replay buffer: step 13152, counter 209867\n",
      "Environment 0: Episode 2629, Score -117.40315029454666, Avg_Score -119.85855841080465\n",
      "Adding trajectory to replay buffer: step 13153, counter 209908\n",
      "Environment 13: Episode 2630, Score -117.90139847265122, Avg_Score -119.84846635305705\n",
      "Adding trajectory to replay buffer: step 13160, counter 209974\n",
      "Environment 7: Episode 2631, Score -116.83839576132519, Avg_Score -119.83915004229875\n",
      "Adding trajectory to replay buffer: step 13171, counter 210040\n",
      "Environment 15: Episode 2632, Score -117.12139222156195, Avg_Score -119.85001675204668\n",
      "Adding trajectory to replay buffer: step 13175, counter 210083\n",
      "Environment 1: Episode 2633, Score -114.84051344023706, Avg_Score -119.81961508791782\n",
      "Adding trajectory to replay buffer: step 13180, counter 210130\n",
      "Environment 8: Episode 2634, Score -117.96383674432275, Avg_Score -119.82238864975434\n",
      "Adding trajectory to replay buffer: step 13181, counter 210186\n",
      "Environment 5: Episode 2635, Score -118.42924735161927, Avg_Score -119.87310589529366\n",
      "Adding trajectory to replay buffer: step 13182, counter 210247\n",
      "Environment 12: Episode 2636, Score -120.56369837049023, Avg_Score -119.88370646878462\n",
      "Adding trajectory to replay buffer: step 13185, counter 210293\n",
      "Environment 4: Episode 2637, Score -120.57807872470033, Avg_Score -119.88719640634893\n",
      "Adding trajectory to replay buffer: step 13194, counter 210342\n",
      "Environment 2: Episode 2638, Score -120.19134867182312, Avg_Score -119.8765955313294\n",
      "Adding trajectory to replay buffer: step 13200, counter 210390\n",
      "Environment 0: Episode 2639, Score -120.17628658474734, Avg_Score -119.85906123905058\n",
      "Adding trajectory to replay buffer: step 13201, counter 210444\n",
      "Environment 10: Episode 2640, Score -113.82074215906175, Avg_Score -119.82880249694921\n",
      "Adding trajectory to replay buffer: step 13206, counter 210490\n",
      "Environment 7: Episode 2641, Score -123.78753684615033, Avg_Score -119.86321940644092\n",
      "Adding trajectory to replay buffer: step 13214, counter 210533\n",
      "Environment 15: Episode 2642, Score -119.93281381222855, Avg_Score -119.84190984861421\n",
      "Adding trajectory to replay buffer: step 13216, counter 210596\n",
      "Environment 13: Episode 2643, Score -114.63048997829544, Avg_Score -119.75927871134012\n",
      "Adding trajectory to replay buffer: step 13226, counter 210641\n",
      "Environment 5: Episode 2644, Score -118.537914131683, Avg_Score -119.71625745788546\n",
      "Adding trajectory to replay buffer: step 13227, counter 210688\n",
      "Environment 8: Episode 2645, Score -116.20245153293541, Avg_Score -119.66622017405813\n",
      "Adding trajectory to replay buffer: step 13228, counter 210731\n",
      "Environment 4: Episode 2646, Score -123.21782901122607, Avg_Score -119.72508773824644\n",
      "Adding trajectory to replay buffer: step 13240, counter 210838\n",
      "Environment 6: Episode 2647, Score -114.60882381361725, Avg_Score -119.64621205940152\n",
      "Adding trajectory to replay buffer: step 13242, counter 210958\n",
      "Environment 11: Episode 2648, Score -118.175199421534, Avg_Score -119.60277218792098\n",
      "Adding trajectory to replay buffer: step 13247, counter 211004\n",
      "Environment 10: Episode 2649, Score -119.95859746305148, Avg_Score -119.5701335339238\n",
      "Adding trajectory to replay buffer: step 13249, counter 211173\n",
      "Environment 9: Episode 2650, Score -133.39322145698904, Avg_Score -119.69083869874919\n",
      "Adding trajectory to replay buffer: step 13254, counter 211221\n",
      "Environment 7: Episode 2651, Score -121.55959333027278, Avg_Score -119.69362751723844\n",
      "Adding trajectory to replay buffer: step 13257, counter 211264\n",
      "Environment 15: Episode 2652, Score -123.16829786569873, Avg_Score -119.69888503069446\n",
      "Adding trajectory to replay buffer: step 13260, counter 211308\n",
      "Environment 13: Episode 2653, Score -122.8250500987079, Avg_Score -119.69782599582905\n",
      "Adding trajectory to replay buffer: step 13267, counter 211484\n",
      "Environment 14: Episode 2654, Score -118.94963944297407, Avg_Score -119.70545140674999\n",
      "Adding trajectory to replay buffer: step 13269, counter 211553\n",
      "Environment 0: Episode 2655, Score -115.03067990953103, Avg_Score -119.63481342868637\n",
      "Adding trajectory to replay buffer: step 13283, counter 211609\n",
      "Environment 8: Episode 2656, Score -114.74977867998058, Avg_Score -119.67362126497628\n",
      "Adding trajectory to replay buffer: step 13287, counter 211654\n",
      "Environment 11: Episode 2657, Score -118.86215581575533, Avg_Score -119.63263064437382\n",
      "Adding trajectory to replay buffer: step 13291, counter 211698\n",
      "Environment 10: Episode 2658, Score -121.66477157251413, Avg_Score -119.6767923749318\n",
      "Adding trajectory to replay buffer: step 13293, counter 211763\n",
      "Environment 4: Episode 2659, Score -116.70423821080087, Avg_Score -119.62498434822574\n",
      "Adding trajectory to replay buffer: step 13294, counter 211875\n",
      "Environment 12: Episode 2660, Score -118.5430839741081, Avg_Score -119.58812853957392\n",
      "Adding trajectory to replay buffer: step 13297, counter 212136\n",
      "Environment 3: Episode 2661, Score -123.2517557246446, Avg_Score -119.60576185279932\n",
      "Adding trajectory to replay buffer: step 13297, counter 212184\n",
      "Environment 9: Episode 2662, Score -117.40601826866157, Avg_Score -119.55486667027634\n",
      "Adding trajectory to replay buffer: step 13301, counter 212228\n",
      "Environment 15: Episode 2663, Score -115.325346309113, Avg_Score -119.55924489546986\n",
      "Adding trajectory to replay buffer: step 13305, counter 212293\n",
      "Environment 6: Episode 2664, Score -116.9062421461735, Avg_Score -119.5630830827752\n",
      "Adding trajectory to replay buffer: step 13307, counter 212374\n",
      "Environment 5: Episode 2665, Score -119.83803251402126, Avg_Score -119.53299405597623\n",
      "Adding trajectory to replay buffer: step 13307, counter 212421\n",
      "Environment 13: Episode 2666, Score -121.90066546317873, Avg_Score -119.5703808220254\n",
      "Adding trajectory to replay buffer: step 13309, counter 212463\n",
      "Environment 14: Episode 2667, Score -117.0265059406031, Avg_Score -119.58934744368567\n",
      "Adding trajectory to replay buffer: step 13316, counter 212510\n",
      "Environment 0: Episode 2668, Score -120.73410846015872, Avg_Score -119.5846297944611\n",
      "Adding trajectory to replay buffer: step 13321, counter 212577\n",
      "Environment 7: Episode 2669, Score -110.97777161073932, Avg_Score -119.47336752345733\n",
      "Adding trajectory to replay buffer: step 13329, counter 212623\n",
      "Environment 8: Episode 2670, Score -120.04407081427115, Avg_Score -119.33872888004434\n",
      "Adding trajectory to replay buffer: step 13330, counter 212759\n",
      "Environment 2: Episode 2671, Score -127.60972478097347, Avg_Score -119.38556009393638\n",
      "Adding trajectory to replay buffer: step 13336, counter 212808\n",
      "Environment 11: Episode 2672, Score -117.25544064860419, Avg_Score -119.28397725306645\n",
      "Adding trajectory to replay buffer: step 13338, counter 212855\n",
      "Environment 10: Episode 2673, Score -120.71459671899366, Avg_Score -119.26223048286386\n",
      "Adding trajectory to replay buffer: step 13338, counter 212899\n",
      "Environment 12: Episode 2674, Score -123.77585164903229, Avg_Score -119.27126138832745\n",
      "Adding trajectory to replay buffer: step 13342, counter 212944\n",
      "Environment 3: Episode 2675, Score -121.49936058009975, Avg_Score -119.32156131186883\n",
      "Adding trajectory to replay buffer: step 13343, counter 212990\n",
      "Environment 9: Episode 2676, Score -119.58309076283872, Avg_Score -119.30529074400742\n",
      "Adding trajectory to replay buffer: step 13344, counter 213033\n",
      "Environment 15: Episode 2677, Score -123.28842644461494, Avg_Score -119.3076053478\n",
      "Adding trajectory to replay buffer: step 13350, counter 213078\n",
      "Environment 6: Episode 2678, Score -122.50580331038063, Avg_Score -119.35467891575168\n",
      "Adding trajectory to replay buffer: step 13351, counter 213122\n",
      "Environment 13: Episode 2679, Score -117.83072412857476, Avg_Score -119.3654082058252\n",
      "Adding trajectory to replay buffer: step 13352, counter 213167\n",
      "Environment 5: Episode 2680, Score -122.8137333026249, Avg_Score -119.37161601837123\n",
      "Adding trajectory to replay buffer: step 13360, counter 213211\n",
      "Environment 0: Episode 2681, Score -121.68079440887459, Avg_Score -119.36608483317028\n",
      "Adding trajectory to replay buffer: step 13367, counter 213257\n",
      "Environment 7: Episode 2682, Score -119.48636158375008, Avg_Score -119.35630247020417\n",
      "Adding trajectory to replay buffer: step 13374, counter 213301\n",
      "Environment 2: Episode 2683, Score -122.4720368083492, Avg_Score -119.33858219174849\n",
      "Adding trajectory to replay buffer: step 13380, counter 213345\n",
      "Environment 11: Episode 2684, Score -122.23794967039531, Avg_Score -119.34619888236404\n",
      "Adding trajectory to replay buffer: step 13386, counter 213389\n",
      "Environment 3: Episode 2685, Score -122.63406581545341, Avg_Score -119.4106828597203\n",
      "Adding trajectory to replay buffer: step 13389, counter 213435\n",
      "Environment 9: Episode 2686, Score -120.13549217375802, Avg_Score -119.45194932141027\n",
      "Adding trajectory to replay buffer: step 13393, counter 213499\n",
      "Environment 8: Episode 2687, Score -115.46376005141438, Avg_Score -119.3860659968563\n",
      "Adding trajectory to replay buffer: step 13394, counter 213543\n",
      "Environment 6: Episode 2688, Score -122.75254291023687, Avg_Score -119.44612753863312\n",
      "Adding trajectory to replay buffer: step 13395, counter 213645\n",
      "Environment 4: Episode 2689, Score -114.19247693212176, Avg_Score -119.42165879985367\n",
      "Adding trajectory to replay buffer: step 13395, counter 213689\n",
      "Environment 13: Episode 2690, Score -119.17620489985558, Avg_Score -119.4524256368884\n",
      "Adding trajectory to replay buffer: step 13396, counter 213910\n",
      "Environment 1: Episode 2691, Score -137.81581460069742, Avg_Score -119.64579444912695\n",
      "Adding trajectory to replay buffer: step 13397, counter 213998\n",
      "Environment 14: Episode 2692, Score -113.26709327562776, Avg_Score -119.6133797113055\n",
      "Adding trajectory to replay buffer: step 13404, counter 214042\n",
      "Environment 0: Episode 2693, Score -122.00108155725151, Avg_Score -119.60757524251204\n",
      "Adding trajectory to replay buffer: step 13412, counter 214087\n",
      "Environment 7: Episode 2694, Score -121.58446298453646, Avg_Score -119.64770749832502\n",
      "Adding trajectory to replay buffer: step 13420, counter 214133\n",
      "Environment 2: Episode 2695, Score -116.22416886420176, Avg_Score -119.63139177301242\n",
      "Adding trajectory to replay buffer: step 13432, counter 214213\n",
      "Environment 5: Episode 2696, Score -118.54592770176103, Avg_Score -119.6092241271959\n",
      "Adding trajectory to replay buffer: step 13435, counter 214310\n",
      "Environment 10: Episode 2697, Score -115.54938545209667, Avg_Score -119.55384184371898\n",
      "Adding trajectory to replay buffer: step 13436, counter 214351\n",
      "Environment 4: Episode 2698, Score -117.7244478995235, Avg_Score -119.5042097295296\n",
      "Adding trajectory to replay buffer: step 13440, counter 214395\n",
      "Environment 1: Episode 2699, Score -122.52728199305386, Avg_Score -119.58192214117706\n",
      "Adding trajectory to replay buffer: step 13440, counter 214441\n",
      "Environment 6: Episode 2700, Score -124.51709168203362, Avg_Score -119.67152405789548\n",
      "Adding trajectory to replay buffer: step 13440, counter 214501\n",
      "Environment 11: Episode 2701, Score -113.90106605262372, Avg_Score -119.62517312058603\n",
      "Adding trajectory to replay buffer: step 13442, counter 214548\n",
      "Environment 13: Episode 2702, Score -119.56798139662389, Avg_Score -119.61970457588814\n",
      "Adding trajectory to replay buffer: step 13451, counter 214595\n",
      "Environment 0: Episode 2703, Score -122.23870979130145, Avg_Score -119.68326279930831\n",
      "Adding trajectory to replay buffer: step 13455, counter 214706\n",
      "Environment 15: Episode 2704, Score -121.49736898518852, Avg_Score -119.69123672132146\n",
      "Adding trajectory to replay buffer: step 13456, counter 214769\n",
      "Environment 8: Episode 2705, Score -117.43466371022166, Avg_Score -119.68660059918477\n",
      "Adding trajectory to replay buffer: step 13461, counter 214844\n",
      "Environment 3: Episode 2706, Score -118.31744693277591, Avg_Score -119.63750336235087\n",
      "Adding trajectory to replay buffer: step 13463, counter 214887\n",
      "Environment 2: Episode 2707, Score -122.60976910865244, Avg_Score -119.6583465037353\n",
      "Adding trajectory to replay buffer: step 13475, counter 214930\n",
      "Environment 5: Episode 2708, Score -122.81666710094115, Avg_Score -119.66886845783758\n",
      "Adding trajectory to replay buffer: step 13479, counter 214973\n",
      "Environment 4: Episode 2709, Score -122.7789931754501, Avg_Score -119.7140839140242\n",
      "Adding trajectory to replay buffer: step 13479, counter 215017\n",
      "Environment 10: Episode 2710, Score -122.55614834601542, Avg_Score -119.71398729234755\n",
      "Adding trajectory to replay buffer: step 13483, counter 215103\n",
      "Environment 14: Episode 2711, Score -122.94677173901385, Avg_Score -119.72708198750173\n",
      "Adding trajectory to replay buffer: step 13484, counter 215147\n",
      "Environment 1: Episode 2712, Score -117.17710266464515, Avg_Score -119.6767266733901\n",
      "Adding trajectory to replay buffer: step 13485, counter 215192\n",
      "Environment 6: Episode 2713, Score -121.3568727297075, Avg_Score -119.77166481529451\n",
      "Adding trajectory to replay buffer: step 13485, counter 215235\n",
      "Environment 13: Episode 2714, Score -122.27448525492846, Avg_Score -119.81456986801935\n",
      "Adding trajectory to replay buffer: step 13488, counter 215334\n",
      "Environment 9: Episode 2715, Score -125.15332256936345, Avg_Score -119.89020992499894\n",
      "Adding trajectory to replay buffer: step 13490, counter 215412\n",
      "Environment 7: Episode 2716, Score -119.47435904702668, Avg_Score -119.88651128121873\n",
      "Adding trajectory to replay buffer: step 13498, counter 215459\n",
      "Environment 0: Episode 2717, Score -122.70141613653426, Avg_Score -119.95644062574736\n",
      "Adding trajectory to replay buffer: step 13505, counter 215508\n",
      "Environment 8: Episode 2718, Score -117.20353366492317, Avg_Score -119.95037082390687\n",
      "Adding trajectory to replay buffer: step 13506, counter 215551\n",
      "Environment 2: Episode 2719, Score -122.66850650773819, Avg_Score -119.95260514555413\n",
      "Adding trajectory to replay buffer: step 13507, counter 215597\n",
      "Environment 3: Episode 2720, Score -123.20469969563496, Avg_Score -119.97230643302746\n",
      "Adding trajectory to replay buffer: step 13523, counter 215665\n",
      "Environment 15: Episode 2721, Score -116.18132520959463, Avg_Score -119.90394225078705\n",
      "Adding trajectory to replay buffer: step 13524, counter 215710\n",
      "Environment 10: Episode 2722, Score -117.47544961293228, Avg_Score -119.86019774065382\n",
      "Adding trajectory to replay buffer: step 13525, counter 215756\n",
      "Environment 4: Episode 2723, Score -120.78299629176594, Avg_Score -119.8764247165161\n",
      "Adding trajectory to replay buffer: step 13526, counter 215798\n",
      "Environment 1: Episode 2724, Score -123.3568098719921, Avg_Score -119.98271822853849\n",
      "Adding trajectory to replay buffer: step 13528, counter 215841\n",
      "Environment 13: Episode 2725, Score -123.02627544038742, Avg_Score -120.0096285239882\n",
      "Adding trajectory to replay buffer: step 13529, counter 215885\n",
      "Environment 6: Episode 2726, Score -122.78302708461135, Avg_Score -120.01263939308195\n",
      "Adding trajectory to replay buffer: step 13529, counter 215931\n",
      "Environment 14: Episode 2727, Score -119.091390845151, Avg_Score -120.01347063248619\n",
      "Adding trajectory to replay buffer: step 13534, counter 215977\n",
      "Environment 9: Episode 2728, Score -121.91239227154541, Avg_Score -120.0309551724572\n",
      "Adding trajectory to replay buffer: step 13541, counter 216028\n",
      "Environment 7: Episode 2729, Score -122.20812184663676, Avg_Score -120.07900488797809\n",
      "Adding trajectory to replay buffer: step 13544, counter 216074\n",
      "Environment 0: Episode 2730, Score -123.80237222993622, Avg_Score -120.13801462555094\n",
      "Adding trajectory to replay buffer: step 13548, counter 216117\n",
      "Environment 8: Episode 2731, Score -122.57922319974301, Avg_Score -120.19542289993515\n",
      "Adding trajectory to replay buffer: step 13549, counter 216160\n",
      "Environment 2: Episode 2732, Score -122.19874796833346, Avg_Score -120.24619645740286\n",
      "Adding trajectory to replay buffer: step 13549, counter 216202\n",
      "Environment 3: Episode 2733, Score -123.45691423220002, Avg_Score -120.3323604653225\n",
      "Adding trajectory to replay buffer: step 13566, counter 216245\n",
      "Environment 15: Episode 2734, Score -122.92718495280853, Avg_Score -120.38199394740734\n",
      "Adding trajectory to replay buffer: step 13569, counter 216289\n",
      "Environment 4: Episode 2735, Score -122.40971255017692, Avg_Score -120.42179859939291\n",
      "Adding trajectory to replay buffer: step 13569, counter 216334\n",
      "Environment 10: Episode 2736, Score -123.01488115469552, Avg_Score -120.44631042723499\n",
      "Adding trajectory to replay buffer: step 13570, counter 216378\n",
      "Environment 1: Episode 2737, Score -123.1459308348832, Avg_Score -120.47198894833681\n",
      "Adding trajectory to replay buffer: step 13574, counter 216423\n",
      "Environment 6: Episode 2738, Score -121.55224045005193, Avg_Score -120.48559786611912\n",
      "Adding trajectory to replay buffer: step 13574, counter 216468\n",
      "Environment 14: Episode 2739, Score -121.28590560010541, Avg_Score -120.49669405627272\n",
      "Adding trajectory to replay buffer: step 13577, counter 216511\n",
      "Environment 9: Episode 2740, Score -122.50640055224795, Avg_Score -120.58355064020458\n",
      "Adding trajectory to replay buffer: step 13583, counter 216566\n",
      "Environment 13: Episode 2741, Score -114.08412924199055, Avg_Score -120.48651656416295\n",
      "Adding trajectory to replay buffer: step 13589, counter 216611\n",
      "Environment 0: Episode 2742, Score -122.94091869509903, Avg_Score -120.51659761299169\n",
      "Adding trajectory to replay buffer: step 13593, counter 216655\n",
      "Environment 2: Episode 2743, Score -120.53922326323391, Avg_Score -120.57568494584106\n",
      "Adding trajectory to replay buffer: step 13593, counter 216699\n",
      "Environment 3: Episode 2744, Score -122.32456857989169, Avg_Score -120.61355149032313\n",
      "Adding trajectory to replay buffer: step 13598, counter 216749\n",
      "Environment 8: Episode 2745, Score -116.90034293793329, Avg_Score -120.62053040437311\n",
      "Adding trajectory to replay buffer: step 13600, counter 216874\n",
      "Environment 5: Episode 2746, Score -127.44573990806909, Avg_Score -120.66280951334153\n",
      "Adding trajectory to replay buffer: step 13602, counter 217138\n",
      "Environment 12: Episode 2747, Score -125.63169944977015, Avg_Score -120.77303826970308\n",
      "Adding trajectory to replay buffer: step 13606, counter 217203\n",
      "Environment 7: Episode 2748, Score -115.3327597723324, Avg_Score -120.74461387321108\n",
      "Adding trajectory to replay buffer: step 13612, counter 217249\n",
      "Environment 15: Episode 2749, Score -124.06207316092356, Avg_Score -120.78564863018981\n",
      "Adding trajectory to replay buffer: step 13613, counter 217292\n",
      "Environment 1: Episode 2750, Score -122.93585898586736, Avg_Score -120.68107500547858\n",
      "Adding trajectory to replay buffer: step 13614, counter 217337\n",
      "Environment 4: Episode 2751, Score -121.6314863343183, Avg_Score -120.68179393551904\n",
      "Adding trajectory to replay buffer: step 13618, counter 217381\n",
      "Environment 14: Episode 2752, Score -121.55552267425072, Avg_Score -120.66566618360457\n",
      "Adding trajectory to replay buffer: step 13623, counter 217430\n",
      "Environment 6: Episode 2753, Score -116.91138356611444, Avg_Score -120.60652951827862\n",
      "Adding trajectory to replay buffer: step 13627, counter 217617\n",
      "Environment 11: Episode 2754, Score -123.11323416921898, Avg_Score -120.64816546554108\n",
      "Adding trajectory to replay buffer: step 13632, counter 217680\n",
      "Environment 10: Episode 2755, Score -113.89886202471145, Avg_Score -120.63684728669288\n",
      "Adding trajectory to replay buffer: step 13634, counter 217725\n",
      "Environment 0: Episode 2756, Score -122.27726505108924, Avg_Score -120.71212215040397\n",
      "Adding trajectory to replay buffer: step 13636, counter 217768\n",
      "Environment 3: Episode 2757, Score -117.8897590522555, Avg_Score -120.70239818276899\n",
      "Adding trajectory to replay buffer: step 13646, counter 217821\n",
      "Environment 2: Episode 2758, Score -119.82738914739713, Avg_Score -120.68402435851783\n",
      "Adding trajectory to replay buffer: step 13648, counter 217867\n",
      "Environment 12: Episode 2759, Score -120.27152760357534, Avg_Score -120.71969725244558\n",
      "Adding trajectory to replay buffer: step 13651, counter 217935\n",
      "Environment 13: Episode 2760, Score -117.54263281637492, Avg_Score -120.70969274086825\n",
      "Adding trajectory to replay buffer: step 13653, counter 217982\n",
      "Environment 7: Episode 2761, Score -121.26487233907791, Avg_Score -120.68982390701257\n",
      "Adding trajectory to replay buffer: step 13661, counter 218043\n",
      "Environment 5: Episode 2762, Score -116.40526492456223, Avg_Score -120.6798163735716\n",
      "Adding trajectory to replay buffer: step 13670, counter 218101\n",
      "Environment 15: Episode 2763, Score -115.26493075976016, Avg_Score -120.67921221807802\n",
      "Adding trajectory to replay buffer: step 13671, counter 218145\n",
      "Environment 11: Episode 2764, Score -118.12185321650468, Avg_Score -120.69136832878132\n",
      "Adding trajectory to replay buffer: step 13672, counter 218194\n",
      "Environment 6: Episode 2765, Score -120.0600246268219, Avg_Score -120.69358824990934\n",
      "Adding trajectory to replay buffer: step 13675, counter 218237\n",
      "Environment 10: Episode 2766, Score -122.6978286879727, Avg_Score -120.70155988215728\n",
      "Adding trajectory to replay buffer: step 13676, counter 218295\n",
      "Environment 14: Episode 2767, Score -116.3894498671076, Avg_Score -120.69518932142232\n",
      "Adding trajectory to replay buffer: step 13677, counter 218338\n",
      "Environment 0: Episode 2768, Score -123.15851438548353, Avg_Score -120.71943338067557\n",
      "Adding trajectory to replay buffer: step 13679, counter 218381\n",
      "Environment 3: Episode 2769, Score -122.52662441273282, Avg_Score -120.83492190869546\n",
      "Adding trajectory to replay buffer: step 13692, counter 218427\n",
      "Environment 2: Episode 2770, Score -120.01023231197365, Avg_Score -120.8345835236725\n",
      "Adding trajectory to replay buffer: step 13694, counter 218473\n",
      "Environment 12: Episode 2771, Score -122.34157319593803, Avg_Score -120.78190200782214\n",
      "Adding trajectory to replay buffer: step 13697, counter 218517\n",
      "Environment 7: Episode 2772, Score -122.02018962648884, Avg_Score -120.82954949760096\n",
      "Adding trajectory to replay buffer: step 13700, counter 218619\n",
      "Environment 8: Episode 2773, Score -116.52608840991836, Avg_Score -120.78766441451026\n",
      "Adding trajectory to replay buffer: step 13704, counter 218662\n",
      "Environment 5: Episode 2774, Score -120.39028940206269, Avg_Score -120.75380879204056\n",
      "Adding trajectory to replay buffer: step 13705, counter 218790\n",
      "Environment 9: Episode 2775, Score -114.0849159464358, Avg_Score -120.67966434570391\n",
      "Adding trajectory to replay buffer: step 13711, counter 218850\n",
      "Environment 13: Episode 2776, Score -115.75441563291227, Avg_Score -120.64137759440466\n",
      "Adding trajectory to replay buffer: step 13714, counter 218892\n",
      "Environment 6: Episode 2777, Score -123.07725178748743, Avg_Score -120.63926584783337\n",
      "Adding trajectory to replay buffer: step 13714, counter 218936\n",
      "Environment 15: Episode 2778, Score -116.05898709332757, Avg_Score -120.57479768566286\n",
      "Adding trajectory to replay buffer: step 13718, counter 218979\n",
      "Environment 10: Episode 2779, Score -122.56106016268022, Avg_Score -120.6221010460039\n",
      "Adding trajectory to replay buffer: step 13720, counter 219022\n",
      "Environment 0: Episode 2780, Score -122.25431731102057, Avg_Score -120.61650688608785\n",
      "Adding trajectory to replay buffer: step 13720, counter 219066\n",
      "Environment 14: Episode 2781, Score -121.58041025758038, Avg_Score -120.61550304457491\n",
      "Adding trajectory to replay buffer: step 13724, counter 219111\n",
      "Environment 3: Episode 2782, Score -118.49328494841482, Avg_Score -120.60557227822153\n",
      "Adding trajectory to replay buffer: step 13740, counter 219154\n",
      "Environment 7: Episode 2783, Score -121.63223331448063, Avg_Score -120.59717424328284\n",
      "Adding trajectory to replay buffer: step 13741, counter 219282\n",
      "Environment 1: Episode 2784, Score -118.69892266751638, Avg_Score -120.56178397325404\n",
      "Adding trajectory to replay buffer: step 13745, counter 219327\n",
      "Environment 8: Episode 2785, Score -121.4848460191302, Avg_Score -120.55029177529083\n",
      "Adding trajectory to replay buffer: step 13750, counter 219372\n",
      "Environment 9: Episode 2786, Score -122.61457777007483, Avg_Score -120.57508263125399\n",
      "Adding trajectory to replay buffer: step 13757, counter 219415\n",
      "Environment 6: Episode 2787, Score -122.69372472886865, Avg_Score -120.64738227802854\n",
      "Adding trajectory to replay buffer: step 13760, counter 219461\n",
      "Environment 15: Episode 2788, Score -120.39618116525187, Avg_Score -120.62381866057869\n",
      "Adding trajectory to replay buffer: step 13762, counter 219503\n",
      "Environment 14: Episode 2789, Score -122.56383595683582, Avg_Score -120.70753225082585\n",
      "Adding trajectory to replay buffer: step 13763, counter 219546\n",
      "Environment 0: Episode 2790, Score -121.44887673245806, Avg_Score -120.73025896915188\n",
      "Adding trajectory to replay buffer: step 13770, counter 219592\n",
      "Environment 3: Episode 2791, Score -122.10942171585623, Avg_Score -120.57319504030347\n",
      "Adding trajectory to replay buffer: step 13776, counter 219676\n",
      "Environment 2: Episode 2792, Score -115.62599413606029, Avg_Score -120.5967840489078\n",
      "Adding trajectory to replay buffer: step 13778, counter 219736\n",
      "Environment 10: Episode 2793, Score -113.45113509232738, Avg_Score -120.51128458425853\n",
      "Adding trajectory to replay buffer: step 13778, counter 219843\n",
      "Environment 11: Episode 2794, Score -118.34575860405526, Avg_Score -120.47889754045372\n",
      "Adding trajectory to replay buffer: step 13784, counter 219887\n",
      "Environment 7: Episode 2795, Score -115.52360120269283, Avg_Score -120.47189186383862\n",
      "Adding trajectory to replay buffer: step 13785, counter 219931\n",
      "Environment 1: Episode 2796, Score -120.39432776105109, Avg_Score -120.49037586443154\n",
      "Adding trajectory to replay buffer: step 13791, counter 219977\n",
      "Environment 8: Episode 2797, Score -120.94298278115565, Avg_Score -120.54431183772215\n",
      "Adding trajectory to replay buffer: step 13798, counter 220025\n",
      "Environment 9: Episode 2798, Score -121.67215675470358, Avg_Score -120.58378892627394\n",
      "Adding trajectory to replay buffer: step 13805, counter 220070\n",
      "Environment 15: Episode 2799, Score -117.96782771959528, Avg_Score -120.53819438353933\n",
      "Adding trajectory to replay buffer: step 13807, counter 220115\n",
      "Environment 14: Episode 2800, Score -118.1077125982639, Avg_Score -120.47410059270167\n",
      "Adding trajectory to replay buffer: step 13808, counter 220160\n",
      "Environment 0: Episode 2801, Score -120.65680727559018, Avg_Score -120.54165800493134\n",
      "Adding trajectory to replay buffer: step 13809, counter 220212\n",
      "Environment 6: Episode 2802, Score -117.78339537722991, Avg_Score -120.52381214473742\n",
      "Adding trajectory to replay buffer: step 13813, counter 220255\n",
      "Environment 3: Episode 2803, Score -122.81655810501613, Avg_Score -120.52959062787457\n",
      "Adding trajectory to replay buffer: step 13819, counter 220363\n",
      "Environment 13: Episode 2804, Score -110.70795494228415, Avg_Score -120.42169648744552\n",
      "Adding trajectory to replay buffer: step 13820, counter 220479\n",
      "Environment 5: Episode 2805, Score -126.99406945724289, Avg_Score -120.51729054491574\n",
      "Adding trajectory to replay buffer: step 13820, counter 220521\n",
      "Environment 10: Episode 2806, Score -118.90364982814776, Avg_Score -120.52315257386945\n",
      "Adding trajectory to replay buffer: step 13822, counter 220567\n",
      "Environment 2: Episode 2807, Score -122.60645825808558, Avg_Score -120.52311946536378\n",
      "Adding trajectory to replay buffer: step 13822, counter 220611\n",
      "Environment 11: Episode 2808, Score -121.68822319130786, Avg_Score -120.51183502626743\n",
      "Adding trajectory to replay buffer: step 13828, counter 220654\n",
      "Environment 1: Episode 2809, Score -122.51265632441081, Avg_Score -120.50917165775704\n",
      "Adding trajectory to replay buffer: step 13832, counter 220702\n",
      "Environment 7: Episode 2810, Score -118.92533984595755, Avg_Score -120.47286357275647\n",
      "Adding trajectory to replay buffer: step 13836, counter 220747\n",
      "Environment 8: Episode 2811, Score -123.15826327098719, Avg_Score -120.47497848807619\n",
      "Adding trajectory to replay buffer: step 13853, counter 220791\n",
      "Environment 6: Episode 2812, Score -120.89413685696013, Avg_Score -120.51214882999933\n",
      "Adding trajectory to replay buffer: step 13853, counter 220837\n",
      "Environment 14: Episode 2813, Score -120.35288359428384, Avg_Score -120.5021089386451\n",
      "Adding trajectory to replay buffer: step 13856, counter 220880\n",
      "Environment 3: Episode 2814, Score -121.94595142278634, Avg_Score -120.49882360032366\n",
      "Adding trajectory to replay buffer: step 13856, counter 221042\n",
      "Environment 12: Episode 2815, Score -122.50415167544907, Avg_Score -120.47233189138453\n",
      "Adding trajectory to replay buffer: step 13865, counter 221087\n",
      "Environment 5: Episode 2816, Score -121.92079452590272, Avg_Score -120.4967962461733\n",
      "Adding trajectory to replay buffer: step 13865, counter 221133\n",
      "Environment 13: Episode 2817, Score -121.15108845764647, Avg_Score -120.48129296938441\n",
      "Adding trajectory to replay buffer: step 13866, counter 221179\n",
      "Environment 10: Episode 2818, Score -119.33826493484527, Avg_Score -120.50264028208363\n",
      "Adding trajectory to replay buffer: step 13868, counter 221225\n",
      "Environment 11: Episode 2819, Score -123.23748356794566, Avg_Score -120.50833005268572\n",
      "Adding trajectory to replay buffer: step 13869, counter 221272\n",
      "Environment 2: Episode 2820, Score -121.69057340279346, Avg_Score -120.49318878975731\n",
      "Adding trajectory to replay buffer: step 13873, counter 221317\n",
      "Environment 1: Episode 2821, Score -120.94458172063902, Avg_Score -120.54082135486775\n",
      "Adding trajectory to replay buffer: step 13877, counter 221580\n",
      "Environment 4: Episode 2822, Score -127.435399286771, Avg_Score -120.64042085160614\n",
      "Adding trajectory to replay buffer: step 13878, counter 221626\n",
      "Environment 7: Episode 2823, Score -119.78150480017376, Avg_Score -120.63040593669021\n",
      "Adding trajectory to replay buffer: step 13879, counter 221697\n",
      "Environment 0: Episode 2824, Score -114.13010585109703, Avg_Score -120.53813889648126\n",
      "Adding trajectory to replay buffer: step 13884, counter 221745\n",
      "Environment 8: Episode 2825, Score -116.96247466516309, Avg_Score -120.477500888729\n",
      "Adding trajectory to replay buffer: step 13897, counter 221789\n",
      "Environment 6: Episode 2826, Score -123.02547460673625, Avg_Score -120.47992536395026\n",
      "Adding trajectory to replay buffer: step 13897, counter 221833\n",
      "Environment 14: Episode 2827, Score -120.65939965588103, Avg_Score -120.49560545205755\n",
      "Adding trajectory to replay buffer: step 13899, counter 221876\n",
      "Environment 12: Episode 2828, Score -116.43692968326559, Avg_Score -120.44085082617475\n",
      "Adding trajectory to replay buffer: step 13900, counter 221920\n",
      "Environment 3: Episode 2829, Score -120.69125734791781, Avg_Score -120.42568218118757\n",
      "Adding trajectory to replay buffer: step 13910, counter 221965\n",
      "Environment 5: Episode 2830, Score -122.5003664991508, Avg_Score -120.41266212387971\n",
      "Adding trajectory to replay buffer: step 13910, counter 222010\n",
      "Environment 13: Episode 2831, Score -122.54248535919811, Avg_Score -120.41229474547428\n",
      "Adding trajectory to replay buffer: step 13911, counter 222055\n",
      "Environment 10: Episode 2832, Score -122.29063457156346, Avg_Score -120.41321361150655\n",
      "Adding trajectory to replay buffer: step 13912, counter 222099\n",
      "Environment 11: Episode 2833, Score -122.47612528844924, Avg_Score -120.40340572206904\n",
      "Adding trajectory to replay buffer: step 13916, counter 222146\n",
      "Environment 2: Episode 2834, Score -119.80969342762977, Avg_Score -120.37223080681724\n",
      "Adding trajectory to replay buffer: step 13920, counter 222189\n",
      "Environment 4: Episode 2835, Score -123.14409314281555, Avg_Score -120.37957461274364\n",
      "Adding trajectory to replay buffer: step 13922, counter 222233\n",
      "Environment 7: Episode 2836, Score -122.64166073325525, Avg_Score -120.37584240852922\n",
      "Adding trajectory to replay buffer: step 13928, counter 222277\n",
      "Environment 8: Episode 2837, Score -121.78120268886411, Avg_Score -120.36219512706904\n",
      "Adding trajectory to replay buffer: step 13943, counter 222321\n",
      "Environment 12: Episode 2838, Score -121.79279097387194, Avg_Score -120.36460063230724\n",
      "Adding trajectory to replay buffer: step 13944, counter 222467\n",
      "Environment 9: Episode 2839, Score -124.90126646588371, Avg_Score -120.40075424096506\n",
      "Adding trajectory to replay buffer: step 13945, counter 222512\n",
      "Environment 3: Episode 2840, Score -121.06580977517118, Avg_Score -120.38634833319425\n",
      "Adding trajectory to replay buffer: step 13946, counter 222561\n",
      "Environment 14: Episode 2841, Score -116.03899751389771, Avg_Score -120.40589701591333\n",
      "Adding trajectory to replay buffer: step 13953, counter 222641\n",
      "Environment 1: Episode 2842, Score -120.25904787884411, Avg_Score -120.37907830775079\n",
      "Adding trajectory to replay buffer: step 13957, counter 222687\n",
      "Environment 10: Episode 2843, Score -118.58493755967666, Avg_Score -120.35953545071523\n",
      "Adding trajectory to replay buffer: step 13973, counter 222744\n",
      "Environment 2: Episode 2844, Score -114.99830638368677, Avg_Score -120.28627282875318\n",
      "Adding trajectory to replay buffer: step 13973, counter 222789\n",
      "Environment 8: Episode 2845, Score -123.12823615269798, Avg_Score -120.34855176090083\n",
      "Adding trajectory to replay buffer: step 13976, counter 222843\n",
      "Environment 7: Episode 2846, Score -115.80952718270508, Avg_Score -120.2321896336472\n",
      "Adding trajectory to replay buffer: step 13982, counter 222905\n",
      "Environment 4: Episode 2847, Score -116.37559467924137, Avg_Score -120.13962858594188\n",
      "Adding trajectory to replay buffer: step 13985, counter 222993\n",
      "Environment 6: Episode 2848, Score -115.09084414305724, Avg_Score -120.13720942964916\n",
      "Adding trajectory to replay buffer: step 13989, counter 223036\n",
      "Environment 14: Episode 2849, Score -123.0776455249687, Avg_Score -120.12736515328962\n",
      "Adding trajectory to replay buffer: step 13990, counter 223083\n",
      "Environment 12: Episode 2850, Score -120.59431839981862, Avg_Score -120.10394974742908\n",
      "Adding trajectory to replay buffer: step 13995, counter 223125\n",
      "Environment 1: Episode 2851, Score -122.55039679404099, Avg_Score -120.11313885202631\n",
      "Adding trajectory to replay buffer: step 14001, counter 223169\n",
      "Environment 10: Episode 2852, Score -122.6417037653861, Avg_Score -120.12400066293768\n",
      "Adding trajectory to replay buffer: step 14004, counter 223368\n",
      "Environment 15: Episode 2853, Score -121.564530692686, Avg_Score -120.17053213420337\n",
      "Adding trajectory to replay buffer: step 14007, counter 223496\n",
      "Environment 0: Episode 2854, Score -117.64281598758635, Avg_Score -120.11582795238706\n",
      "Adding trajectory to replay buffer: step 14007, counter 223593\n",
      "Environment 13: Episode 2855, Score -123.08720793415917, Avg_Score -120.20771141148154\n",
      "Adding trajectory to replay buffer: step 14011, counter 223660\n",
      "Environment 9: Episode 2856, Score -117.22226800042205, Avg_Score -120.15716144097487\n",
      "Adding trajectory to replay buffer: step 14016, counter 223731\n",
      "Environment 3: Episode 2857, Score -115.42642172138083, Avg_Score -120.1325280676661\n",
      "Adding trajectory to replay buffer: step 14017, counter 223775\n",
      "Environment 2: Episode 2858, Score -122.5161429492291, Avg_Score -120.15941560568444\n",
      "Adding trajectory to replay buffer: step 14022, counter 223821\n",
      "Environment 7: Episode 2859, Score -121.28420541881336, Avg_Score -120.16954238383683\n",
      "Adding trajectory to replay buffer: step 14028, counter 223937\n",
      "Environment 11: Episode 2860, Score -117.63708459611672, Avg_Score -120.17048690163425\n",
      "Adding trajectory to replay buffer: step 14034, counter 223989\n",
      "Environment 4: Episode 2861, Score -115.77076647698539, Avg_Score -120.11554584301331\n",
      "Adding trajectory to replay buffer: step 14034, counter 224034\n",
      "Environment 14: Episode 2862, Score -121.34906983414541, Avg_Score -120.16498389210916\n",
      "Adding trajectory to replay buffer: step 14037, counter 224098\n",
      "Environment 8: Episode 2863, Score -116.3698752993308, Avg_Score -120.17603333750485\n",
      "Adding trajectory to replay buffer: step 14040, counter 224143\n",
      "Environment 1: Episode 2864, Score -123.14403205212702, Avg_Score -120.22625512586109\n",
      "Adding trajectory to replay buffer: step 14044, counter 224186\n",
      "Environment 10: Episode 2865, Score -123.49622325455583, Avg_Score -120.26061711213842\n",
      "Adding trajectory to replay buffer: step 14047, counter 224229\n",
      "Environment 15: Episode 2866, Score -122.80732029433851, Avg_Score -120.26171202820207\n",
      "Adding trajectory to replay buffer: step 14050, counter 224272\n",
      "Environment 0: Episode 2867, Score -122.50486769096678, Avg_Score -120.32286620644066\n",
      "Adding trajectory to replay buffer: step 14050, counter 224315\n",
      "Environment 13: Episode 2868, Score -122.39142699566577, Avg_Score -120.31519533254249\n",
      "Adding trajectory to replay buffer: step 14055, counter 224359\n",
      "Environment 9: Episode 2869, Score -122.40378548220794, Avg_Score -120.31396694323723\n",
      "Adding trajectory to replay buffer: step 14064, counter 224438\n",
      "Environment 6: Episode 2870, Score -121.14505055133316, Avg_Score -120.32531512563081\n",
      "Adding trajectory to replay buffer: step 14068, counter 224516\n",
      "Environment 12: Episode 2871, Score -119.15633463568935, Avg_Score -120.29346274002832\n",
      "Adding trajectory to replay buffer: step 14070, counter 224558\n",
      "Environment 11: Episode 2872, Score -122.91708324821354, Avg_Score -120.30243167624558\n",
      "Adding trajectory to replay buffer: step 14075, counter 224616\n",
      "Environment 2: Episode 2873, Score -119.60129614565398, Avg_Score -120.33318375360295\n",
      "Adding trajectory to replay buffer: step 14076, counter 224676\n",
      "Environment 3: Episode 2874, Score -115.97305950466543, Avg_Score -120.28901145462898\n",
      "Adding trajectory to replay buffer: step 14076, counter 224730\n",
      "Environment 7: Episode 2875, Score -119.5386460409003, Avg_Score -120.34354875557364\n",
      "Adding trajectory to replay buffer: step 14077, counter 224773\n",
      "Environment 14: Episode 2876, Score -115.92540277349514, Avg_Score -120.34525862697947\n",
      "Adding trajectory to replay buffer: step 14083, counter 224822\n",
      "Environment 4: Episode 2877, Score -120.44274555348854, Avg_Score -120.31891356463947\n",
      "Adding trajectory to replay buffer: step 14092, counter 224867\n",
      "Environment 15: Episode 2878, Score -121.47837225866566, Avg_Score -120.37310741629285\n",
      "Adding trajectory to replay buffer: step 14093, counter 224910\n",
      "Environment 13: Episode 2879, Score -122.86619427758254, Avg_Score -120.37615875744191\n",
      "Adding trajectory to replay buffer: step 14095, counter 224955\n",
      "Environment 0: Episode 2880, Score -121.03248973187432, Avg_Score -120.36394048165046\n",
      "Adding trajectory to replay buffer: step 14099, counter 224999\n",
      "Environment 9: Episode 2881, Score -122.75529187759199, Avg_Score -120.37568929785057\n",
      "Adding trajectory to replay buffer: step 14104, counter 225063\n",
      "Environment 1: Episode 2882, Score -117.6091365822833, Avg_Score -120.36684781418927\n",
      "Adding trajectory to replay buffer: step 14107, counter 225106\n",
      "Environment 6: Episode 2883, Score -123.0933341228546, Avg_Score -120.381458822273\n",
      "Adding trajectory to replay buffer: step 14113, counter 225151\n",
      "Environment 12: Episode 2884, Score -117.85749728224675, Avg_Score -120.3730445684203\n",
      "Adding trajectory to replay buffer: step 14124, counter 225198\n",
      "Environment 14: Episode 2885, Score -121.02181803701755, Avg_Score -120.36841428859918\n",
      "Adding trajectory to replay buffer: step 14131, counter 225419\n",
      "Environment 5: Episode 2886, Score -124.94067465902833, Avg_Score -120.39167525748871\n",
      "Adding trajectory to replay buffer: step 14134, counter 225461\n",
      "Environment 15: Episode 2887, Score -122.93741361282517, Avg_Score -120.3941121463283\n",
      "Adding trajectory to replay buffer: step 14135, counter 225503\n",
      "Environment 13: Episode 2888, Score -122.88526295464547, Avg_Score -120.41900296422223\n",
      "Adding trajectory to replay buffer: step 14137, counter 225564\n",
      "Environment 7: Episode 2889, Score -116.94738152668812, Avg_Score -120.36283841992075\n",
      "Adding trajectory to replay buffer: step 14138, counter 225607\n",
      "Environment 0: Episode 2890, Score -123.03616944180864, Avg_Score -120.37871134701425\n",
      "Adding trajectory to replay buffer: step 14138, counter 225670\n",
      "Environment 2: Episode 2891, Score -117.7868840328182, Avg_Score -120.33548597018385\n",
      "Adding trajectory to replay buffer: step 14141, counter 225774\n",
      "Environment 8: Episode 2892, Score -114.74201182725342, Avg_Score -120.32664614709576\n",
      "Adding trajectory to replay buffer: step 14142, counter 225817\n",
      "Environment 9: Episode 2893, Score -123.10420520093292, Avg_Score -120.42317684818181\n",
      "Adding trajectory to replay buffer: step 14144, counter 225878\n",
      "Environment 4: Episode 2894, Score -116.43330469076584, Avg_Score -120.40405230904891\n",
      "Adding trajectory to replay buffer: step 14148, counter 225922\n",
      "Environment 1: Episode 2895, Score -123.82385381644531, Avg_Score -120.48705483518643\n",
      "Adding trajectory to replay buffer: step 14148, counter 226000\n",
      "Environment 11: Episode 2896, Score -121.78080697467364, Avg_Score -120.50091962732266\n",
      "Adding trajectory to replay buffer: step 14157, counter 226081\n",
      "Environment 3: Episode 2897, Score -121.66514652911822, Avg_Score -120.50814126480229\n",
      "Adding trajectory to replay buffer: step 14157, counter 226125\n",
      "Environment 12: Episode 2898, Score -122.39552648434042, Avg_Score -120.51537496209866\n",
      "Adding trajectory to replay buffer: step 14167, counter 226185\n",
      "Environment 6: Episode 2899, Score -111.61006659638323, Avg_Score -120.45179735086654\n",
      "Adding trajectory to replay buffer: step 14168, counter 226229\n",
      "Environment 14: Episode 2900, Score -121.97693137683781, Avg_Score -120.49048953865227\n",
      "Adding trajectory to replay buffer: step 14175, counter 226273\n",
      "Environment 5: Episode 2901, Score -122.50225521219025, Avg_Score -120.50894401801828\n",
      "Adding trajectory to replay buffer: step 14177, counter 226316\n",
      "Environment 15: Episode 2902, Score -122.46086214330543, Avg_Score -120.55571868567904\n",
      "Adding trajectory to replay buffer: step 14178, counter 226359\n",
      "Environment 13: Episode 2903, Score -121.23975272543356, Avg_Score -120.53995063188322\n",
      "Adding trajectory to replay buffer: step 14180, counter 226402\n",
      "Environment 7: Episode 2904, Score -122.93457281250382, Avg_Score -120.66221681058543\n",
      "Adding trajectory to replay buffer: step 14181, counter 226445\n",
      "Environment 0: Episode 2905, Score -122.78506059660018, Avg_Score -120.62012672197899\n",
      "Adding trajectory to replay buffer: step 14181, counter 226488\n",
      "Environment 2: Episode 2906, Score -122.83229197494003, Avg_Score -120.65941314344691\n",
      "Adding trajectory to replay buffer: step 14184, counter 226628\n",
      "Environment 10: Episode 2907, Score -119.3573398894049, Avg_Score -120.62692195976007\n",
      "Adding trajectory to replay buffer: step 14187, counter 226673\n",
      "Environment 9: Episode 2908, Score -123.03813593629374, Avg_Score -120.64042108720994\n",
      "Adding trajectory to replay buffer: step 14191, counter 226720\n",
      "Environment 4: Episode 2909, Score -121.3421517939996, Avg_Score -120.62871604190585\n",
      "Adding trajectory to replay buffer: step 14195, counter 226767\n",
      "Environment 11: Episode 2910, Score -122.16754938342609, Avg_Score -120.66113813728055\n",
      "Adding trajectory to replay buffer: step 14204, counter 226814\n",
      "Environment 12: Episode 2911, Score -121.3668862663495, Avg_Score -120.64322436723414\n",
      "Adding trajectory to replay buffer: step 14212, counter 226869\n",
      "Environment 3: Episode 2912, Score -118.73602339124432, Avg_Score -120.62164323257697\n",
      "Adding trajectory to replay buffer: step 14213, counter 226914\n",
      "Environment 14: Episode 2913, Score -122.2094100652275, Avg_Score -120.64020849728638\n",
      "Adding trajectory to replay buffer: step 14220, counter 226959\n",
      "Environment 5: Episode 2914, Score -122.15950068662502, Avg_Score -120.64234398992475\n",
      "Adding trajectory to replay buffer: step 14221, counter 227003\n",
      "Environment 15: Episode 2915, Score -123.19018753798306, Avg_Score -120.64920434855011\n",
      "Adding trajectory to replay buffer: step 14223, counter 227048\n",
      "Environment 13: Episode 2916, Score -121.86187500877088, Avg_Score -120.64861515337881\n",
      "Adding trajectory to replay buffer: step 14225, counter 227092\n",
      "Environment 0: Episode 2917, Score -122.3145715045482, Avg_Score -120.66024998384782\n",
      "Adding trajectory to replay buffer: step 14225, counter 227136\n",
      "Environment 2: Episode 2918, Score -122.34057929382226, Avg_Score -120.69027312743759\n",
      "Adding trajectory to replay buffer: step 14225, counter 227181\n",
      "Environment 7: Episode 2919, Score -122.03811337935119, Avg_Score -120.67827942555165\n",
      "Adding trajectory to replay buffer: step 14230, counter 227227\n",
      "Environment 10: Episode 2920, Score -121.07845347549394, Avg_Score -120.67215822627868\n",
      "Adding trajectory to replay buffer: step 14239, counter 227275\n",
      "Environment 4: Episode 2921, Score -123.63107163466327, Avg_Score -120.69902312541892\n",
      "Adding trajectory to replay buffer: step 14240, counter 227320\n",
      "Environment 11: Episode 2922, Score -122.10720564013099, Avg_Score -120.64574118895254\n",
      "Adding trajectory to replay buffer: step 14245, counter 227378\n",
      "Environment 9: Episode 2923, Score -118.77183093138474, Avg_Score -120.63564445026465\n",
      "Adding trajectory to replay buffer: step 14248, counter 227422\n",
      "Environment 12: Episode 2924, Score -122.5229778510624, Avg_Score -120.7195731702643\n",
      "Adding trajectory to replay buffer: step 14259, counter 227469\n",
      "Environment 3: Episode 2925, Score -117.68168857991137, Avg_Score -120.72676530941179\n",
      "Adding trajectory to replay buffer: step 14259, counter 227515\n",
      "Environment 14: Episode 2926, Score -121.99618649719096, Avg_Score -120.71647242831632\n",
      "Adding trajectory to replay buffer: step 14264, counter 227559\n",
      "Environment 5: Episode 2927, Score -121.16192160509527, Avg_Score -120.72149764780846\n",
      "Adding trajectory to replay buffer: step 14267, counter 227605\n",
      "Environment 15: Episode 2928, Score -121.51178324631974, Avg_Score -120.772246183439\n",
      "Adding trajectory to replay buffer: step 14270, counter 227734\n",
      "Environment 8: Episode 2929, Score -117.42676997260813, Avg_Score -120.73960130968591\n",
      "Adding trajectory to replay buffer: step 14272, counter 227783\n",
      "Environment 13: Episode 2930, Score -119.09777000089858, Avg_Score -120.70557534470338\n",
      "Adding trajectory to replay buffer: step 14274, counter 227832\n",
      "Environment 0: Episode 2931, Score -122.38233133245507, Avg_Score -120.70397380443596\n",
      "Adding trajectory to replay buffer: step 14276, counter 227878\n",
      "Environment 10: Episode 2932, Score -121.21659756555967, Avg_Score -120.69323343437591\n",
      "Adding trajectory to replay buffer: step 14284, counter 228014\n",
      "Environment 1: Episode 2933, Score -128.858902752505, Avg_Score -120.75706120901647\n",
      "Adding trajectory to replay buffer: step 14285, counter 228060\n",
      "Environment 4: Episode 2934, Score -118.45495480429754, Avg_Score -120.74351382278313\n",
      "Adding trajectory to replay buffer: step 14286, counter 228121\n",
      "Environment 2: Episode 2935, Score -113.92307872192809, Avg_Score -120.65130367857427\n",
      "Adding trajectory to replay buffer: step 14286, counter 228167\n",
      "Environment 11: Episode 2936, Score -122.20345910800808, Avg_Score -120.64692166232179\n",
      "Adding trajectory to replay buffer: step 14291, counter 228213\n",
      "Environment 9: Episode 2937, Score -123.09305295414416, Avg_Score -120.66004016497457\n",
      "Adding trajectory to replay buffer: step 14292, counter 228280\n",
      "Environment 7: Episode 2938, Score -117.77302810056196, Avg_Score -120.61984253624149\n",
      "Adding trajectory to replay buffer: step 14295, counter 228327\n",
      "Environment 12: Episode 2939, Score -120.58238161480428, Avg_Score -120.57665368773068\n",
      "Adding trajectory to replay buffer: step 14302, counter 228370\n",
      "Environment 3: Episode 2940, Score -121.597130279975, Avg_Score -120.58196689277872\n",
      "Adding trajectory to replay buffer: step 14302, counter 228413\n",
      "Environment 14: Episode 2941, Score -121.72307777314882, Avg_Score -120.63880769537126\n",
      "Adding trajectory to replay buffer: step 14316, counter 228459\n",
      "Environment 8: Episode 2942, Score -121.77957699644007, Avg_Score -120.6540129865472\n",
      "Adding trajectory to replay buffer: step 14316, counter 228503\n",
      "Environment 13: Episode 2943, Score -122.68880479954805, Avg_Score -120.69505165894593\n",
      "Adding trajectory to replay buffer: step 14320, counter 228549\n",
      "Environment 0: Episode 2944, Score -121.03677345590852, Avg_Score -120.75543632966816\n",
      "Adding trajectory to replay buffer: step 14320, counter 228593\n",
      "Environment 10: Episode 2945, Score -122.89146184823724, Avg_Score -120.75306858662356\n",
      "Adding trajectory to replay buffer: step 14329, counter 228637\n",
      "Environment 4: Episode 2946, Score -116.86695319683228, Avg_Score -120.76364284676484\n",
      "Adding trajectory to replay buffer: step 14333, counter 228706\n",
      "Environment 5: Episode 2947, Score -117.21546322278245, Avg_Score -120.77204153220026\n",
      "Adding trajectory to replay buffer: step 14334, counter 228773\n",
      "Environment 15: Episode 2948, Score -111.38469268414875, Avg_Score -120.73498001761112\n",
      "Adding trajectory to replay buffer: step 14335, counter 228824\n",
      "Environment 1: Episode 2949, Score -119.56697821572051, Avg_Score -120.69987334451864\n",
      "Adding trajectory to replay buffer: step 14341, counter 228870\n",
      "Environment 12: Episode 2950, Score -122.33652281457651, Avg_Score -120.71729538866623\n",
      "Adding trajectory to replay buffer: step 14351, counter 228919\n",
      "Environment 14: Episode 2951, Score -121.48666030574621, Avg_Score -120.70665802378326\n",
      "Adding trajectory to replay buffer: step 14357, counter 228990\n",
      "Environment 2: Episode 2952, Score -113.33509191689826, Avg_Score -120.61359190529836\n",
      "Adding trajectory to replay buffer: step 14364, counter 229063\n",
      "Environment 9: Episode 2953, Score -117.95067451169405, Avg_Score -120.57745334348846\n",
      "Adding trajectory to replay buffer: step 14367, counter 229144\n",
      "Environment 11: Episode 2954, Score -116.49241567716065, Avg_Score -120.5659493403842\n",
      "Adding trajectory to replay buffer: step 14372, counter 229200\n",
      "Environment 8: Episode 2955, Score -113.89704521868067, Avg_Score -120.47404771322942\n",
      "Adding trajectory to replay buffer: step 14373, counter 229240\n",
      "Environment 5: Episode 2956, Score -117.46098362120179, Avg_Score -120.47643486943721\n",
      "Adding trajectory to replay buffer: step 14374, counter 229294\n",
      "Environment 10: Episode 2957, Score -113.69595424792729, Avg_Score -120.45913019470267\n",
      "Adding trajectory to replay buffer: step 14374, counter 229352\n",
      "Environment 13: Episode 2958, Score -115.63591195048889, Avg_Score -120.3903278847153\n",
      "Adding trajectory to replay buffer: step 14376, counter 229561\n",
      "Environment 6: Episode 2959, Score -126.63945072424536, Avg_Score -120.44388033776964\n",
      "Adding trajectory to replay buffer: step 14383, counter 229610\n",
      "Environment 15: Episode 2960, Score -117.36745136037966, Avg_Score -120.44118400541227\n",
      "Adding trajectory to replay buffer: step 14386, counter 229655\n",
      "Environment 12: Episode 2961, Score -116.46437300827603, Avg_Score -120.44812007072514\n",
      "Adding trajectory to replay buffer: step 14387, counter 229722\n",
      "Environment 0: Episode 2962, Score -114.9894972635222, Avg_Score -120.38452434501893\n",
      "Adding trajectory to replay buffer: step 14389, counter 229776\n",
      "Environment 1: Episode 2963, Score -115.24149705222186, Avg_Score -120.37324056254785\n",
      "Adding trajectory to replay buffer: step 14397, counter 229822\n",
      "Environment 14: Episode 2964, Score -119.89460623152429, Avg_Score -120.3407463043418\n",
      "Adding trajectory to replay buffer: step 14401, counter 229866\n",
      "Environment 2: Episode 2965, Score -122.4271707836967, Avg_Score -120.33005577963323\n",
      "Adding trajectory to replay buffer: step 14407, counter 229909\n",
      "Environment 9: Episode 2966, Score -122.18535240227916, Avg_Score -120.32383610071264\n",
      "Adding trajectory to replay buffer: step 14416, counter 230023\n",
      "Environment 3: Episode 2967, Score -127.75192546070771, Avg_Score -120.37630667841006\n",
      "Adding trajectory to replay buffer: step 14417, counter 230067\n",
      "Environment 5: Episode 2968, Score -122.02143870883435, Avg_Score -120.37260679554174\n",
      "Adding trajectory to replay buffer: step 14418, counter 230113\n",
      "Environment 8: Episode 2969, Score -120.80748865176488, Avg_Score -120.35664382723733\n",
      "Adding trajectory to replay buffer: step 14420, counter 230157\n",
      "Environment 6: Episode 2970, Score -115.96712247742587, Avg_Score -120.30486454649825\n",
      "Adding trajectory to replay buffer: step 14427, counter 230201\n",
      "Environment 15: Episode 2971, Score -122.21712220660287, Avg_Score -120.33547242220735\n",
      "Adding trajectory to replay buffer: step 14434, counter 230306\n",
      "Environment 4: Episode 2972, Score -113.33358343122961, Avg_Score -120.23963742403753\n",
      "Adding trajectory to replay buffer: step 14444, counter 230349\n",
      "Environment 2: Episode 2973, Score -115.72372661667876, Avg_Score -120.20086172874777\n",
      "Adding trajectory to replay buffer: step 14449, counter 230391\n",
      "Environment 9: Episode 2974, Score -123.08817804180208, Avg_Score -120.27201291411914\n",
      "Adding trajectory to replay buffer: step 14458, counter 230557\n",
      "Environment 7: Episode 2975, Score -120.24163346976663, Avg_Score -120.27904278840779\n",
      "Adding trajectory to replay buffer: step 14459, counter 230642\n",
      "Environment 13: Episode 2976, Score -115.49891697992447, Avg_Score -120.27477793047208\n",
      "Adding trajectory to replay buffer: step 14461, counter 230714\n",
      "Environment 1: Episode 2977, Score -115.34139462104254, Avg_Score -120.22376442114762\n",
      "Adding trajectory to replay buffer: step 14464, counter 230762\n",
      "Environment 3: Episode 2978, Score -120.7621947811246, Avg_Score -120.21660264637222\n",
      "Adding trajectory to replay buffer: step 14465, counter 230853\n",
      "Environment 10: Episode 2979, Score -121.02693091385376, Avg_Score -120.19821001273493\n",
      "Adding trajectory to replay buffer: step 14466, counter 230902\n",
      "Environment 5: Episode 2980, Score -115.62266470566703, Avg_Score -120.14411176247285\n",
      "Adding trajectory to replay buffer: step 14466, counter 230971\n",
      "Environment 14: Episode 2981, Score -112.36447977747878, Avg_Score -120.04020364147175\n",
      "Adding trajectory to replay buffer: step 14467, counter 231018\n",
      "Environment 6: Episode 2982, Score -120.74051350638146, Avg_Score -120.07151741071272\n",
      "Adding trajectory to replay buffer: step 14472, counter 231063\n",
      "Environment 15: Episode 2983, Score -122.29638762772518, Avg_Score -120.06354794576141\n",
      "Adding trajectory to replay buffer: step 14475, counter 231171\n",
      "Environment 11: Episode 2984, Score -113.63672727845578, Avg_Score -120.02134024572351\n",
      "Adding trajectory to replay buffer: step 14475, counter 231260\n",
      "Environment 12: Episode 2985, Score -114.78027484201641, Avg_Score -119.9589248137735\n",
      "Adding trajectory to replay buffer: step 14476, counter 231302\n",
      "Environment 4: Episode 2986, Score -120.02048537329335, Avg_Score -119.90972292091615\n",
      "Adding trajectory to replay buffer: step 14488, counter 231346\n",
      "Environment 2: Episode 2987, Score -122.57855837259193, Avg_Score -119.90613436851382\n",
      "Adding trajectory to replay buffer: step 14499, counter 231458\n",
      "Environment 0: Episode 2988, Score -126.2071394433112, Avg_Score -119.93935313340049\n",
      "Adding trajectory to replay buffer: step 14502, counter 231502\n",
      "Environment 7: Episode 2989, Score -121.9640349445399, Avg_Score -119.98951966757899\n",
      "Adding trajectory to replay buffer: step 14502, counter 231545\n",
      "Environment 13: Episode 2990, Score -122.94764512466516, Avg_Score -119.98863442440754\n",
      "Adding trajectory to replay buffer: step 14505, counter 231632\n",
      "Environment 8: Episode 2991, Score -115.71770518623045, Avg_Score -119.96794263594167\n",
      "Adding trajectory to replay buffer: step 14507, counter 231675\n",
      "Environment 3: Episode 2992, Score -121.77302185013053, Avg_Score -120.03825273617045\n",
      "Adding trajectory to replay buffer: step 14509, counter 231718\n",
      "Environment 5: Episode 2993, Score -122.82551778891312, Avg_Score -120.03546586205026\n",
      "Adding trajectory to replay buffer: step 14509, counter 231778\n",
      "Environment 9: Episode 2994, Score -114.68282580769062, Avg_Score -120.01796107321951\n",
      "Adding trajectory to replay buffer: step 14509, counter 231822\n",
      "Environment 10: Episode 2995, Score -122.42182471852627, Avg_Score -120.00394078224029\n",
      "Adding trajectory to replay buffer: step 14510, counter 231866\n",
      "Environment 14: Episode 2996, Score -122.68298695395515, Avg_Score -120.01296258203314\n",
      "Adding trajectory to replay buffer: step 14512, counter 231911\n",
      "Environment 6: Episode 2997, Score -120.80693931569779, Avg_Score -120.00438050989895\n",
      "Adding trajectory to replay buffer: step 14513, counter 231952\n",
      "Environment 15: Episode 2998, Score -117.89314105767073, Avg_Score -119.95935665563223\n",
      "Adding trajectory to replay buffer: step 14522, counter 231998\n",
      "Environment 4: Episode 2999, Score -122.818843529492, Avg_Score -120.07144442496333\n",
      "Adding trajectory to replay buffer: step 14522, counter 232045\n",
      "Environment 12: Episode 3000, Score -122.99160231189367, Avg_Score -120.08159113431392\n",
      "Adding trajectory to replay buffer: step 14529, counter 232099\n",
      "Environment 11: Episode 3001, Score -118.36743801612035, Avg_Score -120.04024296235322\n",
      "Adding trajectory to replay buffer: step 14532, counter 232143\n",
      "Environment 2: Episode 3002, Score -123.35223035530622, Avg_Score -120.0491566444732\n",
      "Adding trajectory to replay buffer: step 14544, counter 232188\n",
      "Environment 0: Episode 3003, Score -122.18393200139948, Avg_Score -120.05859843723287\n",
      "Adding trajectory to replay buffer: step 14547, counter 232274\n",
      "Environment 1: Episode 3004, Score -115.01193594145403, Avg_Score -119.97937206852237\n",
      "Adding trajectory to replay buffer: step 14548, counter 232317\n",
      "Environment 8: Episode 3005, Score -122.18925704601904, Avg_Score -119.97341403301654\n",
      "Adding trajectory to replay buffer: step 14550, counter 232360\n",
      "Environment 3: Episode 3006, Score -122.77370149672467, Avg_Score -119.97282812823441\n",
      "Adding trajectory to replay buffer: step 14550, counter 232408\n",
      "Environment 7: Episode 3007, Score -119.05758754887495, Avg_Score -119.9698306048291\n",
      "Adding trajectory to replay buffer: step 14551, counter 232450\n",
      "Environment 9: Episode 3008, Score -116.06643021804715, Avg_Score -119.90011354764665\n",
      "Adding trajectory to replay buffer: step 14552, counter 232493\n",
      "Environment 10: Episode 3009, Score -122.23907100206583, Avg_Score -119.90908273972731\n",
      "Adding trajectory to replay buffer: step 14554, counter 232538\n",
      "Environment 5: Episode 3010, Score -123.34815912590301, Avg_Score -119.92088883715206\n",
      "Adding trajectory to replay buffer: step 14554, counter 232582\n",
      "Environment 14: Episode 3011, Score -122.41798949781433, Avg_Score -119.93139986946674\n",
      "Adding trajectory to replay buffer: step 14556, counter 232626\n",
      "Environment 6: Episode 3012, Score -121.57550023050234, Avg_Score -119.9597946378593\n",
      "Adding trajectory to replay buffer: step 14557, counter 232670\n",
      "Environment 15: Episode 3013, Score -123.40365058471946, Avg_Score -119.97173704305422\n",
      "Adding trajectory to replay buffer: step 14561, counter 232729\n",
      "Environment 13: Episode 3014, Score -113.7569907430575, Avg_Score -119.88771194361856\n",
      "Adding trajectory to replay buffer: step 14564, counter 232771\n",
      "Environment 4: Episode 3015, Score -122.23700379562379, Avg_Score -119.87818010619499\n",
      "Adding trajectory to replay buffer: step 14565, counter 232814\n",
      "Environment 12: Episode 3016, Score -122.61934390841549, Avg_Score -119.88575479519142\n",
      "Adding trajectory to replay buffer: step 14574, counter 232859\n",
      "Environment 11: Episode 3017, Score -122.11980076361695, Avg_Score -119.88380708778213\n",
      "Adding trajectory to replay buffer: step 14578, counter 232905\n",
      "Environment 2: Episode 3018, Score -121.53183938768942, Avg_Score -119.87571968872078\n",
      "Adding trajectory to replay buffer: step 14589, counter 232950\n",
      "Environment 0: Episode 3019, Score -122.01007271003414, Avg_Score -119.87543928202764\n",
      "Adding trajectory to replay buffer: step 14591, counter 232994\n",
      "Environment 1: Episode 3020, Score -122.60847268210972, Avg_Score -119.8907394740938\n",
      "Adding trajectory to replay buffer: step 14593, counter 233039\n",
      "Environment 8: Episode 3021, Score -120.33316694434298, Avg_Score -119.85776042719061\n",
      "Adding trajectory to replay buffer: step 14596, counter 233084\n",
      "Environment 9: Episode 3022, Score -122.18828128010533, Avg_Score -119.85857118359034\n",
      "Adding trajectory to replay buffer: step 14598, counter 233128\n",
      "Environment 14: Episode 3023, Score -121.27741801853168, Avg_Score -119.8836270544618\n",
      "Adding trajectory to replay buffer: step 14599, counter 233173\n",
      "Environment 5: Episode 3024, Score -118.77324370584513, Avg_Score -119.84612971300963\n",
      "Adding trajectory to replay buffer: step 14601, counter 233217\n",
      "Environment 15: Episode 3025, Score -115.76031902084996, Avg_Score -119.82691601741901\n",
      "Adding trajectory to replay buffer: step 14605, counter 233261\n",
      "Environment 13: Episode 3026, Score -122.43077243760601, Avg_Score -119.83126187682315\n",
      "Adding trajectory to replay buffer: step 14609, counter 233305\n",
      "Environment 12: Episode 3027, Score -122.886267180847, Avg_Score -119.8485053325807\n",
      "Adding trajectory to replay buffer: step 14617, counter 233372\n",
      "Environment 3: Episode 3028, Score -117.71745684998855, Avg_Score -119.81056206861739\n",
      "Adding trajectory to replay buffer: step 14620, counter 233418\n",
      "Environment 11: Episode 3029, Score -122.95692384695634, Avg_Score -119.86586360736084\n",
      "Adding trajectory to replay buffer: step 14633, counter 233462\n",
      "Environment 0: Episode 3030, Score -122.94902980347227, Avg_Score -119.90437620538658\n",
      "Adding trajectory to replay buffer: step 14636, counter 233507\n",
      "Environment 1: Episode 3031, Score -121.39893815708781, Avg_Score -119.89454227363291\n",
      "Adding trajectory to replay buffer: step 14637, counter 233551\n",
      "Environment 8: Episode 3032, Score -122.81822418926129, Avg_Score -119.91055853986992\n",
      "Adding trajectory to replay buffer: step 14638, counter 233625\n",
      "Environment 4: Episode 3033, Score -121.38308820515499, Avg_Score -119.83580039439643\n",
      "Adding trajectory to replay buffer: step 14642, counter 233671\n",
      "Environment 9: Episode 3034, Score -122.18588386566502, Avg_Score -119.87310968501009\n",
      "Adding trajectory to replay buffer: step 14644, counter 233717\n",
      "Environment 14: Episode 3035, Score -120.86432120908547, Avg_Score -119.94252210988166\n",
      "Adding trajectory to replay buffer: step 14647, counter 233763\n",
      "Environment 15: Episode 3036, Score -120.40768437828496, Avg_Score -119.92456436258442\n",
      "Adding trajectory to replay buffer: step 14648, counter 233806\n",
      "Environment 13: Episode 3037, Score -122.91388935549185, Avg_Score -119.9227727265979\n",
      "Adding trajectory to replay buffer: step 14651, counter 233848\n",
      "Environment 12: Episode 3038, Score -122.95147500810648, Avg_Score -119.97455719567336\n",
      "Adding trajectory to replay buffer: step 14657, counter 233906\n",
      "Environment 5: Episode 3039, Score -114.90547355679298, Avg_Score -119.91778811509324\n",
      "Adding trajectory to replay buffer: step 14659, counter 233948\n",
      "Environment 3: Episode 3040, Score -122.93734423661604, Avg_Score -119.93119025465965\n",
      "Adding trajectory to replay buffer: step 14664, counter 233992\n",
      "Environment 11: Episode 3041, Score -122.53301677505237, Avg_Score -119.9392896446787\n",
      "Adding trajectory to replay buffer: step 14677, counter 234036\n",
      "Environment 0: Episode 3042, Score -119.52350961087676, Avg_Score -119.91672897082307\n",
      "Adding trajectory to replay buffer: step 14678, counter 234078\n",
      "Environment 1: Episode 3043, Score -122.98544028445085, Avg_Score -119.91969532567208\n",
      "Adding trajectory to replay buffer: step 14679, counter 234120\n",
      "Environment 8: Episode 3044, Score -122.96811673292021, Avg_Score -119.93900875844218\n",
      "Adding trajectory to replay buffer: step 14681, counter 234163\n",
      "Environment 4: Episode 3045, Score -122.74635963337465, Avg_Score -119.93755773629357\n",
      "Adding trajectory to replay buffer: step 14684, counter 234205\n",
      "Environment 9: Episode 3046, Score -122.89850728503554, Avg_Score -119.9978732771756\n",
      "Adding trajectory to replay buffer: step 14687, counter 234248\n",
      "Environment 14: Episode 3047, Score -122.93903394328618, Avg_Score -120.05510898438062\n",
      "Adding trajectory to replay buffer: step 14690, counter 234291\n",
      "Environment 15: Episode 3048, Score -122.73872541403522, Avg_Score -120.16864931167949\n",
      "Adding trajectory to replay buffer: step 14691, counter 234334\n",
      "Environment 13: Episode 3049, Score -122.53231936052316, Avg_Score -120.19830272312751\n",
      "Adding trajectory to replay buffer: step 14694, counter 234377\n",
      "Environment 12: Episode 3050, Score -123.08766933559812, Avg_Score -120.20581418833774\n",
      "Adding trajectory to replay buffer: step 14699, counter 234524\n",
      "Environment 10: Episode 3051, Score -119.07395478240275, Avg_Score -120.18168713310429\n",
      "Adding trajectory to replay buffer: step 14703, counter 234568\n",
      "Environment 3: Episode 3052, Score -121.79322722883894, Avg_Score -120.26626848622371\n",
      "Adding trajectory to replay buffer: step 14703, counter 234614\n",
      "Environment 5: Episode 3053, Score -122.12727268210301, Avg_Score -120.3080344679278\n",
      "Adding trajectory to replay buffer: step 14707, counter 234771\n",
      "Environment 7: Episode 3054, Score -121.16831774236076, Avg_Score -120.3547934885798\n",
      "Adding trajectory to replay buffer: step 14710, counter 234817\n",
      "Environment 11: Episode 3055, Score -118.86579785066408, Avg_Score -120.40448101489962\n",
      "Adding trajectory to replay buffer: step 14713, counter 234952\n",
      "Environment 2: Episode 3056, Score -125.73493334290075, Avg_Score -120.48722051211658\n",
      "Adding trajectory to replay buffer: step 14723, counter 234997\n",
      "Environment 1: Episode 3057, Score -122.08786521756525, Avg_Score -120.57113962181296\n",
      "Adding trajectory to replay buffer: step 14726, counter 235046\n",
      "Environment 0: Episode 3058, Score -120.26629123290753, Avg_Score -120.61744341463712\n",
      "Adding trajectory to replay buffer: step 14727, counter 235094\n",
      "Environment 8: Episode 3059, Score -121.55851899570352, Avg_Score -120.56663409735172\n",
      "Adding trajectory to replay buffer: step 14733, counter 235137\n",
      "Environment 15: Episode 3060, Score -116.87433635257247, Avg_Score -120.56170294727364\n",
      "Adding trajectory to replay buffer: step 14736, counter 235186\n",
      "Environment 14: Episode 3061, Score -120.28127044040461, Avg_Score -120.59987192159493\n",
      "Adding trajectory to replay buffer: step 14738, counter 235230\n",
      "Environment 12: Episode 3062, Score -120.36617026355863, Avg_Score -120.6536386515953\n",
      "Adding trajectory to replay buffer: step 14738, counter 235277\n",
      "Environment 13: Episode 3063, Score -122.30396943670388, Avg_Score -120.72426337544013\n",
      "Adding trajectory to replay buffer: step 14743, counter 235317\n",
      "Environment 5: Episode 3064, Score -116.79810062218023, Avg_Score -120.69329831934667\n",
      "Adding trajectory to replay buffer: step 14747, counter 235361\n",
      "Environment 3: Episode 3065, Score -122.6336966076903, Avg_Score -120.69536357758662\n",
      "Adding trajectory to replay buffer: step 14752, counter 235406\n",
      "Environment 7: Episode 3066, Score -116.6189913222113, Avg_Score -120.63969996678594\n",
      "Adding trajectory to replay buffer: step 14755, counter 235480\n",
      "Environment 4: Episode 3067, Score -113.71278291656512, Avg_Score -120.49930854134453\n",
      "Adding trajectory to replay buffer: step 14759, counter 235526\n",
      "Environment 2: Episode 3068, Score -121.831336372459, Avg_Score -120.49740751798079\n",
      "Adding trajectory to replay buffer: step 14761, counter 235588\n",
      "Environment 10: Episode 3069, Score -116.20131539540007, Avg_Score -120.45134578541712\n",
      "Adding trajectory to replay buffer: step 14766, counter 235631\n",
      "Environment 1: Episode 3070, Score -122.12561625930543, Avg_Score -120.51293072323591\n",
      "Adding trajectory to replay buffer: step 14772, counter 235677\n",
      "Environment 0: Episode 3071, Score -120.1210329353561, Avg_Score -120.49196983052347\n",
      "Adding trajectory to replay buffer: step 14773, counter 235723\n",
      "Environment 8: Episode 3072, Score -121.03672436831053, Avg_Score -120.56900123989426\n",
      "Adding trajectory to replay buffer: step 14779, counter 235818\n",
      "Environment 9: Episode 3073, Score -115.87916568978937, Avg_Score -120.57055563062539\n",
      "Adding trajectory to replay buffer: step 14780, counter 235888\n",
      "Environment 11: Episode 3074, Score -115.71473849374863, Avg_Score -120.49682123514484\n",
      "Adding trajectory to replay buffer: step 14780, counter 235935\n",
      "Environment 15: Episode 3075, Score -119.51166995387587, Avg_Score -120.48952159998593\n",
      "Adding trajectory to replay buffer: step 14782, counter 235981\n",
      "Environment 14: Episode 3076, Score -120.69942041713, Avg_Score -120.541526634358\n",
      "Adding trajectory to replay buffer: step 14783, counter 236026\n",
      "Environment 13: Episode 3077, Score -116.43130429466069, Avg_Score -120.55242573109419\n",
      "Adding trajectory to replay buffer: step 14784, counter 236072\n",
      "Environment 12: Episode 3078, Score -122.29536018954343, Avg_Score -120.56775738517837\n",
      "Adding trajectory to replay buffer: step 14796, counter 236125\n",
      "Environment 5: Episode 3079, Score -117.19440397865449, Avg_Score -120.52943211582637\n",
      "Adding trajectory to replay buffer: step 14797, counter 236366\n",
      "Environment 6: Episode 3080, Score -125.84903701012146, Avg_Score -120.63169583887091\n",
      "Adding trajectory to replay buffer: step 14797, counter 236411\n",
      "Environment 7: Episode 3081, Score -122.20565849982512, Avg_Score -120.73010762609434\n",
      "Adding trajectory to replay buffer: step 14801, counter 236457\n",
      "Environment 4: Episode 3082, Score -122.74867646853563, Avg_Score -120.75018925571591\n",
      "Adding trajectory to replay buffer: step 14803, counter 236501\n",
      "Environment 2: Episode 3083, Score -117.79627525553542, Avg_Score -120.705188131994\n",
      "Adding trajectory to replay buffer: step 14808, counter 236548\n",
      "Environment 10: Episode 3084, Score -121.33838253149825, Avg_Score -120.78220468452442\n",
      "Adding trajectory to replay buffer: step 14812, counter 236594\n",
      "Environment 1: Episode 3085, Score -121.17311298097236, Avg_Score -120.84613306591399\n",
      "Adding trajectory to replay buffer: step 14818, counter 236640\n",
      "Environment 0: Episode 3086, Score -121.82855860392408, Avg_Score -120.86421379822028\n",
      "Adding trajectory to replay buffer: step 14822, counter 236682\n",
      "Environment 11: Episode 3087, Score -120.43225346083318, Avg_Score -120.84275074910269\n",
      "Adding trajectory to replay buffer: step 14823, counter 236726\n",
      "Environment 9: Episode 3088, Score -121.70799569136835, Avg_Score -120.79775931158326\n",
      "Adding trajectory to replay buffer: step 14825, counter 236768\n",
      "Environment 13: Episode 3089, Score -122.94679505451397, Avg_Score -120.807586912683\n",
      "Adding trajectory to replay buffer: step 14825, counter 236813\n",
      "Environment 15: Episode 3090, Score -122.7091436155724, Avg_Score -120.80520189759207\n",
      "Adding trajectory to replay buffer: step 14828, counter 236857\n",
      "Environment 12: Episode 3091, Score -122.99325025278704, Avg_Score -120.87795734825762\n",
      "Adding trajectory to replay buffer: step 14828, counter 236903\n",
      "Environment 14: Episode 3092, Score -121.07381547878559, Avg_Score -120.87096528454418\n",
      "Adding trajectory to replay buffer: step 14842, counter 236948\n",
      "Environment 6: Episode 3093, Score -123.20017415689614, Avg_Score -120.87471184822401\n",
      "Adding trajectory to replay buffer: step 14843, counter 236990\n",
      "Environment 4: Episode 3094, Score -118.75376074540863, Avg_Score -120.91542119760123\n",
      "Adding trajectory to replay buffer: step 14843, counter 237036\n",
      "Environment 7: Episode 3095, Score -122.43603028933269, Avg_Score -120.91556325330929\n",
      "Adding trajectory to replay buffer: step 14849, counter 237082\n",
      "Environment 2: Episode 3096, Score -117.95581350978154, Avg_Score -120.86829151886755\n",
      "Adding trajectory to replay buffer: step 14850, counter 237124\n",
      "Environment 10: Episode 3097, Score -122.9004349764095, Avg_Score -120.88922647547464\n",
      "Adding trajectory to replay buffer: step 14858, counter 237170\n",
      "Environment 1: Episode 3098, Score -122.56133925777488, Avg_Score -120.93590845747569\n",
      "Adding trajectory to replay buffer: step 14862, counter 237214\n",
      "Environment 0: Episode 3099, Score -119.11007267634571, Avg_Score -120.89882074894422\n",
      "Adding trajectory to replay buffer: step 14863, counter 237255\n",
      "Environment 11: Episode 3100, Score -117.21572097921744, Avg_Score -120.84106193561743\n",
      "Adding trajectory to replay buffer: step 14867, counter 237299\n",
      "Environment 9: Episode 3101, Score -117.10178773283151, Avg_Score -120.82840543278455\n",
      "Adding trajectory to replay buffer: step 14869, counter 237343\n",
      "Environment 13: Episode 3102, Score -122.87409929938738, Avg_Score -120.82362412222537\n",
      "Adding trajectory to replay buffer: step 14869, counter 237387\n",
      "Environment 15: Episode 3103, Score -122.87367150604166, Avg_Score -120.8305215172718\n",
      "Adding trajectory to replay buffer: step 14871, counter 237462\n",
      "Environment 5: Episode 3104, Score -115.44471366355693, Avg_Score -120.83484929449286\n",
      "Adding trajectory to replay buffer: step 14871, counter 237505\n",
      "Environment 12: Episode 3105, Score -122.57523219301862, Avg_Score -120.83870904596286\n",
      "Adding trajectory to replay buffer: step 14871, counter 237548\n",
      "Environment 14: Episode 3106, Score -122.57237404093809, Avg_Score -120.83669577140496\n",
      "Adding trajectory to replay buffer: step 14885, counter 237591\n",
      "Environment 6: Episode 3107, Score -122.12844717895393, Avg_Score -120.86740436770575\n",
      "Adding trajectory to replay buffer: step 14888, counter 237636\n",
      "Environment 4: Episode 3108, Score -122.19928279780535, Avg_Score -120.92873289350332\n",
      "Adding trajectory to replay buffer: step 14888, counter 237681\n",
      "Environment 7: Episode 3109, Score -122.14055816797912, Avg_Score -120.92774776516244\n",
      "Adding trajectory to replay buffer: step 14893, counter 237725\n",
      "Environment 2: Episode 3110, Score -121.9296600064126, Avg_Score -120.91356277396753\n",
      "Adding trajectory to replay buffer: step 14896, counter 237771\n",
      "Environment 10: Episode 3111, Score -120.63147981645912, Avg_Score -120.89569767715399\n",
      "Adding trajectory to replay buffer: step 14904, counter 237817\n",
      "Environment 1: Episode 3112, Score -120.19879313865304, Avg_Score -120.88193060623549\n",
      "Adding trajectory to replay buffer: step 14905, counter 237949\n",
      "Environment 8: Episode 3113, Score -117.10086486293872, Avg_Score -120.81890274901772\n",
      "Adding trajectory to replay buffer: step 14916, counter 237998\n",
      "Environment 9: Episode 3114, Score -122.00624045304954, Avg_Score -120.90139524611763\n",
      "Adding trajectory to replay buffer: step 14917, counter 238044\n",
      "Environment 5: Episode 3115, Score -118.22161147510944, Avg_Score -120.86124132291248\n",
      "Adding trajectory to replay buffer: step 14918, counter 238091\n",
      "Environment 12: Episode 3116, Score -119.54234897933404, Avg_Score -120.83047137362168\n",
      "Adding trajectory to replay buffer: step 14918, counter 238138\n",
      "Environment 14: Episode 3117, Score -119.9739402734302, Avg_Score -120.80901276871982\n",
      "Adding trajectory to replay buffer: step 14925, counter 238316\n",
      "Environment 3: Episode 3118, Score -123.99549234132034, Avg_Score -120.83364929825613\n",
      "Adding trajectory to replay buffer: step 14927, counter 238358\n",
      "Environment 6: Episode 3119, Score -115.95802576385998, Avg_Score -120.7731288287944\n",
      "Adding trajectory to replay buffer: step 14940, counter 238410\n",
      "Environment 7: Episode 3120, Score -115.6418584075682, Avg_Score -120.70346268604901\n",
      "Adding trajectory to replay buffer: step 14940, counter 238481\n",
      "Environment 13: Episode 3121, Score -117.32117275568409, Avg_Score -120.67334274416244\n",
      "Adding trajectory to replay buffer: step 14942, counter 238560\n",
      "Environment 11: Episode 3122, Score -119.45468251766204, Avg_Score -120.64600675653799\n",
      "Adding trajectory to replay buffer: step 14947, counter 238603\n",
      "Environment 1: Episode 3123, Score -115.67224424714037, Avg_Score -120.5899550188241\n",
      "Adding trajectory to replay buffer: step 14950, counter 238648\n",
      "Environment 8: Episode 3124, Score -122.14727546362082, Avg_Score -120.62369533640185\n",
      "Adding trajectory to replay buffer: step 14957, counter 238687\n",
      "Environment 12: Episode 3125, Score -116.46844970049725, Avg_Score -120.63077664319832\n",
      "Adding trajectory to replay buffer: step 14958, counter 238729\n",
      "Environment 9: Episode 3126, Score -123.09040090442325, Avg_Score -120.63737292786647\n",
      "Adding trajectory to replay buffer: step 14959, counter 238771\n",
      "Environment 5: Episode 3127, Score -122.92164357719199, Avg_Score -120.63772669182993\n",
      "Adding trajectory to replay buffer: step 14965, counter 238840\n",
      "Environment 10: Episode 3128, Score -118.61027354064956, Avg_Score -120.64665485873655\n",
      "Adding trajectory to replay buffer: step 14969, counter 238884\n",
      "Environment 3: Episode 3129, Score -117.00713413872819, Avg_Score -120.58715696165427\n",
      "Adding trajectory to replay buffer: step 14971, counter 238928\n",
      "Environment 6: Episode 3130, Score -124.68371945261521, Avg_Score -120.6045038581457\n",
      "Adding trajectory to replay buffer: step 14985, counter 238973\n",
      "Environment 13: Episode 3131, Score -120.45351068232084, Avg_Score -120.59504958339804\n",
      "Adding trajectory to replay buffer: step 14986, counter 239019\n",
      "Environment 7: Episode 3132, Score -122.62871129507695, Avg_Score -120.59315445445621\n",
      "Adding trajectory to replay buffer: step 14987, counter 239064\n",
      "Environment 11: Episode 3133, Score -122.0109485690308, Avg_Score -120.59943305809496\n",
      "Adding trajectory to replay buffer: step 14988, counter 239164\n",
      "Environment 4: Episode 3134, Score -113.7405635305612, Avg_Score -120.5149798547439\n",
      "Adding trajectory to replay buffer: step 14992, counter 239209\n",
      "Environment 1: Episode 3135, Score -122.61480618168848, Avg_Score -120.53248470446994\n",
      "Adding trajectory to replay buffer: step 15000, counter 239252\n",
      "Environment 12: Episode 3136, Score -122.38225343085516, Avg_Score -120.55223039499562\n",
      "Adding trajectory to replay buffer: step 15004, counter 239298\n",
      "Environment 9: Episode 3137, Score -121.99759647075958, Avg_Score -120.5430674661483\n",
      "Adding trajectory to replay buffer: step 15009, counter 239445\n",
      "Environment 0: Episode 3138, Score -116.06449313897329, Avg_Score -120.47419764745695\n",
      "Adding trajectory to replay buffer: step 15013, counter 239489\n",
      "Environment 3: Episode 3139, Score -123.04088630641066, Avg_Score -120.55555177495316\n",
      "Adding trajectory to replay buffer: step 15018, counter 239638\n",
      "Environment 15: Episode 3140, Score -131.81102761856155, Avg_Score -120.64428860877263\n",
      "Adding trajectory to replay buffer: step 15024, counter 239712\n",
      "Environment 8: Episode 3141, Score -117.84832126315125, Avg_Score -120.5974416536536\n",
      "Adding trajectory to replay buffer: step 15025, counter 239778\n",
      "Environment 5: Episode 3142, Score -116.55088711482846, Avg_Score -120.56771542869313\n",
      "Adding trajectory to replay buffer: step 15029, counter 239821\n",
      "Environment 7: Episode 3143, Score -122.22697039522913, Avg_Score -120.56013072980093\n",
      "Adding trajectory to replay buffer: step 15029, counter 239865\n",
      "Environment 13: Episode 3144, Score -123.23805160125345, Avg_Score -120.56283007848425\n",
      "Adding trajectory to replay buffer: step 15030, counter 239908\n",
      "Environment 11: Episode 3145, Score -119.41423957440009, Avg_Score -120.5295088778945\n",
      "Adding trajectory to replay buffer: step 15032, counter 239952\n",
      "Environment 4: Episode 3146, Score -121.80774971139742, Avg_Score -120.51860130215812\n",
      "Adding trajectory to replay buffer: step 15035, counter 240094\n",
      "Environment 2: Episode 3147, Score -114.13817418066365, Avg_Score -120.43059270453188\n",
      "Adding trajectory to replay buffer: step 15036, counter 240138\n",
      "Environment 1: Episode 3148, Score -119.97875406707388, Avg_Score -120.40299299106226\n",
      "Adding trajectory to replay buffer: step 15045, counter 240183\n",
      "Environment 12: Episode 3149, Score -121.38709822072461, Avg_Score -120.39154077966424\n",
      "Adding trajectory to replay buffer: step 15049, counter 240228\n",
      "Environment 9: Episode 3150, Score -121.26407217556363, Avg_Score -120.37330480806389\n",
      "Adding trajectory to replay buffer: step 15057, counter 240272\n",
      "Environment 3: Episode 3151, Score -122.41330393967343, Avg_Score -120.40669829963662\n",
      "Adding trajectory to replay buffer: step 15063, counter 240317\n",
      "Environment 15: Episode 3152, Score -122.29635893421496, Avg_Score -120.41172961669034\n",
      "Adding trajectory to replay buffer: step 15068, counter 240360\n",
      "Environment 5: Episode 3153, Score -121.9630990994939, Avg_Score -120.41008788086425\n",
      "Adding trajectory to replay buffer: step 15070, counter 240406\n",
      "Environment 8: Episode 3154, Score -121.45850279384725, Avg_Score -120.41298973137911\n",
      "Adding trajectory to replay buffer: step 15072, counter 240448\n",
      "Environment 11: Episode 3155, Score -123.1165320294785, Avg_Score -120.45549707316727\n",
      "Adding trajectory to replay buffer: step 15075, counter 240494\n",
      "Environment 7: Episode 3156, Score -121.56313434482988, Avg_Score -120.41377908318654\n",
      "Adding trajectory to replay buffer: step 15075, counter 240604\n",
      "Environment 10: Episode 3157, Score -116.6393373372021, Avg_Score -120.35929380438294\n",
      "Adding trajectory to replay buffer: step 15076, counter 240648\n",
      "Environment 4: Episode 3158, Score -121.7481976972452, Avg_Score -120.3741128690263\n",
      "Adding trajectory to replay buffer: step 15078, counter 240691\n",
      "Environment 2: Episode 3159, Score -122.5961105741871, Avg_Score -120.38448878481113\n",
      "Adding trajectory to replay buffer: step 15080, counter 240735\n",
      "Environment 1: Episode 3160, Score -121.82126625237055, Avg_Score -120.43395808380912\n",
      "Adding trajectory to replay buffer: step 15089, counter 240795\n",
      "Environment 13: Episode 3161, Score -113.29482694450394, Avg_Score -120.36409364885012\n",
      "Adding trajectory to replay buffer: step 15093, counter 240839\n",
      "Environment 9: Episode 3162, Score -118.98502920239233, Avg_Score -120.35028223823845\n",
      "Adding trajectory to replay buffer: step 15095, counter 241016\n",
      "Environment 14: Episode 3163, Score -120.84335064243461, Avg_Score -120.33567605029577\n",
      "Adding trajectory to replay buffer: step 15102, counter 241073\n",
      "Environment 12: Episode 3164, Score -114.99291820315892, Avg_Score -120.31762422610555\n",
      "Adding trajectory to replay buffer: step 15107, counter 241123\n",
      "Environment 3: Episode 3165, Score -122.44615766493169, Avg_Score -120.31574883667798\n",
      "Adding trajectory to replay buffer: step 15113, counter 241227\n",
      "Environment 0: Episode 3166, Score -116.22422465125595, Avg_Score -120.31180116996842\n",
      "Adding trajectory to replay buffer: step 15125, counter 241284\n",
      "Environment 5: Episode 3167, Score -112.53424898998625, Avg_Score -120.30001583070265\n",
      "Adding trajectory to replay buffer: step 15128, counter 241334\n",
      "Environment 2: Episode 3168, Score -117.36746372819692, Avg_Score -120.25537710426002\n",
      "Adding trajectory to replay buffer: step 15132, counter 241391\n",
      "Environment 7: Episode 3169, Score -113.5499953339727, Avg_Score -120.22886390364576\n",
      "Adding trajectory to replay buffer: step 15133, counter 241454\n",
      "Environment 8: Episode 3170, Score -113.7357054980211, Avg_Score -120.14496479603291\n",
      "Adding trajectory to replay buffer: step 15136, counter 241518\n",
      "Environment 11: Episode 3171, Score -116.58438324117351, Avg_Score -120.10959829909106\n",
      "Adding trajectory to replay buffer: step 15138, counter 241561\n",
      "Environment 14: Episode 3172, Score -114.47956378428948, Avg_Score -120.04402669325086\n",
      "Adding trajectory to replay buffer: step 15144, counter 241612\n",
      "Environment 9: Episode 3173, Score -117.00829753708965, Avg_Score -120.05531801172384\n",
      "Adding trajectory to replay buffer: step 15147, counter 241657\n",
      "Environment 12: Episode 3174, Score -120.51161421940414, Avg_Score -120.1032867689804\n",
      "Adding trajectory to replay buffer: step 15154, counter 241735\n",
      "Environment 4: Episode 3175, Score -120.2950534561885, Avg_Score -120.11112060400352\n",
      "Adding trajectory to replay buffer: step 15166, counter 241826\n",
      "Environment 10: Episode 3176, Score -121.18885905944401, Avg_Score -120.11601499042666\n",
      "Adding trajectory to replay buffer: step 15169, counter 241870\n",
      "Environment 5: Episode 3177, Score -122.8804136075247, Avg_Score -120.1805060835553\n",
      "Adding trajectory to replay buffer: step 15176, counter 241939\n",
      "Environment 3: Episode 3178, Score -117.60012636070263, Avg_Score -120.13355374526691\n",
      "Adding trajectory to replay buffer: step 15177, counter 241983\n",
      "Environment 8: Episode 3179, Score -121.93605865333664, Avg_Score -120.18097029201374\n",
      "Adding trajectory to replay buffer: step 15179, counter 242026\n",
      "Environment 11: Episode 3180, Score -122.53008464123556, Avg_Score -120.14778076832486\n",
      "Adding trajectory to replay buffer: step 15182, counter 242095\n",
      "Environment 0: Episode 3181, Score -113.44910937887057, Avg_Score -120.06021527711533\n",
      "Adding trajectory to replay buffer: step 15182, counter 242139\n",
      "Environment 14: Episode 3182, Score -122.20266060786818, Avg_Score -120.05475511850868\n",
      "Adding trajectory to replay buffer: step 15188, counter 242247\n",
      "Environment 1: Episode 3183, Score -114.49212771766385, Avg_Score -120.02171364312996\n",
      "Adding trajectory to replay buffer: step 15189, counter 242292\n",
      "Environment 9: Episode 3184, Score -122.79860206301262, Avg_Score -120.03631583844509\n",
      "Adding trajectory to replay buffer: step 15191, counter 242336\n",
      "Environment 12: Episode 3185, Score -121.97902913340118, Avg_Score -120.0443749999694\n",
      "Adding trajectory to replay buffer: step 15199, counter 242564\n",
      "Environment 6: Episode 3186, Score -137.15739771704034, Avg_Score -120.19766339110056\n",
      "Adding trajectory to replay buffer: step 15210, counter 242711\n",
      "Environment 15: Episode 3187, Score -120.71061567628591, Avg_Score -120.20044701325509\n",
      "Adding trajectory to replay buffer: step 15219, counter 242761\n",
      "Environment 5: Episode 3188, Score -117.35966620229868, Avg_Score -120.15696371836441\n",
      "Adding trajectory to replay buffer: step 15222, counter 242851\n",
      "Environment 7: Episode 3189, Score -125.09611558075498, Avg_Score -120.1784569236268\n",
      "Adding trajectory to replay buffer: step 15222, counter 242896\n",
      "Environment 8: Episode 3190, Score -116.86790751221217, Avg_Score -120.12004456259321\n",
      "Adding trajectory to replay buffer: step 15224, counter 242944\n",
      "Environment 3: Episode 3191, Score -123.53560352268568, Avg_Score -120.12546809529218\n",
      "Adding trajectory to replay buffer: step 15227, counter 242989\n",
      "Environment 0: Episode 3192, Score -118.52038439791836, Avg_Score -120.09993378448351\n",
      "Adding trajectory to replay buffer: step 15227, counter 243050\n",
      "Environment 10: Episode 3193, Score -116.14293787634993, Avg_Score -120.02936142167802\n",
      "Adding trajectory to replay buffer: step 15228, counter 243096\n",
      "Environment 14: Episode 3194, Score -118.90311043760616, Avg_Score -120.0308549186\n",
      "Adding trajectory to replay buffer: step 15230, counter 243147\n",
      "Environment 11: Episode 3195, Score -117.31310371360493, Avg_Score -119.97962565284271\n",
      "Adding trajectory to replay buffer: step 15230, counter 243288\n",
      "Environment 13: Episode 3196, Score -118.03575955429548, Avg_Score -119.98042511328785\n",
      "Adding trajectory to replay buffer: step 15231, counter 243331\n",
      "Environment 1: Episode 3197, Score -123.0175673509622, Avg_Score -119.98159643703339\n",
      "Adding trajectory to replay buffer: step 15233, counter 243375\n",
      "Environment 9: Episode 3198, Score -122.73668958987606, Avg_Score -119.9833499403544\n",
      "Adding trajectory to replay buffer: step 15233, counter 243417\n",
      "Environment 12: Episode 3199, Score -122.71887184502744, Avg_Score -120.0194379320412\n",
      "Adding trajectory to replay buffer: step 15234, counter 243523\n",
      "Environment 2: Episode 3200, Score -113.59623640869822, Avg_Score -119.98324308633602\n",
      "Adding trajectory to replay buffer: step 15240, counter 243609\n",
      "Environment 4: Episode 3201, Score -113.73730202786811, Avg_Score -119.94959822928638\n",
      "Adding trajectory to replay buffer: step 15243, counter 243653\n",
      "Environment 6: Episode 3202, Score -122.65634111641657, Avg_Score -119.9474206474567\n",
      "Adding trajectory to replay buffer: step 15255, counter 243698\n",
      "Environment 15: Episode 3203, Score -120.9491889641347, Avg_Score -119.92817582203762\n",
      "Adding trajectory to replay buffer: step 15270, counter 243746\n",
      "Environment 7: Episode 3204, Score -118.7771285255986, Avg_Score -119.96149997065804\n",
      "Adding trajectory to replay buffer: step 15271, counter 243790\n",
      "Environment 0: Episode 3205, Score -122.48857899512909, Avg_Score -119.96063343867917\n",
      "Adding trajectory to replay buffer: step 15271, counter 243834\n",
      "Environment 10: Episode 3206, Score -120.58899069085872, Avg_Score -119.94079960517837\n",
      "Adding trajectory to replay buffer: step 15271, counter 243877\n",
      "Environment 14: Episode 3207, Score -118.35971968697757, Avg_Score -119.9031123302586\n",
      "Adding trajectory to replay buffer: step 15272, counter 243927\n",
      "Environment 8: Episode 3208, Score -115.44401159391305, Avg_Score -119.83555961821966\n",
      "Adding trajectory to replay buffer: step 15272, counter 243969\n",
      "Environment 13: Episode 3209, Score -115.69104034245574, Avg_Score -119.77106443996443\n",
      "Adding trajectory to replay buffer: step 15275, counter 244013\n",
      "Environment 1: Episode 3210, Score -116.60961965340562, Avg_Score -119.71786403643435\n",
      "Adding trajectory to replay buffer: step 15276, counter 244059\n",
      "Environment 11: Episode 3211, Score -122.14691885222743, Avg_Score -119.73301842679204\n",
      "Adding trajectory to replay buffer: step 15277, counter 244103\n",
      "Environment 9: Episode 3212, Score -122.16052047932459, Avg_Score -119.75263570019877\n",
      "Adding trajectory to replay buffer: step 15278, counter 244147\n",
      "Environment 2: Episode 3213, Score -123.79622274429352, Avg_Score -119.81958927901229\n",
      "Adding trajectory to replay buffer: step 15278, counter 244206\n",
      "Environment 5: Episode 3214, Score -121.06422514684562, Avg_Score -119.81016912595025\n",
      "Adding trajectory to replay buffer: step 15278, counter 244251\n",
      "Environment 12: Episode 3215, Score -122.45336631893801, Avg_Score -119.85248667438854\n",
      "Adding trajectory to replay buffer: step 15283, counter 244310\n",
      "Environment 3: Episode 3216, Score -115.3846201249305, Avg_Score -119.8109093858445\n",
      "Adding trajectory to replay buffer: step 15289, counter 244359\n",
      "Environment 4: Episode 3217, Score -122.53614246912859, Avg_Score -119.83653140780149\n",
      "Adding trajectory to replay buffer: step 15300, counter 244404\n",
      "Environment 15: Episode 3218, Score -121.26916387757596, Avg_Score -119.80926812316403\n",
      "Adding trajectory to replay buffer: step 15313, counter 244446\n",
      "Environment 0: Episode 3219, Score -123.16269501736389, Avg_Score -119.88131481569906\n",
      "Adding trajectory to replay buffer: step 15313, counter 244489\n",
      "Environment 7: Episode 3220, Score -123.05313457210984, Avg_Score -119.95542757734447\n",
      "Adding trajectory to replay buffer: step 15313, counter 244531\n",
      "Environment 10: Episode 3221, Score -122.94286823965734, Avg_Score -120.0116445321842\n",
      "Adding trajectory to replay buffer: step 15313, counter 244573\n",
      "Environment 14: Episode 3222, Score -122.91034880547164, Avg_Score -120.0462011950623\n",
      "Adding trajectory to replay buffer: step 15314, counter 244615\n",
      "Environment 8: Episode 3223, Score -123.06435757800192, Avg_Score -120.12012232837093\n",
      "Adding trajectory to replay buffer: step 15314, counter 244657\n",
      "Environment 13: Episode 3224, Score -122.95116435184205, Avg_Score -120.12816121725312\n",
      "Adding trajectory to replay buffer: step 15317, counter 244699\n",
      "Environment 1: Episode 3225, Score -122.67615212030833, Avg_Score -120.19023824145123\n",
      "Adding trajectory to replay buffer: step 15319, counter 244741\n",
      "Environment 9: Episode 3226, Score -122.98071400982143, Avg_Score -120.18914137250523\n",
      "Adding trajectory to replay buffer: step 15319, counter 244784\n",
      "Environment 11: Episode 3227, Score -123.39422772595411, Avg_Score -120.19386721399283\n",
      "Adding trajectory to replay buffer: step 15322, counter 244828\n",
      "Environment 2: Episode 3228, Score -123.41171365105174, Avg_Score -120.24188161509686\n",
      "Adding trajectory to replay buffer: step 15327, counter 244872\n",
      "Environment 3: Episode 3229, Score -122.71024684774628, Avg_Score -120.29891274218706\n",
      "Adding trajectory to replay buffer: step 15330, counter 244959\n",
      "Environment 6: Episode 3230, Score -115.88675909773447, Avg_Score -120.21094313863824\n",
      "Adding trajectory to replay buffer: step 15332, counter 245002\n",
      "Environment 4: Episode 3231, Score -122.86541960324409, Avg_Score -120.23506222784746\n",
      "Adding trajectory to replay buffer: step 15358, counter 245047\n",
      "Environment 0: Episode 3232, Score -121.10839141435736, Avg_Score -120.21985902904026\n",
      "Adding trajectory to replay buffer: step 15358, counter 245092\n",
      "Environment 10: Episode 3233, Score -118.31563020881576, Avg_Score -120.18290584543813\n",
      "Adding trajectory to replay buffer: step 15359, counter 245138\n",
      "Environment 7: Episode 3234, Score -120.6130175280037, Avg_Score -120.25163038541254\n",
      "Adding trajectory to replay buffer: step 15359, counter 245183\n",
      "Environment 13: Episode 3235, Score -121.43784200359384, Avg_Score -120.23986074363161\n",
      "Adding trajectory to replay buffer: step 15360, counter 245224\n",
      "Environment 9: Episode 3236, Score -116.87804183593268, Avg_Score -120.18481862768238\n",
      "Adding trajectory to replay buffer: step 15360, counter 245265\n",
      "Environment 11: Episode 3237, Score -116.84212688178445, Avg_Score -120.13326393179263\n",
      "Adding trajectory to replay buffer: step 15361, counter 245309\n",
      "Environment 1: Episode 3238, Score -122.07550014968837, Avg_Score -120.19337400189976\n",
      "Adding trajectory to replay buffer: step 15366, counter 245397\n",
      "Environment 12: Episode 3239, Score -118.68641697244036, Avg_Score -120.14982930856007\n",
      "Adding trajectory to replay buffer: step 15368, counter 245443\n",
      "Environment 2: Episode 3240, Score -121.32057189131403, Avg_Score -120.0449247512876\n",
      "Adding trajectory to replay buffer: step 15369, counter 245485\n",
      "Environment 3: Episode 3241, Score -119.7989703329342, Avg_Score -120.06443124198545\n",
      "Adding trajectory to replay buffer: step 15370, counter 245555\n",
      "Environment 15: Episode 3242, Score -112.22189790186813, Avg_Score -120.02114134985582\n",
      "Adding trajectory to replay buffer: step 15374, counter 245599\n",
      "Environment 6: Episode 3243, Score -122.5406418770148, Avg_Score -120.02427806467367\n",
      "Adding trajectory to replay buffer: step 15380, counter 245647\n",
      "Environment 4: Episode 3244, Score -120.0678801521243, Avg_Score -119.99257635018239\n",
      "Adding trajectory to replay buffer: step 15401, counter 245690\n",
      "Environment 0: Episode 3245, Score -115.89586926176834, Avg_Score -119.9573926470561\n",
      "Adding trajectory to replay buffer: step 15402, counter 245734\n",
      "Environment 10: Episode 3246, Score -119.32600866374622, Avg_Score -119.9325752365796\n",
      "Adding trajectory to replay buffer: step 15408, counter 245781\n",
      "Environment 1: Episode 3247, Score -113.247346754766, Avg_Score -119.92366696232061\n",
      "Adding trajectory to replay buffer: step 15411, counter 245833\n",
      "Environment 7: Episode 3248, Score -117.66097034696242, Avg_Score -119.90048912511949\n",
      "Adding trajectory to replay buffer: step 15412, counter 245879\n",
      "Environment 12: Episode 3249, Score -123.31791142669755, Avg_Score -119.9197972571792\n",
      "Adding trajectory to replay buffer: step 15412, counter 245921\n",
      "Environment 15: Episode 3250, Score -118.84965923505091, Avg_Score -119.89565312777408\n",
      "Adding trajectory to replay buffer: step 15413, counter 245966\n",
      "Environment 2: Episode 3251, Score -121.31352339822426, Avg_Score -119.88465532235959\n",
      "Adding trajectory to replay buffer: step 15418, counter 246010\n",
      "Environment 6: Episode 3252, Score -122.67728654764966, Avg_Score -119.88846459849395\n",
      "Adding trajectory to replay buffer: step 15422, counter 246072\n",
      "Environment 9: Episode 3253, Score -116.70122882145768, Avg_Score -119.8358458957136\n",
      "Adding trajectory to replay buffer: step 15424, counter 246116\n",
      "Environment 4: Episode 3254, Score -121.44256918610694, Avg_Score -119.8356865596362\n",
      "Adding trajectory to replay buffer: step 15425, counter 246227\n",
      "Environment 8: Episode 3255, Score -123.81979065693854, Avg_Score -119.84271914591079\n",
      "Adding trajectory to replay buffer: step 15428, counter 246296\n",
      "Environment 13: Episode 3256, Score -114.3174342065854, Avg_Score -119.77026214452835\n",
      "Adding trajectory to replay buffer: step 15430, counter 246357\n",
      "Environment 3: Episode 3257, Score -116.35241304216473, Avg_Score -119.76739290157799\n",
      "Adding trajectory to replay buffer: step 15438, counter 246435\n",
      "Environment 11: Episode 3258, Score -121.46623987619392, Avg_Score -119.76457332336747\n",
      "Adding trajectory to replay buffer: step 15445, counter 246479\n",
      "Environment 0: Episode 3259, Score -122.8465765702172, Avg_Score -119.76707798332777\n",
      "Adding trajectory to replay buffer: step 15451, counter 246522\n",
      "Environment 1: Episode 3260, Score -121.35985680825512, Avg_Score -119.76246388888661\n",
      "Adding trajectory to replay buffer: step 15455, counter 246565\n",
      "Environment 12: Episode 3261, Score -122.73748439110133, Avg_Score -119.8568904633526\n",
      "Adding trajectory to replay buffer: step 15456, counter 246609\n",
      "Environment 15: Episode 3262, Score -123.03533607003155, Avg_Score -119.89739353202896\n",
      "Adding trajectory to replay buffer: step 15457, counter 246653\n",
      "Environment 2: Episode 3263, Score -123.12834562657586, Avg_Score -119.92024348187039\n",
      "Adding trajectory to replay buffer: step 15461, counter 246836\n",
      "Environment 5: Episode 3264, Score -121.46144462258118, Avg_Score -119.9849287460646\n",
      "Adding trajectory to replay buffer: step 15463, counter 246881\n",
      "Environment 6: Episode 3265, Score -121.40980193515432, Avg_Score -119.97456518876683\n",
      "Adding trajectory to replay buffer: step 15465, counter 246924\n",
      "Environment 9: Episode 3266, Score -122.86682520795799, Avg_Score -120.04099119433387\n",
      "Adding trajectory to replay buffer: step 15468, counter 246968\n",
      "Environment 4: Episode 3267, Score -118.54116156433213, Avg_Score -120.10106032007732\n",
      "Adding trajectory to replay buffer: step 15470, counter 247013\n",
      "Environment 8: Episode 3268, Score -120.68124284188636, Avg_Score -120.13419811121423\n",
      "Adding trajectory to replay buffer: step 15471, counter 247056\n",
      "Environment 13: Episode 3269, Score -119.58272767230744, Avg_Score -120.19452543459758\n",
      "Adding trajectory to replay buffer: step 15473, counter 247127\n",
      "Environment 10: Episode 3270, Score -118.47441073260394, Avg_Score -120.2419124869434\n",
      "Adding trajectory to replay buffer: step 15476, counter 247173\n",
      "Environment 3: Episode 3271, Score -116.28022543487667, Avg_Score -120.23887090888043\n",
      "Adding trajectory to replay buffer: step 15485, counter 247247\n",
      "Environment 7: Episode 3272, Score -111.7294288682503, Avg_Score -120.21136955972004\n",
      "Adding trajectory to replay buffer: step 15489, counter 247291\n",
      "Environment 0: Episode 3273, Score -119.53342771866359, Avg_Score -120.23662086153577\n",
      "Adding trajectory to replay buffer: step 15496, counter 247336\n",
      "Environment 1: Episode 3274, Score -122.24416152137519, Avg_Score -120.25394633455545\n",
      "Adding trajectory to replay buffer: step 15499, counter 247397\n",
      "Environment 11: Episode 3275, Score -116.54603578671006, Avg_Score -120.2164561578607\n",
      "Adding trajectory to replay buffer: step 15499, counter 247441\n",
      "Environment 12: Episode 3276, Score -123.13770417058468, Avg_Score -120.23594460897212\n",
      "Adding trajectory to replay buffer: step 15502, counter 247486\n",
      "Environment 2: Episode 3277, Score -121.7004342615741, Avg_Score -120.22414481551259\n",
      "Adding trajectory to replay buffer: step 15502, counter 247532\n",
      "Environment 15: Episode 3278, Score -123.12941100205234, Avg_Score -120.2794376619261\n",
      "Adding trajectory to replay buffer: step 15506, counter 247577\n",
      "Environment 5: Episode 3279, Score -121.71893974697652, Avg_Score -120.27726647286251\n",
      "Adding trajectory to replay buffer: step 15509, counter 247623\n",
      "Environment 6: Episode 3280, Score -121.02601062335633, Avg_Score -120.2622257326837\n",
      "Adding trajectory to replay buffer: step 15511, counter 247669\n",
      "Environment 9: Episode 3281, Score -121.51229623600841, Avg_Score -120.34285760125509\n",
      "Adding trajectory to replay buffer: step 15513, counter 247714\n",
      "Environment 4: Episode 3282, Score -122.82619497342408, Avg_Score -120.34909294491067\n",
      "Adding trajectory to replay buffer: step 15516, counter 247760\n",
      "Environment 8: Episode 3283, Score -122.11154783415918, Avg_Score -120.42528714607562\n",
      "Adding trajectory to replay buffer: step 15516, counter 247803\n",
      "Environment 10: Episode 3284, Score -123.39480791442655, Avg_Score -120.43124920458975\n",
      "Adding trajectory to replay buffer: step 15518, counter 247850\n",
      "Environment 13: Episode 3285, Score -118.94896208759883, Avg_Score -120.40094853413171\n",
      "Adding trajectory to replay buffer: step 15520, counter 247894\n",
      "Environment 3: Episode 3286, Score -120.62185609628136, Avg_Score -120.23559311792414\n",
      "Adding trajectory to replay buffer: step 15527, counter 247936\n",
      "Environment 7: Episode 3287, Score -115.13641558176776, Avg_Score -120.17985111697897\n",
      "Adding trajectory to replay buffer: step 15533, counter 247980\n",
      "Environment 0: Episode 3288, Score -121.87725436804506, Avg_Score -120.22502699863642\n",
      "Adding trajectory to replay buffer: step 15541, counter 248022\n",
      "Environment 12: Episode 3289, Score -122.83091816120594, Avg_Score -120.20237502444091\n",
      "Adding trajectory to replay buffer: step 15542, counter 248065\n",
      "Environment 11: Episode 3290, Score -123.03911066394174, Avg_Score -120.26408705595823\n",
      "Adding trajectory to replay buffer: step 15548, counter 248111\n",
      "Environment 2: Episode 3291, Score -121.90324009839073, Avg_Score -120.2477634217153\n",
      "Adding trajectory to replay buffer: step 15548, counter 248157\n",
      "Environment 15: Episode 3292, Score -121.67691888721473, Avg_Score -120.27932876660827\n",
      "Adding trajectory to replay buffer: step 15552, counter 248203\n",
      "Environment 5: Episode 3293, Score -120.09180839668525, Avg_Score -120.31881747181158\n",
      "Adding trajectory to replay buffer: step 15552, counter 248246\n",
      "Environment 6: Episode 3294, Score -122.66234882073041, Avg_Score -120.35640985564285\n",
      "Adding trajectory to replay buffer: step 15554, counter 248289\n",
      "Environment 9: Episode 3295, Score -117.58764728240732, Avg_Score -120.35915529133086\n",
      "Adding trajectory to replay buffer: step 15556, counter 248332\n",
      "Environment 4: Episode 3296, Score -115.95590709989891, Avg_Score -120.33835676678689\n",
      "Adding trajectory to replay buffer: step 15559, counter 248375\n",
      "Environment 10: Episode 3297, Score -122.59739060130282, Avg_Score -120.33415499929028\n",
      "Adding trajectory to replay buffer: step 15561, counter 248420\n",
      "Environment 8: Episode 3298, Score -122.50504064413781, Avg_Score -120.33183850983289\n",
      "Adding trajectory to replay buffer: step 15562, counter 248669\n",
      "Environment 14: Episode 3299, Score -128.53642791326965, Avg_Score -120.39001407051532\n",
      "Adding trajectory to replay buffer: step 15570, counter 248743\n",
      "Environment 1: Episode 3300, Score -117.88001186265362, Avg_Score -120.43285182505485\n",
      "Adding trajectory to replay buffer: step 15571, counter 248796\n",
      "Environment 13: Episode 3301, Score -114.04783637480561, Avg_Score -120.43595716852424\n",
      "Adding trajectory to replay buffer: step 15572, counter 248841\n",
      "Environment 7: Episode 3302, Score -122.50390353771547, Avg_Score -120.43443279273723\n",
      "Adding trajectory to replay buffer: step 15577, counter 248885\n",
      "Environment 0: Episode 3303, Score -122.05250732417404, Avg_Score -120.44546597633763\n",
      "Adding trajectory to replay buffer: step 15583, counter 248927\n",
      "Environment 12: Episode 3304, Score -116.5641832472682, Avg_Score -120.42333652355434\n",
      "Adding trajectory to replay buffer: step 15586, counter 248971\n",
      "Environment 11: Episode 3305, Score -122.09505475072065, Avg_Score -120.41940128111024\n",
      "Adding trajectory to replay buffer: step 15590, counter 249041\n",
      "Environment 3: Episode 3306, Score -112.98961046338393, Avg_Score -120.34340747883553\n",
      "Adding trajectory to replay buffer: step 15591, counter 249084\n",
      "Environment 2: Episode 3307, Score -116.02583788393748, Avg_Score -120.32006866080512\n",
      "Adding trajectory to replay buffer: step 15593, counter 249129\n",
      "Environment 15: Episode 3308, Score -123.65651533632663, Avg_Score -120.4021936982292\n",
      "Adding trajectory to replay buffer: step 15597, counter 249174\n",
      "Environment 6: Episode 3309, Score -121.74348297424676, Avg_Score -120.46271812454712\n",
      "Adding trajectory to replay buffer: step 15597, counter 249217\n",
      "Environment 9: Episode 3310, Score -122.27459180915852, Avg_Score -120.51936784610466\n",
      "Adding trajectory to replay buffer: step 15601, counter 249262\n",
      "Environment 4: Episode 3311, Score -121.7756212254893, Avg_Score -120.5156548698373\n",
      "Adding trajectory to replay buffer: step 15605, counter 249308\n",
      "Environment 10: Episode 3312, Score -120.67658204994225, Avg_Score -120.50081548554346\n",
      "Adding trajectory to replay buffer: step 15606, counter 249353\n",
      "Environment 8: Episode 3313, Score -118.14905880661868, Avg_Score -120.4443438461667\n",
      "Adding trajectory to replay buffer: step 15606, counter 249397\n",
      "Environment 14: Episode 3314, Score -122.7013329645718, Avg_Score -120.46071492434397\n",
      "Adding trajectory to replay buffer: step 15610, counter 249455\n",
      "Environment 5: Episode 3315, Score -114.39065453189735, Avg_Score -120.38008780647358\n",
      "Adding trajectory to replay buffer: step 15614, counter 249499\n",
      "Environment 1: Episode 3316, Score -122.44279314374117, Avg_Score -120.45066953666166\n",
      "Adding trajectory to replay buffer: step 15631, counter 249544\n",
      "Environment 11: Episode 3317, Score -121.70508576226545, Avg_Score -120.44235896959303\n",
      "Adding trajectory to replay buffer: step 15634, counter 249588\n",
      "Environment 3: Episode 3318, Score -123.58090716812947, Avg_Score -120.46547640249855\n",
      "Adding trajectory to replay buffer: step 15636, counter 249633\n",
      "Environment 2: Episode 3319, Score -123.51376527613215, Avg_Score -120.46898710508623\n",
      "Adding trajectory to replay buffer: step 15638, counter 249694\n",
      "Environment 0: Episode 3320, Score -111.13841876350375, Avg_Score -120.3498399470002\n",
      "Adding trajectory to replay buffer: step 15639, counter 249736\n",
      "Environment 6: Episode 3321, Score -122.9847192638653, Avg_Score -120.35025845724223\n",
      "Adding trajectory to replay buffer: step 15639, counter 249778\n",
      "Environment 9: Episode 3322, Score -123.12181784540414, Avg_Score -120.35237314764159\n",
      "Adding trajectory to replay buffer: step 15641, counter 249848\n",
      "Environment 13: Episode 3323, Score -116.48733816832366, Avg_Score -120.28660295354477\n",
      "Adding trajectory to replay buffer: step 15643, counter 249890\n",
      "Environment 4: Episode 3324, Score -123.12827943574575, Avg_Score -120.2883741043838\n",
      "Adding trajectory to replay buffer: step 15647, counter 249932\n",
      "Environment 10: Episode 3325, Score -123.25336617307552, Avg_Score -120.29414624491147\n",
      "Adding trajectory to replay buffer: step 15648, counter 249974\n",
      "Environment 8: Episode 3326, Score -123.52399505122007, Avg_Score -120.29957905532548\n",
      "Adding trajectory to replay buffer: step 15648, counter 250016\n",
      "Environment 14: Episode 3327, Score -123.23614857108643, Avg_Score -120.29799826377683\n",
      "Adding trajectory to replay buffer: step 15652, counter 250058\n",
      "Environment 5: Episode 3328, Score -122.92412505971889, Avg_Score -120.29312237786347\n",
      "Adding trajectory to replay buffer: step 15655, counter 250120\n",
      "Environment 15: Episode 3329, Score -114.99744552539724, Avg_Score -120.21599436464\n",
      "Adding trajectory to replay buffer: step 15656, counter 250162\n",
      "Environment 1: Episode 3330, Score -122.4873691746976, Avg_Score -120.28200046540962\n",
      "Adding trajectory to replay buffer: step 15672, counter 250251\n",
      "Environment 12: Episode 3331, Score -123.16671486591889, Avg_Score -120.2850134180364\n",
      "Adding trajectory to replay buffer: step 15676, counter 250296\n",
      "Environment 11: Episode 3332, Score -122.76274786196028, Avg_Score -120.3015569825124\n",
      "Adding trajectory to replay buffer: step 15682, counter 250340\n",
      "Environment 0: Episode 3333, Score -123.3324322248809, Avg_Score -120.35172500267304\n",
      "Adding trajectory to replay buffer: step 15683, counter 250387\n",
      "Environment 2: Episode 3334, Score -121.0703748746502, Avg_Score -120.35629857613952\n",
      "Adding trajectory to replay buffer: step 15683, counter 250436\n",
      "Environment 3: Episode 3335, Score -117.04741045041398, Avg_Score -120.31239426060773\n",
      "Adding trajectory to replay buffer: step 15683, counter 250480\n",
      "Environment 6: Episode 3336, Score -121.7361315752125, Avg_Score -120.36097515800054\n",
      "Adding trajectory to replay buffer: step 15683, counter 250524\n",
      "Environment 9: Episode 3337, Score -120.45446692327099, Avg_Score -120.39709855841542\n",
      "Adding trajectory to replay buffer: step 15685, counter 250568\n",
      "Environment 13: Episode 3338, Score -122.14394749466764, Avg_Score -120.39778303186519\n",
      "Adding trajectory to replay buffer: step 15691, counter 250612\n",
      "Environment 10: Episode 3339, Score -122.22745983274157, Avg_Score -120.4331934604682\n",
      "Adding trajectory to replay buffer: step 15694, counter 250658\n",
      "Environment 14: Episode 3340, Score -122.62974386297788, Avg_Score -120.44628518018482\n",
      "Adding trajectory to replay buffer: step 15698, counter 250704\n",
      "Environment 5: Episode 3341, Score -121.0556298929428, Avg_Score -120.45885177578488\n",
      "Adding trajectory to replay buffer: step 15699, counter 250748\n",
      "Environment 15: Episode 3342, Score -123.00876082933073, Avg_Score -120.56672040505953\n",
      "Adding trajectory to replay buffer: step 15702, counter 250878\n",
      "Environment 7: Episode 3343, Score -126.0665470536109, Avg_Score -120.6019794568255\n",
      "Adding trajectory to replay buffer: step 15705, counter 250940\n",
      "Environment 4: Episode 3344, Score -113.10577470954073, Avg_Score -120.53235840239967\n",
      "Adding trajectory to replay buffer: step 15718, counter 250986\n",
      "Environment 12: Episode 3345, Score -120.944948620469, Avg_Score -120.58284919598664\n",
      "Adding trajectory to replay buffer: step 15721, counter 251051\n",
      "Environment 1: Episode 3346, Score -113.28524973552985, Avg_Score -120.52244160670449\n",
      "Adding trajectory to replay buffer: step 15726, counter 251095\n",
      "Environment 0: Episode 3347, Score -121.76610435824097, Avg_Score -120.60762918273922\n",
      "Adding trajectory to replay buffer: step 15726, counter 251145\n",
      "Environment 11: Episode 3348, Score -118.6333015053682, Avg_Score -120.61735249432331\n",
      "Adding trajectory to replay buffer: step 15727, counter 251189\n",
      "Environment 9: Episode 3349, Score -121.75364020108121, Avg_Score -120.60170978206716\n",
      "Adding trajectory to replay buffer: step 15728, counter 251234\n",
      "Environment 6: Episode 3350, Score -120.66531117672659, Avg_Score -120.6198663014839\n",
      "Adding trajectory to replay buffer: step 15729, counter 251280\n",
      "Environment 3: Episode 3351, Score -118.06424057278969, Avg_Score -120.58737347322958\n",
      "Adding trajectory to replay buffer: step 15730, counter 251327\n",
      "Environment 2: Episode 3352, Score -117.647941032568, Avg_Score -120.53708001807878\n",
      "Adding trajectory to replay buffer: step 15730, counter 251372\n",
      "Environment 13: Episode 3353, Score -120.84662464094596, Avg_Score -120.57853397627366\n",
      "Adding trajectory to replay buffer: step 15737, counter 251418\n",
      "Environment 10: Episode 3354, Score -122.00095639776578, Avg_Score -120.58411784839025\n",
      "Adding trajectory to replay buffer: step 15740, counter 251464\n",
      "Environment 14: Episode 3355, Score -121.61350624827105, Avg_Score -120.5620550043036\n",
      "Adding trajectory to replay buffer: step 15770, counter 251508\n",
      "Environment 0: Episode 3356, Score -122.3516594995999, Avg_Score -120.64239725723374\n",
      "Adding trajectory to replay buffer: step 15770, counter 251552\n",
      "Environment 11: Episode 3357, Score -122.33791250665672, Avg_Score -120.70225225187868\n",
      "Adding trajectory to replay buffer: step 15771, counter 251595\n",
      "Environment 6: Episode 3358, Score -122.62283341838918, Avg_Score -120.7138181873006\n",
      "Adding trajectory to replay buffer: step 15771, counter 251718\n",
      "Environment 8: Episode 3359, Score -126.51630837706912, Avg_Score -120.75051550536914\n",
      "Adding trajectory to replay buffer: step 15771, counter 251762\n",
      "Environment 9: Episode 3360, Score -122.33922002743743, Avg_Score -120.76030913756097\n",
      "Adding trajectory to replay buffer: step 15772, counter 251805\n",
      "Environment 3: Episode 3361, Score -122.65698554799582, Avg_Score -120.7595041491299\n",
      "Adding trajectory to replay buffer: step 15774, counter 251849\n",
      "Environment 2: Episode 3362, Score -123.01533774101486, Avg_Score -120.75930416583974\n",
      "Adding trajectory to replay buffer: step 15774, counter 251893\n",
      "Environment 13: Episode 3363, Score -123.0106163996396, Avg_Score -120.75812687357036\n",
      "Adding trajectory to replay buffer: step 15779, counter 251935\n",
      "Environment 10: Episode 3364, Score -122.76985718910956, Avg_Score -120.77121099923565\n",
      "Adding trajectory to replay buffer: step 15782, counter 252019\n",
      "Environment 5: Episode 3365, Score -115.29026866678498, Avg_Score -120.71001566655195\n",
      "Adding trajectory to replay buffer: step 15783, counter 252062\n",
      "Environment 14: Episode 3366, Score -123.08085050467464, Avg_Score -120.71215591951913\n",
      "Adding trajectory to replay buffer: step 15797, counter 252157\n",
      "Environment 7: Episode 3367, Score -120.66977045484569, Avg_Score -120.73344200842428\n",
      "Adding trajectory to replay buffer: step 15807, counter 252265\n",
      "Environment 15: Episode 3368, Score -124.09842387750481, Avg_Score -120.76761381878045\n",
      "Adding trajectory to replay buffer: step 15812, counter 252307\n",
      "Environment 11: Episode 3369, Score -115.75413848322195, Avg_Score -120.7293279268896\n",
      "Adding trajectory to replay buffer: step 15815, counter 252352\n",
      "Environment 0: Episode 3370, Score -121.55527177586966, Avg_Score -120.76013653732225\n",
      "Adding trajectory to replay buffer: step 15816, counter 252397\n",
      "Environment 6: Episode 3371, Score -120.1330655399443, Avg_Score -120.79866493837294\n",
      "Adding trajectory to replay buffer: step 15816, counter 252442\n",
      "Environment 8: Episode 3372, Score -121.57887943800664, Avg_Score -120.89715944407048\n",
      "Adding trajectory to replay buffer: step 15816, counter 252540\n",
      "Environment 12: Episode 3373, Score -113.0028974512797, Avg_Score -120.83185414139665\n",
      "Adding trajectory to replay buffer: step 15817, counter 252585\n",
      "Environment 3: Episode 3374, Score -122.8598139121371, Avg_Score -120.83801066530428\n",
      "Adding trajectory to replay buffer: step 15817, counter 252631\n",
      "Environment 9: Episode 3375, Score -119.16636980263547, Avg_Score -120.86421400546352\n",
      "Adding trajectory to replay buffer: step 15819, counter 252676\n",
      "Environment 2: Episode 3376, Score -117.77438173587123, Avg_Score -120.81058078111639\n",
      "Adding trajectory to replay buffer: step 15821, counter 252723\n",
      "Environment 13: Episode 3377, Score -121.93472414140311, Avg_Score -120.81292367991469\n",
      "Adding trajectory to replay buffer: step 15824, counter 252768\n",
      "Environment 10: Episode 3378, Score -123.37830156818094, Avg_Score -120.81541258557598\n",
      "Adding trajectory to replay buffer: step 15825, counter 252811\n",
      "Environment 5: Episode 3379, Score -121.95994433093381, Avg_Score -120.81782263141555\n",
      "Adding trajectory to replay buffer: step 15827, counter 252855\n",
      "Environment 14: Episode 3380, Score -121.90156809434916, Avg_Score -120.82657820612548\n",
      "Adding trajectory to replay buffer: step 15851, counter 252899\n",
      "Environment 15: Episode 3381, Score -122.45334813004608, Avg_Score -120.83598872506583\n",
      "Adding trajectory to replay buffer: step 15854, counter 252956\n",
      "Environment 7: Episode 3382, Score -115.43719066702698, Avg_Score -120.76209868200189\n",
      "Adding trajectory to replay buffer: step 15857, counter 253001\n",
      "Environment 11: Episode 3383, Score -117.0164025397274, Avg_Score -120.71114722905754\n",
      "Adding trajectory to replay buffer: step 15862, counter 253047\n",
      "Environment 6: Episode 3384, Score -120.21855865692409, Avg_Score -120.67938473648253\n",
      "Adding trajectory to replay buffer: step 15863, counter 253095\n",
      "Environment 0: Episode 3385, Score -122.8075695148222, Avg_Score -120.71797081075476\n",
      "Adding trajectory to replay buffer: step 15865, counter 253139\n",
      "Environment 13: Episode 3386, Score -122.76289393225996, Avg_Score -120.73938118911455\n",
      "Adding trajectory to replay buffer: step 15870, counter 253184\n",
      "Environment 5: Episode 3387, Score -121.39811594870284, Avg_Score -120.8019981927839\n",
      "Adding trajectory to replay buffer: step 15873, counter 253240\n",
      "Environment 9: Episode 3388, Score -115.79884895447022, Avg_Score -120.74121413864813\n",
      "Adding trajectory to replay buffer: step 15873, counter 253286\n",
      "Environment 14: Episode 3389, Score -116.74266600053198, Avg_Score -120.68033161704139\n",
      "Adding trajectory to replay buffer: step 15874, counter 253344\n",
      "Environment 12: Episode 3390, Score -111.8566290376652, Avg_Score -120.56850680077864\n",
      "Adding trajectory to replay buffer: step 15884, counter 253412\n",
      "Environment 8: Episode 3391, Score -116.57526719028752, Avg_Score -120.51522707169761\n",
      "Adding trajectory to replay buffer: step 15890, counter 253597\n",
      "Environment 4: Episode 3392, Score -117.65480594256846, Avg_Score -120.47500594225113\n",
      "Adding trajectory to replay buffer: step 15891, counter 253664\n",
      "Environment 10: Episode 3393, Score -116.75549594714182, Avg_Score -120.4416428177557\n",
      "Adding trajectory to replay buffer: step 15899, counter 253709\n",
      "Environment 7: Episode 3394, Score -120.15142209870064, Avg_Score -120.4165335505354\n",
      "Adding trajectory to replay buffer: step 15904, counter 253892\n",
      "Environment 1: Episode 3395, Score -124.10562812808156, Avg_Score -120.48171335899215\n",
      "Adding trajectory to replay buffer: step 15904, counter 253934\n",
      "Environment 6: Episode 3396, Score -117.37465404263202, Avg_Score -120.49590082841951\n",
      "Adding trajectory to replay buffer: step 15907, counter 253978\n",
      "Environment 0: Episode 3397, Score -122.24641972438184, Avg_Score -120.49239111965029\n",
      "Adding trajectory to replay buffer: step 15910, counter 254023\n",
      "Environment 13: Episode 3398, Score -122.24671000139291, Avg_Score -120.48980781322284\n",
      "Adding trajectory to replay buffer: step 15916, counter 254069\n",
      "Environment 5: Episode 3399, Score -121.42191407573968, Avg_Score -120.41866267484751\n",
      "Adding trajectory to replay buffer: step 15916, counter 254134\n",
      "Environment 15: Episode 3400, Score -117.65866339518254, Avg_Score -120.41644919017281\n",
      "Adding trajectory to replay buffer: step 15917, counter 254194\n",
      "Environment 11: Episode 3401, Score -115.46538049393955, Avg_Score -120.43062463136414\n",
      "Adding trajectory to replay buffer: step 15927, counter 254248\n",
      "Environment 14: Episode 3402, Score -115.23774012452674, Avg_Score -120.35796299723226\n",
      "Adding trajectory to replay buffer: step 15929, counter 254293\n",
      "Environment 8: Episode 3403, Score -121.40630519859306, Avg_Score -120.35150097597644\n",
      "Adding trajectory to replay buffer: step 15934, counter 254337\n",
      "Environment 4: Episode 3404, Score -121.90438639664899, Avg_Score -120.40490300747024\n",
      "Adding trajectory to replay buffer: step 15935, counter 254381\n",
      "Environment 10: Episode 3405, Score -122.11885832211003, Avg_Score -120.40514104318416\n",
      "Adding trajectory to replay buffer: step 15944, counter 254426\n",
      "Environment 7: Episode 3406, Score -122.96682663728919, Avg_Score -120.50491320492323\n",
      "Adding trajectory to replay buffer: step 15949, counter 254501\n",
      "Environment 12: Episode 3407, Score -117.23595070541215, Avg_Score -120.51701433313798\n",
      "Adding trajectory to replay buffer: step 15950, counter 254544\n",
      "Environment 0: Episode 3408, Score -123.81105652474426, Avg_Score -120.51855974502215\n",
      "Adding trajectory to replay buffer: step 15951, counter 254678\n",
      "Environment 3: Episode 3409, Score -116.48697983601068, Avg_Score -120.46599471363979\n",
      "Adding trajectory to replay buffer: step 15953, counter 254721\n",
      "Environment 13: Episode 3410, Score -120.40183736793387, Avg_Score -120.44726716922754\n",
      "Adding trajectory to replay buffer: step 15956, counter 254773\n",
      "Environment 1: Episode 3411, Score -117.1083449010458, Avg_Score -120.4005944059831\n",
      "Adding trajectory to replay buffer: step 15959, counter 254828\n",
      "Environment 6: Episode 3412, Score -113.80584925301994, Avg_Score -120.33188707801386\n",
      "Adding trajectory to replay buffer: step 15960, counter 254871\n",
      "Environment 11: Episode 3413, Score -122.3773689865917, Avg_Score -120.37417017981363\n",
      "Adding trajectory to replay buffer: step 15960, counter 254915\n",
      "Environment 15: Episode 3414, Score -122.41548884946044, Avg_Score -120.3713117386625\n",
      "Adding trajectory to replay buffer: step 15961, counter 254960\n",
      "Environment 5: Episode 3415, Score -121.95882960994108, Avg_Score -120.44699348944295\n",
      "Adding trajectory to replay buffer: step 15962, counter 255103\n",
      "Environment 2: Episode 3416, Score -117.58467273434499, Avg_Score -120.39841228534897\n",
      "Adding trajectory to replay buffer: step 15980, counter 255148\n",
      "Environment 10: Episode 3417, Score -121.67330983407794, Avg_Score -120.39809452606708\n",
      "Adding trajectory to replay buffer: step 15987, counter 255191\n",
      "Environment 7: Episode 3418, Score -122.09978287226458, Avg_Score -120.38328328310843\n",
      "Adding trajectory to replay buffer: step 15990, counter 255308\n",
      "Environment 9: Episode 3419, Score -113.5008681847689, Avg_Score -120.28315431219478\n",
      "Adding trajectory to replay buffer: step 15993, counter 255352\n",
      "Environment 12: Episode 3420, Score -117.91952773704504, Avg_Score -120.35096540193022\n",
      "Adding trajectory to replay buffer: step 15996, counter 255397\n",
      "Environment 3: Episode 3421, Score -116.9205190944622, Avg_Score -120.29032340023622\n",
      "Adding trajectory to replay buffer: step 16000, counter 255441\n",
      "Environment 1: Episode 3422, Score -121.89456114028954, Avg_Score -120.27805083318506\n",
      "Adding trajectory to replay buffer: step 16002, counter 255484\n",
      "Environment 6: Episode 3423, Score -123.51797568983709, Avg_Score -120.34835720840017\n",
      "Adding trajectory to replay buffer: step 16003, counter 255527\n",
      "Environment 11: Episode 3424, Score -122.70308581027948, Avg_Score -120.34410527214551\n",
      "Adding trajectory to replay buffer: step 16003, counter 255570\n",
      "Environment 15: Episode 3425, Score -122.7384888202753, Avg_Score -120.33895649861753\n",
      "Adding trajectory to replay buffer: step 16005, counter 255614\n",
      "Environment 5: Episode 3426, Score -122.32474142247004, Avg_Score -120.32696396233001\n",
      "Adding trajectory to replay buffer: step 16006, counter 255658\n",
      "Environment 2: Episode 3427, Score -122.18656940632562, Avg_Score -120.3164681706824\n",
      "Adding trajectory to replay buffer: step 16006, counter 255730\n",
      "Environment 4: Episode 3428, Score -116.58019587314423, Avg_Score -120.25302887881666\n",
      "Adding trajectory to replay buffer: step 16007, counter 255787\n",
      "Environment 0: Episode 3429, Score -115.87018682853257, Avg_Score -120.26175629184802\n",
      "Adding trajectory to replay buffer: step 16018, counter 255876\n",
      "Environment 8: Episode 3430, Score -125.27845027066395, Avg_Score -120.28966710280767\n",
      "Adding trajectory to replay buffer: step 16019, counter 255942\n",
      "Environment 13: Episode 3431, Score -117.8092588056506, Avg_Score -120.23609254220497\n",
      "Adding trajectory to replay buffer: step 16023, counter 255985\n",
      "Environment 10: Episode 3432, Score -123.41165828910346, Avg_Score -120.24258164647642\n",
      "Adding trajectory to replay buffer: step 16030, counter 256028\n",
      "Environment 7: Episode 3433, Score -122.93458209820527, Avg_Score -120.23860314520967\n",
      "Adding trajectory to replay buffer: step 16035, counter 256136\n",
      "Environment 14: Episode 3434, Score -124.8835376466581, Avg_Score -120.27673477292974\n",
      "Adding trajectory to replay buffer: step 16038, counter 256184\n",
      "Environment 9: Episode 3435, Score -120.928119052299, Avg_Score -120.31554185894862\n",
      "Adding trajectory to replay buffer: step 16044, counter 256232\n",
      "Environment 3: Episode 3436, Score -118.44307952094948, Avg_Score -120.28261133840597\n",
      "Adding trajectory to replay buffer: step 16045, counter 256277\n",
      "Environment 1: Episode 3437, Score -122.2536845493534, Avg_Score -120.3006035146668\n",
      "Adding trajectory to replay buffer: step 16049, counter 256319\n",
      "Environment 0: Episode 3438, Score -122.71292250856136, Avg_Score -120.30629326480573\n",
      "Adding trajectory to replay buffer: step 16053, counter 256369\n",
      "Environment 11: Episode 3439, Score -116.46524689568083, Avg_Score -120.24867113543513\n",
      "Adding trajectory to replay buffer: step 16061, counter 256412\n",
      "Environment 8: Episode 3440, Score -123.48516554678231, Avg_Score -120.2572253522732\n",
      "Adding trajectory to replay buffer: step 16063, counter 256456\n",
      "Environment 13: Episode 3441, Score -120.76504260179712, Avg_Score -120.25431947936175\n",
      "Adding trajectory to replay buffer: step 16065, counter 256519\n",
      "Environment 6: Episode 3442, Score -114.95766193824448, Avg_Score -120.17380849045087\n",
      "Adding trajectory to replay buffer: step 16066, counter 256562\n",
      "Environment 10: Episode 3443, Score -123.00325944852892, Avg_Score -120.14317561440006\n",
      "Adding trajectory to replay buffer: step 16070, counter 256626\n",
      "Environment 4: Episode 3444, Score -116.24426576087501, Avg_Score -120.17456052491339\n",
      "Adding trajectory to replay buffer: step 16072, counter 256693\n",
      "Environment 5: Episode 3445, Score -118.26502316504344, Avg_Score -120.14776127035915\n",
      "Adding trajectory to replay buffer: step 16073, counter 256736\n",
      "Environment 7: Episode 3446, Score -122.79339687319472, Avg_Score -120.24284274173579\n",
      "Adding trajectory to replay buffer: step 16080, counter 256781\n",
      "Environment 14: Episode 3447, Score -121.6918697161985, Avg_Score -120.24210039531536\n",
      "Adding trajectory to replay buffer: step 16083, counter 256826\n",
      "Environment 9: Episode 3448, Score -116.49298409919503, Avg_Score -120.22069722125363\n",
      "Adding trajectory to replay buffer: step 16084, counter 256866\n",
      "Environment 3: Episode 3449, Score -117.27569667139463, Avg_Score -120.17591778595678\n",
      "Adding trajectory to replay buffer: step 16092, counter 256909\n",
      "Environment 0: Episode 3450, Score -122.96150113372008, Avg_Score -120.19887968552673\n",
      "Adding trajectory to replay buffer: step 16095, counter 257011\n",
      "Environment 12: Episode 3451, Score -122.3337071312281, Avg_Score -120.24157435111113\n",
      "Adding trajectory to replay buffer: step 16103, counter 257061\n",
      "Environment 11: Episode 3452, Score -118.04583528423062, Avg_Score -120.24555329362775\n",
      "Adding trajectory to replay buffer: step 16116, counter 257105\n",
      "Environment 5: Episode 3453, Score -122.19115845844088, Avg_Score -120.2589986318027\n",
      "Adding trajectory to replay buffer: step 16120, counter 257152\n",
      "Environment 7: Episode 3454, Score -119.78791301343031, Avg_Score -120.23686819795934\n",
      "Adding trajectory to replay buffer: step 16123, counter 257210\n",
      "Environment 6: Episode 3455, Score -118.78240427769224, Avg_Score -120.20855717825354\n",
      "Adding trajectory to replay buffer: step 16123, counter 257253\n",
      "Environment 14: Episode 3456, Score -123.05424325193775, Avg_Score -120.21558301577691\n",
      "Adding trajectory to replay buffer: step 16126, counter 257295\n",
      "Environment 3: Episode 3457, Score -123.25442150349355, Avg_Score -120.22474810574529\n",
      "Adding trajectory to replay buffer: step 16126, counter 257338\n",
      "Environment 9: Episode 3458, Score -122.99677863748309, Avg_Score -120.22848755793625\n",
      "Adding trajectory to replay buffer: step 16135, counter 257470\n",
      "Environment 15: Episode 3459, Score -118.01952935038818, Avg_Score -120.14351976766942\n",
      "Adding trajectory to replay buffer: step 16137, counter 257515\n",
      "Environment 0: Episode 3460, Score -121.69834383923808, Avg_Score -120.1371110057874\n",
      "Adding trajectory to replay buffer: step 16137, counter 257586\n",
      "Environment 10: Episode 3461, Score -114.02402481573138, Avg_Score -120.05078139846476\n",
      "Adding trajectory to replay buffer: step 16140, counter 257631\n",
      "Environment 12: Episode 3462, Score -122.95191579836421, Avg_Score -120.05014717903829\n",
      "Adding trajectory to replay buffer: step 16149, counter 257677\n",
      "Environment 11: Episode 3463, Score -117.90278110694948, Avg_Score -119.99906882611138\n",
      "Adding trajectory to replay buffer: step 16151, counter 257758\n",
      "Environment 4: Episode 3464, Score -113.35910515037071, Avg_Score -119.90496130572399\n",
      "Adding trajectory to replay buffer: step 16158, counter 257800\n",
      "Environment 5: Episode 3465, Score -122.82585059061896, Avg_Score -119.98031712496233\n",
      "Adding trajectory to replay buffer: step 16163, counter 257843\n",
      "Environment 7: Episode 3466, Score -123.55120711906068, Avg_Score -119.98502069110619\n",
      "Adding trajectory to replay buffer: step 16165, counter 257885\n",
      "Environment 14: Episode 3467, Score -122.74093779604138, Avg_Score -120.00573236451811\n",
      "Adding trajectory to replay buffer: step 16166, counter 257928\n",
      "Environment 6: Episode 3468, Score -123.07954970222899, Avg_Score -119.99554362276534\n",
      "Adding trajectory to replay buffer: step 16169, counter 257971\n",
      "Environment 3: Episode 3469, Score -122.51377687161539, Avg_Score -120.06314000664928\n",
      "Adding trajectory to replay buffer: step 16171, counter 258016\n",
      "Environment 9: Episode 3470, Score -122.05720868291023, Avg_Score -120.06815937571967\n",
      "Adding trajectory to replay buffer: step 16179, counter 258132\n",
      "Environment 13: Episode 3471, Score -125.78864114823627, Avg_Score -120.1247151318026\n",
      "Adding trajectory to replay buffer: step 16183, counter 258254\n",
      "Environment 8: Episode 3472, Score -130.80077356006717, Avg_Score -120.2169340730232\n",
      "Adding trajectory to replay buffer: step 16185, counter 258299\n",
      "Environment 12: Episode 3473, Score -122.4348582263564, Avg_Score -120.31125368077397\n",
      "Adding trajectory to replay buffer: step 16193, counter 258343\n",
      "Environment 11: Episode 3474, Score -121.76863252939408, Avg_Score -120.30034186694652\n",
      "Adding trajectory to replay buffer: step 16196, counter 258388\n",
      "Environment 4: Episode 3475, Score -122.86745206419253, Avg_Score -120.33735268956211\n",
      "Adding trajectory to replay buffer: step 16202, counter 258453\n",
      "Environment 0: Episode 3476, Score -118.47753469057506, Avg_Score -120.34438421910914\n",
      "Adding trajectory to replay buffer: step 16202, counter 258497\n",
      "Environment 5: Episode 3477, Score -122.47406217858506, Avg_Score -120.34977759948097\n",
      "Adding trajectory to replay buffer: step 16207, counter 258541\n",
      "Environment 7: Episode 3478, Score -122.58409068523162, Avg_Score -120.34183549065148\n",
      "Adding trajectory to replay buffer: step 16210, counter 258616\n",
      "Environment 15: Episode 3479, Score -116.22729274723802, Avg_Score -120.28450897481453\n",
      "Adding trajectory to replay buffer: step 16211, counter 258821\n",
      "Environment 2: Episode 3480, Score -125.04725905727035, Avg_Score -120.31596588444374\n",
      "Adding trajectory to replay buffer: step 16211, counter 258867\n",
      "Environment 14: Episode 3481, Score -120.20441339230973, Avg_Score -120.29347653706638\n",
      "Adding trajectory to replay buffer: step 16212, counter 258913\n",
      "Environment 6: Episode 3482, Score -121.83638112076807, Avg_Score -120.35746844160377\n",
      "Adding trajectory to replay buffer: step 16214, counter 258956\n",
      "Environment 9: Episode 3483, Score -121.53908893823562, Avg_Score -120.40269530558884\n",
      "Adding trajectory to replay buffer: step 16222, counter 258999\n",
      "Environment 13: Episode 3484, Score -122.8039665615124, Avg_Score -120.42854938463474\n",
      "Adding trajectory to replay buffer: step 16227, counter 259057\n",
      "Environment 3: Episode 3485, Score -115.57784293058204, Avg_Score -120.35625211879234\n",
      "Adding trajectory to replay buffer: step 16227, counter 259147\n",
      "Environment 10: Episode 3486, Score -113.49584858899065, Avg_Score -120.26358166535967\n",
      "Adding trajectory to replay buffer: step 16228, counter 259192\n",
      "Environment 8: Episode 3487, Score -122.39487265141682, Avg_Score -120.2735492323868\n",
      "Adding trajectory to replay buffer: step 16229, counter 259236\n",
      "Environment 12: Episode 3488, Score -123.09079159122643, Avg_Score -120.34646865875436\n",
      "Adding trajectory to replay buffer: step 16236, counter 259279\n",
      "Environment 11: Episode 3489, Score -123.11598368803783, Avg_Score -120.41020183562941\n",
      "Adding trajectory to replay buffer: step 16242, counter 259325\n",
      "Environment 4: Episode 3490, Score -120.5265472697746, Avg_Score -120.49690101795049\n",
      "Adding trajectory to replay buffer: step 16246, counter 259369\n",
      "Environment 0: Episode 3491, Score -119.02113311034068, Avg_Score -120.52135967715101\n",
      "Adding trajectory to replay buffer: step 16246, counter 259413\n",
      "Environment 5: Episode 3492, Score -121.33917667715872, Avg_Score -120.55820338449692\n",
      "Adding trajectory to replay buffer: step 16251, counter 259619\n",
      "Environment 1: Episode 3493, Score -118.0505774908699, Avg_Score -120.57115419993421\n",
      "Adding trajectory to replay buffer: step 16251, counter 259663\n",
      "Environment 7: Episode 3494, Score -121.47308564579549, Avg_Score -120.58437083540515\n",
      "Adding trajectory to replay buffer: step 16254, counter 259706\n",
      "Environment 14: Episode 3495, Score -122.40523602564198, Avg_Score -120.56736691438077\n",
      "Adding trajectory to replay buffer: step 16256, counter 259751\n",
      "Environment 2: Episode 3496, Score -122.1745499359512, Avg_Score -120.61536587331395\n",
      "Adding trajectory to replay buffer: step 16257, counter 259796\n",
      "Environment 6: Episode 3497, Score -121.89556366123695, Avg_Score -120.61185731268252\n",
      "Adding trajectory to replay buffer: step 16265, counter 259851\n",
      "Environment 15: Episode 3498, Score -113.02359384761378, Avg_Score -120.51962615114473\n",
      "Adding trajectory to replay buffer: step 16273, counter 259896\n",
      "Environment 8: Episode 3499, Score -118.82073634656928, Avg_Score -120.49361437385303\n",
      "Adding trajectory to replay buffer: step 16273, counter 259942\n",
      "Environment 10: Episode 3500, Score -122.38814109896992, Avg_Score -120.54090915089091\n",
      "Adding trajectory to replay buffer: step 16279, counter 259985\n",
      "Environment 11: Episode 3501, Score -123.08037673595547, Avg_Score -120.61705911331109\n",
      "Adding trajectory to replay buffer: step 16281, counter 260044\n",
      "Environment 13: Episode 3502, Score -120.19937617414259, Avg_Score -120.66667547380725\n",
      "Adding trajectory to replay buffer: step 16288, counter 260090\n",
      "Environment 4: Episode 3503, Score -121.66380648055114, Avg_Score -120.66925048662684\n",
      "Adding trajectory to replay buffer: step 16299, counter 260133\n",
      "Environment 2: Episode 3504, Score -115.2709609198564, Avg_Score -120.60291623185891\n",
      "Adding trajectory to replay buffer: step 16299, counter 260178\n",
      "Environment 14: Episode 3505, Score -121.14695468799646, Avg_Score -120.59319719551779\n",
      "Adding trajectory to replay buffer: step 16304, counter 260225\n",
      "Environment 6: Episode 3506, Score -117.72046701372601, Avg_Score -120.54073359928213\n",
      "Adding trajectory to replay buffer: step 16307, counter 260281\n",
      "Environment 1: Episode 3507, Score -112.33270398419103, Avg_Score -120.49170113206992\n",
      "Adding trajectory to replay buffer: step 16308, counter 260360\n",
      "Environment 12: Episode 3508, Score -122.49269919819508, Avg_Score -120.47851755880443\n",
      "Adding trajectory to replay buffer: step 16309, counter 260404\n",
      "Environment 15: Episode 3509, Score -121.96058849938618, Avg_Score -120.5332536454382\n",
      "Adding trajectory to replay buffer: step 16310, counter 260463\n",
      "Environment 7: Episode 3510, Score -113.8729611270471, Avg_Score -120.46796488302931\n",
      "Adding trajectory to replay buffer: step 16313, counter 260530\n",
      "Environment 5: Episode 3511, Score -113.3015887255799, Avg_Score -120.42989732127464\n",
      "Adding trajectory to replay buffer: step 16321, counter 260578\n",
      "Environment 10: Episode 3512, Score -120.70015396692231, Avg_Score -120.49884036841368\n",
      "Adding trajectory to replay buffer: step 16329, counter 260628\n",
      "Environment 11: Episode 3513, Score -117.66646090179744, Avg_Score -120.45173128756572\n",
      "Adding trajectory to replay buffer: step 16329, counter 260676\n",
      "Environment 13: Episode 3514, Score -118.67104030676683, Avg_Score -120.41428680213876\n",
      "Adding trajectory to replay buffer: step 16344, counter 260721\n",
      "Environment 2: Episode 3515, Score -121.22363398766828, Avg_Score -120.40693484591606\n",
      "Adding trajectory to replay buffer: step 16345, counter 260767\n",
      "Environment 14: Episode 3516, Score -120.85690179308753, Avg_Score -120.43965713650348\n",
      "Adding trajectory to replay buffer: step 16352, counter 260809\n",
      "Environment 7: Episode 3517, Score -115.28808467233492, Avg_Score -120.37580488488605\n",
      "Adding trajectory to replay buffer: step 16353, counter 260855\n",
      "Environment 1: Episode 3518, Score -123.1765687672291, Avg_Score -120.38657274383566\n",
      "Adding trajectory to replay buffer: step 16353, counter 260900\n",
      "Environment 12: Episode 3519, Score -122.43881060027081, Avg_Score -120.47595216799068\n",
      "Adding trajectory to replay buffer: step 16354, counter 260945\n",
      "Environment 15: Episode 3520, Score -122.6043590182718, Avg_Score -120.52280048080296\n",
      "Adding trajectory to replay buffer: step 16355, counter 261012\n",
      "Environment 4: Episode 3521, Score -115.74717171001434, Avg_Score -120.51106700695851\n",
      "Adding trajectory to replay buffer: step 16365, counter 261104\n",
      "Environment 8: Episode 3522, Score -121.76029392993016, Avg_Score -120.50972433485492\n",
      "Adding trajectory to replay buffer: step 16367, counter 261150\n",
      "Environment 10: Episode 3523, Score -123.22173454370039, Avg_Score -120.50676192339357\n",
      "Adding trajectory to replay buffer: step 16376, counter 261280\n",
      "Environment 0: Episode 3524, Score -116.85610649266653, Avg_Score -120.44829213021747\n",
      "Adding trajectory to replay buffer: step 16376, counter 261352\n",
      "Environment 6: Episode 3525, Score -115.21346635484447, Avg_Score -120.37304190556311\n",
      "Adding trajectory to replay buffer: step 16387, counter 261410\n",
      "Environment 11: Episode 3526, Score -115.9492809985218, Avg_Score -120.30928730132365\n",
      "Adding trajectory to replay buffer: step 16390, counter 261456\n",
      "Environment 2: Episode 3527, Score -121.95357277360621, Avg_Score -120.30695733499644\n",
      "Adding trajectory to replay buffer: step 16394, counter 261521\n",
      "Environment 13: Episode 3528, Score -115.87857014400268, Avg_Score -120.299941077705\n",
      "Adding trajectory to replay buffer: step 16397, counter 261566\n",
      "Environment 7: Episode 3529, Score -122.30490117641413, Avg_Score -120.36428822118383\n",
      "Adding trajectory to replay buffer: step 16398, counter 261611\n",
      "Environment 1: Episode 3530, Score -120.37648282032895, Avg_Score -120.31526854668049\n",
      "Adding trajectory to replay buffer: step 16398, counter 261654\n",
      "Environment 4: Episode 3531, Score -122.83568847228152, Avg_Score -120.36553284334681\n",
      "Adding trajectory to replay buffer: step 16399, counter 261700\n",
      "Environment 12: Episode 3532, Score -121.10542993436258, Avg_Score -120.34247055979937\n",
      "Adding trajectory to replay buffer: step 16399, counter 261745\n",
      "Environment 15: Episode 3533, Score -120.62872382884659, Avg_Score -120.31941197710577\n",
      "Adding trajectory to replay buffer: step 16411, counter 261791\n",
      "Environment 8: Episode 3534, Score -122.17991053879138, Avg_Score -120.29237570602709\n",
      "Adding trajectory to replay buffer: step 16415, counter 261992\n",
      "Environment 9: Episode 3535, Score -124.1085592056637, Avg_Score -120.32418010756074\n",
      "Adding trajectory to replay buffer: step 16420, counter 262036\n",
      "Environment 0: Episode 3536, Score -122.35956992490652, Avg_Score -120.36334501160032\n",
      "Adding trajectory to replay buffer: step 16420, counter 262080\n",
      "Environment 6: Episode 3537, Score -119.73819318093545, Avg_Score -120.33819009791615\n",
      "Adding trajectory to replay buffer: step 16421, counter 262156\n",
      "Environment 14: Episode 3538, Score -119.8485015397432, Avg_Score -120.30954588822797\n",
      "Adding trajectory to replay buffer: step 16431, counter 262197\n",
      "Environment 2: Episode 3539, Score -117.08316945694077, Avg_Score -120.3157251138406\n",
      "Adding trajectory to replay buffer: step 16434, counter 262244\n",
      "Environment 11: Episode 3540, Score -121.80892872758582, Avg_Score -120.29896274564858\n",
      "Adding trajectory to replay buffer: step 16436, counter 262286\n",
      "Environment 13: Episode 3541, Score -123.54327890078537, Avg_Score -120.32674510863846\n",
      "Adding trajectory to replay buffer: step 16440, counter 262329\n",
      "Environment 7: Episode 3542, Score -122.98168175963437, Avg_Score -120.40698530685239\n",
      "Adding trajectory to replay buffer: step 16441, counter 262372\n",
      "Environment 1: Episode 3543, Score -123.04504706521891, Avg_Score -120.40740318301928\n",
      "Adding trajectory to replay buffer: step 16441, counter 262415\n",
      "Environment 4: Episode 3544, Score -121.05143955364389, Avg_Score -120.45547492094698\n",
      "Adding trajectory to replay buffer: step 16442, counter 262458\n",
      "Environment 12: Episode 3545, Score -122.23613749559534, Avg_Score -120.49518606425251\n",
      "Adding trajectory to replay buffer: step 16442, counter 262501\n",
      "Environment 15: Episode 3546, Score -122.01223802214923, Avg_Score -120.48737447574206\n",
      "Adding trajectory to replay buffer: step 16454, counter 262544\n",
      "Environment 8: Episode 3547, Score -123.27850566899917, Avg_Score -120.50324083527002\n",
      "Adding trajectory to replay buffer: step 16459, counter 262776\n",
      "Environment 3: Episode 3548, Score -122.4237436439873, Avg_Score -120.56254843071797\n",
      "Adding trajectory to replay buffer: step 16459, counter 262820\n",
      "Environment 9: Episode 3549, Score -122.18507162927837, Avg_Score -120.61164218029683\n",
      "Adding trajectory to replay buffer: step 16463, counter 262863\n",
      "Environment 0: Episode 3550, Score -122.41125456242885, Avg_Score -120.60613971458388\n",
      "Adding trajectory to replay buffer: step 16463, counter 262906\n",
      "Environment 6: Episode 3551, Score -122.30872533441647, Avg_Score -120.60588989661578\n",
      "Adding trajectory to replay buffer: step 16464, counter 262949\n",
      "Environment 14: Episode 3552, Score -123.03048571870228, Avg_Score -120.65573640096049\n",
      "Adding trajectory to replay buffer: step 16474, counter 262992\n",
      "Environment 2: Episode 3553, Score -122.11986094479573, Avg_Score -120.65502342582403\n",
      "Adding trajectory to replay buffer: step 16477, counter 263035\n",
      "Environment 11: Episode 3554, Score -121.15059215694914, Avg_Score -120.66865021725921\n",
      "Adding trajectory to replay buffer: step 16480, counter 263079\n",
      "Environment 13: Episode 3555, Score -122.2299559429828, Avg_Score -120.70312573391213\n",
      "Adding trajectory to replay buffer: step 16483, counter 263122\n",
      "Environment 7: Episode 3556, Score -115.82516927327775, Avg_Score -120.63083499412551\n",
      "Adding trajectory to replay buffer: step 16498, counter 263178\n",
      "Environment 15: Episode 3557, Score -117.85113374127758, Avg_Score -120.57680211650337\n",
      "Adding trajectory to replay buffer: step 16499, counter 263223\n",
      "Environment 8: Episode 3558, Score -120.80618175772639, Avg_Score -120.5548961477058\n",
      "Adding trajectory to replay buffer: step 16506, counter 263266\n",
      "Environment 6: Episode 3559, Score -114.96879821790382, Avg_Score -120.52438883638096\n",
      "Adding trajectory to replay buffer: step 16508, counter 263333\n",
      "Environment 4: Episode 3560, Score -116.67532506284304, Avg_Score -120.474158648617\n",
      "Adding trajectory to replay buffer: step 16509, counter 263475\n",
      "Environment 10: Episode 3561, Score -130.2986775838006, Avg_Score -120.6369051762977\n",
      "Adding trajectory to replay buffer: step 16521, counter 263519\n",
      "Environment 11: Episode 3562, Score -121.90671140920806, Avg_Score -120.62645313240613\n",
      "Adding trajectory to replay buffer: step 16524, counter 263563\n",
      "Environment 13: Episode 3563, Score -122.61771313758692, Avg_Score -120.67360245271249\n",
      "Adding trajectory to replay buffer: step 16527, counter 263777\n",
      "Environment 5: Episode 3564, Score -129.70656385895012, Avg_Score -120.8370770397983\n",
      "Adding trajectory to replay buffer: step 16527, counter 263821\n",
      "Environment 7: Episode 3565, Score -116.83618868752879, Avg_Score -120.7771804207674\n",
      "Adding trajectory to replay buffer: step 16531, counter 263893\n",
      "Environment 9: Episode 3566, Score -119.05950931454822, Avg_Score -120.73226344272227\n",
      "Adding trajectory to replay buffer: step 16539, counter 263973\n",
      "Environment 3: Episode 3567, Score -119.9657110691915, Avg_Score -120.70451117545376\n",
      "Adding trajectory to replay buffer: step 16541, counter 264015\n",
      "Environment 8: Episode 3568, Score -123.03586461243157, Avg_Score -120.70407432455576\n",
      "Adding trajectory to replay buffer: step 16541, counter 264058\n",
      "Environment 15: Episode 3569, Score -123.26073020886568, Avg_Score -120.71154385792826\n",
      "Adding trajectory to replay buffer: step 16542, counter 264126\n",
      "Environment 2: Episode 3570, Score -119.87956154771211, Avg_Score -120.68976738657629\n",
      "Adding trajectory to replay buffer: step 16548, counter 264211\n",
      "Environment 0: Episode 3571, Score -121.16076102964456, Avg_Score -120.64348858539037\n",
      "Adding trajectory to replay buffer: step 16550, counter 264255\n",
      "Environment 6: Episode 3572, Score -122.59211713392412, Avg_Score -120.56140202112894\n",
      "Adding trajectory to replay buffer: step 16553, counter 264300\n",
      "Environment 4: Episode 3573, Score -122.23839967863077, Avg_Score -120.55943743565169\n",
      "Adding trajectory to replay buffer: step 16566, counter 264345\n",
      "Environment 11: Episode 3574, Score -118.0057909464327, Avg_Score -120.52180901982209\n",
      "Adding trajectory to replay buffer: step 16568, counter 264472\n",
      "Environment 1: Episode 3575, Score -127.23123111331589, Avg_Score -120.56544681031329\n",
      "Adding trajectory to replay buffer: step 16571, counter 264519\n",
      "Environment 13: Episode 3576, Score -120.02413521356307, Avg_Score -120.58091281554317\n",
      "Adding trajectory to replay buffer: step 16573, counter 264565\n",
      "Environment 7: Episode 3577, Score -122.33271906122938, Avg_Score -120.57949938436964\n",
      "Adding trajectory to replay buffer: step 16574, counter 264608\n",
      "Environment 9: Episode 3578, Score -121.58074878792839, Avg_Score -120.56946596539662\n",
      "Adding trajectory to replay buffer: step 16584, counter 264650\n",
      "Environment 2: Episode 3579, Score -117.6859859723517, Avg_Score -120.58405289764774\n",
      "Adding trajectory to replay buffer: step 16595, counter 264695\n",
      "Environment 6: Episode 3580, Score -121.71351163690444, Avg_Score -120.55071542344407\n",
      "Adding trajectory to replay buffer: step 16595, counter 264749\n",
      "Environment 8: Episode 3581, Score -118.21910160637088, Avg_Score -120.53086230558469\n",
      "Adding trajectory to replay buffer: step 16597, counter 264793\n",
      "Environment 4: Episode 3582, Score -122.02741016880671, Avg_Score -120.53277259606506\n",
      "Adding trajectory to replay buffer: step 16604, counter 264849\n",
      "Environment 0: Episode 3583, Score -115.35596906564075, Avg_Score -120.47094139733912\n",
      "Adding trajectory to replay buffer: step 16605, counter 264990\n",
      "Environment 14: Episode 3584, Score -121.34079574814314, Avg_Score -120.45630968920543\n",
      "Adding trajectory to replay buffer: step 16606, counter 265057\n",
      "Environment 3: Episode 3585, Score -117.88534704855829, Avg_Score -120.47938473038518\n",
      "Adding trajectory to replay buffer: step 16613, counter 265104\n",
      "Environment 11: Episode 3586, Score -119.95809289667756, Avg_Score -120.54400717346203\n",
      "Adding trajectory to replay buffer: step 16615, counter 265148\n",
      "Environment 13: Episode 3587, Score -119.98805533979213, Avg_Score -120.51993900034579\n",
      "Adding trajectory to replay buffer: step 16617, counter 265224\n",
      "Environment 15: Episode 3588, Score -115.19930270132484, Avg_Score -120.44102411144678\n",
      "Adding trajectory to replay buffer: step 16618, counter 265268\n",
      "Environment 9: Episode 3589, Score -122.46045141285285, Avg_Score -120.43446878869491\n",
      "Adding trajectory to replay buffer: step 16628, counter 265312\n",
      "Environment 2: Episode 3590, Score -119.91753324575473, Avg_Score -120.42837864845471\n",
      "Adding trajectory to replay buffer: step 16639, counter 265356\n",
      "Environment 8: Episode 3591, Score -123.05897521619188, Avg_Score -120.46875706951325\n",
      "Adding trajectory to replay buffer: step 16640, counter 265399\n",
      "Environment 4: Episode 3592, Score -123.19939728577, Avg_Score -120.48735927559936\n",
      "Adding trajectory to replay buffer: step 16647, counter 265441\n",
      "Environment 14: Episode 3593, Score -123.25244458648935, Avg_Score -120.53937794655555\n",
      "Adding trajectory to replay buffer: step 16648, counter 265485\n",
      "Environment 0: Episode 3594, Score -122.35957723639844, Avg_Score -120.54824286246156\n",
      "Adding trajectory to replay buffer: step 16649, counter 265528\n",
      "Environment 3: Episode 3595, Score -122.53065675774155, Avg_Score -120.54949706978257\n",
      "Adding trajectory to replay buffer: step 16658, counter 265613\n",
      "Environment 7: Episode 3596, Score -113.61300780645385, Avg_Score -120.46388164848759\n",
      "Adding trajectory to replay buffer: step 16658, counter 265762\n",
      "Environment 10: Episode 3597, Score -116.23170949320806, Avg_Score -120.40724310680733\n",
      "Adding trajectory to replay buffer: step 16661, counter 265805\n",
      "Environment 9: Episode 3598, Score -123.00514444377211, Avg_Score -120.5070586127689\n",
      "Adding trajectory to replay buffer: step 16663, counter 265873\n",
      "Environment 6: Episode 3599, Score -115.27310621456118, Avg_Score -120.47158231144881\n",
      "Adding trajectory to replay buffer: step 16669, counter 266100\n",
      "Environment 12: Episode 3600, Score -124.13288076792533, Avg_Score -120.48902970813837\n",
      "Adding trajectory to replay buffer: step 16670, counter 266155\n",
      "Environment 13: Episode 3601, Score -115.77228690840987, Avg_Score -120.41594880986293\n",
      "Adding trajectory to replay buffer: step 16672, counter 266199\n",
      "Environment 2: Episode 3602, Score -122.83943125871197, Avg_Score -120.44234936070862\n",
      "Adding trajectory to replay buffer: step 16684, counter 266244\n",
      "Environment 8: Episode 3603, Score -121.18347354107972, Avg_Score -120.43754603131389\n",
      "Adding trajectory to replay buffer: step 16685, counter 266289\n",
      "Environment 4: Episode 3604, Score -122.42233015807594, Avg_Score -120.5090597236961\n",
      "Adding trajectory to replay buffer: step 16705, counter 266333\n",
      "Environment 9: Episode 3605, Score -121.71270156490431, Avg_Score -120.51471719246518\n",
      "Adding trajectory to replay buffer: step 16706, counter 266471\n",
      "Environment 1: Episode 3606, Score -114.22589432289016, Avg_Score -120.47977146555682\n",
      "Adding trajectory to replay buffer: step 16706, counter 266514\n",
      "Environment 6: Episode 3607, Score -116.53755142673913, Avg_Score -120.52181993998232\n",
      "Adding trajectory to replay buffer: step 16708, counter 266605\n",
      "Environment 15: Episode 3608, Score -123.92605563956934, Avg_Score -120.53615350439605\n",
      "Adding trajectory to replay buffer: step 16713, counter 266670\n",
      "Environment 0: Episode 3609, Score -117.77696468634468, Avg_Score -120.49431726626564\n",
      "Adding trajectory to replay buffer: step 16714, counter 266715\n",
      "Environment 12: Episode 3610, Score -121.61063820842591, Avg_Score -120.57169403707944\n",
      "Adding trajectory to replay buffer: step 16716, counter 266761\n",
      "Environment 13: Episode 3611, Score -120.14902844043822, Avg_Score -120.640168434228\n",
      "Adding trajectory to replay buffer: step 16720, counter 266823\n",
      "Environment 7: Episode 3612, Score -116.34047519731459, Avg_Score -120.59657164653191\n",
      "Adding trajectory to replay buffer: step 16722, counter 266887\n",
      "Environment 10: Episode 3613, Score -116.13052839184118, Avg_Score -120.58121232143236\n",
      "Adding trajectory to replay buffer: step 16725, counter 266965\n",
      "Environment 14: Episode 3614, Score -122.2339801504345, Avg_Score -120.61684171986904\n",
      "Adding trajectory to replay buffer: step 16728, counter 267009\n",
      "Environment 8: Episode 3615, Score -122.25792812035792, Avg_Score -120.62718466119594\n",
      "Adding trajectory to replay buffer: step 16730, counter 267054\n",
      "Environment 4: Episode 3616, Score -121.52762514949342, Avg_Score -120.63389189476001\n",
      "Adding trajectory to replay buffer: step 16748, counter 267097\n",
      "Environment 9: Episode 3617, Score -123.04414472595415, Avg_Score -120.71145249529621\n",
      "Adding trajectory to replay buffer: step 16749, counter 267140\n",
      "Environment 1: Episode 3618, Score -122.9408342498665, Avg_Score -120.7090951501226\n",
      "Adding trajectory to replay buffer: step 16749, counter 267183\n",
      "Environment 6: Episode 3619, Score -122.82200790340143, Avg_Score -120.71292712315389\n",
      "Adding trajectory to replay buffer: step 16752, counter 267227\n",
      "Environment 15: Episode 3620, Score -122.53835860201964, Avg_Score -120.71226711899139\n",
      "Adding trajectory to replay buffer: step 16756, counter 267270\n",
      "Environment 0: Episode 3621, Score -123.24178880890707, Avg_Score -120.78721328998031\n",
      "Adding trajectory to replay buffer: step 16756, counter 267354\n",
      "Environment 2: Episode 3622, Score -114.56360047577508, Avg_Score -120.71524635543878\n",
      "Adding trajectory to replay buffer: step 16757, counter 267397\n",
      "Environment 12: Episode 3623, Score -122.36852619102224, Avg_Score -120.70671427191196\n",
      "Adding trajectory to replay buffer: step 16759, counter 267440\n",
      "Environment 13: Episode 3624, Score -122.71844258246074, Avg_Score -120.76533763280993\n",
      "Adding trajectory to replay buffer: step 16762, counter 267553\n",
      "Environment 3: Episode 3625, Score -126.18885939454684, Avg_Score -120.87509156320694\n",
      "Adding trajectory to replay buffer: step 16765, counter 267598\n",
      "Environment 7: Episode 3626, Score -121.70027047689383, Avg_Score -120.93260145799064\n",
      "Adding trajectory to replay buffer: step 16786, counter 267857\n",
      "Environment 5: Episode 3627, Score -129.33694858643784, Avg_Score -121.00643521611899\n",
      "Adding trajectory to replay buffer: step 16791, counter 267923\n",
      "Environment 14: Episode 3628, Score -117.77633252004597, Avg_Score -121.02541283987942\n",
      "Adding trajectory to replay buffer: step 16793, counter 267964\n",
      "Environment 15: Episode 3629, Score -118.06539997483418, Avg_Score -120.98301782786365\n",
      "Adding trajectory to replay buffer: step 16794, counter 268009\n",
      "Environment 1: Episode 3630, Score -121.66004655868747, Avg_Score -120.9958534652472\n",
      "Adding trajectory to replay buffer: step 16795, counter 268055\n",
      "Environment 6: Episode 3631, Score -122.29036728749051, Avg_Score -120.99040025339929\n",
      "Adding trajectory to replay buffer: step 16795, counter 268102\n",
      "Environment 9: Episode 3632, Score -121.0505726492765, Avg_Score -120.98985168054841\n",
      "Adding trajectory to replay buffer: step 16800, counter 268146\n",
      "Environment 0: Episode 3633, Score -122.489956886122, Avg_Score -121.00846401112118\n",
      "Adding trajectory to replay buffer: step 16802, counter 268192\n",
      "Environment 2: Episode 3634, Score -123.95000915377338, Avg_Score -121.02616499727101\n",
      "Adding trajectory to replay buffer: step 16802, counter 268237\n",
      "Environment 12: Episode 3635, Score -123.08417533802117, Avg_Score -121.0159211585946\n",
      "Adding trajectory to replay buffer: step 16803, counter 268281\n",
      "Environment 13: Episode 3636, Score -123.07726668688096, Avg_Score -121.02309812621435\n",
      "Adding trajectory to replay buffer: step 16805, counter 268324\n",
      "Environment 3: Episode 3637, Score -122.35480193118813, Avg_Score -121.04926421371687\n",
      "Adding trajectory to replay buffer: step 16807, counter 268366\n",
      "Environment 7: Episode 3638, Score -122.76483420745905, Avg_Score -121.07842754039405\n",
      "Adding trajectory to replay buffer: step 16829, counter 268465\n",
      "Environment 4: Episode 3639, Score -119.6890606226058, Avg_Score -121.10448645205068\n",
      "Adding trajectory to replay buffer: step 16830, counter 268509\n",
      "Environment 5: Episode 3640, Score -118.98095894334962, Avg_Score -121.0762067542083\n",
      "Adding trajectory to replay buffer: step 16834, counter 268615\n",
      "Environment 8: Episode 3641, Score -124.58455995609921, Avg_Score -121.08661956476143\n",
      "Adding trajectory to replay buffer: step 16835, counter 268657\n",
      "Environment 15: Episode 3642, Score -115.21551587222517, Avg_Score -121.00895790588737\n",
      "Adding trajectory to replay buffer: step 16837, counter 268772\n",
      "Environment 10: Episode 3643, Score -125.52796780837878, Avg_Score -121.03378711331894\n",
      "Adding trajectory to replay buffer: step 16839, counter 268817\n",
      "Environment 1: Episode 3644, Score -117.55867449126082, Avg_Score -120.99885946269515\n",
      "Adding trajectory to replay buffer: step 16840, counter 268866\n",
      "Environment 14: Episode 3645, Score -123.2559014443606, Avg_Score -121.00905710218281\n",
      "Adding trajectory to replay buffer: step 16842, counter 268913\n",
      "Environment 9: Episode 3646, Score -122.97347438762709, Avg_Score -121.01866946583758\n",
      "Adding trajectory to replay buffer: step 16843, counter 269143\n",
      "Environment 11: Episode 3647, Score -125.60076214272217, Avg_Score -121.04189203057479\n",
      "Adding trajectory to replay buffer: step 16845, counter 269188\n",
      "Environment 0: Episode 3648, Score -121.1058649522302, Avg_Score -121.02871324365722\n",
      "Adding trajectory to replay buffer: step 16846, counter 269232\n",
      "Environment 2: Episode 3649, Score -122.28364297458717, Avg_Score -121.0296989571103\n",
      "Adding trajectory to replay buffer: step 16847, counter 269277\n",
      "Environment 12: Episode 3650, Score -122.27570255365471, Avg_Score -121.02834343702257\n",
      "Adding trajectory to replay buffer: step 16847, counter 269321\n",
      "Environment 13: Episode 3651, Score -122.26829707042252, Avg_Score -121.02793915438264\n",
      "Adding trajectory to replay buffer: step 16849, counter 269365\n",
      "Environment 3: Episode 3652, Score -122.32055525473878, Avg_Score -121.02083984974298\n",
      "Adding trajectory to replay buffer: step 16850, counter 269408\n",
      "Environment 7: Episode 3653, Score -119.94690605290117, Avg_Score -120.99911030082403\n",
      "Adding trajectory to replay buffer: step 16863, counter 269476\n",
      "Environment 6: Episode 3654, Score -117.13798494247037, Avg_Score -120.95898422867926\n",
      "Adding trajectory to replay buffer: step 16872, counter 269519\n",
      "Environment 4: Episode 3655, Score -122.48733297825233, Avg_Score -120.96155799903198\n",
      "Adding trajectory to replay buffer: step 16879, counter 269563\n",
      "Environment 15: Episode 3656, Score -122.84803463049357, Avg_Score -121.03178665260413\n",
      "Adding trajectory to replay buffer: step 16881, counter 269605\n",
      "Environment 1: Episode 3657, Score -123.3573366767466, Avg_Score -121.08684868195881\n",
      "Adding trajectory to replay buffer: step 16883, counter 269651\n",
      "Environment 10: Episode 3658, Score -117.06676920962582, Avg_Score -121.0494545564778\n",
      "Adding trajectory to replay buffer: step 16885, counter 269696\n",
      "Environment 14: Episode 3659, Score -121.89941651097685, Avg_Score -121.11876073940853\n",
      "Adding trajectory to replay buffer: step 16886, counter 269740\n",
      "Environment 9: Episode 3660, Score -116.92415281684697, Avg_Score -121.12124901694855\n",
      "Adding trajectory to replay buffer: step 16887, counter 269784\n",
      "Environment 11: Episode 3661, Score -122.35616341083124, Avg_Score -121.04182387521887\n",
      "Adding trajectory to replay buffer: step 16888, counter 269827\n",
      "Environment 0: Episode 3662, Score -123.46475968478497, Avg_Score -121.0574043579746\n",
      "Adding trajectory to replay buffer: step 16890, counter 269870\n",
      "Environment 12: Episode 3663, Score -122.64138337160213, Avg_Score -121.05764106031478\n",
      "Adding trajectory to replay buffer: step 16891, counter 269912\n",
      "Environment 3: Episode 3664, Score -122.57258868596952, Avg_Score -120.98630130858498\n",
      "Adding trajectory to replay buffer: step 16891, counter 269956\n",
      "Environment 13: Episode 3665, Score -120.70183304645121, Avg_Score -121.02495775217422\n",
      "Adding trajectory to replay buffer: step 16892, counter 270014\n",
      "Environment 8: Episode 3666, Score -116.19667825455716, Avg_Score -120.99632944157429\n",
      "Adding trajectory to replay buffer: step 16896, counter 270060\n",
      "Environment 7: Episode 3667, Score -120.6640590790355, Avg_Score -121.00331292167273\n",
      "Adding trajectory to replay buffer: step 16907, counter 270104\n",
      "Environment 6: Episode 3668, Score -122.74521158056086, Avg_Score -121.000406391354\n",
      "Adding trajectory to replay buffer: step 16915, counter 270147\n",
      "Environment 4: Episode 3669, Score -122.68479315073354, Avg_Score -120.99464702077267\n",
      "Adding trajectory to replay buffer: step 16924, counter 270190\n",
      "Environment 1: Episode 3670, Score -122.13323044592887, Avg_Score -121.01718370975486\n",
      "Adding trajectory to replay buffer: step 16926, counter 270233\n",
      "Environment 10: Episode 3671, Score -119.94785200512595, Avg_Score -121.00505461950966\n",
      "Adding trajectory to replay buffer: step 16930, counter 270277\n",
      "Environment 9: Episode 3672, Score -116.9275971033002, Avg_Score -120.94840941920341\n",
      "Adding trajectory to replay buffer: step 16930, counter 270320\n",
      "Environment 11: Episode 3673, Score -121.75426433134514, Avg_Score -120.94356806573056\n",
      "Adding trajectory to replay buffer: step 16930, counter 270365\n",
      "Environment 14: Episode 3674, Score -122.36182619582054, Avg_Score -120.98712841822442\n",
      "Adding trajectory to replay buffer: step 16935, counter 270410\n",
      "Environment 12: Episode 3675, Score -121.88907622191248, Avg_Score -120.93370686931037\n",
      "Adding trajectory to replay buffer: step 16939, counter 270453\n",
      "Environment 7: Episode 3676, Score -120.6725408393362, Avg_Score -120.94019092556813\n",
      "Adding trajectory to replay buffer: step 16940, counter 270501\n",
      "Environment 8: Episode 3677, Score -121.55352481044704, Avg_Score -120.93239898306028\n",
      "Adding trajectory to replay buffer: step 16941, counter 270551\n",
      "Environment 3: Episode 3678, Score -111.30777324960879, Avg_Score -120.8296692276771\n",
      "Adding trajectory to replay buffer: step 16954, counter 270598\n",
      "Environment 6: Episode 3679, Score -121.86205020926552, Avg_Score -120.87142987004623\n",
      "Adding trajectory to replay buffer: step 16957, counter 270676\n",
      "Environment 15: Episode 3680, Score -113.29891108476005, Avg_Score -120.7872838645248\n",
      "Adding trajectory to replay buffer: step 16960, counter 270721\n",
      "Environment 4: Episode 3681, Score -121.87916473128313, Avg_Score -120.8238844957739\n",
      "Adding trajectory to replay buffer: step 16961, counter 270791\n",
      "Environment 13: Episode 3682, Score -115.09461725640048, Avg_Score -120.75455656664984\n",
      "Adding trajectory to replay buffer: step 16976, counter 270837\n",
      "Environment 9: Episode 3683, Score -121.82107969753444, Avg_Score -120.81920767296879\n",
      "Adding trajectory to replay buffer: step 16976, counter 270883\n",
      "Environment 11: Episode 3684, Score -119.16485060625027, Avg_Score -120.79744822154986\n",
      "Adding trajectory to replay buffer: step 16977, counter 270930\n",
      "Environment 14: Episode 3685, Score -120.83915955568291, Avg_Score -120.82698634662108\n",
      "Adding trajectory to replay buffer: step 16983, counter 270978\n",
      "Environment 12: Episode 3686, Score -116.38394729135807, Avg_Score -120.79124489056788\n",
      "Adding trajectory to replay buffer: step 16988, counter 271025\n",
      "Environment 3: Episode 3687, Score -122.24765819754514, Avg_Score -120.8138409191454\n",
      "Adding trajectory to replay buffer: step 16990, counter 271075\n",
      "Environment 8: Episode 3688, Score -121.59578425524508, Avg_Score -120.87780573468463\n",
      "Adding trajectory to replay buffer: step 16991, counter 271178\n",
      "Environment 0: Episode 3689, Score -124.78141289287682, Avg_Score -120.90101534948484\n",
      "Adding trajectory to replay buffer: step 16997, counter 271221\n",
      "Environment 6: Episode 3690, Score -117.8749848245399, Avg_Score -120.88058986527271\n",
      "Adding trajectory to replay buffer: step 17000, counter 271297\n",
      "Environment 1: Episode 3691, Score -115.21920355982891, Avg_Score -120.80219214870907\n",
      "Adding trajectory to replay buffer: step 17001, counter 271341\n",
      "Environment 15: Episode 3692, Score -123.03939842980107, Avg_Score -120.80059216014939\n",
      "Adding trajectory to replay buffer: step 17003, counter 271384\n",
      "Environment 4: Episode 3693, Score -123.33521824541191, Avg_Score -120.80141989673862\n",
      "Adding trajectory to replay buffer: step 17004, counter 271427\n",
      "Environment 13: Episode 3694, Score -123.26405582092329, Avg_Score -120.81046468258388\n",
      "Adding trajectory to replay buffer: step 17020, counter 271471\n",
      "Environment 9: Episode 3695, Score -120.9426296077054, Avg_Score -120.79458441108352\n",
      "Adding trajectory to replay buffer: step 17024, counter 271556\n",
      "Environment 7: Episode 3696, Score -121.98812385702381, Avg_Score -120.87833557158923\n",
      "Adding trajectory to replay buffer: step 17026, counter 271599\n",
      "Environment 12: Episode 3697, Score -117.15662190948241, Avg_Score -120.88758469575197\n",
      "Adding trajectory to replay buffer: step 17036, counter 271709\n",
      "Environment 10: Episode 3698, Score -124.99173861699987, Avg_Score -120.90745063748426\n",
      "Adding trajectory to replay buffer: step 17039, counter 271902\n",
      "Environment 2: Episode 3699, Score -120.08369303491568, Avg_Score -120.9555565056878\n",
      "Adding trajectory to replay buffer: step 17039, counter 272111\n",
      "Environment 5: Episode 3700, Score -123.89107378180572, Avg_Score -120.9531384358266\n",
      "Adding trajectory to replay buffer: step 17040, counter 272154\n",
      "Environment 6: Episode 3701, Score -122.82790567973692, Avg_Score -121.02369462353988\n",
      "Adding trajectory to replay buffer: step 17044, counter 272197\n",
      "Environment 15: Episode 3702, Score -122.60726176504728, Avg_Score -121.02137292860326\n",
      "Adding trajectory to replay buffer: step 17048, counter 272242\n",
      "Environment 4: Episode 3703, Score -122.25920864651755, Avg_Score -121.03213027965764\n",
      "Adding trajectory to replay buffer: step 17048, counter 272314\n",
      "Environment 11: Episode 3704, Score -119.75650715751635, Avg_Score -121.00547204965206\n",
      "Adding trajectory to replay buffer: step 17049, counter 272359\n",
      "Environment 13: Episode 3705, Score -121.4759217971818, Avg_Score -121.00310425197482\n",
      "Adding trajectory to replay buffer: step 17064, counter 272423\n",
      "Environment 1: Episode 3706, Score -115.14583658646544, Avg_Score -121.01230367461059\n",
      "Adding trajectory to replay buffer: step 17067, counter 272466\n",
      "Environment 7: Episode 3707, Score -122.52143792181649, Avg_Score -121.07214253956136\n",
      "Adding trajectory to replay buffer: step 17069, counter 272515\n",
      "Environment 9: Episode 3708, Score -120.27656630800168, Avg_Score -121.03564764624569\n",
      "Adding trajectory to replay buffer: step 17069, counter 272558\n",
      "Environment 12: Episode 3709, Score -123.86723747254845, Avg_Score -121.09655037410772\n",
      "Adding trajectory to replay buffer: step 17072, counter 272640\n",
      "Environment 8: Episode 3710, Score -112.10380434086112, Avg_Score -121.00148203543203\n",
      "Adding trajectory to replay buffer: step 17084, counter 272685\n",
      "Environment 2: Episode 3711, Score -122.37530675070298, Avg_Score -121.02374481853471\n",
      "Adding trajectory to replay buffer: step 17084, counter 272730\n",
      "Environment 5: Episode 3712, Score -122.53613928813797, Avg_Score -121.08570145944296\n",
      "Adding trajectory to replay buffer: step 17085, counter 272775\n",
      "Environment 6: Episode 3713, Score -121.42222577217098, Avg_Score -121.13861843324625\n",
      "Adding trajectory to replay buffer: step 17085, counter 272824\n",
      "Environment 10: Episode 3714, Score -118.37175317759625, Avg_Score -121.09999616351786\n",
      "Adding trajectory to replay buffer: step 17091, counter 272938\n",
      "Environment 14: Episode 3715, Score -126.91338957122495, Avg_Score -121.14655077802654\n",
      "Adding trajectory to replay buffer: step 17091, counter 272985\n",
      "Environment 15: Episode 3716, Score -118.28460405832219, Avg_Score -121.11412056711482\n",
      "Adding trajectory to replay buffer: step 17094, counter 273030\n",
      "Environment 13: Episode 3717, Score -122.21967606958934, Avg_Score -121.10587588055118\n",
      "Adding trajectory to replay buffer: step 17104, counter 273086\n",
      "Environment 11: Episode 3718, Score -115.03192277757327, Avg_Score -121.02678676582826\n",
      "Adding trajectory to replay buffer: step 17108, counter 273130\n",
      "Environment 1: Episode 3719, Score -121.9041825641344, Avg_Score -121.01760851243556\n",
      "Adding trajectory to replay buffer: step 17110, counter 273173\n",
      "Environment 7: Episode 3720, Score -122.37894688027414, Avg_Score -121.01601439521812\n",
      "Adding trajectory to replay buffer: step 17112, counter 273216\n",
      "Environment 9: Episode 3721, Score -123.20787623434126, Avg_Score -121.01567526947245\n",
      "Adding trajectory to replay buffer: step 17113, counter 273260\n",
      "Environment 12: Episode 3722, Score -121.66822208768687, Avg_Score -121.08672148559155\n",
      "Adding trajectory to replay buffer: step 17116, counter 273328\n",
      "Environment 4: Episode 3723, Score -116.07302899883192, Avg_Score -121.02376651366966\n",
      "Adding trajectory to replay buffer: step 17117, counter 273373\n",
      "Environment 8: Episode 3724, Score -121.739861535009, Avg_Score -121.01398070319514\n",
      "Adding trajectory to replay buffer: step 17129, counter 273418\n",
      "Environment 2: Episode 3725, Score -118.39982321457751, Avg_Score -120.93609034139543\n",
      "Adding trajectory to replay buffer: step 17129, counter 273462\n",
      "Environment 6: Episode 3726, Score -123.04167783781328, Avg_Score -120.9495044150046\n",
      "Adding trajectory to replay buffer: step 17129, counter 273506\n",
      "Environment 10: Episode 3727, Score -122.90673773098612, Avg_Score -120.8852023064501\n",
      "Adding trajectory to replay buffer: step 17130, counter 273552\n",
      "Environment 5: Episode 3728, Score -119.94787472517727, Avg_Score -120.90691772850141\n",
      "Adding trajectory to replay buffer: step 17134, counter 273595\n",
      "Environment 15: Episode 3729, Score -122.18786845226276, Avg_Score -120.94814241327569\n",
      "Adding trajectory to replay buffer: step 17135, counter 273639\n",
      "Environment 14: Episode 3730, Score -122.01932391268573, Avg_Score -120.9517351868157\n",
      "Adding trajectory to replay buffer: step 17137, counter 273788\n",
      "Environment 3: Episode 3731, Score -130.5335852359192, Avg_Score -121.03416736629995\n",
      "Adding trajectory to replay buffer: step 17139, counter 273833\n",
      "Environment 13: Episode 3732, Score -123.23840463971347, Avg_Score -121.0560456862043\n",
      "Adding trajectory to replay buffer: step 17148, counter 273877\n",
      "Environment 11: Episode 3733, Score -122.39863291453446, Avg_Score -121.05513244648846\n",
      "Adding trajectory to replay buffer: step 17152, counter 273921\n",
      "Environment 1: Episode 3734, Score -122.40172250202609, Avg_Score -121.03964957997097\n",
      "Adding trajectory to replay buffer: step 17154, counter 273965\n",
      "Environment 7: Episode 3735, Score -122.975375606232, Avg_Score -121.03856158265307\n",
      "Adding trajectory to replay buffer: step 17156, counter 274009\n",
      "Environment 9: Episode 3736, Score -120.95416442659436, Avg_Score -121.01733056005021\n",
      "Adding trajectory to replay buffer: step 17159, counter 274055\n",
      "Environment 12: Episode 3737, Score -120.5361112087338, Avg_Score -120.99914365282564\n",
      "Adding trajectory to replay buffer: step 17161, counter 274100\n",
      "Environment 4: Episode 3738, Score -122.73528599565601, Avg_Score -120.9988481707076\n",
      "Adding trajectory to replay buffer: step 17161, counter 274144\n",
      "Environment 8: Episode 3739, Score -120.28501242868292, Avg_Score -121.00480768876838\n",
      "Adding trajectory to replay buffer: step 17162, counter 274315\n",
      "Environment 0: Episode 3740, Score -117.93517030695267, Avg_Score -120.99434980240443\n",
      "Adding trajectory to replay buffer: step 17173, counter 274358\n",
      "Environment 5: Episode 3741, Score -123.04246537511858, Avg_Score -120.97892885659464\n",
      "Adding trajectory to replay buffer: step 17174, counter 274403\n",
      "Environment 2: Episode 3742, Score -121.58635852204387, Avg_Score -121.0426372830928\n",
      "Adding trajectory to replay buffer: step 17174, counter 274448\n",
      "Environment 6: Episode 3743, Score -121.40310048265692, Avg_Score -121.0013886098356\n",
      "Adding trajectory to replay buffer: step 17174, counter 274493\n",
      "Environment 10: Episode 3744, Score -121.36083958023693, Avg_Score -121.03941026072536\n",
      "Adding trajectory to replay buffer: step 17177, counter 274536\n",
      "Environment 15: Episode 3745, Score -123.05811756276091, Avg_Score -121.03743242190939\n",
      "Adding trajectory to replay buffer: step 17178, counter 274579\n",
      "Environment 14: Episode 3746, Score -123.31028897067284, Avg_Score -121.04080056773982\n",
      "Adding trajectory to replay buffer: step 17180, counter 274622\n",
      "Environment 3: Episode 3747, Score -123.16776697516504, Avg_Score -121.01647061606425\n",
      "Adding trajectory to replay buffer: step 17181, counter 274664\n",
      "Environment 13: Episode 3748, Score -123.32924502505549, Avg_Score -121.0387044167925\n",
      "Adding trajectory to replay buffer: step 17193, counter 274709\n",
      "Environment 11: Episode 3749, Score -121.77355202672942, Avg_Score -121.03360350731391\n",
      "Adding trajectory to replay buffer: step 17195, counter 274752\n",
      "Environment 1: Episode 3750, Score -123.16988312667547, Avg_Score -121.0425453130441\n",
      "Adding trajectory to replay buffer: step 17196, counter 274794\n",
      "Environment 7: Episode 3751, Score -122.66295280298962, Avg_Score -121.04649187036979\n",
      "Adding trajectory to replay buffer: step 17196, counter 274834\n",
      "Environment 9: Episode 3752, Score -117.08267038443995, Avg_Score -120.9941130216668\n",
      "Adding trajectory to replay buffer: step 17201, counter 274876\n",
      "Environment 12: Episode 3753, Score -123.30447791855596, Avg_Score -121.02768874032336\n",
      "Adding trajectory to replay buffer: step 17204, counter 274918\n",
      "Environment 0: Episode 3754, Score -122.68387466260232, Avg_Score -121.08314763752468\n",
      "Adding trajectory to replay buffer: step 17204, counter 274961\n",
      "Environment 4: Episode 3755, Score -122.73244686304902, Avg_Score -121.08559877637265\n",
      "Adding trajectory to replay buffer: step 17204, counter 275004\n",
      "Environment 8: Episode 3756, Score -122.6555029437753, Avg_Score -121.08367345950548\n",
      "Adding trajectory to replay buffer: step 17217, counter 275047\n",
      "Environment 2: Episode 3757, Score -123.26209440323153, Avg_Score -121.08272103677032\n",
      "Adding trajectory to replay buffer: step 17217, counter 275090\n",
      "Environment 6: Episode 3758, Score -123.2519113586632, Avg_Score -121.14457245826074\n",
      "Adding trajectory to replay buffer: step 17217, counter 275133\n",
      "Environment 10: Episode 3759, Score -123.29266837907583, Avg_Score -121.15850497694171\n",
      "Adding trajectory to replay buffer: step 17218, counter 275178\n",
      "Environment 5: Episode 3760, Score -122.28059880314255, Avg_Score -121.21206943680467\n",
      "Adding trajectory to replay buffer: step 17220, counter 275221\n",
      "Environment 15: Episode 3761, Score -123.04457053298876, Avg_Score -121.21895350802625\n",
      "Adding trajectory to replay buffer: step 17221, counter 275264\n",
      "Environment 14: Episode 3762, Score -123.22635719915914, Avg_Score -121.21656948316999\n",
      "Adding trajectory to replay buffer: step 17225, counter 275309\n",
      "Environment 3: Episode 3763, Score -119.71817445463253, Avg_Score -121.18733739400028\n",
      "Adding trajectory to replay buffer: step 17228, counter 275356\n",
      "Environment 13: Episode 3764, Score -116.73813271117459, Avg_Score -121.12899283425233\n",
      "Adding trajectory to replay buffer: step 17242, counter 275402\n",
      "Environment 7: Episode 3765, Score -120.01042173942302, Avg_Score -121.12207872118206\n",
      "Adding trajectory to replay buffer: step 17247, counter 275448\n",
      "Environment 12: Episode 3766, Score -121.71024166680357, Avg_Score -121.17721435530453\n",
      "Adding trajectory to replay buffer: step 17248, counter 275492\n",
      "Environment 4: Episode 3767, Score -120.37244038554219, Avg_Score -121.1742981683696\n",
      "Adding trajectory to replay buffer: step 17248, counter 275536\n",
      "Environment 8: Episode 3768, Score -122.19225746151122, Avg_Score -121.1687686271791\n",
      "Adding trajectory to replay buffer: step 17255, counter 275598\n",
      "Environment 11: Episode 3769, Score -115.30172068941097, Avg_Score -121.09493790256589\n",
      "Adding trajectory to replay buffer: step 17260, counter 275641\n",
      "Environment 2: Episode 3770, Score -121.5821816129728, Avg_Score -121.08942741423633\n",
      "Adding trajectory to replay buffer: step 17261, counter 275685\n",
      "Environment 10: Episode 3771, Score -123.05879293097183, Avg_Score -121.12053682349479\n",
      "Adding trajectory to replay buffer: step 17263, counter 275730\n",
      "Environment 5: Episode 3772, Score -121.63094327531134, Avg_Score -121.1675702852149\n",
      "Adding trajectory to replay buffer: step 17264, counter 275773\n",
      "Environment 14: Episode 3773, Score -122.27194325133661, Avg_Score -121.17274707441481\n",
      "Adding trajectory to replay buffer: step 17265, counter 275842\n",
      "Environment 9: Episode 3774, Score -117.61496337956562, Avg_Score -121.12527844625225\n",
      "Adding trajectory to replay buffer: step 17267, counter 275889\n",
      "Environment 15: Episode 3775, Score -120.54394684568979, Avg_Score -121.11182715249004\n",
      "Adding trajectory to replay buffer: step 17275, counter 275936\n",
      "Environment 13: Episode 3776, Score -123.18389662374945, Avg_Score -121.13694071033416\n",
      "Adding trajectory to replay buffer: step 17286, counter 276018\n",
      "Environment 0: Episode 3777, Score -113.31039184596824, Avg_Score -121.05450938068938\n",
      "Adding trajectory to replay buffer: step 17286, counter 276109\n",
      "Environment 1: Episode 3778, Score -125.88686025718287, Avg_Score -121.20030025076511\n",
      "Adding trajectory to replay buffer: step 17286, counter 276153\n",
      "Environment 7: Episode 3779, Score -123.08718835609841, Avg_Score -121.21255163223343\n",
      "Adding trajectory to replay buffer: step 17289, counter 276225\n",
      "Environment 6: Episode 3780, Score -114.58011405715719, Avg_Score -121.22536366195737\n",
      "Adding trajectory to replay buffer: step 17290, counter 276268\n",
      "Environment 12: Episode 3781, Score -121.68042984452038, Avg_Score -121.22337631308976\n",
      "Adding trajectory to replay buffer: step 17298, counter 276311\n",
      "Environment 11: Episode 3782, Score -122.36445005555453, Avg_Score -121.2960746410813\n",
      "Adding trajectory to replay buffer: step 17299, counter 276385\n",
      "Environment 3: Episode 3783, Score -114.37466986975198, Avg_Score -121.2216105428035\n",
      "Adding trajectory to replay buffer: step 17306, counter 276430\n",
      "Environment 10: Episode 3784, Score -122.51516868165757, Avg_Score -121.25511372355757\n",
      "Adding trajectory to replay buffer: step 17308, counter 276475\n",
      "Environment 5: Episode 3785, Score -123.29264820187478, Avg_Score -121.27964861001948\n",
      "Adding trajectory to replay buffer: step 17308, counter 276518\n",
      "Environment 9: Episode 3786, Score -122.12154180563739, Avg_Score -121.33702455516227\n",
      "Adding trajectory to replay buffer: step 17308, counter 276562\n",
      "Environment 14: Episode 3787, Score -123.00109997864409, Avg_Score -121.34455897297329\n",
      "Adding trajectory to replay buffer: step 17310, counter 276605\n",
      "Environment 15: Episode 3788, Score -122.70589170276313, Avg_Score -121.35566004744845\n",
      "Adding trajectory to replay buffer: step 17320, counter 276650\n",
      "Environment 13: Episode 3789, Score -122.20369204804673, Avg_Score -121.32988283900015\n",
      "Adding trajectory to replay buffer: step 17329, counter 276693\n",
      "Environment 0: Episode 3790, Score -117.48797461164992, Avg_Score -121.32601273687123\n",
      "Adding trajectory to replay buffer: step 17329, counter 276736\n",
      "Environment 7: Episode 3791, Score -117.57386170444265, Avg_Score -121.34955931831738\n",
      "Adding trajectory to replay buffer: step 17331, counter 276781\n",
      "Environment 1: Episode 3792, Score -121.63875006958408, Avg_Score -121.33555283471519\n",
      "Adding trajectory to replay buffer: step 17332, counter 276823\n",
      "Environment 12: Episode 3793, Score -122.69153310287186, Avg_Score -121.32911598328978\n",
      "Adding trajectory to replay buffer: step 17333, counter 276867\n",
      "Environment 6: Episode 3794, Score -122.1118948984407, Avg_Score -121.31759437406495\n",
      "Adding trajectory to replay buffer: step 17336, counter 276955\n",
      "Environment 8: Episode 3795, Score -117.04090479362756, Avg_Score -121.2785771259242\n",
      "Adding trajectory to replay buffer: step 17344, counter 277001\n",
      "Environment 11: Episode 3796, Score -118.53533313985852, Avg_Score -121.24404921875254\n",
      "Adding trajectory to replay buffer: step 17345, counter 277086\n",
      "Environment 2: Episode 3797, Score -112.80488452574394, Avg_Score -121.20053184491513\n",
      "Adding trajectory to replay buffer: step 17350, counter 277128\n",
      "Environment 9: Episode 3798, Score -122.82759665517261, Avg_Score -121.17889042529686\n",
      "Adding trajectory to replay buffer: step 17350, counter 277172\n",
      "Environment 10: Episode 3799, Score -122.85913402364156, Avg_Score -121.20664483518411\n",
      "Adding trajectory to replay buffer: step 17351, counter 277215\n",
      "Environment 5: Episode 3800, Score -123.5543927371204, Avg_Score -121.20327802473724\n",
      "Adding trajectory to replay buffer: step 17351, counter 277258\n",
      "Environment 14: Episode 3801, Score -123.41708512273742, Avg_Score -121.20916981916726\n",
      "Adding trajectory to replay buffer: step 17353, counter 277312\n",
      "Environment 3: Episode 3802, Score -118.47542446969699, Avg_Score -121.16785144621376\n",
      "Adding trajectory to replay buffer: step 17353, counter 277355\n",
      "Environment 15: Episode 3803, Score -121.2865361171042, Avg_Score -121.15812472091962\n",
      "Adding trajectory to replay buffer: step 17362, counter 277397\n",
      "Environment 13: Episode 3804, Score -116.18537383169246, Avg_Score -121.1224133876614\n",
      "Adding trajectory to replay buffer: step 17375, counter 277443\n",
      "Environment 0: Episode 3805, Score -122.00046636607871, Avg_Score -121.12765883335037\n",
      "Adding trajectory to replay buffer: step 17378, counter 277488\n",
      "Environment 6: Episode 3806, Score -122.65122037504428, Avg_Score -121.20271267123616\n",
      "Adding trajectory to replay buffer: step 17378, counter 277534\n",
      "Environment 12: Episode 3807, Score -122.45608992678052, Avg_Score -121.20205919128578\n",
      "Adding trajectory to replay buffer: step 17379, counter 277584\n",
      "Environment 7: Episode 3808, Score -117.1923670026306, Avg_Score -121.17121719823209\n",
      "Adding trajectory to replay buffer: step 17384, counter 277632\n",
      "Environment 8: Episode 3809, Score -117.8556722699857, Avg_Score -121.11110154620643\n",
      "Adding trajectory to replay buffer: step 17391, counter 277678\n",
      "Environment 2: Episode 3810, Score -121.995153571068, Avg_Score -121.21001503850852\n",
      "Adding trajectory to replay buffer: step 17391, counter 277725\n",
      "Environment 11: Episode 3811, Score -120.45185179109748, Avg_Score -121.19078048891248\n",
      "Adding trajectory to replay buffer: step 17393, counter 277767\n",
      "Environment 5: Episode 3812, Score -115.4440595287575, Avg_Score -121.1198596913187\n",
      "Adding trajectory to replay buffer: step 17396, counter 277813\n",
      "Environment 9: Episode 3813, Score -121.79346800276669, Avg_Score -121.12357211362465\n",
      "Adding trajectory to replay buffer: step 17396, counter 277858\n",
      "Environment 14: Episode 3814, Score -123.20222914056139, Avg_Score -121.17187687325429\n",
      "Adding trajectory to replay buffer: step 17397, counter 277902\n",
      "Environment 3: Episode 3815, Score -122.42955736643324, Avg_Score -121.12703855120638\n",
      "Adding trajectory to replay buffer: step 17397, counter 277949\n",
      "Environment 10: Episode 3816, Score -120.32450597161551, Avg_Score -121.14743757033932\n",
      "Adding trajectory to replay buffer: step 17397, counter 277993\n",
      "Environment 15: Episode 3817, Score -122.56600234887253, Avg_Score -121.15090083313214\n",
      "Adding trajectory to replay buffer: step 17405, counter 278150\n",
      "Environment 4: Episode 3818, Score -116.52536458352084, Avg_Score -121.1658352511916\n",
      "Adding trajectory to replay buffer: step 17407, counter 278195\n",
      "Environment 13: Episode 3819, Score -120.23543497209997, Avg_Score -121.14914777527129\n",
      "Adding trajectory to replay buffer: step 17420, counter 278240\n",
      "Environment 0: Episode 3820, Score -122.12502336308424, Avg_Score -121.14660854009936\n",
      "Adding trajectory to replay buffer: step 17421, counter 278283\n",
      "Environment 6: Episode 3821, Score -123.2759426187401, Avg_Score -121.14728920394336\n",
      "Adding trajectory to replay buffer: step 17421, counter 278325\n",
      "Environment 7: Episode 3822, Score -122.85411761355152, Avg_Score -121.15914815920203\n",
      "Adding trajectory to replay buffer: step 17421, counter 278368\n",
      "Environment 12: Episode 3823, Score -123.28718960570482, Avg_Score -121.23128976527076\n",
      "Adding trajectory to replay buffer: step 17426, counter 278463\n",
      "Environment 1: Episode 3824, Score -114.69454718396938, Avg_Score -121.16083662176037\n",
      "Adding trajectory to replay buffer: step 17426, counter 278505\n",
      "Environment 8: Episode 3825, Score -122.89880598559046, Avg_Score -121.20582644947049\n",
      "Adding trajectory to replay buffer: step 17434, counter 278548\n",
      "Environment 2: Episode 3826, Score -123.41370431298638, Avg_Score -121.20954671422223\n",
      "Adding trajectory to replay buffer: step 17435, counter 278592\n",
      "Environment 11: Episode 3827, Score -120.38270015727107, Avg_Score -121.1843063384851\n",
      "Adding trajectory to replay buffer: step 17438, counter 278637\n",
      "Environment 5: Episode 3828, Score -122.27870684488676, Avg_Score -121.20761465968218\n",
      "Adding trajectory to replay buffer: step 17441, counter 278682\n",
      "Environment 9: Episode 3829, Score -123.85093192582453, Avg_Score -121.22424529441781\n",
      "Adding trajectory to replay buffer: step 17444, counter 278730\n",
      "Environment 14: Episode 3830, Score -120.03074513058115, Avg_Score -121.20435950659677\n",
      "Adding trajectory to replay buffer: step 17452, counter 278777\n",
      "Environment 4: Episode 3831, Score -121.5798510842833, Avg_Score -121.1148221650804\n",
      "Adding trajectory to replay buffer: step 17453, counter 278833\n",
      "Environment 3: Episode 3832, Score -113.85729292728628, Avg_Score -121.02101104795616\n",
      "Adding trajectory to replay buffer: step 17465, counter 278877\n",
      "Environment 12: Episode 3833, Score -124.02824909975256, Avg_Score -121.03730720980833\n",
      "Adding trajectory to replay buffer: step 17467, counter 278937\n",
      "Environment 13: Episode 3834, Score -114.69328714096112, Avg_Score -120.96022285619767\n",
      "Adding trajectory to replay buffer: step 17468, counter 278984\n",
      "Environment 6: Episode 3835, Score -121.74550865667189, Avg_Score -120.94792418670205\n",
      "Adding trajectory to replay buffer: step 17470, counter 279028\n",
      "Environment 1: Episode 3836, Score -121.07914991987434, Avg_Score -120.94917404163488\n",
      "Adding trajectory to replay buffer: step 17471, counter 279078\n",
      "Environment 7: Episode 3837, Score -120.02797519002544, Avg_Score -120.94409268144778\n",
      "Adding trajectory to replay buffer: step 17476, counter 279119\n",
      "Environment 11: Episode 3838, Score -118.03441150774125, Avg_Score -120.89708393656862\n",
      "Adding trajectory to replay buffer: step 17479, counter 279164\n",
      "Environment 2: Episode 3839, Score -121.47543607070111, Avg_Score -120.9089881729888\n",
      "Adding trajectory to replay buffer: step 17481, counter 279225\n",
      "Environment 0: Episode 3840, Score -115.67899426426241, Avg_Score -120.8864264125619\n",
      "Adding trajectory to replay buffer: step 17481, counter 279268\n",
      "Environment 5: Episode 3841, Score -120.2486191345149, Avg_Score -120.85848795015586\n",
      "Adding trajectory to replay buffer: step 17483, counter 279354\n",
      "Environment 15: Episode 3842, Score -122.93609340579033, Avg_Score -120.87198529899335\n",
      "Adding trajectory to replay buffer: step 17491, counter 279404\n",
      "Environment 9: Episode 3843, Score -117.69562616982843, Avg_Score -120.83491055586506\n",
      "Adding trajectory to replay buffer: step 17497, counter 279449\n",
      "Environment 4: Episode 3844, Score -119.30876382221655, Avg_Score -120.81438979828485\n",
      "Adding trajectory to replay buffer: step 17498, counter 279494\n",
      "Environment 3: Episode 3845, Score -122.56267622385609, Avg_Score -120.80943538489579\n",
      "Adding trajectory to replay buffer: step 17509, counter 279577\n",
      "Environment 8: Episode 3846, Score -116.59633377170252, Avg_Score -120.7422958329061\n",
      "Adding trajectory to replay buffer: step 17509, counter 279621\n",
      "Environment 12: Episode 3847, Score -121.95444406353434, Avg_Score -120.73016260378978\n",
      "Adding trajectory to replay buffer: step 17511, counter 279665\n",
      "Environment 13: Episode 3848, Score -121.26002506281995, Avg_Score -120.70947040416742\n",
      "Adding trajectory to replay buffer: step 17514, counter 279711\n",
      "Environment 6: Episode 3849, Score -121.2082934634791, Avg_Score -120.70381781853492\n",
      "Adding trajectory to replay buffer: step 17516, counter 279757\n",
      "Environment 1: Episode 3850, Score -120.06126012378682, Avg_Score -120.67273158850601\n",
      "Adding trajectory to replay buffer: step 17518, counter 279804\n",
      "Environment 7: Episode 3851, Score -121.96568022401073, Avg_Score -120.66575886271625\n",
      "Adding trajectory to replay buffer: step 17519, counter 279847\n",
      "Environment 11: Episode 3852, Score -122.30523944695543, Avg_Score -120.71798455334141\n",
      "Adding trajectory to replay buffer: step 17523, counter 279891\n",
      "Environment 2: Episode 3853, Score -122.64472337259228, Avg_Score -120.71138700788175\n",
      "Adding trajectory to replay buffer: step 17523, counter 279933\n",
      "Environment 5: Episode 3854, Score -122.34913559903329, Avg_Score -120.70803961724607\n",
      "Adding trajectory to replay buffer: step 17527, counter 279979\n",
      "Environment 0: Episode 3855, Score -121.51599551730914, Avg_Score -120.6958751037887\n",
      "Adding trajectory to replay buffer: step 17528, counter 280024\n",
      "Environment 15: Episode 3856, Score -121.27168201814064, Avg_Score -120.68203689453235\n",
      "Adding trajectory to replay buffer: step 17533, counter 280113\n",
      "Environment 14: Episode 3857, Score -122.49173377961418, Avg_Score -120.67433328829617\n",
      "Adding trajectory to replay buffer: step 17538, counter 280160\n",
      "Environment 9: Episode 3858, Score -119.72291916645257, Avg_Score -120.63904336637404\n",
      "Adding trajectory to replay buffer: step 17542, counter 280204\n",
      "Environment 3: Episode 3859, Score -120.88594645891214, Avg_Score -120.6149761471724\n",
      "Adding trajectory to replay buffer: step 17542, counter 280249\n",
      "Environment 4: Episode 3860, Score -121.76023463425398, Avg_Score -120.60977250548348\n",
      "Adding trajectory to replay buffer: step 17554, counter 280294\n",
      "Environment 8: Episode 3861, Score -119.77082592485783, Avg_Score -120.57703505940216\n",
      "Adding trajectory to replay buffer: step 17556, counter 280336\n",
      "Environment 6: Episode 3862, Score -122.91484967679281, Avg_Score -120.57391998417852\n",
      "Adding trajectory to replay buffer: step 17559, counter 280379\n",
      "Environment 1: Episode 3863, Score -122.69745736163047, Avg_Score -120.60371281324849\n",
      "Adding trajectory to replay buffer: step 17560, counter 280430\n",
      "Environment 12: Episode 3864, Score -123.72701439294529, Avg_Score -120.67360163006619\n",
      "Adding trajectory to replay buffer: step 17562, counter 280474\n",
      "Environment 7: Episode 3865, Score -122.74253207712806, Avg_Score -120.70092273344324\n",
      "Adding trajectory to replay buffer: step 17563, counter 280518\n",
      "Environment 11: Episode 3866, Score -122.21150682404266, Avg_Score -120.70593538501564\n",
      "Adding trajectory to replay buffer: step 17568, counter 280563\n",
      "Environment 2: Episode 3867, Score -121.8434902066694, Avg_Score -120.72064588322688\n",
      "Adding trajectory to replay buffer: step 17568, counter 280608\n",
      "Environment 5: Episode 3868, Score -122.47207534646057, Avg_Score -120.7234440620764\n",
      "Adding trajectory to replay buffer: step 17571, counter 280652\n",
      "Environment 0: Episode 3869, Score -121.4930612936467, Avg_Score -120.7853574681187\n",
      "Adding trajectory to replay buffer: step 17571, counter 280695\n",
      "Environment 15: Episode 3870, Score -122.25539694011181, Avg_Score -120.79208962139012\n",
      "Adding trajectory to replay buffer: step 17576, counter 280738\n",
      "Environment 14: Episode 3871, Score -123.11509455069528, Avg_Score -120.79265263758738\n",
      "Adding trajectory to replay buffer: step 17581, counter 280781\n",
      "Environment 9: Episode 3872, Score -122.12010693265002, Avg_Score -120.79754427416077\n",
      "Adding trajectory to replay buffer: step 17585, counter 280824\n",
      "Environment 4: Episode 3873, Score -122.57094295335375, Avg_Score -120.80053427118095\n",
      "Adding trajectory to replay buffer: step 17585, counter 280898\n",
      "Environment 13: Episode 3874, Score -116.52768689827496, Avg_Score -120.78966150636805\n",
      "Adding trajectory to replay buffer: step 17587, counter 280943\n",
      "Environment 3: Episode 3875, Score -121.58646170358918, Avg_Score -120.80008665494705\n",
      "Adding trajectory to replay buffer: step 17588, counter 281134\n",
      "Environment 10: Episode 3876, Score -120.15682590188054, Avg_Score -120.76981594772836\n",
      "Adding trajectory to replay buffer: step 17597, counter 281177\n",
      "Environment 8: Episode 3877, Score -122.67451705551893, Avg_Score -120.86345719982388\n",
      "Adding trajectory to replay buffer: step 17599, counter 281220\n",
      "Environment 6: Episode 3878, Score -121.82845813679509, Avg_Score -120.82287317862001\n",
      "Adding trajectory to replay buffer: step 17605, counter 281265\n",
      "Environment 12: Episode 3879, Score -121.41750913654641, Avg_Score -120.80617638642447\n",
      "Adding trajectory to replay buffer: step 17606, counter 281308\n",
      "Environment 11: Episode 3880, Score -122.47319649611724, Avg_Score -120.88510721081407\n",
      "Adding trajectory to replay buffer: step 17608, counter 281354\n",
      "Environment 7: Episode 3881, Score -120.11584616216335, Avg_Score -120.8694613739905\n",
      "Adding trajectory to replay buffer: step 17609, counter 281395\n",
      "Environment 2: Episode 3882, Score -117.84496140724917, Avg_Score -120.82426648750747\n",
      "Adding trajectory to replay buffer: step 17615, counter 281442\n",
      "Environment 5: Episode 3883, Score -119.54946720854194, Avg_Score -120.87601446089536\n",
      "Adding trajectory to replay buffer: step 17627, counter 281493\n",
      "Environment 14: Episode 3884, Score -117.39240394508776, Avg_Score -120.82478681352967\n",
      "Adding trajectory to replay buffer: step 17629, counter 281551\n",
      "Environment 0: Episode 3885, Score -114.95890335465036, Avg_Score -120.74144936505743\n",
      "Adding trajectory to replay buffer: step 17630, counter 281594\n",
      "Environment 3: Episode 3886, Score -119.74150274477526, Avg_Score -120.71764897444878\n",
      "Adding trajectory to replay buffer: step 17630, counter 281639\n",
      "Environment 4: Episode 3887, Score -121.69779598531312, Avg_Score -120.7046159345155\n",
      "Adding trajectory to replay buffer: step 17632, counter 281686\n",
      "Environment 13: Episode 3888, Score -118.01657301372073, Avg_Score -120.65772274762507\n",
      "Adding trajectory to replay buffer: step 17644, counter 281749\n",
      "Environment 9: Episode 3889, Score -116.35537945871428, Avg_Score -120.59923962173174\n",
      "Adding trajectory to replay buffer: step 17648, counter 281798\n",
      "Environment 6: Episode 3890, Score -120.85219948593651, Avg_Score -120.63288187047463\n",
      "Adding trajectory to replay buffer: step 17649, counter 281841\n",
      "Environment 11: Episode 3891, Score -123.82911214842213, Avg_Score -120.69543437491443\n",
      "Adding trajectory to replay buffer: step 17653, counter 281885\n",
      "Environment 2: Episode 3892, Score -119.05559461333291, Avg_Score -120.66960282035193\n",
      "Adding trajectory to replay buffer: step 17653, counter 281930\n",
      "Environment 7: Episode 3893, Score -122.36830211067385, Avg_Score -120.66637051042993\n",
      "Adding trajectory to replay buffer: step 17658, counter 282029\n",
      "Environment 1: Episode 3894, Score -114.48153971108795, Avg_Score -120.59006695855642\n",
      "Adding trajectory to replay buffer: step 17661, counter 282075\n",
      "Environment 5: Episode 3895, Score -120.0328161470114, Avg_Score -120.61998607209027\n",
      "Adding trajectory to replay buffer: step 17662, counter 282166\n",
      "Environment 15: Episode 3896, Score -121.15407097003174, Avg_Score -120.646173450392\n",
      "Adding trajectory to replay buffer: step 17671, counter 282210\n",
      "Environment 14: Episode 3897, Score -122.212558321096, Avg_Score -120.74025018834554\n",
      "Adding trajectory to replay buffer: step 17673, counter 282253\n",
      "Environment 3: Episode 3898, Score -116.03100536954155, Avg_Score -120.67228427548923\n",
      "Adding trajectory to replay buffer: step 17674, counter 282298\n",
      "Environment 0: Episode 3899, Score -122.57193527376404, Avg_Score -120.66941228799047\n",
      "Adding trajectory to replay buffer: step 17674, counter 282367\n",
      "Environment 12: Episode 3900, Score -114.91561198075178, Avg_Score -120.58302448042677\n",
      "Adding trajectory to replay buffer: step 17675, counter 282412\n",
      "Environment 4: Episode 3901, Score -122.32681343629956, Avg_Score -120.57212176356241\n",
      "Adding trajectory to replay buffer: step 17677, counter 282457\n",
      "Environment 13: Episode 3902, Score -122.90363399833068, Avg_Score -120.61640385884874\n",
      "Adding trajectory to replay buffer: step 17678, counter 282538\n",
      "Environment 8: Episode 3903, Score -113.89250653757993, Avg_Score -120.5424635630535\n",
      "Adding trajectory to replay buffer: step 17688, counter 282582\n",
      "Environment 9: Episode 3904, Score -122.70115223699995, Avg_Score -120.60762134710656\n",
      "Adding trajectory to replay buffer: step 17692, counter 282626\n",
      "Environment 6: Episode 3905, Score -120.97122032246305, Avg_Score -120.5973288866704\n",
      "Adding trajectory to replay buffer: step 17696, counter 282673\n",
      "Environment 11: Episode 3906, Score -119.06986072526314, Avg_Score -120.56151529017258\n",
      "Adding trajectory to replay buffer: step 17703, counter 282718\n",
      "Environment 1: Episode 3907, Score -121.6697442410207, Avg_Score -120.55365183331496\n",
      "Adding trajectory to replay buffer: step 17705, counter 282762\n",
      "Environment 5: Episode 3908, Score -122.73395043850132, Avg_Score -120.60906766767368\n",
      "Adding trajectory to replay buffer: step 17707, counter 282807\n",
      "Environment 15: Episode 3909, Score -117.75465169831614, Avg_Score -120.60805746195697\n",
      "Adding trajectory to replay buffer: step 17714, counter 282868\n",
      "Environment 2: Episode 3910, Score -116.27666173912057, Avg_Score -120.55087254363748\n",
      "Adding trajectory to replay buffer: step 17715, counter 282909\n",
      "Environment 12: Episode 3911, Score -115.72291224886538, Avg_Score -120.50358314821518\n",
      "Adding trajectory to replay buffer: step 17716, counter 282951\n",
      "Environment 0: Episode 3912, Score -116.042843179912, Avg_Score -120.50957098472672\n",
      "Adding trajectory to replay buffer: step 17717, counter 282997\n",
      "Environment 14: Episode 3913, Score -121.94476565919817, Avg_Score -120.51108396129104\n",
      "Adding trajectory to replay buffer: step 17720, counter 283044\n",
      "Environment 3: Episode 3914, Score -120.79163283165731, Avg_Score -120.48697799820201\n",
      "Adding trajectory to replay buffer: step 17721, counter 283088\n",
      "Environment 13: Episode 3915, Score -122.55566240232562, Avg_Score -120.48823904856091\n",
      "Adding trajectory to replay buffer: step 17722, counter 283135\n",
      "Environment 4: Episode 3916, Score -119.42155917508501, Avg_Score -120.4792095805956\n",
      "Adding trajectory to replay buffer: step 17723, counter 283180\n",
      "Environment 8: Episode 3917, Score -120.31452135650504, Avg_Score -120.45669477067193\n",
      "Adding trajectory to replay buffer: step 17733, counter 283225\n",
      "Environment 9: Episode 3918, Score -122.04562900657581, Avg_Score -120.51189741490246\n",
      "Adding trajectory to replay buffer: step 17735, counter 283268\n",
      "Environment 6: Episode 3919, Score -122.34033354161247, Avg_Score -120.5329464005976\n",
      "Adding trajectory to replay buffer: step 17740, counter 283312\n",
      "Environment 11: Episode 3920, Score -123.25786252350734, Avg_Score -120.54427479220182\n",
      "Adding trajectory to replay buffer: step 17749, counter 283358\n",
      "Environment 1: Episode 3921, Score -121.73945217796975, Avg_Score -120.52890988779409\n",
      "Adding trajectory to replay buffer: step 17749, counter 283402\n",
      "Environment 5: Episode 3922, Score -123.28447946149359, Avg_Score -120.53321350627351\n",
      "Adding trajectory to replay buffer: step 17757, counter 283445\n",
      "Environment 2: Episode 3923, Score -122.37173486565179, Avg_Score -120.524058958873\n",
      "Adding trajectory to replay buffer: step 17759, counter 283488\n",
      "Environment 0: Episode 3924, Score -119.09017505377966, Avg_Score -120.56801523757109\n",
      "Adding trajectory to replay buffer: step 17760, counter 283531\n",
      "Environment 14: Episode 3925, Score -115.98545392376495, Avg_Score -120.49888171695284\n",
      "Adding trajectory to replay buffer: step 17765, counter 283581\n",
      "Environment 12: Episode 3926, Score -115.09911961846923, Avg_Score -120.41573587000767\n",
      "Adding trajectory to replay buffer: step 17768, counter 283626\n",
      "Environment 8: Episode 3927, Score -123.2144565237624, Avg_Score -120.44405343367258\n",
      "Adding trajectory to replay buffer: step 17771, counter 283675\n",
      "Environment 4: Episode 3928, Score -120.88134920036917, Avg_Score -120.4300798572274\n",
      "Adding trajectory to replay buffer: step 17776, counter 283718\n",
      "Environment 9: Episode 3929, Score -121.41145227036314, Avg_Score -120.40568506067281\n",
      "Adding trajectory to replay buffer: step 17781, counter 283764\n",
      "Environment 6: Episode 3930, Score -119.98316385001627, Avg_Score -120.40520924786716\n",
      "Adding trajectory to replay buffer: step 17784, counter 283841\n",
      "Environment 15: Episode 3931, Score -114.54233031181494, Avg_Score -120.33483404014248\n",
      "Adding trajectory to replay buffer: step 17785, counter 283886\n",
      "Environment 11: Episode 3932, Score -121.1834943037387, Avg_Score -120.40809605390702\n",
      "Adding trajectory to replay buffer: step 17789, counter 284087\n",
      "Environment 10: Episode 3933, Score -122.96343660075031, Avg_Score -120.39744792891699\n",
      "Adding trajectory to replay buffer: step 17792, counter 284130\n",
      "Environment 1: Episode 3934, Score -116.94197405089872, Avg_Score -120.41993479801639\n",
      "Adding trajectory to replay buffer: step 17793, counter 284174\n",
      "Environment 5: Episode 3935, Score -122.27156292674442, Avg_Score -120.42519534071707\n",
      "Adding trajectory to replay buffer: step 17795, counter 284249\n",
      "Environment 3: Episode 3936, Score -116.84326864289616, Avg_Score -120.3828365279473\n",
      "Adding trajectory to replay buffer: step 17803, counter 284293\n",
      "Environment 0: Episode 3937, Score -122.46502023389098, Avg_Score -120.40720697838599\n",
      "Adding trajectory to replay buffer: step 17804, counter 284337\n",
      "Environment 14: Episode 3938, Score -123.19016014759988, Avg_Score -120.45876446478457\n",
      "Adding trajectory to replay buffer: step 17813, counter 284393\n",
      "Environment 2: Episode 3939, Score -118.52479838942551, Avg_Score -120.42925808797183\n",
      "Adding trajectory to replay buffer: step 17814, counter 284439\n",
      "Environment 8: Episode 3940, Score -121.58320840709098, Avg_Score -120.4883002294001\n",
      "Adding trajectory to replay buffer: step 17819, counter 284605\n",
      "Environment 7: Episode 3941, Score -118.64832111444264, Avg_Score -120.47229724919939\n",
      "Adding trajectory to replay buffer: step 17825, counter 284649\n",
      "Environment 6: Episode 3942, Score -121.85002320002438, Avg_Score -120.46143654714172\n",
      "Adding trajectory to replay buffer: step 17826, counter 284710\n",
      "Environment 12: Episode 3943, Score -117.2228892254252, Avg_Score -120.4567091776977\n",
      "Adding trajectory to replay buffer: step 17828, counter 284753\n",
      "Environment 11: Episode 3944, Score -116.45149335892002, Avg_Score -120.4281364730647\n",
      "Adding trajectory to replay buffer: step 17832, counter 284796\n",
      "Environment 10: Episode 3945, Score -123.09554632318331, Avg_Score -120.433465174058\n",
      "Adding trajectory to replay buffer: step 17835, counter 284839\n",
      "Environment 1: Episode 3946, Score -120.86796368691698, Avg_Score -120.47618147321013\n",
      "Adding trajectory to replay buffer: step 17835, counter 284898\n",
      "Environment 9: Episode 3947, Score -114.43771558206528, Avg_Score -120.40101418839545\n",
      "Adding trajectory to replay buffer: step 17838, counter 284941\n",
      "Environment 3: Episode 3948, Score -122.2356989689842, Avg_Score -120.4107709274571\n",
      "Adding trajectory to replay buffer: step 17842, counter 285012\n",
      "Environment 4: Episode 3949, Score -120.26165518077337, Avg_Score -120.40130454463004\n",
      "Adding trajectory to replay buffer: step 17842, counter 285070\n",
      "Environment 15: Episode 3950, Score -115.95562733042675, Avg_Score -120.36024821669642\n",
      "Adding trajectory to replay buffer: step 17850, counter 285117\n",
      "Environment 0: Episode 3951, Score -120.73849760374178, Avg_Score -120.3479763904937\n",
      "Adding trajectory to replay buffer: step 17851, counter 285164\n",
      "Environment 14: Episode 3952, Score -119.97377755923259, Avg_Score -120.3246617716165\n",
      "Adding trajectory to replay buffer: step 17855, counter 285206\n",
      "Environment 2: Episode 3953, Score -123.62235542833186, Avg_Score -120.3344380921739\n",
      "Adding trajectory to replay buffer: step 17861, counter 285248\n",
      "Environment 7: Episode 3954, Score -116.35299515845689, Avg_Score -120.27447668776813\n",
      "Adding trajectory to replay buffer: step 17861, counter 285295\n",
      "Environment 8: Episode 3955, Score -118.29324337142891, Avg_Score -120.2422491663093\n",
      "Adding trajectory to replay buffer: step 17873, counter 285340\n",
      "Environment 11: Episode 3956, Score -121.14932827879613, Avg_Score -120.24102562891585\n",
      "Adding trajectory to replay buffer: step 17876, counter 285391\n",
      "Environment 6: Episode 3957, Score -122.27425864913315, Avg_Score -120.23885087761104\n",
      "Adding trajectory to replay buffer: step 17876, counter 285435\n",
      "Environment 10: Episode 3958, Score -121.95962720936971, Avg_Score -120.26121795804022\n",
      "Adding trajectory to replay buffer: step 17879, counter 285479\n",
      "Environment 9: Episode 3959, Score -123.82897704876773, Avg_Score -120.29064826393878\n",
      "Adding trajectory to replay buffer: step 17883, counter 285524\n",
      "Environment 3: Episode 3960, Score -119.98182178287021, Avg_Score -120.27286413542495\n",
      "Adding trajectory to replay buffer: step 17886, counter 285568\n",
      "Environment 4: Episode 3961, Score -122.80118617586915, Avg_Score -120.30316773793507\n",
      "Adding trajectory to replay buffer: step 17886, counter 285612\n",
      "Environment 15: Episode 3962, Score -122.74081509308269, Avg_Score -120.30142739209793\n",
      "Adding trajectory to replay buffer: step 17893, counter 285679\n",
      "Environment 12: Episode 3963, Score -117.9617922996829, Avg_Score -120.25407074147849\n",
      "Adding trajectory to replay buffer: step 17894, counter 285723\n",
      "Environment 0: Episode 3964, Score -123.01707886543063, Avg_Score -120.2469713862033\n",
      "Adding trajectory to replay buffer: step 17896, counter 285768\n",
      "Environment 14: Episode 3965, Score -121.98235958480586, Avg_Score -120.23936966128011\n",
      "Adding trajectory to replay buffer: step 17900, counter 285833\n",
      "Environment 1: Episode 3966, Score -118.51059047813249, Avg_Score -120.20236049782098\n",
      "Adding trajectory to replay buffer: step 17904, counter 285882\n",
      "Environment 2: Episode 3967, Score -115.36549539951005, Avg_Score -120.13758054974939\n",
      "Adding trajectory to replay buffer: step 17906, counter 285927\n",
      "Environment 8: Episode 3968, Score -117.48602240277765, Avg_Score -120.08772002031256\n",
      "Adding trajectory to replay buffer: step 17916, counter 285970\n",
      "Environment 11: Episode 3969, Score -122.56089704141641, Avg_Score -120.09839837779028\n",
      "Adding trajectory to replay buffer: step 17916, counter 286165\n",
      "Environment 13: Episode 3970, Score -121.27343505573643, Avg_Score -120.08857875894651\n",
      "Adding trajectory to replay buffer: step 17919, counter 286208\n",
      "Environment 6: Episode 3971, Score -122.13238733539544, Avg_Score -120.07875168679351\n",
      "Adding trajectory to replay buffer: step 17920, counter 286252\n",
      "Environment 10: Episode 3972, Score -122.26438406270184, Avg_Score -120.08019445809401\n",
      "Adding trajectory to replay buffer: step 17923, counter 286296\n",
      "Environment 9: Episode 3973, Score -122.21343493499049, Avg_Score -120.07661937791039\n",
      "Adding trajectory to replay buffer: step 17926, counter 286361\n",
      "Environment 7: Episode 3974, Score -111.06426301715462, Avg_Score -120.02198513909917\n",
      "Adding trajectory to replay buffer: step 17927, counter 286405\n",
      "Environment 3: Episode 3975, Score -116.53133163333374, Avg_Score -119.97143383839665\n",
      "Adding trajectory to replay buffer: step 17929, counter 286448\n",
      "Environment 15: Episode 3976, Score -121.76858090072375, Avg_Score -119.98755138838506\n",
      "Adding trajectory to replay buffer: step 17930, counter 286492\n",
      "Environment 4: Episode 3977, Score -121.77573319109034, Avg_Score -119.97856354974077\n",
      "Adding trajectory to replay buffer: step 17939, counter 286537\n",
      "Environment 0: Episode 3978, Score -120.17116399069533, Avg_Score -119.96199060827978\n",
      "Adding trajectory to replay buffer: step 17944, counter 286581\n",
      "Environment 1: Episode 3979, Score -121.91548926039599, Avg_Score -119.9669704095183\n",
      "Adding trajectory to replay buffer: step 17949, counter 286634\n",
      "Environment 14: Episode 3980, Score -118.43851868987642, Avg_Score -119.9266236314559\n",
      "Adding trajectory to replay buffer: step 17950, counter 286680\n",
      "Environment 2: Episode 3981, Score -122.1271954713265, Avg_Score -119.94673712454752\n",
      "Adding trajectory to replay buffer: step 17951, counter 286725\n",
      "Environment 8: Episode 3982, Score -122.58831503155766, Avg_Score -119.99417066079063\n",
      "Adding trajectory to replay buffer: step 17952, counter 286784\n",
      "Environment 12: Episode 3983, Score -115.81597303227709, Avg_Score -119.95683571902796\n",
      "Adding trajectory to replay buffer: step 17960, counter 286828\n",
      "Environment 13: Episode 3984, Score -120.10747459846301, Avg_Score -119.98398642556172\n",
      "Adding trajectory to replay buffer: step 17962, counter 286874\n",
      "Environment 11: Episode 3985, Score -121.11525116297169, Avg_Score -120.04554990364491\n",
      "Adding trajectory to replay buffer: step 17967, counter 287048\n",
      "Environment 5: Episode 3986, Score -118.1721125904775, Avg_Score -120.02985600210195\n",
      "Adding trajectory to replay buffer: step 17967, counter 287092\n",
      "Environment 9: Episode 3987, Score -122.91396350255299, Avg_Score -120.04201767727434\n",
      "Adding trajectory to replay buffer: step 17970, counter 287136\n",
      "Environment 7: Episode 3988, Score -122.52017374882661, Avg_Score -120.0870536846254\n",
      "Adding trajectory to replay buffer: step 17972, counter 287181\n",
      "Environment 3: Episode 3989, Score -122.35098981014576, Avg_Score -120.14700978813971\n",
      "Adding trajectory to replay buffer: step 17972, counter 287224\n",
      "Environment 15: Episode 3990, Score -122.35812545838643, Avg_Score -120.16206904786421\n",
      "Adding trajectory to replay buffer: step 17975, counter 287269\n",
      "Environment 4: Episode 3991, Score -122.09828499077014, Avg_Score -120.14476077628768\n",
      "Adding trajectory to replay buffer: step 17981, counter 287331\n",
      "Environment 6: Episode 3992, Score -116.24278937546102, Avg_Score -120.11663272390896\n",
      "Adding trajectory to replay buffer: step 17983, counter 287375\n",
      "Environment 0: Episode 3993, Score -122.76382068901758, Avg_Score -120.1205879096924\n",
      "Adding trajectory to replay buffer: step 17987, counter 287418\n",
      "Environment 1: Episode 3994, Score -121.25350837791277, Avg_Score -120.18830759636064\n",
      "Adding trajectory to replay buffer: step 17993, counter 287462\n",
      "Environment 14: Episode 3995, Score -121.41792282538705, Avg_Score -120.20215866314442\n",
      "Adding trajectory to replay buffer: step 17995, counter 287505\n",
      "Environment 12: Episode 3996, Score -115.92655997406102, Avg_Score -120.1498835531847\n",
      "Adding trajectory to replay buffer: step 18001, counter 287556\n",
      "Environment 2: Episode 3997, Score -117.52856027056588, Avg_Score -120.1030435726794\n",
      "Adding trajectory to replay buffer: step 18004, counter 287598\n",
      "Environment 11: Episode 3998, Score -114.85018663339007, Avg_Score -120.09123538531789\n",
      "Adding trajectory to replay buffer: step 18012, counter 287638\n",
      "Environment 15: Episode 3999, Score -115.3901145933289, Avg_Score -120.01941717851354\n",
      "Adding trajectory to replay buffer: step 18014, counter 287685\n",
      "Environment 9: Episode 4000, Score -119.42531397568746, Avg_Score -120.06451419846292\n",
      "Adding trajectory to replay buffer: step 18019, counter 287732\n",
      "Environment 3: Episode 4001, Score -120.68759238494933, Avg_Score -120.04812198794941\n",
      "Adding trajectory to replay buffer: step 18019, counter 287781\n",
      "Environment 7: Episode 4002, Score -120.16215731800224, Avg_Score -120.02070722114611\n",
      "Adding trajectory to replay buffer: step 18023, counter 287829\n",
      "Environment 4: Episode 4003, Score -118.11377476307129, Avg_Score -120.06291990340104\n",
      "Adding trajectory to replay buffer: step 18026, counter 287872\n",
      "Environment 0: Episode 4004, Score -119.22294840060361, Avg_Score -120.02813786503707\n",
      "Adding trajectory to replay buffer: step 18034, counter 287939\n",
      "Environment 5: Episode 4005, Score -114.08184210400034, Avg_Score -119.95924408285242\n",
      "Adding trajectory to replay buffer: step 18036, counter 287982\n",
      "Environment 14: Episode 4006, Score -123.55833554792342, Avg_Score -120.00412883107903\n",
      "Adding trajectory to replay buffer: step 18038, counter 288025\n",
      "Environment 12: Episode 4007, Score -116.75222101759911, Avg_Score -119.95495359884482\n",
      "Adding trajectory to replay buffer: step 18039, counter 288144\n",
      "Environment 10: Episode 4008, Score -113.03758811031159, Avg_Score -119.85798997556293\n",
      "Adding trajectory to replay buffer: step 18044, counter 288187\n",
      "Environment 2: Episode 4009, Score -122.12711307635655, Avg_Score -119.90171458934334\n",
      "Adding trajectory to replay buffer: step 18047, counter 288230\n",
      "Environment 11: Episode 4010, Score -123.29720911564553, Avg_Score -119.9719200631086\n",
      "Adding trajectory to replay buffer: step 18058, counter 288276\n",
      "Environment 15: Episode 4011, Score -120.39334415640062, Avg_Score -120.01862438218393\n",
      "Adding trajectory to replay buffer: step 18062, counter 288319\n",
      "Environment 3: Episode 4012, Score -123.0344348340165, Avg_Score -120.08854029872496\n",
      "Adding trajectory to replay buffer: step 18062, counter 288362\n",
      "Environment 7: Episode 4013, Score -123.16733927459643, Avg_Score -120.10076603487894\n",
      "Adding trajectory to replay buffer: step 18065, counter 288404\n",
      "Environment 4: Episode 4014, Score -122.8572449838128, Avg_Score -120.12142215640047\n",
      "Adding trajectory to replay buffer: step 18068, counter 288491\n",
      "Environment 6: Episode 4015, Score -122.56265469655085, Avg_Score -120.12149207934273\n",
      "Adding trajectory to replay buffer: step 18070, counter 288574\n",
      "Environment 1: Episode 4016, Score -122.05395678608181, Avg_Score -120.14781605545271\n",
      "Adding trajectory to replay buffer: step 18079, counter 288619\n",
      "Environment 5: Episode 4017, Score -121.4783239570052, Avg_Score -120.15945408145771\n",
      "Adding trajectory to replay buffer: step 18080, counter 288663\n",
      "Environment 14: Episode 4018, Score -122.28974998078371, Avg_Score -120.1618952911998\n",
      "Adding trajectory to replay buffer: step 18083, counter 288732\n",
      "Environment 9: Episode 4019, Score -116.53654203095039, Avg_Score -120.10385737609319\n",
      "Adding trajectory to replay buffer: step 18083, counter 288777\n",
      "Environment 12: Episode 4020, Score -121.11848545089923, Avg_Score -120.08246360536712\n",
      "Adding trajectory to replay buffer: step 18085, counter 288823\n",
      "Environment 10: Episode 4021, Score -115.04628934813539, Avg_Score -120.01553197706879\n",
      "Adding trajectory to replay buffer: step 18106, counter 288867\n",
      "Environment 3: Episode 4022, Score -122.20458131589244, Avg_Score -120.00473299561277\n",
      "Adding trajectory to replay buffer: step 18106, counter 288911\n",
      "Environment 7: Episode 4023, Score -122.11135648474408, Avg_Score -120.00212921180369\n",
      "Adding trajectory to replay buffer: step 18109, counter 288955\n",
      "Environment 4: Episode 4024, Score -121.1781930143647, Avg_Score -120.02300939140954\n",
      "Adding trajectory to replay buffer: step 18113, counter 289000\n",
      "Environment 6: Episode 4025, Score -120.86678014997393, Avg_Score -120.07182265367165\n",
      "Adding trajectory to replay buffer: step 18113, counter 289162\n",
      "Environment 8: Episode 4026, Score -121.41109240948099, Avg_Score -120.13494238158174\n",
      "Adding trajectory to replay buffer: step 18116, counter 289208\n",
      "Environment 1: Episode 4027, Score -121.63156913891001, Avg_Score -120.11911350773322\n",
      "Adding trajectory to replay buffer: step 18121, counter 289282\n",
      "Environment 11: Episode 4028, Score -121.42326516689546, Avg_Score -120.12453266739847\n",
      "Adding trajectory to replay buffer: step 18122, counter 289325\n",
      "Environment 5: Episode 4029, Score -122.10981759269472, Avg_Score -120.1315163206218\n",
      "Adding trajectory to replay buffer: step 18123, counter 289368\n",
      "Environment 14: Episode 4030, Score -121.36360716223902, Avg_Score -120.14532075374402\n",
      "Adding trajectory to replay buffer: step 18124, counter 289466\n",
      "Environment 0: Episode 4031, Score -113.58000325045177, Avg_Score -120.13569748313039\n",
      "Adding trajectory to replay buffer: step 18127, counter 289510\n",
      "Environment 12: Episode 4032, Score -122.49855684814602, Avg_Score -120.14884810857447\n",
      "Adding trajectory to replay buffer: step 18129, counter 289556\n",
      "Environment 9: Episode 4033, Score -120.42894595281345, Avg_Score -120.1235032020951\n",
      "Adding trajectory to replay buffer: step 18131, counter 289602\n",
      "Environment 10: Episode 4034, Score -120.15093540478901, Avg_Score -120.155592815634\n",
      "Adding trajectory to replay buffer: step 18150, counter 289646\n",
      "Environment 3: Episode 4035, Score -123.9319053383091, Avg_Score -120.17219623974962\n",
      "Adding trajectory to replay buffer: step 18153, counter 289693\n",
      "Environment 7: Episode 4036, Score -121.05270610440213, Avg_Score -120.21429061436469\n",
      "Adding trajectory to replay buffer: step 18156, counter 289736\n",
      "Environment 8: Episode 4037, Score -123.29834698977882, Avg_Score -120.22262388192357\n",
      "Adding trajectory to replay buffer: step 18157, counter 289784\n",
      "Environment 4: Episode 4038, Score -121.362722228614, Avg_Score -120.2043495027337\n",
      "Adding trajectory to replay buffer: step 18157, counter 289828\n",
      "Environment 6: Episode 4039, Score -120.26949535295927, Avg_Score -120.22179647236904\n",
      "Adding trajectory to replay buffer: step 18159, counter 289871\n",
      "Environment 1: Episode 4040, Score -121.64502333975398, Avg_Score -120.22241462169568\n",
      "Adding trajectory to replay buffer: step 18164, counter 289914\n",
      "Environment 11: Episode 4041, Score -122.80360153582382, Avg_Score -120.26396742590947\n",
      "Adding trajectory to replay buffer: step 18165, counter 289957\n",
      "Environment 5: Episode 4042, Score -122.92590438037118, Avg_Score -120.27472623771293\n",
      "Adding trajectory to replay buffer: step 18167, counter 290001\n",
      "Environment 14: Episode 4043, Score -123.19523320300381, Avg_Score -120.3344496774887\n",
      "Adding trajectory to replay buffer: step 18171, counter 290048\n",
      "Environment 0: Episode 4044, Score -121.49476322009042, Avg_Score -120.38488237610044\n",
      "Adding trajectory to replay buffer: step 18171, counter 290092\n",
      "Environment 12: Episode 4045, Score -122.91828516201551, Avg_Score -120.38310976448875\n",
      "Adding trajectory to replay buffer: step 18172, counter 290135\n",
      "Environment 9: Episode 4046, Score -123.40376790426487, Avg_Score -120.40846780666223\n",
      "Adding trajectory to replay buffer: step 18174, counter 290178\n",
      "Environment 10: Episode 4047, Score -122.49278581782245, Avg_Score -120.4890185090198\n",
      "Adding trajectory to replay buffer: step 18194, counter 290219\n",
      "Environment 7: Episode 4048, Score -115.68137387377273, Avg_Score -120.42347525806768\n",
      "Adding trajectory to replay buffer: step 18195, counter 290370\n",
      "Environment 2: Episode 4049, Score -118.21816309850477, Avg_Score -120.40304033724499\n",
      "Adding trajectory to replay buffer: step 18196, counter 290508\n",
      "Environment 15: Episode 4050, Score -117.50052615211789, Avg_Score -120.4184893254619\n",
      "Adding trajectory to replay buffer: step 18202, counter 290551\n",
      "Environment 1: Episode 4051, Score -122.6905216183141, Avg_Score -120.43800956560764\n",
      "Adding trajectory to replay buffer: step 18204, counter 290795\n",
      "Environment 13: Episode 4052, Score -127.50196977670554, Avg_Score -120.51329148778237\n",
      "Adding trajectory to replay buffer: step 18207, counter 290837\n",
      "Environment 5: Episode 4053, Score -122.8756090708462, Avg_Score -120.50582402420748\n",
      "Adding trajectory to replay buffer: step 18207, counter 290880\n",
      "Environment 11: Episode 4054, Score -121.40405231473284, Avg_Score -120.55633459577025\n",
      "Adding trajectory to replay buffer: step 18212, counter 290925\n",
      "Environment 14: Episode 4055, Score -122.4818594999481, Avg_Score -120.59822075705544\n",
      "Adding trajectory to replay buffer: step 18215, counter 290969\n",
      "Environment 0: Episode 4056, Score -116.79649370801775, Avg_Score -120.55469241134766\n",
      "Adding trajectory to replay buffer: step 18218, counter 291013\n",
      "Environment 10: Episode 4057, Score -117.51713260727563, Avg_Score -120.5071211509291\n",
      "Adding trajectory to replay buffer: step 18231, counter 291088\n",
      "Environment 8: Episode 4058, Score -114.88618381493073, Avg_Score -120.43638671698473\n",
      "Adding trajectory to replay buffer: step 18234, counter 291165\n",
      "Environment 6: Episode 4059, Score -112.88037735639327, Avg_Score -120.32690072006099\n",
      "Adding trajectory to replay buffer: step 18236, counter 291207\n",
      "Environment 7: Episode 4060, Score -123.2040069383861, Avg_Score -120.35912257161614\n",
      "Adding trajectory to replay buffer: step 18238, counter 291250\n",
      "Environment 2: Episode 4061, Score -121.96662023401385, Avg_Score -120.35077691219757\n",
      "Adding trajectory to replay buffer: step 18239, counter 291293\n",
      "Environment 15: Episode 4062, Score -121.33663584646087, Avg_Score -120.33673511973134\n",
      "Adding trajectory to replay buffer: step 18247, counter 291338\n",
      "Environment 1: Episode 4063, Score -116.75524189132949, Avg_Score -120.32466961564782\n",
      "Adding trajectory to replay buffer: step 18249, counter 291383\n",
      "Environment 13: Episode 4064, Score -118.69245350935124, Avg_Score -120.28142336208703\n",
      "Adding trajectory to replay buffer: step 18254, counter 291430\n",
      "Environment 5: Episode 4065, Score -122.0318684028412, Avg_Score -120.28191845026738\n",
      "Adding trajectory to replay buffer: step 18254, counter 291512\n",
      "Environment 9: Episode 4066, Score -121.02725233081357, Avg_Score -120.30708506879421\n",
      "Adding trajectory to replay buffer: step 18258, counter 291563\n",
      "Environment 11: Episode 4067, Score -117.06364568779865, Avg_Score -120.32406657167708\n",
      "Adding trajectory to replay buffer: step 18262, counter 291613\n",
      "Environment 14: Episode 4068, Score -121.67107378527211, Avg_Score -120.36591708550202\n",
      "Adding trajectory to replay buffer: step 18263, counter 291658\n",
      "Environment 10: Episode 4069, Score -121.91766844893128, Avg_Score -120.35948479957719\n",
      "Adding trajectory to replay buffer: step 18270, counter 291778\n",
      "Environment 3: Episode 4070, Score -115.19755797870846, Avg_Score -120.29872602880691\n",
      "Adding trajectory to replay buffer: step 18270, counter 291891\n",
      "Environment 4: Episode 4071, Score -114.12286168859589, Avg_Score -120.21863077233893\n",
      "Adding trajectory to replay buffer: step 18278, counter 291938\n",
      "Environment 8: Episode 4072, Score -121.12830819595544, Avg_Score -120.20727001367146\n",
      "Adding trajectory to replay buffer: step 18279, counter 291981\n",
      "Environment 7: Episode 4073, Score -114.85567268975265, Avg_Score -120.1336923912191\n",
      "Adding trajectory to replay buffer: step 18282, counter 292024\n",
      "Environment 15: Episode 4074, Score -117.23325793431388, Avg_Score -120.1953823403907\n",
      "Adding trajectory to replay buffer: step 18284, counter 292093\n",
      "Environment 0: Episode 4075, Score -118.43073404081787, Avg_Score -120.21437636446555\n",
      "Adding trajectory to replay buffer: step 18284, counter 292143\n",
      "Environment 6: Episode 4076, Score -117.23003668461617, Avg_Score -120.16899092230447\n",
      "Adding trajectory to replay buffer: step 18293, counter 292189\n",
      "Environment 1: Episode 4077, Score -122.16513832887263, Avg_Score -120.17288497368229\n",
      "Adding trajectory to replay buffer: step 18302, counter 292233\n",
      "Environment 11: Episode 4078, Score -116.17760695459074, Avg_Score -120.13294940332123\n",
      "Adding trajectory to replay buffer: step 18305, counter 292284\n",
      "Environment 9: Episode 4079, Score -112.10038201853074, Avg_Score -120.03479833090258\n",
      "Adding trajectory to replay buffer: step 18306, counter 292328\n",
      "Environment 14: Episode 4080, Score -122.05517130584083, Avg_Score -120.07096485706222\n",
      "Adding trajectory to replay buffer: step 18310, counter 292400\n",
      "Environment 2: Episode 4081, Score -113.59761731599085, Avg_Score -119.98566907550887\n",
      "Adding trajectory to replay buffer: step 18311, counter 292441\n",
      "Environment 3: Episode 4082, Score -116.47662942347583, Avg_Score -119.92455221942807\n",
      "Adding trajectory to replay buffer: step 18313, counter 292500\n",
      "Environment 5: Episode 4083, Score -115.78371137252947, Avg_Score -119.92422960283058\n",
      "Adding trajectory to replay buffer: step 18323, counter 292544\n",
      "Environment 7: Episode 4084, Score -116.6348315844244, Avg_Score -119.88950317269018\n",
      "Adding trajectory to replay buffer: step 18325, counter 292591\n",
      "Environment 8: Episode 4085, Score -121.43456921614458, Avg_Score -119.89269635322191\n",
      "Adding trajectory to replay buffer: step 18327, counter 292636\n",
      "Environment 15: Episode 4086, Score -122.81900282174287, Avg_Score -119.93916525553458\n",
      "Adding trajectory to replay buffer: step 18331, counter 292683\n",
      "Environment 0: Episode 4087, Score -121.74066220401849, Avg_Score -119.92743224254922\n",
      "Adding trajectory to replay buffer: step 18331, counter 292744\n",
      "Environment 4: Episode 4088, Score -111.12642086592938, Avg_Score -119.81349471372025\n",
      "Adding trajectory to replay buffer: step 18333, counter 292793\n",
      "Environment 6: Episode 4089, Score -116.98660815589938, Avg_Score -119.75985089717778\n",
      "Adding trajectory to replay buffer: step 18339, counter 292839\n",
      "Environment 1: Episode 4090, Score -122.89562959461722, Avg_Score -119.76522593854008\n",
      "Adding trajectory to replay buffer: step 18344, counter 293012\n",
      "Environment 12: Episode 4091, Score -120.63459674258158, Avg_Score -119.75058905605822\n",
      "Adding trajectory to replay buffer: step 18345, counter 293055\n",
      "Environment 11: Episode 4092, Score -122.70328918896057, Avg_Score -119.81519405419321\n",
      "Adding trajectory to replay buffer: step 18348, counter 293098\n",
      "Environment 9: Episode 4093, Score -123.51335373116099, Avg_Score -119.82268938461466\n",
      "Adding trajectory to replay buffer: step 18348, counter 293140\n",
      "Environment 14: Episode 4094, Score -123.33479991794688, Avg_Score -119.843502300015\n",
      "Adding trajectory to replay buffer: step 18355, counter 293185\n",
      "Environment 2: Episode 4095, Score -121.58795933401026, Avg_Score -119.84520266510124\n",
      "Adding trajectory to replay buffer: step 18355, counter 293277\n",
      "Environment 10: Episode 4096, Score -120.33327582532664, Avg_Score -119.8892698236139\n",
      "Adding trajectory to replay buffer: step 18356, counter 293322\n",
      "Environment 3: Episode 4097, Score -123.33708471461401, Avg_Score -119.94735506805438\n",
      "Adding trajectory to replay buffer: step 18368, counter 293367\n",
      "Environment 7: Episode 4098, Score -121.99854621347288, Avg_Score -120.01883866385522\n",
      "Adding trajectory to replay buffer: step 18372, counter 293414\n",
      "Environment 8: Episode 4099, Score -120.14021914662544, Avg_Score -120.06633970938819\n",
      "Adding trajectory to replay buffer: step 18373, counter 293456\n",
      "Environment 0: Episode 4100, Score -123.045596399026, Avg_Score -120.10254253362159\n",
      "Adding trajectory to replay buffer: step 18373, counter 293498\n",
      "Environment 4: Episode 4101, Score -123.07233776556328, Avg_Score -120.1263899874277\n",
      "Adding trajectory to replay buffer: step 18376, counter 293541\n",
      "Environment 6: Episode 4102, Score -123.27637418602966, Avg_Score -120.15753215610798\n",
      "Adding trajectory to replay buffer: step 18382, counter 293584\n",
      "Environment 1: Episode 4103, Score -122.47593139618075, Avg_Score -120.20115372243905\n",
      "Adding trajectory to replay buffer: step 18387, counter 293658\n",
      "Environment 5: Episode 4104, Score -113.4926983299038, Avg_Score -120.14385122173206\n",
      "Adding trajectory to replay buffer: step 18387, counter 293701\n",
      "Environment 12: Episode 4105, Score -122.43221576535268, Avg_Score -120.22735495834559\n",
      "Adding trajectory to replay buffer: step 18388, counter 293744\n",
      "Environment 11: Episode 4106, Score -122.53569286055055, Avg_Score -120.21712853147186\n",
      "Adding trajectory to replay buffer: step 18388, counter 293805\n",
      "Environment 15: Episode 4107, Score -117.29585816730125, Avg_Score -120.2225649029689\n",
      "Adding trajectory to replay buffer: step 18391, counter 293848\n",
      "Environment 9: Episode 4108, Score -117.10467475326483, Avg_Score -120.26323576939843\n",
      "Adding trajectory to replay buffer: step 18395, counter 293895\n",
      "Environment 14: Episode 4109, Score -115.61456718241361, Avg_Score -120.198110310459\n",
      "Adding trajectory to replay buffer: step 18406, counter 293945\n",
      "Environment 3: Episode 4110, Score -120.01368853854326, Avg_Score -120.165275104688\n",
      "Adding trajectory to replay buffer: step 18411, counter 293988\n",
      "Environment 7: Episode 4111, Score -122.38030593999724, Avg_Score -120.18514472252396\n",
      "Adding trajectory to replay buffer: step 18416, counter 294049\n",
      "Environment 2: Episode 4112, Score -114.58896706476808, Avg_Score -120.1006900448315\n",
      "Adding trajectory to replay buffer: step 18416, counter 294093\n",
      "Environment 8: Episode 4113, Score -122.59355788168683, Avg_Score -120.09495223090236\n",
      "Adding trajectory to replay buffer: step 18417, counter 294137\n",
      "Environment 0: Episode 4114, Score -123.16800710131167, Avg_Score -120.09805985207738\n",
      "Adding trajectory to replay buffer: step 18417, counter 294199\n",
      "Environment 10: Episode 4115, Score -120.55348936001212, Avg_Score -120.077968198712\n",
      "Adding trajectory to replay buffer: step 18421, counter 294247\n",
      "Environment 4: Episode 4116, Score -119.12169183201095, Avg_Score -120.04864554917128\n",
      "Adding trajectory to replay buffer: step 18421, counter 294292\n",
      "Environment 6: Episode 4117, Score -122.5428515137254, Avg_Score -120.05929082473847\n",
      "Adding trajectory to replay buffer: step 18426, counter 294336\n",
      "Environment 1: Episode 4118, Score -120.24154434031, Avg_Score -120.03880876833375\n",
      "Adding trajectory to replay buffer: step 18431, counter 294379\n",
      "Environment 15: Episode 4119, Score -123.71130499317621, Avg_Score -120.11055639795599\n",
      "Adding trajectory to replay buffer: step 18433, counter 294425\n",
      "Environment 12: Episode 4120, Score -121.71477203885838, Avg_Score -120.11651926383558\n",
      "Adding trajectory to replay buffer: step 18434, counter 294472\n",
      "Environment 5: Episode 4121, Score -119.52470089822387, Avg_Score -120.16130337933646\n",
      "Adding trajectory to replay buffer: step 18434, counter 294518\n",
      "Environment 11: Episode 4122, Score -122.0017650141896, Avg_Score -120.15927521631944\n",
      "Adding trajectory to replay buffer: step 18436, counter 294563\n",
      "Environment 9: Episode 4123, Score -119.03471948279906, Avg_Score -120.1285088463\n",
      "Adding trajectory to replay buffer: step 18440, counter 294608\n",
      "Environment 14: Episode 4124, Score -122.04427002622684, Avg_Score -120.13716961641857\n",
      "Adding trajectory to replay buffer: step 18451, counter 294653\n",
      "Environment 3: Episode 4125, Score -120.16135386119782, Avg_Score -120.13011535353084\n",
      "Adding trajectory to replay buffer: step 18455, counter 294697\n",
      "Environment 7: Episode 4126, Score -121.94807813929766, Avg_Score -120.13548521082899\n",
      "Adding trajectory to replay buffer: step 18460, counter 294741\n",
      "Environment 2: Episode 4127, Score -120.6821310513206, Avg_Score -120.1259908299531\n",
      "Adding trajectory to replay buffer: step 18460, counter 294785\n",
      "Environment 8: Episode 4128, Score -122.40603492183983, Avg_Score -120.13581852750252\n",
      "Adding trajectory to replay buffer: step 18461, counter 294829\n",
      "Environment 0: Episode 4129, Score -122.76086111855895, Avg_Score -120.14232896276117\n",
      "Adding trajectory to replay buffer: step 18462, counter 294874\n",
      "Environment 10: Episode 4130, Score -120.76464237748024, Avg_Score -120.13633931491357\n",
      "Adding trajectory to replay buffer: step 18465, counter 294918\n",
      "Environment 4: Episode 4131, Score -121.63501799440509, Avg_Score -120.21688946235312\n",
      "Adding trajectory to replay buffer: step 18465, counter 294962\n",
      "Environment 6: Episode 4132, Score -121.91711741684627, Avg_Score -120.2110750680401\n",
      "Adding trajectory to replay buffer: step 18471, counter 295007\n",
      "Environment 1: Episode 4133, Score -119.25871955337252, Avg_Score -120.19937280404571\n",
      "Adding trajectory to replay buffer: step 18478, counter 295052\n",
      "Environment 12: Episode 4134, Score -117.46152846494441, Avg_Score -120.17247873464729\n",
      "Adding trajectory to replay buffer: step 18480, counter 295098\n",
      "Environment 5: Episode 4135, Score -120.55209634940884, Avg_Score -120.13868064475827\n",
      "Adding trajectory to replay buffer: step 18483, counter 295145\n",
      "Environment 9: Episode 4136, Score -115.08983577661341, Avg_Score -120.07905194148036\n",
      "Adding trajectory to replay buffer: step 18494, counter 295188\n",
      "Environment 3: Episode 4137, Score -115.02324686862032, Avg_Score -119.99630094026878\n",
      "Adding trajectory to replay buffer: step 18499, counter 295232\n",
      "Environment 7: Episode 4138, Score -121.05990517533385, Avg_Score -119.993272769736\n",
      "Adding trajectory to replay buffer: step 18501, counter 295293\n",
      "Environment 14: Episode 4139, Score -115.14755828899705, Avg_Score -119.94205339909635\n",
      "Adding trajectory to replay buffer: step 18504, counter 295337\n",
      "Environment 2: Episode 4140, Score -119.72090717454566, Avg_Score -119.92281223744426\n",
      "Adding trajectory to replay buffer: step 18505, counter 295382\n",
      "Environment 8: Episode 4141, Score -121.76573116407482, Avg_Score -119.91243353372674\n",
      "Adding trajectory to replay buffer: step 18510, counter 295427\n",
      "Environment 4: Episode 4142, Score -118.72561057418089, Avg_Score -119.87043059566486\n",
      "Adding trajectory to replay buffer: step 18510, counter 295472\n",
      "Environment 6: Episode 4143, Score -121.19768638416969, Avg_Score -119.85045512747651\n",
      "Adding trajectory to replay buffer: step 18515, counter 295516\n",
      "Environment 1: Episode 4144, Score -121.83538855379386, Avg_Score -119.85386138081356\n",
      "Adding trajectory to replay buffer: step 18522, counter 295576\n",
      "Environment 10: Episode 4145, Score -116.27146429517555, Avg_Score -119.78739317214517\n",
      "Adding trajectory to replay buffer: step 18522, counter 295620\n",
      "Environment 12: Episode 4146, Score -122.46608581520368, Avg_Score -119.77801635125454\n",
      "Adding trajectory to replay buffer: step 18523, counter 295663\n",
      "Environment 5: Episode 4147, Score -123.7219260478411, Avg_Score -119.79030775355473\n",
      "Adding trajectory to replay buffer: step 18523, counter 295752\n",
      "Environment 11: Episode 4148, Score -121.64089208227955, Avg_Score -119.8499029356398\n",
      "Adding trajectory to replay buffer: step 18523, counter 296026\n",
      "Environment 13: Episode 4149, Score -131.90501422666787, Avg_Score -119.98677144692142\n",
      "Adding trajectory to replay buffer: step 18526, counter 296069\n",
      "Environment 9: Episode 4150, Score -122.52715839363137, Avg_Score -120.03703776933658\n",
      "Adding trajectory to replay buffer: step 18541, counter 296116\n",
      "Environment 3: Episode 4151, Score -120.308797622066, Avg_Score -120.01322052937411\n",
      "Adding trajectory to replay buffer: step 18542, counter 296159\n",
      "Environment 7: Episode 4152, Score -121.26785847212375, Avg_Score -119.95087941632828\n",
      "Adding trajectory to replay buffer: step 18544, counter 296202\n",
      "Environment 14: Episode 4153, Score -122.3023282916954, Avg_Score -119.94514660853676\n",
      "Adding trajectory to replay buffer: step 18548, counter 296246\n",
      "Environment 2: Episode 4154, Score -123.03311705460771, Avg_Score -119.96143725593551\n",
      "Adding trajectory to replay buffer: step 18548, counter 296289\n",
      "Environment 8: Episode 4155, Score -122.4893469552373, Avg_Score -119.96151213048842\n",
      "Adding trajectory to replay buffer: step 18553, counter 296332\n",
      "Environment 4: Episode 4156, Score -116.65501163427035, Avg_Score -119.9600973097509\n",
      "Adding trajectory to replay buffer: step 18554, counter 296376\n",
      "Environment 6: Episode 4157, Score -117.06119926502804, Avg_Score -119.95553797632846\n",
      "Adding trajectory to replay buffer: step 18554, counter 296499\n",
      "Environment 15: Episode 4158, Score -114.73503609839952, Avg_Score -119.95402649916315\n",
      "Adding trajectory to replay buffer: step 18560, counter 296544\n",
      "Environment 1: Episode 4159, Score -121.67639450580006, Avg_Score -120.04198667065721\n",
      "Adding trajectory to replay buffer: step 18566, counter 296587\n",
      "Environment 5: Episode 4160, Score -122.79153161202123, Avg_Score -120.03786191739357\n",
      "Adding trajectory to replay buffer: step 18566, counter 296631\n",
      "Environment 10: Episode 4161, Score -121.56694671759257, Avg_Score -120.03386518222933\n",
      "Adding trajectory to replay buffer: step 18567, counter 296675\n",
      "Environment 13: Episode 4162, Score -122.95491912355894, Avg_Score -120.0500480150003\n",
      "Adding trajectory to replay buffer: step 18568, counter 296721\n",
      "Environment 12: Episode 4163, Score -120.59470310865342, Avg_Score -120.08844262717354\n",
      "Adding trajectory to replay buffer: step 18570, counter 296768\n",
      "Environment 11: Episode 4164, Score -121.01590720826822, Avg_Score -120.1116771641627\n",
      "Adding trajectory to replay buffer: step 18571, counter 296813\n",
      "Environment 9: Episode 4165, Score -121.41418925635578, Avg_Score -120.10550037269785\n",
      "Adding trajectory to replay buffer: step 18577, counter 296929\n",
      "Environment 0: Episode 4166, Score -114.64727605191806, Avg_Score -120.0417006099089\n",
      "Adding trajectory to replay buffer: step 18587, counter 296974\n",
      "Environment 7: Episode 4167, Score -122.01039131304188, Avg_Score -120.09116806616133\n",
      "Adding trajectory to replay buffer: step 18587, counter 297017\n",
      "Environment 14: Episode 4168, Score -122.5550351634156, Avg_Score -120.10000767994278\n",
      "Adding trajectory to replay buffer: step 18596, counter 297060\n",
      "Environment 4: Episode 4169, Score -121.65323470706679, Avg_Score -120.09736334252412\n",
      "Adding trajectory to replay buffer: step 18597, counter 297103\n",
      "Environment 6: Episode 4170, Score -115.71108064983227, Avg_Score -120.10249856923537\n",
      "Adding trajectory to replay buffer: step 18600, counter 297149\n",
      "Environment 15: Episode 4171, Score -120.6112029491421, Avg_Score -120.16738198184083\n",
      "Adding trajectory to replay buffer: step 18610, counter 297193\n",
      "Environment 10: Episode 4172, Score -115.40221210200775, Avg_Score -120.11012102090135\n",
      "Adding trajectory to replay buffer: step 18611, counter 297238\n",
      "Environment 5: Episode 4173, Score -121.52936970506856, Avg_Score -120.17685799105453\n",
      "Adding trajectory to replay buffer: step 18615, counter 297282\n",
      "Environment 9: Episode 4174, Score -116.39085588118682, Avg_Score -120.16843397052325\n",
      "Adding trajectory to replay buffer: step 18616, counter 297357\n",
      "Environment 3: Episode 4175, Score -113.41013922424676, Avg_Score -120.11822802235754\n",
      "Adding trajectory to replay buffer: step 18618, counter 297427\n",
      "Environment 2: Episode 4176, Score -117.31134800213638, Avg_Score -120.11904113553274\n",
      "Adding trajectory to replay buffer: step 18624, counter 297481\n",
      "Environment 11: Episode 4177, Score -118.03987626901767, Avg_Score -120.0777885149342\n",
      "Adding trajectory to replay buffer: step 18635, counter 297539\n",
      "Environment 0: Episode 4178, Score -116.33339773042066, Avg_Score -120.07934642269248\n",
      "Adding trajectory to replay buffer: step 18639, counter 297591\n",
      "Environment 7: Episode 4179, Score -118.27037207905451, Avg_Score -120.14104632329773\n",
      "Adding trajectory to replay buffer: step 18643, counter 297637\n",
      "Environment 6: Episode 4180, Score -118.35491209970415, Avg_Score -120.10404373123637\n",
      "Adding trajectory to replay buffer: step 18643, counter 297680\n",
      "Environment 15: Episode 4181, Score -123.43464314902945, Avg_Score -120.20241398956678\n",
      "Adding trajectory to replay buffer: step 18649, counter 297762\n",
      "Environment 13: Episode 4182, Score -118.66744839978591, Avg_Score -120.22432217932986\n",
      "Adding trajectory to replay buffer: step 18651, counter 297845\n",
      "Environment 12: Episode 4183, Score -109.90922904581764, Avg_Score -120.16557735606273\n",
      "Adding trajectory to replay buffer: step 18652, counter 297887\n",
      "Environment 10: Episode 4184, Score -122.81672804109566, Avg_Score -120.22739632062947\n",
      "Adding trajectory to replay buffer: step 18655, counter 297931\n",
      "Environment 5: Episode 4185, Score -122.87869772598893, Avg_Score -120.24183760572792\n",
      "Adding trajectory to replay buffer: step 18658, counter 297974\n",
      "Environment 9: Episode 4186, Score -122.4356056455498, Avg_Score -120.23800363396599\n",
      "Adding trajectory to replay buffer: step 18659, counter 298017\n",
      "Environment 3: Episode 4187, Score -122.53307672921693, Avg_Score -120.24592777921798\n",
      "Adding trajectory to replay buffer: step 18660, counter 298129\n",
      "Environment 8: Episode 4188, Score -113.78971919867583, Avg_Score -120.27256076254544\n",
      "Adding trajectory to replay buffer: step 18662, counter 298173\n",
      "Environment 2: Episode 4189, Score -122.40823029551346, Avg_Score -120.32677698394158\n",
      "Adding trajectory to replay buffer: step 18668, counter 298245\n",
      "Environment 4: Episode 4190, Score -120.12241521937526, Avg_Score -120.29904484018917\n",
      "Adding trajectory to replay buffer: step 18668, counter 298289\n",
      "Environment 11: Episode 4191, Score -122.91967539268111, Avg_Score -120.32189562669016\n",
      "Adding trajectory to replay buffer: step 18672, counter 298401\n",
      "Environment 1: Episode 4192, Score -114.42171520836621, Avg_Score -120.23907988688421\n",
      "Adding trajectory to replay buffer: step 18678, counter 298444\n",
      "Environment 0: Episode 4193, Score -123.27664955647415, Avg_Score -120.23671284513733\n",
      "Adding trajectory to replay buffer: step 18682, counter 298487\n",
      "Environment 7: Episode 4194, Score -122.39016642256703, Avg_Score -120.22726651018354\n",
      "Adding trajectory to replay buffer: step 18687, counter 298531\n",
      "Environment 6: Episode 4195, Score -121.84190983025047, Avg_Score -120.22980601514595\n",
      "Adding trajectory to replay buffer: step 18688, counter 298576\n",
      "Environment 15: Episode 4196, Score -119.32804199355778, Avg_Score -120.21975367682828\n",
      "Adding trajectory to replay buffer: step 18694, counter 298621\n",
      "Environment 13: Episode 4197, Score -121.52472866301486, Avg_Score -120.20163011631229\n",
      "Adding trajectory to replay buffer: step 18695, counter 298664\n",
      "Environment 10: Episode 4198, Score -122.41478716553003, Avg_Score -120.20579252583283\n",
      "Adding trajectory to replay buffer: step 18696, counter 298709\n",
      "Environment 12: Episode 4199, Score -120.8582562019353, Avg_Score -120.21297289638595\n",
      "Adding trajectory to replay buffer: step 18701, counter 298823\n",
      "Environment 14: Episode 4200, Score -112.79664412842071, Avg_Score -120.11048337367988\n",
      "Adding trajectory to replay buffer: step 18702, counter 298870\n",
      "Environment 5: Episode 4201, Score -120.31585291214536, Avg_Score -120.0829185251457\n",
      "Adding trajectory to replay buffer: step 18705, counter 298915\n",
      "Environment 8: Episode 4202, Score -122.6809062480672, Avg_Score -120.07696384576607\n",
      "Adding trajectory to replay buffer: step 18706, counter 298962\n",
      "Environment 3: Episode 4203, Score -123.15573987039676, Avg_Score -120.08376193050825\n",
      "Adding trajectory to replay buffer: step 18710, counter 299004\n",
      "Environment 11: Episode 4204, Score -122.6591600630482, Avg_Score -120.17542654783969\n",
      "Adding trajectory to replay buffer: step 18711, counter 299047\n",
      "Environment 4: Episode 4205, Score -123.63605996579616, Avg_Score -120.18746498984412\n",
      "Adding trajectory to replay buffer: step 18717, counter 299092\n",
      "Environment 1: Episode 4206, Score -118.42592146072661, Avg_Score -120.14636727584589\n",
      "Adding trajectory to replay buffer: step 18719, counter 299149\n",
      "Environment 2: Episode 4207, Score -113.46653663917569, Avg_Score -120.10807406056463\n",
      "Adding trajectory to replay buffer: step 18730, counter 299192\n",
      "Environment 6: Episode 4208, Score -122.4626023489628, Avg_Score -120.16165333652161\n",
      "Adding trajectory to replay buffer: step 18731, counter 299235\n",
      "Environment 15: Episode 4209, Score -122.1661327492067, Avg_Score -120.22716899218955\n",
      "Adding trajectory to replay buffer: step 18739, counter 299280\n",
      "Environment 13: Episode 4210, Score -122.11915871013515, Avg_Score -120.24822369390547\n",
      "Adding trajectory to replay buffer: step 18740, counter 299338\n",
      "Environment 7: Episode 4211, Score -115.33772009837503, Avg_Score -120.17779783548922\n",
      "Adding trajectory to replay buffer: step 18740, counter 299383\n",
      "Environment 10: Episode 4212, Score -119.91239220753188, Avg_Score -120.23103208691687\n",
      "Adding trajectory to replay buffer: step 18740, counter 299427\n",
      "Environment 12: Episode 4213, Score -122.58153610501626, Avg_Score -120.23091186915013\n",
      "Adding trajectory to replay buffer: step 18745, counter 299470\n",
      "Environment 5: Episode 4214, Score -117.14120993354172, Avg_Score -120.17064389747243\n",
      "Adding trajectory to replay buffer: step 18747, counter 299516\n",
      "Environment 14: Episode 4215, Score -121.34413934710497, Avg_Score -120.17855039734337\n",
      "Adding trajectory to replay buffer: step 18749, counter 299559\n",
      "Environment 3: Episode 4216, Score -121.00625561823323, Avg_Score -120.19739603520557\n",
      "Adding trajectory to replay buffer: step 18749, counter 299603\n",
      "Environment 8: Episode 4217, Score -120.930783846071, Avg_Score -120.18127535852904\n",
      "Adding trajectory to replay buffer: step 18754, counter 299647\n",
      "Environment 11: Episode 4218, Score -122.93946520899857, Avg_Score -120.20825456721593\n",
      "Adding trajectory to replay buffer: step 18757, counter 299693\n",
      "Environment 4: Episode 4219, Score -121.04638864279539, Avg_Score -120.18160540371213\n",
      "Adding trajectory to replay buffer: step 18765, counter 299739\n",
      "Environment 2: Episode 4220, Score -122.01250899451847, Avg_Score -120.1845827732687\n",
      "Adding trajectory to replay buffer: step 18773, counter 299782\n",
      "Environment 6: Episode 4221, Score -123.10470745768461, Avg_Score -120.22038283886333\n",
      "Adding trajectory to replay buffer: step 18774, counter 299825\n",
      "Environment 15: Episode 4222, Score -122.79264990243627, Avg_Score -120.22829168774581\n",
      "Adding trajectory to replay buffer: step 18786, counter 299871\n",
      "Environment 7: Episode 4223, Score -121.85792811654423, Avg_Score -120.25652377408325\n",
      "Adding trajectory to replay buffer: step 18787, counter 299918\n",
      "Environment 10: Episode 4224, Score -122.85512357179572, Avg_Score -120.26463230953895\n",
      "Adding trajectory to replay buffer: step 18791, counter 299969\n",
      "Environment 12: Episode 4225, Score -116.82289225063853, Avg_Score -120.23124769343335\n",
      "Adding trajectory to replay buffer: step 18797, counter 300088\n",
      "Environment 0: Episode 4226, Score -122.8334448304105, Avg_Score -120.24010136034445\n",
      "Adding trajectory to replay buffer: step 18797, counter 300131\n",
      "Environment 11: Episode 4227, Score -122.97536561568019, Avg_Score -120.26303370598808\n",
      "Adding trajectory to replay buffer: step 18799, counter 300191\n",
      "Environment 13: Episode 4228, Score -112.38589628525513, Avg_Score -120.16283231962221\n",
      "Adding trajectory to replay buffer: step 18801, counter 300235\n",
      "Environment 4: Episode 4229, Score -121.9031566916493, Avg_Score -120.1542552753531\n",
      "Adding trajectory to replay buffer: step 18804, counter 300294\n",
      "Environment 5: Episode 4230, Score -116.29533810311618, Avg_Score -120.10956223260952\n",
      "Adding trajectory to replay buffer: step 18805, counter 300350\n",
      "Environment 3: Episode 4231, Score -115.10626673975649, Avg_Score -120.04427472006302\n",
      "Adding trajectory to replay buffer: step 18807, counter 300392\n",
      "Environment 2: Episode 4232, Score -122.7844321542581, Avg_Score -120.05294786743713\n",
      "Adding trajectory to replay buffer: step 18811, counter 300545\n",
      "Environment 9: Episode 4233, Score -122.94789642653178, Avg_Score -120.08983963616875\n",
      "Adding trajectory to replay buffer: step 18814, counter 300586\n",
      "Environment 6: Episode 4234, Score -117.27970373809896, Avg_Score -120.08802138890029\n",
      "Adding trajectory to replay buffer: step 18823, counter 300635\n",
      "Environment 15: Episode 4235, Score -120.637271644447, Avg_Score -120.08887314185067\n",
      "Adding trajectory to replay buffer: step 18830, counter 300679\n",
      "Environment 7: Episode 4236, Score -122.47032468549659, Avg_Score -120.16267803093952\n",
      "Adding trajectory to replay buffer: step 18832, counter 300724\n",
      "Environment 10: Episode 4237, Score -122.3003508052764, Avg_Score -120.23544907030606\n",
      "Adding trajectory to replay buffer: step 18832, counter 300809\n",
      "Environment 14: Episode 4238, Score -124.76095613917336, Avg_Score -120.27245957994444\n",
      "Adding trajectory to replay buffer: step 18836, counter 300854\n",
      "Environment 12: Episode 4239, Score -122.3455152949011, Avg_Score -120.34443915000347\n",
      "Adding trajectory to replay buffer: step 18840, counter 300897\n",
      "Environment 0: Episode 4240, Score -122.32261919320189, Avg_Score -120.37045627019005\n",
      "Adding trajectory to replay buffer: step 18840, counter 300940\n",
      "Environment 11: Episode 4241, Score -122.37023295141447, Avg_Score -120.37650128806342\n",
      "Adding trajectory to replay buffer: step 18842, counter 300983\n",
      "Environment 13: Episode 4242, Score -122.26348207800959, Avg_Score -120.4118800031017\n",
      "Adding trajectory to replay buffer: step 18845, counter 301027\n",
      "Environment 4: Episode 4243, Score -120.63727396143724, Avg_Score -120.40627587887438\n",
      "Adding trajectory to replay buffer: step 18849, counter 301072\n",
      "Environment 5: Episode 4244, Score -122.64605577172216, Avg_Score -120.41438255105368\n",
      "Adding trajectory to replay buffer: step 18850, counter 301117\n",
      "Environment 3: Episode 4245, Score -122.30591964378519, Avg_Score -120.47472710453978\n",
      "Adding trajectory to replay buffer: step 18857, counter 301163\n",
      "Environment 9: Episode 4246, Score -118.90970499815295, Avg_Score -120.43916329636927\n",
      "Adding trajectory to replay buffer: step 18860, counter 301209\n",
      "Environment 6: Episode 4247, Score -123.28004079930237, Avg_Score -120.43474444388389\n",
      "Adding trajectory to replay buffer: step 18867, counter 301253\n",
      "Environment 15: Episode 4248, Score -122.17137058843115, Avg_Score -120.44004922894537\n",
      "Adding trajectory to replay buffer: step 18872, counter 301295\n",
      "Environment 7: Episode 4249, Score -123.14239617208702, Avg_Score -120.35242304839956\n",
      "Adding trajectory to replay buffer: step 18875, counter 301338\n",
      "Environment 14: Episode 4250, Score -122.88440147596909, Avg_Score -120.35599547922294\n",
      "Adding trajectory to replay buffer: step 18877, counter 301383\n",
      "Environment 10: Episode 4251, Score -123.05976023711762, Avg_Score -120.38350510537346\n",
      "Adding trajectory to replay buffer: step 18879, counter 301426\n",
      "Environment 12: Episode 4252, Score -123.16599807112726, Avg_Score -120.4024865013635\n",
      "Adding trajectory to replay buffer: step 18881, counter 301590\n",
      "Environment 1: Episode 4253, Score -118.29213607751453, Avg_Score -120.36238457922168\n",
      "Adding trajectory to replay buffer: step 18883, counter 301633\n",
      "Environment 0: Episode 4254, Score -123.50447351956367, Avg_Score -120.36709814387126\n",
      "Adding trajectory to replay buffer: step 18883, counter 301676\n",
      "Environment 11: Episode 4255, Score -123.46302538057851, Avg_Score -120.37683492812468\n",
      "Adding trajectory to replay buffer: step 18887, counter 301721\n",
      "Environment 13: Episode 4256, Score -120.52324144083727, Avg_Score -120.41551722619035\n",
      "Adding trajectory to replay buffer: step 18889, counter 301765\n",
      "Environment 4: Episode 4257, Score -122.29979463194559, Avg_Score -120.46790317985953\n",
      "Adding trajectory to replay buffer: step 18895, counter 301811\n",
      "Environment 5: Episode 4258, Score -121.70380919431832, Avg_Score -120.5375909108187\n",
      "Adding trajectory to replay buffer: step 18904, counter 301865\n",
      "Environment 3: Episode 4259, Score -114.39962006891395, Avg_Score -120.46482316644986\n",
      "Adding trajectory to replay buffer: step 18907, counter 301915\n",
      "Environment 9: Episode 4260, Score -120.53115213795915, Avg_Score -120.44221937170921\n",
      "Adding trajectory to replay buffer: step 18908, counter 301963\n",
      "Environment 6: Episode 4261, Score -118.07615045910391, Avg_Score -120.40731140912436\n",
      "Adding trajectory to replay buffer: step 18915, counter 302011\n",
      "Environment 15: Episode 4262, Score -121.24008364153343, Avg_Score -120.39016305430408\n",
      "Adding trajectory to replay buffer: step 18921, counter 302055\n",
      "Environment 10: Episode 4263, Score -119.11633464104, Avg_Score -120.37537936962794\n",
      "Adding trajectory to replay buffer: step 18924, counter 302100\n",
      "Environment 12: Episode 4264, Score -122.25777066594674, Avg_Score -120.38779800420473\n",
      "Adding trajectory to replay buffer: step 18928, counter 302147\n",
      "Environment 1: Episode 4265, Score -120.45206684271372, Avg_Score -120.3781767800683\n",
      "Adding trajectory to replay buffer: step 18934, counter 302192\n",
      "Environment 4: Episode 4266, Score -121.356489635547, Avg_Score -120.44526891590456\n",
      "Adding trajectory to replay buffer: step 18934, counter 302239\n",
      "Environment 13: Episode 4267, Score -120.92713757196628, Avg_Score -120.43443637849381\n",
      "Adding trajectory to replay buffer: step 18940, counter 302307\n",
      "Environment 7: Episode 4268, Score -114.8792525925748, Avg_Score -120.3576785527854\n",
      "Adding trajectory to replay buffer: step 18941, counter 302353\n",
      "Environment 5: Episode 4269, Score -120.84575069358324, Avg_Score -120.34960371265055\n",
      "Adding trajectory to replay buffer: step 18943, counter 302421\n",
      "Environment 14: Episode 4270, Score -114.37162377461891, Avg_Score -120.33620914389843\n",
      "Adding trajectory to replay buffer: step 18947, counter 302464\n",
      "Environment 3: Episode 4271, Score -122.26915997029926, Avg_Score -120.35278871410998\n",
      "Adding trajectory to replay buffer: step 18949, counter 302664\n",
      "Environment 8: Episode 4272, Score -126.10719905684205, Avg_Score -120.45983858365832\n",
      "Adding trajectory to replay buffer: step 18951, counter 302707\n",
      "Environment 6: Episode 4273, Score -117.30577906032526, Avg_Score -120.4176026772109\n",
      "Adding trajectory to replay buffer: step 18951, counter 302751\n",
      "Environment 9: Episode 4274, Score -122.6173072412815, Avg_Score -120.47986719081186\n",
      "Adding trajectory to replay buffer: step 18959, counter 302795\n",
      "Environment 15: Episode 4275, Score -121.65242889425468, Avg_Score -120.56229008751193\n",
      "Adding trajectory to replay buffer: step 18965, counter 302839\n",
      "Environment 10: Episode 4276, Score -122.27039492811883, Avg_Score -120.61188055677175\n",
      "Adding trajectory to replay buffer: step 18968, counter 302883\n",
      "Environment 12: Episode 4277, Score -121.77599909748014, Avg_Score -120.64924178505636\n",
      "Adding trajectory to replay buffer: step 18972, counter 302927\n",
      "Environment 1: Episode 4278, Score -122.38028328284683, Avg_Score -120.70971064058061\n",
      "Adding trajectory to replay buffer: step 18980, counter 302973\n",
      "Environment 4: Episode 4279, Score -121.54665672881777, Avg_Score -120.74247348707827\n",
      "Adding trajectory to replay buffer: step 18982, counter 303021\n",
      "Environment 13: Episode 4280, Score -120.56521896883287, Avg_Score -120.76457655576957\n",
      "Adding trajectory to replay buffer: step 18990, counter 303068\n",
      "Environment 14: Episode 4281, Score -119.79483761759847, Avg_Score -120.72817850045529\n",
      "Adding trajectory to replay buffer: step 18991, counter 303112\n",
      "Environment 3: Episode 4282, Score -122.19709237533374, Avg_Score -120.76347494021078\n",
      "Adding trajectory to replay buffer: step 18993, counter 303154\n",
      "Environment 9: Episode 4283, Score -123.1649044190074, Avg_Score -120.89603169394265\n",
      "Adding trajectory to replay buffer: step 18994, counter 303197\n",
      "Environment 6: Episode 4284, Score -122.792493520579, Avg_Score -120.89578934873747\n",
      "Adding trajectory to replay buffer: step 18996, counter 303310\n",
      "Environment 0: Episode 4285, Score -114.08017192285074, Avg_Score -120.8078040907061\n",
      "Adding trajectory to replay buffer: step 18998, counter 303368\n",
      "Environment 7: Episode 4286, Score -113.74040250045869, Avg_Score -120.72085205925521\n",
      "Adding trajectory to replay buffer: step 18999, counter 303426\n",
      "Environment 5: Episode 4287, Score -110.90951452115178, Avg_Score -120.60461643717456\n",
      "Adding trajectory to replay buffer: step 19010, counter 303468\n",
      "Environment 12: Episode 4288, Score -123.13522299766974, Avg_Score -120.69807147516447\n",
      "Adding trajectory to replay buffer: step 19012, counter 303531\n",
      "Environment 8: Episode 4289, Score -114.0095962516889, Avg_Score -120.61408513472625\n",
      "Adding trajectory to replay buffer: step 19016, counter 303575\n",
      "Environment 1: Episode 4290, Score -121.95679134665367, Avg_Score -120.63242889599906\n",
      "Adding trajectory to replay buffer: step 19017, counter 303627\n",
      "Environment 10: Episode 4291, Score -118.86160396030856, Avg_Score -120.59184818167532\n",
      "Adding trajectory to replay buffer: step 19024, counter 303671\n",
      "Environment 4: Episode 4292, Score -116.91877775443935, Avg_Score -120.61681880713608\n",
      "Adding trajectory to replay buffer: step 19024, counter 303713\n",
      "Environment 13: Episode 4293, Score -115.692896509137, Avg_Score -120.54098127666269\n",
      "Adding trajectory to replay buffer: step 19035, counter 303758\n",
      "Environment 14: Episode 4294, Score -122.53988531051266, Avg_Score -120.54247846554216\n",
      "Adding trajectory to replay buffer: step 19036, counter 303803\n",
      "Environment 3: Episode 4295, Score -122.34500384108537, Avg_Score -120.5475094056505\n",
      "Adding trajectory to replay buffer: step 19038, counter 303848\n",
      "Environment 9: Episode 4296, Score -118.3017855162656, Avg_Score -120.53724684087757\n",
      "Adding trajectory to replay buffer: step 19041, counter 303891\n",
      "Environment 7: Episode 4297, Score -116.88149722431538, Avg_Score -120.4908145264906\n",
      "Adding trajectory to replay buffer: step 19042, counter 304050\n",
      "Environment 11: Episode 4298, Score -119.05067447180426, Avg_Score -120.45717339955333\n",
      "Adding trajectory to replay buffer: step 19046, counter 304097\n",
      "Environment 5: Episode 4299, Score -115.94903241844041, Avg_Score -120.40808116171839\n",
      "Adding trajectory to replay buffer: step 19059, counter 304140\n",
      "Environment 1: Episode 4300, Score -114.99099436291618, Avg_Score -120.43002466406335\n",
      "Adding trajectory to replay buffer: step 19059, counter 304189\n",
      "Environment 12: Episode 4301, Score -119.80641834793674, Avg_Score -120.42493031842126\n",
      "Adding trajectory to replay buffer: step 19067, counter 304449\n",
      "Environment 2: Episode 4302, Score -128.4526218496695, Avg_Score -120.48264747443727\n",
      "Adding trajectory to replay buffer: step 19068, counter 304500\n",
      "Environment 10: Episode 4303, Score -119.90031052122451, Avg_Score -120.45009318094554\n",
      "Adding trajectory to replay buffer: step 19070, counter 304558\n",
      "Environment 8: Episode 4304, Score -113.28634326762295, Avg_Score -120.3563650129913\n",
      "Adding trajectory to replay buffer: step 19070, counter 304604\n",
      "Environment 13: Episode 4305, Score -122.12953115307882, Avg_Score -120.34129972486413\n",
      "Adding trajectory to replay buffer: step 19071, counter 304651\n",
      "Environment 4: Episode 4306, Score -122.51781397852271, Avg_Score -120.38221865004209\n",
      "Adding trajectory to replay buffer: step 19075, counter 304767\n",
      "Environment 15: Episode 4307, Score -116.98732484819615, Avg_Score -120.41742653213228\n",
      "Adding trajectory to replay buffer: step 19081, counter 304852\n",
      "Environment 0: Episode 4308, Score -115.186772653707, Avg_Score -120.34466823517974\n",
      "Adding trajectory to replay buffer: step 19081, counter 304897\n",
      "Environment 3: Episode 4309, Score -118.5333874509673, Avg_Score -120.30834078219732\n",
      "Adding trajectory to replay buffer: step 19081, counter 304943\n",
      "Environment 14: Episode 4310, Score -121.50134721921074, Avg_Score -120.30216266728809\n",
      "Adding trajectory to replay buffer: step 19084, counter 305033\n",
      "Environment 6: Episode 4311, Score -122.1439201098696, Avg_Score -120.37022466740302\n",
      "Adding trajectory to replay buffer: step 19088, counter 305080\n",
      "Environment 7: Episode 4312, Score -111.52705861857409, Avg_Score -120.28637133151345\n",
      "Adding trajectory to replay buffer: step 19090, counter 305124\n",
      "Environment 5: Episode 4313, Score -118.89848390121324, Avg_Score -120.2495408094754\n",
      "Adding trajectory to replay buffer: step 19102, counter 305184\n",
      "Environment 11: Episode 4314, Score -120.16180813728646, Avg_Score -120.27974679151286\n",
      "Adding trajectory to replay buffer: step 19103, counter 305249\n",
      "Environment 9: Episode 4315, Score -121.12464693045897, Avg_Score -120.2775518673464\n",
      "Adding trajectory to replay buffer: step 19112, counter 305291\n",
      "Environment 8: Episode 4316, Score -122.69720997479311, Avg_Score -120.29446141091199\n",
      "Adding trajectory to replay buffer: step 19112, counter 305344\n",
      "Environment 12: Episode 4317, Score -122.58326118305513, Avg_Score -120.31098618428183\n",
      "Adding trajectory to replay buffer: step 19113, counter 305386\n",
      "Environment 4: Episode 4318, Score -115.83621495969842, Avg_Score -120.23995368178885\n",
      "Adding trajectory to replay buffer: step 19114, counter 305432\n",
      "Environment 10: Episode 4319, Score -119.29866496226998, Avg_Score -120.2224764449836\n",
      "Adding trajectory to replay buffer: step 19114, counter 305476\n",
      "Environment 13: Episode 4320, Score -122.6732347391074, Avg_Score -120.2290837024295\n",
      "Adding trajectory to replay buffer: step 19117, counter 305534\n",
      "Environment 1: Episode 4321, Score -116.7068576344351, Avg_Score -120.165105204197\n",
      "Adding trajectory to replay buffer: step 19119, counter 305578\n",
      "Environment 15: Episode 4322, Score -121.62577194117259, Avg_Score -120.15343642458438\n",
      "Adding trajectory to replay buffer: step 19125, counter 305622\n",
      "Environment 0: Episode 4323, Score -119.18555004861392, Avg_Score -120.12671264390507\n",
      "Adding trajectory to replay buffer: step 19125, counter 305666\n",
      "Environment 3: Episode 4324, Score -117.75505899172214, Avg_Score -120.07571199810434\n",
      "Adding trajectory to replay buffer: step 19126, counter 305711\n",
      "Environment 14: Episode 4325, Score -122.18720768520546, Avg_Score -120.12935515245\n",
      "Adding trajectory to replay buffer: step 19131, counter 305775\n",
      "Environment 2: Episode 4326, Score -110.71743375995506, Avg_Score -120.00819504174545\n",
      "Adding trajectory to replay buffer: step 19134, counter 305819\n",
      "Environment 5: Episode 4327, Score -121.70648158858344, Avg_Score -119.99550620147451\n",
      "Adding trajectory to replay buffer: step 19137, counter 305868\n",
      "Environment 7: Episode 4328, Score -119.24942064296889, Avg_Score -120.06414144505165\n",
      "Adding trajectory to replay buffer: step 19144, counter 305910\n",
      "Environment 11: Episode 4329, Score -115.55154503323014, Avg_Score -120.00062532846744\n",
      "Adding trajectory to replay buffer: step 19148, counter 305955\n",
      "Environment 9: Episode 4330, Score -121.968426967298, Avg_Score -120.05735621710924\n",
      "Adding trajectory to replay buffer: step 19154, counter 306025\n",
      "Environment 6: Episode 4331, Score -119.49363866347882, Avg_Score -120.10122993634646\n",
      "Adding trajectory to replay buffer: step 19155, counter 306068\n",
      "Environment 8: Episode 4332, Score -118.670017125329, Avg_Score -120.06008578605717\n",
      "Adding trajectory to replay buffer: step 19156, counter 306110\n",
      "Environment 10: Episode 4333, Score -115.0838333897305, Avg_Score -119.98144515568914\n",
      "Adding trajectory to replay buffer: step 19157, counter 306155\n",
      "Environment 12: Episode 4334, Score -123.10261443489713, Avg_Score -120.03967426265713\n",
      "Adding trajectory to replay buffer: step 19159, counter 306201\n",
      "Environment 4: Episode 4335, Score -121.76039274243234, Avg_Score -120.05090547363697\n",
      "Adding trajectory to replay buffer: step 19159, counter 306246\n",
      "Environment 13: Episode 4336, Score -123.25540704496049, Avg_Score -120.05875629723161\n",
      "Adding trajectory to replay buffer: step 19162, counter 306291\n",
      "Environment 1: Episode 4337, Score -122.04204077199287, Avg_Score -120.05617319689877\n",
      "Adding trajectory to replay buffer: step 19168, counter 306334\n",
      "Environment 0: Episode 4338, Score -123.1606989156194, Avg_Score -120.04017062466322\n",
      "Adding trajectory to replay buffer: step 19168, counter 306377\n",
      "Environment 3: Episode 4339, Score -123.21093472747134, Avg_Score -120.04882481898892\n",
      "Adding trajectory to replay buffer: step 19169, counter 306420\n",
      "Environment 14: Episode 4340, Score -122.84227891173214, Avg_Score -120.05402141617424\n",
      "Adding trajectory to replay buffer: step 19171, counter 306472\n",
      "Environment 15: Episode 4341, Score -115.32104368420318, Avg_Score -119.98352952350211\n",
      "Adding trajectory to replay buffer: step 19175, counter 306516\n",
      "Environment 2: Episode 4342, Score -122.14372718371823, Avg_Score -119.9823319745592\n",
      "Adding trajectory to replay buffer: step 19179, counter 306561\n",
      "Environment 5: Episode 4343, Score -121.45828283741449, Avg_Score -119.99054206331898\n",
      "Adding trajectory to replay buffer: step 19183, counter 306607\n",
      "Environment 7: Episode 4344, Score -120.85355133799588, Avg_Score -119.97261701898171\n",
      "Adding trajectory to replay buffer: step 19188, counter 306651\n",
      "Environment 11: Episode 4345, Score -115.272382082227, Avg_Score -119.90228164336614\n",
      "Adding trajectory to replay buffer: step 19202, counter 306698\n",
      "Environment 8: Episode 4346, Score -120.66157130255303, Avg_Score -119.91980030641014\n",
      "Adding trajectory to replay buffer: step 19202, counter 306744\n",
      "Environment 10: Episode 4347, Score -120.51068524123045, Avg_Score -119.89210675082943\n",
      "Adding trajectory to replay buffer: step 19205, counter 306790\n",
      "Environment 13: Episode 4348, Score -122.4688878261329, Avg_Score -119.89508192320643\n",
      "Adding trajectory to replay buffer: step 19206, counter 306837\n",
      "Environment 4: Episode 4349, Score -118.0607668434574, Avg_Score -119.84426562992013\n",
      "Adding trajectory to replay buffer: step 19211, counter 306880\n",
      "Environment 3: Episode 4350, Score -119.93033488391899, Avg_Score -119.81472496399961\n",
      "Adding trajectory to replay buffer: step 19214, counter 306946\n",
      "Environment 9: Episode 4351, Score -111.29456317314629, Avg_Score -119.69707299335991\n",
      "Adding trajectory to replay buffer: step 19215, counter 306992\n",
      "Environment 14: Episode 4352, Score -122.01368208086056, Avg_Score -119.6855498334572\n",
      "Adding trajectory to replay buffer: step 19218, counter 307039\n",
      "Environment 15: Episode 4353, Score -120.09723526074602, Avg_Score -119.7036008252895\n",
      "Adding trajectory to replay buffer: step 19221, counter 307085\n",
      "Environment 2: Episode 4354, Score -119.21938314966857, Avg_Score -119.66074992159056\n",
      "Adding trajectory to replay buffer: step 19222, counter 307128\n",
      "Environment 5: Episode 4355, Score -122.85685659536844, Avg_Score -119.65468823373844\n",
      "Adding trajectory to replay buffer: step 19228, counter 307173\n",
      "Environment 7: Episode 4356, Score -124.0565297337373, Avg_Score -119.69002111666745\n",
      "Adding trajectory to replay buffer: step 19229, counter 307234\n",
      "Environment 0: Episode 4357, Score -116.29289122607652, Avg_Score -119.62995208260874\n",
      "Adding trajectory to replay buffer: step 19231, counter 307277\n",
      "Environment 11: Episode 4358, Score -122.97530947807121, Avg_Score -119.64266708544628\n",
      "Adding trajectory to replay buffer: step 19249, counter 307321\n",
      "Environment 13: Episode 4359, Score -121.9972524051722, Avg_Score -119.71864340880884\n",
      "Adding trajectory to replay buffer: step 19255, counter 307365\n",
      "Environment 3: Episode 4360, Score -116.37841827334277, Avg_Score -119.67711607016268\n",
      "Adding trajectory to replay buffer: step 19259, counter 307409\n",
      "Environment 14: Episode 4361, Score -122.33860085461522, Avg_Score -119.71974057411781\n",
      "Adding trajectory to replay buffer: step 19272, counter 307453\n",
      "Environment 7: Episode 4362, Score -120.48557858125989, Avg_Score -119.71219552351508\n",
      "Adding trajectory to replay buffer: step 19275, counter 307514\n",
      "Environment 9: Episode 4363, Score -113.1596957721853, Avg_Score -119.65262913482653\n",
      "Adding trajectory to replay buffer: step 19276, counter 307559\n",
      "Environment 11: Episode 4364, Score -122.21403705806533, Avg_Score -119.65219179874772\n",
      "Adding trajectory to replay buffer: step 19281, counter 307686\n",
      "Environment 6: Episode 4365, Score -125.53424711236637, Avg_Score -119.70301360144424\n",
      "Adding trajectory to replay buffer: step 19285, counter 307750\n",
      "Environment 2: Episode 4366, Score -113.31675578986915, Avg_Score -119.62261626298745\n",
      "Adding trajectory to replay buffer: step 19293, counter 307841\n",
      "Environment 10: Episode 4367, Score -121.69806124356359, Avg_Score -119.63032549970343\n",
      "Adding trajectory to replay buffer: step 19304, counter 307896\n",
      "Environment 13: Episode 4368, Score -115.43480684519, Avg_Score -119.6358810422296\n",
      "Adding trajectory to replay buffer: step 19305, counter 307942\n",
      "Environment 14: Episode 4369, Score -121.79640129937044, Avg_Score -119.64538754828749\n",
      "Adding trajectory to replay buffer: step 19317, counter 307987\n",
      "Environment 7: Episode 4370, Score -122.29954173253353, Avg_Score -119.72466672786662\n",
      "Adding trajectory to replay buffer: step 19320, counter 308032\n",
      "Environment 9: Episode 4371, Score -122.1848025950336, Avg_Score -119.72382315411396\n",
      "Adding trajectory to replay buffer: step 19320, counter 308076\n",
      "Environment 11: Episode 4372, Score -122.31933293087904, Avg_Score -119.68594449285435\n",
      "Adding trajectory to replay buffer: step 19324, counter 308119\n",
      "Environment 6: Episode 4373, Score -123.08935525537655, Avg_Score -119.74378025480485\n",
      "Adding trajectory to replay buffer: step 19339, counter 308165\n",
      "Environment 10: Episode 4374, Score -119.04671169112747, Avg_Score -119.70807429930329\n",
      "Adding trajectory to replay buffer: step 19348, counter 308209\n",
      "Environment 13: Episode 4375, Score -119.2135381161347, Avg_Score -119.68368539152212\n",
      "Adding trajectory to replay buffer: step 19349, counter 308329\n",
      "Environment 0: Episode 4376, Score -115.00968812891654, Avg_Score -119.6110783235301\n",
      "Adding trajectory to replay buffer: step 19351, counter 308375\n",
      "Environment 14: Episode 4377, Score -119.19854186120878, Avg_Score -119.58530375116742\n",
      "Adding trajectory to replay buffer: step 19353, counter 308443\n",
      "Environment 2: Episode 4378, Score -116.19426312486269, Avg_Score -119.52344354958755\n",
      "Adding trajectory to replay buffer: step 19355, counter 308576\n",
      "Environment 5: Episode 4379, Score -115.98055207482601, Avg_Score -119.46778250304766\n",
      "Adding trajectory to replay buffer: step 19363, counter 308622\n",
      "Environment 7: Episode 4380, Score -121.32020285118993, Avg_Score -119.47533234187122\n",
      "Adding trajectory to replay buffer: step 19364, counter 308666\n",
      "Environment 9: Episode 4381, Score -121.797058546658, Avg_Score -119.49535455116184\n",
      "Adding trajectory to replay buffer: step 19366, counter 308870\n",
      "Environment 1: Episode 4382, Score -124.05402882401116, Avg_Score -119.51392391564859\n",
      "Adding trajectory to replay buffer: step 19366, counter 308916\n",
      "Environment 11: Episode 4383, Score -122.60444556357277, Avg_Score -119.50831932709427\n",
      "Adding trajectory to replay buffer: step 19385, counter 308962\n",
      "Environment 10: Episode 4384, Score -116.2682951831501, Avg_Score -119.44307734371999\n",
      "Adding trajectory to replay buffer: step 19387, counter 309147\n",
      "Environment 8: Episode 4385, Score -124.33565038982545, Avg_Score -119.54563212838974\n",
      "Adding trajectory to replay buffer: step 19389, counter 309212\n",
      "Environment 6: Episode 4386, Score -116.0264191159755, Avg_Score -119.56849229454491\n",
      "Adding trajectory to replay buffer: step 19397, counter 309256\n",
      "Environment 2: Episode 4387, Score -122.44375688506625, Avg_Score -119.68383471818403\n",
      "Adding trajectory to replay buffer: step 19397, counter 309305\n",
      "Environment 13: Episode 4388, Score -117.59828731044134, Avg_Score -119.62846536131175\n",
      "Adding trajectory to replay buffer: step 19398, counter 309497\n",
      "Environment 4: Episode 4389, Score -119.28120567441236, Avg_Score -119.681181455539\n",
      "Adding trajectory to replay buffer: step 19401, counter 309543\n",
      "Environment 5: Episode 4390, Score -122.07540481769853, Avg_Score -119.68236759024943\n",
      "Adding trajectory to replay buffer: step 19408, counter 309602\n",
      "Environment 0: Episode 4391, Score -115.81168082665528, Avg_Score -119.6518683589129\n",
      "Adding trajectory to replay buffer: step 19408, counter 309646\n",
      "Environment 9: Episode 4392, Score -118.64938611643575, Avg_Score -119.66917444253288\n",
      "Adding trajectory to replay buffer: step 19412, counter 309803\n",
      "Environment 3: Episode 4393, Score -116.378659673358, Avg_Score -119.67603207417505\n",
      "Adding trajectory to replay buffer: step 19414, counter 309866\n",
      "Environment 14: Episode 4394, Score -115.8681540820406, Avg_Score -119.6093147618903\n",
      "Adding trajectory to replay buffer: step 19430, counter 309930\n",
      "Environment 1: Episode 4395, Score -118.11152844763485, Avg_Score -119.5669800079558\n",
      "Adding trajectory to replay buffer: step 19430, counter 309975\n",
      "Environment 10: Episode 4396, Score -121.50765947032038, Avg_Score -119.59903874749637\n",
      "Adding trajectory to replay buffer: step 19430, counter 310248\n",
      "Environment 12: Episode 4397, Score -128.8782339508302, Avg_Score -119.71900611476154\n",
      "Adding trajectory to replay buffer: step 19434, counter 310293\n",
      "Environment 6: Episode 4398, Score -122.26177990599598, Avg_Score -119.75111716910345\n",
      "Adding trajectory to replay buffer: step 19445, counter 310337\n",
      "Environment 5: Episode 4399, Score -120.05104546116542, Avg_Score -119.7921372995307\n",
      "Adding trajectory to replay buffer: step 19450, counter 310379\n",
      "Environment 9: Episode 4400, Score -120.76536572070545, Avg_Score -119.84988101310857\n",
      "Adding trajectory to replay buffer: step 19452, counter 310423\n",
      "Environment 0: Episode 4401, Score -121.60800815845467, Avg_Score -119.86789691121378\n",
      "Adding trajectory to replay buffer: step 19456, counter 310467\n",
      "Environment 3: Episode 4402, Score -122.19391470340764, Avg_Score -119.80530983975115\n",
      "Adding trajectory to replay buffer: step 19461, counter 310531\n",
      "Environment 2: Episode 4403, Score -112.8802277109356, Avg_Score -119.73510901164828\n",
      "Adding trajectory to replay buffer: step 19461, counter 310626\n",
      "Environment 11: Episode 4404, Score -116.10623879949686, Avg_Score -119.763307966967\n",
      "Adding trajectory to replay buffer: step 19461, counter 310673\n",
      "Environment 14: Episode 4405, Score -116.20774605202551, Avg_Score -119.70409011595645\n",
      "Adding trajectory to replay buffer: step 19468, counter 310923\n",
      "Environment 15: Episode 4406, Score -127.28432954309568, Avg_Score -119.75175527160218\n",
      "Adding trajectory to replay buffer: step 19472, counter 311032\n",
      "Environment 7: Episode 4407, Score -123.27205237748225, Avg_Score -119.81460254689505\n",
      "Adding trajectory to replay buffer: step 19476, counter 311078\n",
      "Environment 10: Episode 4408, Score -120.80849966055837, Avg_Score -119.87081981696352\n",
      "Adding trajectory to replay buffer: step 19478, counter 311122\n",
      "Environment 6: Episode 4409, Score -122.15750927881773, Avg_Score -119.90706103524204\n",
      "Adding trajectory to replay buffer: step 19488, counter 311180\n",
      "Environment 1: Episode 4410, Score -114.9843173359291, Avg_Score -119.84189073640923\n",
      "Adding trajectory to replay buffer: step 19488, counter 311270\n",
      "Environment 4: Episode 4411, Score -117.93175615373688, Avg_Score -119.79976909684791\n",
      "Adding trajectory to replay buffer: step 19497, counter 311317\n",
      "Environment 9: Episode 4412, Score -122.62803550045938, Avg_Score -119.91077886566676\n",
      "Adding trajectory to replay buffer: step 19498, counter 311363\n",
      "Environment 0: Episode 4413, Score -123.80772813898946, Avg_Score -119.95987130804451\n",
      "Adding trajectory to replay buffer: step 19502, counter 311478\n",
      "Environment 8: Episode 4414, Score -117.20866501831077, Avg_Score -119.93033987685475\n",
      "Adding trajectory to replay buffer: step 19502, counter 311550\n",
      "Environment 12: Episode 4415, Score -119.505370964348, Avg_Score -119.91414711719364\n",
      "Adding trajectory to replay buffer: step 19505, counter 311594\n",
      "Environment 2: Episode 4416, Score -122.1030950777537, Avg_Score -119.90820596822323\n",
      "Adding trajectory to replay buffer: step 19505, counter 311638\n",
      "Environment 11: Episode 4417, Score -121.73575820589936, Avg_Score -119.8997309384517\n",
      "Adding trajectory to replay buffer: step 19506, counter 311683\n",
      "Environment 14: Episode 4418, Score -122.04758621613124, Avg_Score -119.96184465101604\n",
      "Adding trajectory to replay buffer: step 19516, counter 311743\n",
      "Environment 3: Episode 4419, Score -116.15260840401861, Avg_Score -119.93038408543349\n",
      "Adding trajectory to replay buffer: step 19518, counter 311789\n",
      "Environment 7: Episode 4420, Score -118.06216215791243, Avg_Score -119.88427335962153\n",
      "Adding trajectory to replay buffer: step 19522, counter 311833\n",
      "Environment 6: Episode 4421, Score -121.04884653621788, Avg_Score -119.92769324863936\n",
      "Adding trajectory to replay buffer: step 19527, counter 311963\n",
      "Environment 13: Episode 4422, Score -117.22267599635259, Avg_Score -119.88366228919114\n",
      "Adding trajectory to replay buffer: step 19528, counter 312046\n",
      "Environment 5: Episode 4423, Score -114.77232415702007, Avg_Score -119.83953003027521\n",
      "Adding trajectory to replay buffer: step 19530, counter 312088\n",
      "Environment 1: Episode 4424, Score -123.47406991825268, Avg_Score -119.89672013954052\n",
      "Adding trajectory to replay buffer: step 19531, counter 312131\n",
      "Environment 4: Episode 4425, Score -123.0347083222388, Avg_Score -119.9051951459109\n",
      "Adding trajectory to replay buffer: step 19539, counter 312173\n",
      "Environment 9: Episode 4426, Score -116.68734759058927, Avg_Score -119.96489428421724\n",
      "Adding trajectory to replay buffer: step 19541, counter 312238\n",
      "Environment 10: Episode 4427, Score -117.5494215004171, Avg_Score -119.92332368333555\n",
      "Adding trajectory to replay buffer: step 19546, counter 312282\n",
      "Environment 12: Episode 4428, Score -122.17248454454106, Avg_Score -119.95255432235126\n",
      "Adding trajectory to replay buffer: step 19547, counter 312327\n",
      "Environment 8: Episode 4429, Score -121.96608768282023, Avg_Score -120.01669974884719\n",
      "Adding trajectory to replay buffer: step 19548, counter 312370\n",
      "Environment 2: Episode 4430, Score -122.36749373269578, Avg_Score -120.02069041650117\n",
      "Adding trajectory to replay buffer: step 19548, counter 312413\n",
      "Environment 11: Episode 4431, Score -122.17159345267899, Avg_Score -120.04746996439314\n",
      "Adding trajectory to replay buffer: step 19549, counter 312456\n",
      "Environment 14: Episode 4432, Score -122.08793361337956, Avg_Score -120.08164912927367\n",
      "Adding trajectory to replay buffer: step 19571, counter 312500\n",
      "Environment 13: Episode 4433, Score -120.61548465456013, Avg_Score -120.13696564192196\n",
      "Adding trajectory to replay buffer: step 19572, counter 312544\n",
      "Environment 5: Episode 4434, Score -121.93917175117755, Avg_Score -120.12533121508478\n",
      "Adding trajectory to replay buffer: step 19573, counter 312587\n",
      "Environment 1: Episode 4435, Score -122.6263965293703, Avg_Score -120.13399125295415\n",
      "Adding trajectory to replay buffer: step 19573, counter 312638\n",
      "Environment 6: Episode 4436, Score -118.201506121117, Avg_Score -120.08345224371573\n",
      "Adding trajectory to replay buffer: step 19579, counter 312749\n",
      "Environment 15: Episode 4437, Score -118.65017655327978, Avg_Score -120.04953360152858\n",
      "Adding trajectory to replay buffer: step 19580, counter 312813\n",
      "Environment 3: Episode 4438, Score -114.26720507353731, Avg_Score -119.96059866310776\n",
      "Adding trajectory to replay buffer: step 19588, counter 312853\n",
      "Environment 2: Episode 4439, Score -116.76337547814909, Avg_Score -119.89612307061458\n",
      "Adding trajectory to replay buffer: step 19588, counter 312902\n",
      "Environment 9: Episode 4440, Score -121.76798546597486, Avg_Score -119.885380136157\n",
      "Adding trajectory to replay buffer: step 19589, counter 312960\n",
      "Environment 4: Episode 4441, Score -112.80616705384416, Avg_Score -119.86023136985342\n",
      "Adding trajectory to replay buffer: step 19591, counter 313004\n",
      "Environment 8: Episode 4442, Score -121.8116363629407, Avg_Score -119.85691046164564\n",
      "Adding trajectory to replay buffer: step 19591, counter 313049\n",
      "Environment 12: Episode 4443, Score -120.4382323639995, Avg_Score -119.8467099569115\n",
      "Adding trajectory to replay buffer: step 19597, counter 313098\n",
      "Environment 11: Episode 4444, Score -117.08978350519389, Avg_Score -119.8090722785835\n",
      "Adding trajectory to replay buffer: step 19615, counter 313142\n",
      "Environment 13: Episode 4445, Score -123.1547957351928, Avg_Score -119.88789641511313\n",
      "Adding trajectory to replay buffer: step 19617, counter 313186\n",
      "Environment 6: Episode 4446, Score -122.59630948964912, Avg_Score -119.90724379698412\n",
      "Adding trajectory to replay buffer: step 19619, counter 313232\n",
      "Environment 1: Episode 4447, Score -122.4324095640257, Avg_Score -119.92646104021208\n",
      "Adding trajectory to replay buffer: step 19620, counter 313311\n",
      "Environment 10: Episode 4448, Score -116.9032118133735, Avg_Score -119.87080428008447\n",
      "Adding trajectory to replay buffer: step 19621, counter 313414\n",
      "Environment 7: Episode 4449, Score -112.92702320664687, Avg_Score -119.81946684371637\n",
      "Adding trajectory to replay buffer: step 19623, counter 313458\n",
      "Environment 15: Episode 4450, Score -122.55628177302518, Avg_Score -119.84572631260744\n",
      "Adding trajectory to replay buffer: step 19624, counter 313502\n",
      "Environment 3: Episode 4451, Score -122.69372032308144, Avg_Score -119.9597178841068\n",
      "Adding trajectory to replay buffer: step 19624, counter 313554\n",
      "Environment 5: Episode 4452, Score -115.18973640902527, Avg_Score -119.89147842738845\n",
      "Adding trajectory to replay buffer: step 19631, counter 313636\n",
      "Environment 14: Episode 4453, Score -120.58815694379186, Avg_Score -119.8963876442189\n",
      "Adding trajectory to replay buffer: step 19632, counter 313680\n",
      "Environment 2: Episode 4454, Score -116.5366633711656, Avg_Score -119.86956044643384\n",
      "Adding trajectory to replay buffer: step 19638, counter 313730\n",
      "Environment 9: Episode 4455, Score -118.75073313350107, Avg_Score -119.82849921181518\n",
      "Adding trajectory to replay buffer: step 19642, counter 313775\n",
      "Environment 11: Episode 4456, Score -123.23086602435014, Avg_Score -119.82024257472135\n",
      "Adding trajectory to replay buffer: step 19658, counter 313842\n",
      "Environment 8: Episode 4457, Score -115.39231791129212, Avg_Score -119.81123684157347\n",
      "Adding trajectory to replay buffer: step 19661, counter 313888\n",
      "Environment 13: Episode 4458, Score -122.05592581970618, Avg_Score -119.80204300498984\n",
      "Adding trajectory to replay buffer: step 19663, counter 313932\n",
      "Environment 1: Episode 4459, Score -117.41380192356308, Avg_Score -119.75620850017376\n",
      "Adding trajectory to replay buffer: step 19664, counter 313976\n",
      "Environment 10: Episode 4460, Score -116.30250261615961, Avg_Score -119.75544934360195\n",
      "Adding trajectory to replay buffer: step 19668, counter 314023\n",
      "Environment 7: Episode 4461, Score -118.88201141157747, Avg_Score -119.72088344917157\n",
      "Adding trajectory to replay buffer: step 19669, counter 314068\n",
      "Environment 5: Episode 4462, Score -121.88698985847893, Avg_Score -119.73489756194377\n",
      "Adding trajectory to replay buffer: step 19674, counter 314153\n",
      "Environment 4: Episode 4463, Score -113.51312731048402, Avg_Score -119.73843187732673\n",
      "Adding trajectory to replay buffer: step 19676, counter 314197\n",
      "Environment 2: Episode 4464, Score -117.17559050657896, Avg_Score -119.68804741181185\n",
      "Adding trajectory to replay buffer: step 19677, counter 314243\n",
      "Environment 14: Episode 4465, Score -122.87364472915915, Avg_Score -119.66144138797979\n",
      "Adding trajectory to replay buffer: step 19679, counter 314331\n",
      "Environment 12: Episode 4466, Score -118.26940734678196, Avg_Score -119.71096790354892\n",
      "Adding trajectory to replay buffer: step 19682, counter 314375\n",
      "Environment 9: Episode 4467, Score -122.21135947374876, Avg_Score -119.71610088585078\n",
      "Adding trajectory to replay buffer: step 19683, counter 314435\n",
      "Environment 15: Episode 4468, Score -116.0056450024123, Avg_Score -119.72180926742301\n",
      "Adding trajectory to replay buffer: step 19687, counter 314505\n",
      "Environment 6: Episode 4469, Score -115.45015013745147, Avg_Score -119.65834675580382\n",
      "Adding trajectory to replay buffer: step 19688, counter 314551\n",
      "Environment 11: Episode 4470, Score -121.27461159344887, Avg_Score -119.64809745441296\n",
      "Adding trajectory to replay buffer: step 19692, counter 314619\n",
      "Environment 3: Episode 4471, Score -113.34603951576476, Avg_Score -119.55970982362028\n",
      "Adding trajectory to replay buffer: step 19709, counter 314665\n",
      "Environment 1: Episode 4472, Score -122.1514305285352, Avg_Score -119.55803079959684\n",
      "Adding trajectory to replay buffer: step 19711, counter 314708\n",
      "Environment 7: Episode 4473, Score -123.47574994428271, Avg_Score -119.56189474648589\n",
      "Adding trajectory to replay buffer: step 19713, counter 314752\n",
      "Environment 5: Episode 4474, Score -121.93671037200777, Avg_Score -119.59079473329466\n",
      "Adding trajectory to replay buffer: step 19715, counter 314969\n",
      "Environment 0: Episode 4475, Score -122.5822706354121, Avg_Score -119.62448205848743\n",
      "Adding trajectory to replay buffer: step 19717, counter 315022\n",
      "Environment 10: Episode 4476, Score -117.03097008152928, Avg_Score -119.64469487801354\n",
      "Adding trajectory to replay buffer: step 19720, counter 315068\n",
      "Environment 4: Episode 4477, Score -121.6161732147454, Avg_Score -119.66887119154892\n",
      "Adding trajectory to replay buffer: step 19723, counter 315114\n",
      "Environment 14: Episode 4478, Score -118.13426479481471, Avg_Score -119.68827120824842\n",
      "Adding trajectory to replay buffer: step 19724, counter 315159\n",
      "Environment 12: Episode 4479, Score -122.82438575061288, Avg_Score -119.75670954500629\n",
      "Adding trajectory to replay buffer: step 19728, counter 315200\n",
      "Environment 6: Episode 4480, Score -116.13398641896248, Avg_Score -119.70484738068404\n",
      "Adding trajectory to replay buffer: step 19732, counter 315249\n",
      "Environment 15: Episode 4481, Score -117.06969223738399, Avg_Score -119.65757371759129\n",
      "Adding trajectory to replay buffer: step 19735, counter 315292\n",
      "Environment 3: Episode 4482, Score -123.36709935955705, Avg_Score -119.65070442294675\n",
      "Adding trajectory to replay buffer: step 19742, counter 315373\n",
      "Environment 13: Episode 4483, Score -122.8994672216475, Avg_Score -119.65365463952749\n",
      "Adding trajectory to replay buffer: step 19749, counter 315440\n",
      "Environment 9: Episode 4484, Score -118.04274151943872, Avg_Score -119.67139910289039\n",
      "Adding trajectory to replay buffer: step 19750, counter 315514\n",
      "Environment 2: Episode 4485, Score -113.5441976131418, Avg_Score -119.56348457512357\n",
      "Adding trajectory to replay buffer: step 19761, counter 315558\n",
      "Environment 10: Episode 4486, Score -122.1875480591498, Avg_Score -119.62509586455529\n",
      "Adding trajectory to replay buffer: step 19764, counter 315602\n",
      "Environment 4: Episode 4487, Score -123.79879162894252, Avg_Score -119.63864621199406\n",
      "Adding trajectory to replay buffer: step 19764, counter 315678\n",
      "Environment 11: Episode 4488, Score -120.42206470056064, Avg_Score -119.66688398589524\n",
      "Adding trajectory to replay buffer: step 19769, counter 315723\n",
      "Environment 12: Episode 4489, Score -122.24607162842464, Avg_Score -119.69653264543534\n",
      "Adding trajectory to replay buffer: step 19769, counter 315769\n",
      "Environment 14: Episode 4490, Score -122.20609199426384, Avg_Score -119.69783951720099\n",
      "Adding trajectory to replay buffer: step 19771, counter 315812\n",
      "Environment 6: Episode 4491, Score -123.57192116317836, Avg_Score -119.77544192056622\n",
      "Adding trajectory to replay buffer: step 19774, counter 315854\n",
      "Environment 15: Episode 4492, Score -123.30094264392989, Avg_Score -119.82195748584115\n",
      "Adding trajectory to replay buffer: step 19781, counter 315924\n",
      "Environment 7: Episode 4493, Score -119.28830115933033, Avg_Score -119.8510539007009\n",
      "Adding trajectory to replay buffer: step 19785, counter 315967\n",
      "Environment 13: Episode 4494, Score -122.1404986573166, Avg_Score -119.91377734645366\n",
      "Adding trajectory to replay buffer: step 19788, counter 316040\n",
      "Environment 0: Episode 4495, Score -117.1179781175948, Avg_Score -119.90384184315324\n",
      "Adding trajectory to replay buffer: step 19788, counter 316115\n",
      "Environment 5: Episode 4496, Score -119.81265006722883, Avg_Score -119.88689174912233\n",
      "Adding trajectory to replay buffer: step 19790, counter 316196\n",
      "Environment 1: Episode 4497, Score -114.21954741941579, Avg_Score -119.74030488380821\n",
      "Adding trajectory to replay buffer: step 19794, counter 316241\n",
      "Environment 9: Episode 4498, Score -120.36096864779665, Avg_Score -119.72129677122619\n",
      "Adding trajectory to replay buffer: step 19805, counter 316285\n",
      "Environment 10: Episode 4499, Score -122.77978263950224, Avg_Score -119.74858414300955\n",
      "Adding trajectory to replay buffer: step 19810, counter 316331\n",
      "Environment 4: Episode 4500, Score -119.0970812756922, Avg_Score -119.73190129855942\n",
      "Adding trajectory to replay buffer: step 19810, counter 316377\n",
      "Environment 11: Episode 4501, Score -119.02360212411234, Avg_Score -119.706057238216\n",
      "Adding trajectory to replay buffer: step 19812, counter 316420\n",
      "Environment 14: Episode 4502, Score -122.95151659407404, Avg_Score -119.71363325712268\n",
      "Adding trajectory to replay buffer: step 19814, counter 316463\n",
      "Environment 6: Episode 4503, Score -121.34591636924694, Avg_Score -119.7982901437058\n",
      "Adding trajectory to replay buffer: step 19814, counter 316508\n",
      "Environment 12: Episode 4504, Score -122.15011445670265, Avg_Score -119.85872890027785\n",
      "Adding trajectory to replay buffer: step 19825, counter 316583\n",
      "Environment 2: Episode 4505, Score -116.18881487674639, Avg_Score -119.85853958852505\n",
      "Adding trajectory to replay buffer: step 19832, counter 316641\n",
      "Environment 15: Episode 4506, Score -115.57446297859774, Avg_Score -119.7414409228801\n",
      "Adding trajectory to replay buffer: step 19833, counter 316684\n",
      "Environment 1: Episode 4507, Score -122.62140593626289, Avg_Score -119.73493445846792\n",
      "Adding trajectory to replay buffer: step 19837, counter 316727\n",
      "Environment 9: Episode 4508, Score -122.6012746515535, Avg_Score -119.75286220837788\n",
      "Adding trajectory to replay buffer: step 19852, counter 316769\n",
      "Environment 4: Episode 4509, Score -122.89635401089924, Avg_Score -119.7602506556987\n",
      "Adding trajectory to replay buffer: step 19853, counter 316812\n",
      "Environment 11: Episode 4510, Score -123.5004184130244, Avg_Score -119.84541166646964\n",
      "Adding trajectory to replay buffer: step 19855, counter 316855\n",
      "Environment 14: Episode 4511, Score -122.97080859186624, Avg_Score -119.89580219085096\n",
      "Adding trajectory to replay buffer: step 19856, counter 316897\n",
      "Environment 12: Episode 4512, Score -123.08848000590814, Avg_Score -119.90040663590544\n",
      "Adding trajectory to replay buffer: step 19857, counter 316966\n",
      "Environment 0: Episode 4513, Score -116.72102438732671, Avg_Score -119.82953959838882\n",
      "Adding trajectory to replay buffer: step 19857, counter 317009\n",
      "Environment 6: Episode 4514, Score -123.70887593750531, Avg_Score -119.89454170758076\n",
      "Adding trajectory to replay buffer: step 19867, counter 317095\n",
      "Environment 7: Episode 4515, Score -121.72042437455616, Avg_Score -119.91669224168285\n",
      "Adding trajectory to replay buffer: step 19869, counter 317139\n",
      "Environment 2: Episode 4516, Score -122.93583744529325, Avg_Score -119.92501966535825\n",
      "Adding trajectory to replay buffer: step 19873, counter 317227\n",
      "Environment 13: Episode 4517, Score -114.79409648309462, Avg_Score -119.85560304813018\n",
      "Adding trajectory to replay buffer: step 19876, counter 317445\n",
      "Environment 8: Episode 4518, Score -127.01256552739596, Avg_Score -119.90525284124284\n",
      "Adding trajectory to replay buffer: step 19882, counter 317592\n",
      "Environment 3: Episode 4519, Score -122.29044682852862, Avg_Score -119.96663122548793\n",
      "Adding trajectory to replay buffer: step 19885, counter 317689\n",
      "Environment 5: Episode 4520, Score -123.57334425230232, Avg_Score -120.02174304643182\n",
      "Adding trajectory to replay buffer: step 19889, counter 317773\n",
      "Environment 10: Episode 4521, Score -120.61130228089604, Avg_Score -120.01736760387861\n",
      "Adding trajectory to replay buffer: step 19898, counter 317819\n",
      "Environment 4: Episode 4522, Score -122.1777602163038, Avg_Score -120.06691844607812\n",
      "Adding trajectory to replay buffer: step 19898, counter 317864\n",
      "Environment 11: Episode 4523, Score -122.48098214858274, Avg_Score -120.14400502599375\n",
      "Adding trajectory to replay buffer: step 19900, counter 317907\n",
      "Environment 0: Episode 4524, Score -122.5309645873389, Avg_Score -120.13457397268463\n",
      "Adding trajectory to replay buffer: step 19900, counter 317951\n",
      "Environment 12: Episode 4525, Score -122.45128917642558, Avg_Score -120.12873978122649\n",
      "Adding trajectory to replay buffer: step 19900, counter 317996\n",
      "Environment 14: Episode 4526, Score -122.88396099184143, Avg_Score -120.19070591523905\n",
      "Adding trajectory to replay buffer: step 19901, counter 318040\n",
      "Environment 6: Episode 4527, Score -123.2705000486821, Avg_Score -120.24791670072169\n",
      "Adding trajectory to replay buffer: step 19903, counter 318111\n",
      "Environment 15: Episode 4528, Score -114.18081826715975, Avg_Score -120.16800003794786\n",
      "Adding trajectory to replay buffer: step 19909, counter 318187\n",
      "Environment 1: Episode 4529, Score -119.63744863805485, Avg_Score -120.14471364750021\n",
      "Adding trajectory to replay buffer: step 19911, counter 318231\n",
      "Environment 7: Episode 4530, Score -122.18010817234031, Avg_Score -120.14283979189663\n",
      "Adding trajectory to replay buffer: step 19916, counter 318278\n",
      "Environment 2: Episode 4531, Score -120.06507261901845, Avg_Score -120.12177458356005\n",
      "Adding trajectory to replay buffer: step 19921, counter 318323\n",
      "Environment 8: Episode 4532, Score -120.30421666759689, Avg_Score -120.10393741410222\n",
      "Adding trajectory to replay buffer: step 19926, counter 318367\n",
      "Environment 3: Episode 4533, Score -120.79959757316796, Avg_Score -120.10577854328832\n",
      "Adding trajectory to replay buffer: step 19927, counter 318421\n",
      "Environment 13: Episode 4534, Score -119.72578850692821, Avg_Score -120.08364471084582\n",
      "Adding trajectory to replay buffer: step 19934, counter 318470\n",
      "Environment 5: Episode 4535, Score -115.8002535990992, Avg_Score -120.01538328154312\n",
      "Adding trajectory to replay buffer: step 19946, counter 318515\n",
      "Environment 6: Episode 4536, Score -117.45698261195359, Avg_Score -120.00793804645149\n",
      "Adding trajectory to replay buffer: step 19948, counter 318565\n",
      "Environment 11: Episode 4537, Score -116.34267957194834, Avg_Score -119.98486307663816\n",
      "Adding trajectory to replay buffer: step 19949, counter 318616\n",
      "Environment 4: Episode 4538, Score -118.53882625031659, Avg_Score -120.02757928840595\n",
      "Adding trajectory to replay buffer: step 19952, counter 318659\n",
      "Environment 1: Episode 4539, Score -123.32791994636445, Avg_Score -120.0932247330881\n",
      "Adding trajectory to replay buffer: step 19957, counter 318705\n",
      "Environment 7: Episode 4540, Score -122.28996720374005, Avg_Score -120.09844455046574\n",
      "Adding trajectory to replay buffer: step 19960, counter 318749\n",
      "Environment 2: Episode 4541, Score -122.89106302592965, Avg_Score -120.19929351018658\n",
      "Adding trajectory to replay buffer: step 19963, counter 318812\n",
      "Environment 0: Episode 4542, Score -114.13548933649963, Avg_Score -120.12253203992215\n",
      "Adding trajectory to replay buffer: step 19965, counter 318856\n",
      "Environment 8: Episode 4543, Score -122.2123821121088, Avg_Score -120.14027353740326\n",
      "Adding trajectory to replay buffer: step 19967, counter 318986\n",
      "Environment 9: Episode 4544, Score -124.13607678028352, Avg_Score -120.21073647015415\n",
      "Adding trajectory to replay buffer: step 19967, counter 319053\n",
      "Environment 12: Episode 4545, Score -115.12766728683002, Avg_Score -120.13046518567052\n",
      "Adding trajectory to replay buffer: step 19970, counter 319097\n",
      "Environment 3: Episode 4546, Score -117.68215814853025, Avg_Score -120.08132367225934\n",
      "Adding trajectory to replay buffer: step 19970, counter 319140\n",
      "Environment 13: Episode 4547, Score -123.20965089083276, Avg_Score -120.08909608552742\n",
      "Adding trajectory to replay buffer: step 19974, counter 319211\n",
      "Environment 15: Episode 4548, Score -119.10146072313127, Avg_Score -120.11107857462501\n",
      "Adding trajectory to replay buffer: step 19985, counter 319262\n",
      "Environment 5: Episode 4549, Score -119.70036439698252, Avg_Score -120.17881198652839\n",
      "Adding trajectory to replay buffer: step 19993, counter 319307\n",
      "Environment 11: Episode 4550, Score -117.22380385883463, Avg_Score -120.12548720738646\n",
      "Adding trajectory to replay buffer: step 19994, counter 319352\n",
      "Environment 4: Episode 4551, Score -123.37989508928544, Avg_Score -120.13234895504853\n",
      "Adding trajectory to replay buffer: step 19996, counter 319396\n",
      "Environment 1: Episode 4552, Score -121.45116266808472, Avg_Score -120.19496321763913\n",
      "Adding trajectory to replay buffer: step 20006, counter 319442\n",
      "Environment 2: Episode 4553, Score -122.92102450123119, Avg_Score -120.21829189321348\n",
      "Adding trajectory to replay buffer: step 20006, counter 319491\n",
      "Environment 7: Episode 4554, Score -119.43519745594604, Avg_Score -120.24727723406131\n",
      "Adding trajectory to replay buffer: step 20008, counter 319534\n",
      "Environment 8: Episode 4555, Score -123.08394348215995, Avg_Score -120.29060933754786\n",
      "Adding trajectory to replay buffer: step 20010, counter 319577\n",
      "Environment 9: Episode 4556, Score -121.66260603848968, Avg_Score -120.27492673768926\n",
      "Adding trajectory to replay buffer: step 20010, counter 319620\n",
      "Environment 12: Episode 4557, Score -121.58043816659786, Avg_Score -120.33680794024231\n",
      "Adding trajectory to replay buffer: step 20010, counter 319730\n",
      "Environment 14: Episode 4558, Score -123.78650306203961, Avg_Score -120.35411371266564\n",
      "Adding trajectory to replay buffer: step 20011, counter 319778\n",
      "Environment 0: Episode 4559, Score -121.27835215502543, Avg_Score -120.39275921498027\n",
      "Adding trajectory to replay buffer: step 20015, counter 319823\n",
      "Environment 3: Episode 4560, Score -121.36600433548105, Avg_Score -120.44339423217349\n",
      "Adding trajectory to replay buffer: step 20015, counter 319868\n",
      "Environment 13: Episode 4561, Score -123.05462463167123, Avg_Score -120.48512036437444\n",
      "Adding trajectory to replay buffer: step 20015, counter 319909\n",
      "Environment 15: Episode 4562, Score -114.98669083698155, Avg_Score -120.41611737415944\n",
      "Adding trajectory to replay buffer: step 20029, counter 319953\n",
      "Environment 5: Episode 4563, Score -123.11079013283303, Avg_Score -120.51209400238295\n",
      "Adding trajectory to replay buffer: step 20042, counter 319999\n",
      "Environment 1: Episode 4564, Score -121.32227881681175, Avg_Score -120.55356088548525\n",
      "Adding trajectory to replay buffer: step 20042, counter 320095\n",
      "Environment 6: Episode 4565, Score -125.23476537121583, Avg_Score -120.57717209190581\n",
      "Adding trajectory to replay buffer: step 20042, counter 320144\n",
      "Environment 11: Episode 4566, Score -118.88320832372453, Avg_Score -120.58331010167524\n",
      "Adding trajectory to replay buffer: step 20051, counter 320187\n",
      "Environment 8: Episode 4567, Score -116.58333408472626, Avg_Score -120.527029847785\n",
      "Adding trajectory to replay buffer: step 20055, counter 320232\n",
      "Environment 9: Episode 4568, Score -122.59747721033855, Avg_Score -120.59294816986424\n",
      "Adding trajectory to replay buffer: step 20056, counter 320277\n",
      "Environment 0: Episode 4569, Score -123.68668133395167, Avg_Score -120.67531348182925\n",
      "Adding trajectory to replay buffer: step 20058, counter 320320\n",
      "Environment 3: Episode 4570, Score -123.34512258483284, Avg_Score -120.69601859174308\n",
      "Adding trajectory to replay buffer: step 20058, counter 320363\n",
      "Environment 13: Episode 4571, Score -123.27603418607949, Avg_Score -120.79531853844624\n",
      "Adding trajectory to replay buffer: step 20058, counter 320406\n",
      "Environment 15: Episode 4572, Score -123.2922176518552, Avg_Score -120.80672640967941\n",
      "Adding trajectory to replay buffer: step 20061, counter 320461\n",
      "Environment 2: Episode 4573, Score -114.65276143788299, Avg_Score -120.71849652461546\n",
      "Adding trajectory to replay buffer: step 20073, counter 320645\n",
      "Environment 10: Episode 4574, Score -119.9258814511119, Avg_Score -120.69838823540651\n",
      "Adding trajectory to replay buffer: step 20076, counter 320727\n",
      "Environment 4: Episode 4575, Score -116.20866550136543, Avg_Score -120.63465218406604\n",
      "Adding trajectory to replay buffer: step 20087, counter 320772\n",
      "Environment 1: Episode 4576, Score -117.48479179192086, Avg_Score -120.63919040116994\n",
      "Adding trajectory to replay buffer: step 20087, counter 320817\n",
      "Environment 6: Episode 4577, Score -122.23861760761837, Avg_Score -120.64541484509867\n",
      "Adding trajectory to replay buffer: step 20089, counter 320896\n",
      "Environment 12: Episode 4578, Score -121.56147867024472, Avg_Score -120.67968698385297\n",
      "Adding trajectory to replay buffer: step 20093, counter 320979\n",
      "Environment 14: Episode 4579, Score -115.7104372417846, Avg_Score -120.6085474987647\n",
      "Adding trajectory to replay buffer: step 20095, counter 321045\n",
      "Environment 5: Episode 4580, Score -115.84311044044172, Avg_Score -120.60563873897947\n",
      "Adding trajectory to replay buffer: step 20095, counter 321089\n",
      "Environment 8: Episode 4581, Score -123.00148150310106, Avg_Score -120.66495663163661\n",
      "Adding trajectory to replay buffer: step 20101, counter 321132\n",
      "Environment 3: Episode 4582, Score -122.93107652121782, Avg_Score -120.66059640325324\n",
      "Adding trajectory to replay buffer: step 20101, counter 321175\n",
      "Environment 15: Episode 4583, Score -122.75896494396278, Avg_Score -120.6591913804764\n",
      "Adding trajectory to replay buffer: step 20102, counter 321271\n",
      "Environment 7: Episode 4584, Score -125.99829725000635, Avg_Score -120.73874693778207\n",
      "Adding trajectory to replay buffer: step 20104, counter 321314\n",
      "Environment 2: Episode 4585, Score -123.70286592636009, Avg_Score -120.84033362091427\n",
      "Adding trajectory to replay buffer: step 20116, counter 321357\n",
      "Environment 10: Episode 4586, Score -124.15683678847489, Avg_Score -120.8600265082075\n",
      "Adding trajectory to replay buffer: step 20121, counter 321402\n",
      "Environment 4: Episode 4587, Score -120.943832854677, Avg_Score -120.83147692046485\n",
      "Adding trajectory to replay buffer: step 20129, counter 321444\n",
      "Environment 1: Episode 4588, Score -122.72400062770708, Avg_Score -120.85449627973632\n",
      "Adding trajectory to replay buffer: step 20129, counter 321486\n",
      "Environment 6: Episode 4589, Score -122.81187532925358, Avg_Score -120.8601543167446\n",
      "Adding trajectory to replay buffer: step 20129, counter 321557\n",
      "Environment 13: Episode 4590, Score -115.59966463508829, Avg_Score -120.79409004315285\n",
      "Adding trajectory to replay buffer: step 20132, counter 321600\n",
      "Environment 12: Episode 4591, Score -123.49507203272917, Avg_Score -120.79332155184836\n",
      "Adding trajectory to replay buffer: step 20140, counter 321645\n",
      "Environment 8: Episode 4592, Score -116.46294059671771, Avg_Score -120.72494153137623\n",
      "Adding trajectory to replay buffer: step 20140, counter 321743\n",
      "Environment 11: Episode 4593, Score -124.24059644569147, Avg_Score -120.77446448423986\n",
      "Adding trajectory to replay buffer: step 20142, counter 321792\n",
      "Environment 14: Episode 4594, Score -120.84459212583428, Avg_Score -120.76150541892501\n",
      "Adding trajectory to replay buffer: step 20144, counter 321880\n",
      "Environment 0: Episode 4595, Score -117.11905930738368, Avg_Score -120.76151623082292\n",
      "Adding trajectory to replay buffer: step 20144, counter 321923\n",
      "Environment 3: Episode 4596, Score -122.49661879582331, Avg_Score -120.78835591810886\n",
      "Adding trajectory to replay buffer: step 20145, counter 321967\n",
      "Environment 15: Episode 4597, Score -122.65064397186181, Avg_Score -120.87266688363331\n",
      "Adding trajectory to replay buffer: step 20146, counter 322058\n",
      "Environment 9: Episode 4598, Score -122.36332965587016, Avg_Score -120.89269049371406\n",
      "Adding trajectory to replay buffer: step 20147, counter 322103\n",
      "Environment 7: Episode 4599, Score -121.56690190206157, Avg_Score -120.88056168633968\n",
      "Adding trajectory to replay buffer: step 20167, counter 322149\n",
      "Environment 4: Episode 4600, Score -118.97352042180859, Avg_Score -120.87932607780084\n",
      "Adding trajectory to replay buffer: step 20173, counter 322193\n",
      "Environment 1: Episode 4601, Score -116.36173905271106, Avg_Score -120.85270744708683\n",
      "Adding trajectory to replay buffer: step 20182, counter 322246\n",
      "Environment 6: Episode 4602, Score -118.10409551451153, Avg_Score -120.80423323629118\n",
      "Adding trajectory to replay buffer: step 20184, counter 322326\n",
      "Environment 2: Episode 4603, Score -121.25498974157156, Avg_Score -120.80332397001445\n",
      "Adding trajectory to replay buffer: step 20187, counter 322384\n",
      "Environment 13: Episode 4604, Score -117.95093269519322, Avg_Score -120.76133215239938\n",
      "Adding trajectory to replay buffer: step 20188, counter 322428\n",
      "Environment 0: Episode 4605, Score -121.63359096680955, Avg_Score -120.81577991329999\n",
      "Adding trajectory to replay buffer: step 20190, counter 322473\n",
      "Environment 15: Episode 4606, Score -118.75211417879848, Avg_Score -120.84755642530199\n",
      "Adding trajectory to replay buffer: step 20191, counter 322520\n",
      "Environment 3: Episode 4607, Score -121.37903291764856, Avg_Score -120.83513269511585\n",
      "Adding trajectory to replay buffer: step 20194, counter 322567\n",
      "Environment 7: Episode 4608, Score -120.93374742557728, Avg_Score -120.8184574228561\n",
      "Adding trajectory to replay buffer: step 20194, counter 322615\n",
      "Environment 9: Episode 4609, Score -116.93557594266036, Avg_Score -120.7588496421737\n",
      "Adding trajectory to replay buffer: step 20195, counter 322670\n",
      "Environment 8: Episode 4610, Score -113.70448401775832, Avg_Score -120.66089029822103\n",
      "Adding trajectory to replay buffer: step 20199, counter 322729\n",
      "Environment 11: Episode 4611, Score -114.6163131159935, Avg_Score -120.5773453434623\n",
      "Adding trajectory to replay buffer: step 20215, counter 322812\n",
      "Environment 12: Episode 4612, Score -111.8090285417335, Avg_Score -120.46455082882056\n",
      "Adding trajectory to replay buffer: step 20220, counter 322859\n",
      "Environment 1: Episode 4613, Score -122.44908823558812, Avg_Score -120.52183146730316\n",
      "Adding trajectory to replay buffer: step 20228, counter 322905\n",
      "Environment 6: Episode 4614, Score -121.90099144919186, Avg_Score -120.50375262242004\n",
      "Adding trajectory to replay buffer: step 20231, counter 322969\n",
      "Environment 4: Episode 4615, Score -117.15162885393326, Avg_Score -120.45806466721379\n",
      "Adding trajectory to replay buffer: step 20233, counter 323011\n",
      "Environment 3: Episode 4616, Score -119.36823307132596, Avg_Score -120.4223886234741\n",
      "Adding trajectory to replay buffer: step 20236, counter 323057\n",
      "Environment 15: Episode 4617, Score -121.68265976473627, Avg_Score -120.49127425629054\n",
      "Adding trajectory to replay buffer: step 20238, counter 323101\n",
      "Environment 9: Episode 4618, Score -116.36487072710817, Avg_Score -120.38479730828763\n",
      "Adding trajectory to replay buffer: step 20240, counter 323153\n",
      "Environment 0: Episode 4619, Score -115.96925563524974, Avg_Score -120.32158539635486\n",
      "Adding trajectory to replay buffer: step 20245, counter 323199\n",
      "Environment 11: Episode 4620, Score -120.64802591626719, Avg_Score -120.2923322129945\n",
      "Adding trajectory to replay buffer: step 20251, counter 323255\n",
      "Environment 8: Episode 4621, Score -112.22973726050928, Avg_Score -120.20851656279065\n",
      "Adding trajectory to replay buffer: step 20257, counter 323297\n",
      "Environment 12: Episode 4622, Score -123.39592723971543, Avg_Score -120.22069823302476\n",
      "Adding trajectory to replay buffer: step 20264, counter 323341\n",
      "Environment 1: Episode 4623, Score -122.06184585472383, Avg_Score -120.21650687008616\n",
      "Adding trajectory to replay buffer: step 20268, counter 323415\n",
      "Environment 7: Episode 4624, Score -112.0176245224475, Avg_Score -120.11137346943725\n",
      "Adding trajectory to replay buffer: step 20270, counter 323457\n",
      "Environment 6: Episode 4625, Score -115.52971891272566, Avg_Score -120.04215776680024\n",
      "Adding trajectory to replay buffer: step 20275, counter 323637\n",
      "Environment 5: Episode 4626, Score -118.38049727621254, Avg_Score -119.99712312964395\n",
      "Adding trajectory to replay buffer: step 20279, counter 323683\n",
      "Environment 3: Episode 4627, Score -118.81177843984278, Avg_Score -119.95253591355555\n",
      "Adding trajectory to replay buffer: step 20280, counter 323727\n",
      "Environment 15: Episode 4628, Score -122.9217603902587, Avg_Score -120.03994533478654\n",
      "Adding trajectory to replay buffer: step 20281, counter 323770\n",
      "Environment 9: Episode 4629, Score -122.8903037698418, Avg_Score -120.07247388610443\n",
      "Adding trajectory to replay buffer: step 20283, counter 323813\n",
      "Environment 0: Episode 4630, Score -122.82591552317763, Avg_Score -120.07893195961279\n",
      "Adding trajectory to replay buffer: step 20288, counter 323856\n",
      "Environment 11: Episode 4631, Score -121.98718041782516, Avg_Score -120.09815303760087\n",
      "Adding trajectory to replay buffer: step 20295, counter 323900\n",
      "Environment 8: Episode 4632, Score -117.4304192888476, Avg_Score -120.06941506381335\n",
      "Adding trajectory to replay buffer: step 20307, counter 323943\n",
      "Environment 1: Episode 4633, Score -123.01196543980018, Avg_Score -120.09153874247968\n",
      "Adding trajectory to replay buffer: step 20311, counter 323986\n",
      "Environment 7: Episode 4634, Score -123.45040946422517, Avg_Score -120.12878495205267\n",
      "Adding trajectory to replay buffer: step 20319, counter 324030\n",
      "Environment 5: Episode 4635, Score -122.81027924395539, Avg_Score -120.19888520850121\n",
      "Adding trajectory to replay buffer: step 20323, counter 324074\n",
      "Environment 3: Episode 4636, Score -122.46187291343635, Avg_Score -120.24893411151602\n",
      "Adding trajectory to replay buffer: step 20323, counter 324116\n",
      "Environment 9: Episode 4637, Score -123.25011503798888, Avg_Score -120.31800846617642\n",
      "Adding trajectory to replay buffer: step 20323, counter 324159\n",
      "Environment 15: Episode 4638, Score -122.92323687051672, Avg_Score -120.36185257237842\n",
      "Adding trajectory to replay buffer: step 20324, counter 324341\n",
      "Environment 14: Episode 4639, Score -117.76106483316732, Avg_Score -120.30618402124645\n",
      "Adding trajectory to replay buffer: step 20326, counter 324384\n",
      "Environment 0: Episode 4640, Score -123.17644138793275, Avg_Score -120.31504876308838\n",
      "Adding trajectory to replay buffer: step 20327, counter 324595\n",
      "Environment 10: Episode 4641, Score -122.80666139867157, Avg_Score -120.31420474681579\n",
      "Adding trajectory to replay buffer: step 20331, counter 324638\n",
      "Environment 11: Episode 4642, Score -121.95890965755781, Avg_Score -120.39243895002639\n",
      "Adding trajectory to replay buffer: step 20340, counter 324683\n",
      "Environment 8: Episode 4643, Score -122.62968221032061, Avg_Score -120.39661195100851\n",
      "Adding trajectory to replay buffer: step 20344, counter 324770\n",
      "Environment 12: Episode 4644, Score -112.4724173852969, Avg_Score -120.27997535705869\n",
      "Adding trajectory to replay buffer: step 20350, counter 324813\n",
      "Environment 1: Episode 4645, Score -122.47680564554533, Avg_Score -120.35346674064584\n",
      "Adding trajectory to replay buffer: step 20352, counter 324981\n",
      "Environment 2: Episode 4646, Score -121.89575477149151, Avg_Score -120.39560270687544\n",
      "Adding trajectory to replay buffer: step 20354, counter 325024\n",
      "Environment 7: Episode 4647, Score -123.24500382742845, Avg_Score -120.3959562362414\n",
      "Adding trajectory to replay buffer: step 20354, counter 325191\n",
      "Environment 13: Episode 4648, Score -118.7366997033741, Avg_Score -120.39230862604383\n",
      "Adding trajectory to replay buffer: step 20363, counter 325235\n",
      "Environment 5: Episode 4649, Score -122.92334700219706, Avg_Score -120.42453845209597\n",
      "Adding trajectory to replay buffer: step 20367, counter 325279\n",
      "Environment 3: Episode 4650, Score -121.47634660750938, Avg_Score -120.46706387958272\n",
      "Adding trajectory to replay buffer: step 20368, counter 325321\n",
      "Environment 0: Episode 4651, Score -123.58908380917832, Avg_Score -120.46915576678165\n",
      "Adding trajectory to replay buffer: step 20368, counter 325366\n",
      "Environment 9: Episode 4652, Score -122.21826358493479, Avg_Score -120.47682677595017\n",
      "Adding trajectory to replay buffer: step 20368, counter 325410\n",
      "Environment 14: Episode 4653, Score -122.72586637885868, Avg_Score -120.47487519472642\n",
      "Adding trajectory to replay buffer: step 20368, counter 325455\n",
      "Environment 15: Episode 4654, Score -122.23807882696453, Avg_Score -120.5029040084366\n",
      "Adding trajectory to replay buffer: step 20371, counter 325499\n",
      "Environment 10: Episode 4655, Score -121.74244765864685, Avg_Score -120.4894890502015\n",
      "Adding trajectory to replay buffer: step 20375, counter 325543\n",
      "Environment 11: Episode 4656, Score -123.03013904349693, Avg_Score -120.50316438025155\n",
      "Adding trajectory to replay buffer: step 20384, counter 325657\n",
      "Environment 6: Episode 4657, Score -116.50647285366512, Avg_Score -120.45242472712222\n",
      "Adding trajectory to replay buffer: step 20386, counter 325812\n",
      "Environment 4: Episode 4658, Score -119.91948965024513, Avg_Score -120.41375459300428\n",
      "Adding trajectory to replay buffer: step 20387, counter 325859\n",
      "Environment 8: Episode 4659, Score -120.7547214535332, Avg_Score -120.40851828598939\n",
      "Adding trajectory to replay buffer: step 20387, counter 325902\n",
      "Environment 12: Episode 4660, Score -120.77017219503287, Avg_Score -120.4025599645849\n",
      "Adding trajectory to replay buffer: step 20394, counter 325946\n",
      "Environment 1: Episode 4661, Score -122.53286410521468, Avg_Score -120.39734235932033\n",
      "Adding trajectory to replay buffer: step 20394, counter 325988\n",
      "Environment 2: Episode 4662, Score -122.87289593289793, Avg_Score -120.47620441027952\n",
      "Adding trajectory to replay buffer: step 20397, counter 326031\n",
      "Environment 7: Episode 4663, Score -123.12190016279867, Avg_Score -120.47631551057917\n",
      "Adding trajectory to replay buffer: step 20397, counter 326074\n",
      "Environment 13: Episode 4664, Score -123.10522279060073, Avg_Score -120.49414495031704\n",
      "Adding trajectory to replay buffer: step 20411, counter 326117\n",
      "Environment 9: Episode 4665, Score -121.86112903362545, Avg_Score -120.46040858694113\n",
      "Adding trajectory to replay buffer: step 20411, counter 326160\n",
      "Environment 15: Episode 4666, Score -123.23862883221979, Avg_Score -120.5039627920261\n",
      "Adding trajectory to replay buffer: step 20413, counter 326205\n",
      "Environment 14: Episode 4667, Score -119.7606562366554, Avg_Score -120.53573601354537\n",
      "Adding trajectory to replay buffer: step 20415, counter 326249\n",
      "Environment 10: Episode 4668, Score -122.85208401721655, Avg_Score -120.53828208161418\n",
      "Adding trajectory to replay buffer: step 20418, counter 326292\n",
      "Environment 11: Episode 4669, Score -123.44442074133457, Avg_Score -120.53585947568797\n",
      "Adding trajectory to replay buffer: step 20429, counter 326337\n",
      "Environment 6: Episode 4670, Score -122.43422378058452, Avg_Score -120.52675048764553\n",
      "Adding trajectory to replay buffer: step 20432, counter 326382\n",
      "Environment 8: Episode 4671, Score -123.15717096088392, Avg_Score -120.52556185539359\n",
      "Adding trajectory to replay buffer: step 20433, counter 326428\n",
      "Environment 12: Episode 4672, Score -121.66449094124945, Avg_Score -120.5092845882875\n",
      "Adding trajectory to replay buffer: step 20436, counter 326470\n",
      "Environment 1: Episode 4673, Score -123.06163158114751, Avg_Score -120.59337328972015\n",
      "Adding trajectory to replay buffer: step 20438, counter 326514\n",
      "Environment 2: Episode 4674, Score -121.32464243366569, Avg_Score -120.6073608995457\n",
      "Adding trajectory to replay buffer: step 20438, counter 326585\n",
      "Environment 3: Episode 4675, Score -119.85417950138326, Avg_Score -120.64381603954584\n",
      "Adding trajectory to replay buffer: step 20440, counter 326628\n",
      "Environment 7: Episode 4676, Score -120.98626719817644, Avg_Score -120.6788307936084\n",
      "Adding trajectory to replay buffer: step 20441, counter 326706\n",
      "Environment 5: Episode 4677, Score -122.6928528086481, Avg_Score -120.6833731456187\n",
      "Adding trajectory to replay buffer: step 20448, counter 326786\n",
      "Environment 0: Episode 4678, Score -112.84898033671641, Avg_Score -120.59624816228339\n",
      "Adding trajectory to replay buffer: step 20454, counter 326829\n",
      "Environment 9: Episode 4679, Score -122.73628792516514, Avg_Score -120.66650666911718\n",
      "Adding trajectory to replay buffer: step 20454, counter 326872\n",
      "Environment 15: Episode 4680, Score -122.69134403881662, Avg_Score -120.73498900510093\n",
      "Adding trajectory to replay buffer: step 20456, counter 326915\n",
      "Environment 14: Episode 4681, Score -122.40068438661844, Avg_Score -120.72898103393612\n",
      "Adding trajectory to replay buffer: step 20458, counter 326958\n",
      "Environment 10: Episode 4682, Score -122.31437231426686, Avg_Score -120.72281399186663\n",
      "Adding trajectory to replay buffer: step 20460, counter 327032\n",
      "Environment 4: Episode 4683, Score -118.19362022044447, Avg_Score -120.67716054463142\n",
      "Adding trajectory to replay buffer: step 20461, counter 327075\n",
      "Environment 11: Episode 4684, Score -120.18901461053392, Avg_Score -120.6190677182367\n",
      "Adding trajectory to replay buffer: step 20472, counter 327118\n",
      "Environment 6: Episode 4685, Score -122.37114269137258, Avg_Score -120.60575048588684\n",
      "Adding trajectory to replay buffer: step 20478, counter 327164\n",
      "Environment 8: Episode 4686, Score -122.04461963515543, Avg_Score -120.58462831435365\n",
      "Adding trajectory to replay buffer: step 20478, counter 327209\n",
      "Environment 12: Episode 4687, Score -121.76015104364356, Avg_Score -120.59279149624332\n",
      "Adding trajectory to replay buffer: step 20483, counter 327256\n",
      "Environment 1: Episode 4688, Score -119.37601795948484, Avg_Score -120.55931166956108\n",
      "Adding trajectory to replay buffer: step 20486, counter 327302\n",
      "Environment 7: Episode 4689, Score -118.94914334688087, Avg_Score -120.52068434973738\n",
      "Adding trajectory to replay buffer: step 20487, counter 327348\n",
      "Environment 5: Episode 4690, Score -121.90679646083836, Avg_Score -120.58375566799486\n",
      "Adding trajectory to replay buffer: step 20491, counter 327391\n",
      "Environment 0: Episode 4691, Score -122.83529509051361, Avg_Score -120.57715789857271\n",
      "Adding trajectory to replay buffer: step 20495, counter 327432\n",
      "Environment 15: Episode 4692, Score -115.43514981990245, Avg_Score -120.56687999080458\n",
      "Adding trajectory to replay buffer: step 20499, counter 327477\n",
      "Environment 9: Episode 4693, Score -120.40112241306777, Avg_Score -120.52848525047831\n",
      "Adding trajectory to replay buffer: step 20501, counter 327520\n",
      "Environment 10: Episode 4694, Score -117.31880530556664, Avg_Score -120.49322738227568\n",
      "Adding trajectory to replay buffer: step 20501, counter 327565\n",
      "Environment 14: Episode 4695, Score -122.36728492882537, Avg_Score -120.5457096384901\n",
      "Adding trajectory to replay buffer: step 20504, counter 327608\n",
      "Environment 11: Episode 4696, Score -116.983843686445, Avg_Score -120.49058188739632\n",
      "Adding trajectory to replay buffer: step 20517, counter 327687\n",
      "Environment 2: Episode 4697, Score -113.97167851251116, Avg_Score -120.4037922328028\n",
      "Adding trajectory to replay buffer: step 20521, counter 327730\n",
      "Environment 12: Episode 4698, Score -123.00890365028692, Avg_Score -120.41024797274699\n",
      "Adding trajectory to replay buffer: step 20522, counter 327774\n",
      "Environment 8: Episode 4699, Score -121.45108059585668, Avg_Score -120.40908975968492\n",
      "Adding trajectory to replay buffer: step 20523, counter 327837\n",
      "Environment 4: Episode 4700, Score -117.48028919234935, Avg_Score -120.39415744739036\n",
      "Adding trajectory to replay buffer: step 20526, counter 327880\n",
      "Environment 1: Episode 4701, Score -121.60518717181806, Avg_Score -120.4465919285814\n",
      "Adding trajectory to replay buffer: step 20533, counter 327926\n",
      "Environment 5: Episode 4702, Score -122.73555367518341, Avg_Score -120.49290651018813\n",
      "Adding trajectory to replay buffer: step 20535, counter 327975\n",
      "Environment 7: Episode 4703, Score -115.83690202159288, Avg_Score -120.43872563298835\n",
      "Adding trajectory to replay buffer: step 20544, counter 328020\n",
      "Environment 9: Episode 4704, Score -122.89956655858086, Avg_Score -120.4882119716222\n",
      "Adding trajectory to replay buffer: step 20548, counter 328064\n",
      "Environment 11: Episode 4705, Score -123.44008505935068, Avg_Score -120.50627691254762\n",
      "Adding trajectory to replay buffer: step 20562, counter 328109\n",
      "Environment 2: Episode 4706, Score -121.60550456051033, Avg_Score -120.53481081636475\n",
      "Adding trajectory to replay buffer: step 20565, counter 328277\n",
      "Environment 13: Episode 4707, Score -117.31417956893208, Avg_Score -120.49416228287758\n",
      "Adding trajectory to replay buffer: step 20567, counter 328321\n",
      "Environment 4: Episode 4708, Score -119.37285282430861, Avg_Score -120.47855333686489\n",
      "Adding trajectory to replay buffer: step 20567, counter 328366\n",
      "Environment 8: Episode 4709, Score -121.30752406843565, Avg_Score -120.52227281812264\n",
      "Adding trajectory to replay buffer: step 20571, counter 328411\n",
      "Environment 1: Episode 4710, Score -121.49152004621861, Avg_Score -120.60014317840725\n",
      "Adding trajectory to replay buffer: step 20577, counter 328487\n",
      "Environment 10: Episode 4711, Score -116.18390588202824, Avg_Score -120.61581910606759\n",
      "Adding trajectory to replay buffer: step 20578, counter 328593\n",
      "Environment 6: Episode 4712, Score -116.29530417437851, Avg_Score -120.66068186239404\n",
      "Adding trajectory to replay buffer: step 20580, counter 328640\n",
      "Environment 5: Episode 4713, Score -121.69207658065483, Avg_Score -120.65311174584473\n",
      "Adding trajectory to replay buffer: step 20586, counter 328788\n",
      "Environment 3: Episode 4714, Score -119.9885215003419, Avg_Score -120.6339870463562\n",
      "Adding trajectory to replay buffer: step 20588, counter 328832\n",
      "Environment 9: Episode 4715, Score -121.8537846251304, Avg_Score -120.68100860406817\n",
      "Adding trajectory to replay buffer: step 20590, counter 328887\n",
      "Environment 7: Episode 4716, Score -115.14567161465467, Avg_Score -120.63878298950145\n",
      "Adding trajectory to replay buffer: step 20599, counter 328985\n",
      "Environment 14: Episode 4717, Score -117.51057964030034, Avg_Score -120.59706218825711\n",
      "Adding trajectory to replay buffer: step 20603, counter 329040\n",
      "Environment 11: Episode 4718, Score -118.88470941164407, Avg_Score -120.62226057510249\n",
      "Adding trajectory to replay buffer: step 20605, counter 329083\n",
      "Environment 2: Episode 4719, Score -121.35191214921574, Avg_Score -120.67608714024213\n",
      "Adding trajectory to replay buffer: step 20605, counter 329193\n",
      "Environment 15: Episode 4720, Score -113.61686084773206, Avg_Score -120.6057754895568\n",
      "Adding trajectory to replay buffer: step 20609, counter 329237\n",
      "Environment 13: Episode 4721, Score -123.24640943620106, Avg_Score -120.7159422113137\n",
      "Adding trajectory to replay buffer: step 20613, counter 329283\n",
      "Environment 8: Episode 4722, Score -122.5364015264747, Avg_Score -120.7073469541813\n",
      "Adding trajectory to replay buffer: step 20614, counter 329330\n",
      "Environment 4: Episode 4723, Score -122.72140546993414, Avg_Score -120.7139425503334\n",
      "Adding trajectory to replay buffer: step 20616, counter 329425\n",
      "Environment 12: Episode 4724, Score -126.33506668950805, Avg_Score -120.85711697200401\n",
      "Adding trajectory to replay buffer: step 20619, counter 329473\n",
      "Environment 1: Episode 4725, Score -122.1046234095556, Avg_Score -120.92286601697229\n",
      "Adding trajectory to replay buffer: step 20620, counter 329516\n",
      "Environment 10: Episode 4726, Score -122.6065308710225, Avg_Score -120.96512635292038\n",
      "Adding trajectory to replay buffer: step 20622, counter 329560\n",
      "Environment 6: Episode 4727, Score -122.5621643503805, Avg_Score -121.00263021202576\n",
      "Adding trajectory to replay buffer: step 20623, counter 329603\n",
      "Environment 5: Episode 4728, Score -123.13114899888944, Avg_Score -121.00472409811205\n",
      "Adding trajectory to replay buffer: step 20628, counter 329645\n",
      "Environment 3: Episode 4729, Score -121.6040745163858, Avg_Score -120.9918618055775\n",
      "Adding trajectory to replay buffer: step 20633, counter 329690\n",
      "Environment 9: Episode 4730, Score -123.2445806732749, Avg_Score -120.99604845707849\n",
      "Adding trajectory to replay buffer: step 20635, counter 329735\n",
      "Environment 7: Episode 4731, Score -124.01153992147682, Avg_Score -121.016292052115\n",
      "Adding trajectory to replay buffer: step 20643, counter 329779\n",
      "Environment 14: Episode 4732, Score -122.35853455705444, Avg_Score -121.06557320479706\n",
      "Adding trajectory to replay buffer: step 20647, counter 329823\n",
      "Environment 11: Episode 4733, Score -122.7855745213547, Avg_Score -121.0633092956126\n",
      "Adding trajectory to replay buffer: step 20653, counter 329867\n",
      "Environment 13: Episode 4734, Score -121.91741147603095, Avg_Score -121.04797931573066\n",
      "Adding trajectory to replay buffer: step 20657, counter 329911\n",
      "Environment 8: Episode 4735, Score -123.1647817118199, Avg_Score -121.05152434040933\n",
      "Adding trajectory to replay buffer: step 20663, counter 329969\n",
      "Environment 2: Episode 4736, Score -117.05973645265897, Avg_Score -120.99750297580154\n",
      "Adding trajectory to replay buffer: step 20664, counter 330013\n",
      "Environment 10: Episode 4737, Score -123.52730010505394, Avg_Score -121.00027482647218\n",
      "Adding trajectory to replay buffer: step 20665, counter 330059\n",
      "Environment 1: Episode 4738, Score -117.32411234605064, Avg_Score -120.94428358122754\n",
      "Adding trajectory to replay buffer: step 20667, counter 330104\n",
      "Environment 6: Episode 4739, Score -122.1431685369642, Avg_Score -120.9881046182655\n",
      "Adding trajectory to replay buffer: step 20675, counter 330151\n",
      "Environment 3: Episode 4740, Score -117.05071683671636, Avg_Score -120.92684737275333\n",
      "Adding trajectory to replay buffer: step 20675, counter 330212\n",
      "Environment 4: Episode 4741, Score -116.58744099061408, Avg_Score -120.86465516867275\n",
      "Adding trajectory to replay buffer: step 20687, counter 330264\n",
      "Environment 7: Episode 4742, Score -115.98131062323569, Avg_Score -120.80487917832954\n",
      "Adding trajectory to replay buffer: step 20696, counter 330313\n",
      "Environment 11: Episode 4743, Score -122.6647921914626, Avg_Score -120.80523027814095\n",
      "Adding trajectory to replay buffer: step 20698, counter 330358\n",
      "Environment 13: Episode 4744, Score -120.07911162531997, Avg_Score -120.8812972205412\n",
      "Adding trajectory to replay buffer: step 20699, counter 330424\n",
      "Environment 9: Episode 4745, Score -122.09280172932085, Avg_Score -120.87745718137896\n",
      "Adding trajectory to replay buffer: step 20703, counter 330470\n",
      "Environment 8: Episode 4746, Score -120.71561775778358, Avg_Score -120.86565581124187\n",
      "Adding trajectory to replay buffer: step 20707, counter 330513\n",
      "Environment 10: Episode 4747, Score -122.95580139536038, Avg_Score -120.8627637869212\n",
      "Adding trajectory to replay buffer: step 20709, counter 330559\n",
      "Environment 2: Episode 4748, Score -121.14267609870248, Avg_Score -120.88682355087447\n",
      "Adding trajectory to replay buffer: step 20713, counter 330607\n",
      "Environment 1: Episode 4749, Score -121.48723400823772, Avg_Score -120.8724624209349\n",
      "Adding trajectory to replay buffer: step 20717, counter 330681\n",
      "Environment 14: Episode 4750, Score -118.84234842552307, Avg_Score -120.84612243911505\n",
      "Adding trajectory to replay buffer: step 20718, counter 330724\n",
      "Environment 4: Episode 4751, Score -115.4389419927268, Avg_Score -120.76462102095054\n",
      "Adding trajectory to replay buffer: step 20724, counter 330957\n",
      "Environment 0: Episode 4752, Score -125.08740437203801, Avg_Score -120.79331242882154\n",
      "Adding trajectory to replay buffer: step 20731, counter 331001\n",
      "Environment 7: Episode 4753, Score -122.27370745986451, Avg_Score -120.7887908396316\n",
      "Adding trajectory to replay buffer: step 20732, counter 331110\n",
      "Environment 5: Episode 4754, Score -113.33272304782395, Avg_Score -120.69973728184021\n",
      "Adding trajectory to replay buffer: step 20739, counter 331153\n",
      "Environment 11: Episode 4755, Score -123.13228737655966, Avg_Score -120.71363567901933\n",
      "Adding trajectory to replay buffer: step 20741, counter 331196\n",
      "Environment 13: Episode 4756, Score -122.97867830996029, Avg_Score -120.71312107168399\n",
      "Adding trajectory to replay buffer: step 20742, counter 331239\n",
      "Environment 9: Episode 4757, Score -122.70170973806083, Avg_Score -120.77507344052793\n",
      "Adding trajectory to replay buffer: step 20744, counter 331367\n",
      "Environment 12: Episode 4758, Score -117.01953048457392, Avg_Score -120.74607384887123\n",
      "Adding trajectory to replay buffer: step 20746, counter 331410\n",
      "Environment 8: Episode 4759, Score -123.03562489784073, Avg_Score -120.76888288331433\n",
      "Adding trajectory to replay buffer: step 20748, counter 331483\n",
      "Environment 3: Episode 4760, Score -110.36512104559007, Avg_Score -120.66483237181988\n",
      "Adding trajectory to replay buffer: step 20748, counter 331626\n",
      "Environment 15: Episode 4761, Score -121.0993790924164, Avg_Score -120.6504975216919\n",
      "Adding trajectory to replay buffer: step 20751, counter 331670\n",
      "Environment 10: Episode 4762, Score -123.22324767530523, Avg_Score -120.65400103911597\n",
      "Adding trajectory to replay buffer: step 20761, counter 331714\n",
      "Environment 14: Episode 4763, Score -122.56764233373045, Avg_Score -120.64845846082528\n",
      "Adding trajectory to replay buffer: step 20762, counter 331758\n",
      "Environment 4: Episode 4764, Score -122.79526315037162, Avg_Score -120.645358864423\n",
      "Adding trajectory to replay buffer: step 20767, counter 331801\n",
      "Environment 0: Episode 4765, Score -123.52485953944239, Avg_Score -120.66199616948117\n",
      "Adding trajectory to replay buffer: step 20775, counter 331844\n",
      "Environment 5: Episode 4766, Score -123.40093672888416, Avg_Score -120.66361924844779\n",
      "Adding trajectory to replay buffer: step 20777, counter 331890\n",
      "Environment 7: Episode 4767, Score -121.2807499793085, Avg_Score -120.67882018587434\n",
      "Adding trajectory to replay buffer: step 20784, counter 331935\n",
      "Environment 11: Episode 4768, Score -121.60084763188289, Avg_Score -120.666307822021\n",
      "Adding trajectory to replay buffer: step 20784, counter 331978\n",
      "Environment 13: Episode 4769, Score -122.80363652847149, Avg_Score -120.65989997989237\n",
      "Adding trajectory to replay buffer: step 20785, counter 332021\n",
      "Environment 9: Episode 4770, Score -122.65617708752119, Avg_Score -120.66211951296174\n",
      "Adding trajectory to replay buffer: step 20789, counter 332066\n",
      "Environment 12: Episode 4771, Score -121.85596538147082, Avg_Score -120.64910745716757\n",
      "Adding trajectory to replay buffer: step 20791, counter 332148\n",
      "Environment 2: Episode 4772, Score -113.7844485363563, Avg_Score -120.57030703311864\n",
      "Adding trajectory to replay buffer: step 20793, counter 332274\n",
      "Environment 6: Episode 4773, Score -125.22006579296044, Avg_Score -120.59189137523681\n",
      "Adding trajectory to replay buffer: step 20793, counter 332319\n",
      "Environment 15: Episode 4774, Score -121.93677265590927, Avg_Score -120.59801267745921\n",
      "Adding trajectory to replay buffer: step 20794, counter 332365\n",
      "Environment 3: Episode 4775, Score -119.63683132672993, Avg_Score -120.59583919571268\n",
      "Adding trajectory to replay buffer: step 20798, counter 332412\n",
      "Environment 10: Episode 4776, Score -120.09462722682457, Avg_Score -120.5869227959992\n",
      "Adding trajectory to replay buffer: step 20803, counter 332454\n",
      "Environment 14: Episode 4777, Score -115.74760874171432, Avg_Score -120.51747035532986\n",
      "Adding trajectory to replay buffer: step 20805, counter 332513\n",
      "Environment 8: Episode 4778, Score -122.7304859794105, Avg_Score -120.61628541175682\n",
      "Adding trajectory to replay buffer: step 20821, counter 332572\n",
      "Environment 4: Episode 4779, Score -111.33243390630186, Avg_Score -120.50224687156816\n",
      "Adding trajectory to replay buffer: step 20823, counter 332620\n",
      "Environment 5: Episode 4780, Score -122.84927533961584, Avg_Score -120.50382618457617\n",
      "Adding trajectory to replay buffer: step 20826, counter 332679\n",
      "Environment 0: Episode 4781, Score -112.20360443151928, Avg_Score -120.40185538502516\n",
      "Adding trajectory to replay buffer: step 20834, counter 332722\n",
      "Environment 2: Episode 4782, Score -123.12414535676565, Avg_Score -120.40995311545014\n",
      "Adding trajectory to replay buffer: step 20836, counter 332769\n",
      "Environment 12: Episode 4783, Score -120.86617969737762, Avg_Score -120.43667871021945\n",
      "Adding trajectory to replay buffer: step 20837, counter 332812\n",
      "Environment 3: Episode 4784, Score -121.88411014444878, Avg_Score -120.4536296655586\n",
      "Adding trajectory to replay buffer: step 20837, counter 332856\n",
      "Environment 6: Episode 4785, Score -121.83493912659027, Avg_Score -120.4482676299108\n",
      "Adding trajectory to replay buffer: step 20837, counter 332900\n",
      "Environment 15: Episode 4786, Score -121.81253935484092, Avg_Score -120.44594682710763\n",
      "Adding trajectory to replay buffer: step 20838, counter 332954\n",
      "Environment 13: Episode 4787, Score -113.3789896288782, Avg_Score -120.36213521295997\n",
      "Adding trajectory to replay buffer: step 20843, counter 333012\n",
      "Environment 9: Episode 4788, Score -116.1144156592476, Avg_Score -120.3295191899576\n",
      "Adding trajectory to replay buffer: step 20843, counter 333057\n",
      "Environment 10: Episode 4789, Score -121.89483502103062, Avg_Score -120.35897610669909\n",
      "Adding trajectory to replay buffer: step 20849, counter 333101\n",
      "Environment 8: Episode 4790, Score -120.67046434401333, Avg_Score -120.34661278553084\n",
      "Adding trajectory to replay buffer: step 20864, counter 333144\n",
      "Environment 4: Episode 4791, Score -123.14958812453163, Avg_Score -120.34975571587101\n",
      "Adding trajectory to replay buffer: step 20866, counter 333187\n",
      "Environment 5: Episode 4792, Score -123.12338287129998, Avg_Score -120.42663804638497\n",
      "Adding trajectory to replay buffer: step 20868, counter 333278\n",
      "Environment 7: Episode 4793, Score -123.01100507258842, Avg_Score -120.45273687298021\n",
      "Adding trajectory to replay buffer: step 20872, counter 333324\n",
      "Environment 0: Episode 4794, Score -123.49587098756432, Avg_Score -120.51450752980017\n",
      "Adding trajectory to replay buffer: step 20879, counter 333366\n",
      "Environment 6: Episode 4795, Score -119.99585822039471, Avg_Score -120.49079326271587\n",
      "Adding trajectory to replay buffer: step 20879, counter 333408\n",
      "Environment 15: Episode 4796, Score -120.0634766955624, Avg_Score -120.52158959280705\n",
      "Adding trajectory to replay buffer: step 20880, counter 333454\n",
      "Environment 2: Episode 4797, Score -119.72323931295549, Avg_Score -120.5791052008115\n",
      "Adding trajectory to replay buffer: step 20880, counter 333498\n",
      "Environment 12: Episode 4798, Score -122.77419262690718, Avg_Score -120.5767580905777\n",
      "Adding trajectory to replay buffer: step 20882, counter 333543\n",
      "Environment 3: Episode 4799, Score -121.98860208447712, Avg_Score -120.5821333054639\n",
      "Adding trajectory to replay buffer: step 20883, counter 333588\n",
      "Environment 13: Episode 4800, Score -116.98382228585395, Avg_Score -120.57716863639897\n",
      "Adding trajectory to replay buffer: step 20886, counter 333631\n",
      "Environment 9: Episode 4801, Score -120.62603898086772, Avg_Score -120.56737715448946\n",
      "Adding trajectory to replay buffer: step 20886, counter 333674\n",
      "Environment 10: Episode 4802, Score -122.3517182351059, Avg_Score -120.56353880008868\n",
      "Adding trajectory to replay buffer: step 20888, counter 333759\n",
      "Environment 14: Episode 4803, Score -122.3872818119737, Avg_Score -120.62904259799252\n",
      "Adding trajectory to replay buffer: step 20892, counter 333802\n",
      "Environment 8: Episode 4804, Score -118.5630206696329, Avg_Score -120.58567713910304\n",
      "Adding trajectory to replay buffer: step 20910, counter 333846\n",
      "Environment 5: Episode 4805, Score -122.92140572934225, Avg_Score -120.58049034580297\n",
      "Adding trajectory to replay buffer: step 20914, counter 333892\n",
      "Environment 7: Episode 4806, Score -120.60021419315366, Avg_Score -120.5704374421294\n",
      "Adding trajectory to replay buffer: step 20918, counter 333938\n",
      "Environment 0: Episode 4807, Score -121.47567443162265, Avg_Score -120.61205239075629\n",
      "Adding trajectory to replay buffer: step 20922, counter 333981\n",
      "Environment 6: Episode 4808, Score -123.95236718204306, Avg_Score -120.65784753433363\n",
      "Adding trajectory to replay buffer: step 20922, counter 334024\n",
      "Environment 15: Episode 4809, Score -121.80064008610509, Avg_Score -120.66277869451032\n",
      "Adding trajectory to replay buffer: step 20923, counter 334067\n",
      "Environment 2: Episode 4810, Score -122.53551910446149, Avg_Score -120.67321868509276\n",
      "Adding trajectory to replay buffer: step 20926, counter 334111\n",
      "Environment 3: Episode 4811, Score -122.36158743922908, Avg_Score -120.73499550066477\n",
      "Adding trajectory to replay buffer: step 20926, counter 334157\n",
      "Environment 12: Episode 4812, Score -121.77877281849086, Avg_Score -120.78983018710589\n",
      "Adding trajectory to replay buffer: step 20930, counter 334199\n",
      "Environment 14: Episode 4813, Score -115.76516764936814, Avg_Score -120.73056109779301\n",
      "Adding trajectory to replay buffer: step 20932, counter 334267\n",
      "Environment 4: Episode 4814, Score -114.90929768195748, Avg_Score -120.67976885960915\n",
      "Adding trajectory to replay buffer: step 20933, counter 334314\n",
      "Environment 9: Episode 4815, Score -120.77710451222895, Avg_Score -120.66900205848015\n",
      "Adding trajectory to replay buffer: step 20936, counter 334358\n",
      "Environment 8: Episode 4816, Score -123.09925824413139, Avg_Score -120.74853792477492\n",
      "Adding trajectory to replay buffer: step 20953, counter 334401\n",
      "Environment 5: Episode 4817, Score -122.8420477694124, Avg_Score -120.80185260606602\n",
      "Adding trajectory to replay buffer: step 20954, counter 334469\n",
      "Environment 10: Episode 4818, Score -115.06010095470275, Avg_Score -120.76360652149663\n",
      "Adding trajectory to replay buffer: step 20957, counter 334512\n",
      "Environment 7: Episode 4819, Score -122.95934050598679, Avg_Score -120.77968080506433\n",
      "Adding trajectory to replay buffer: step 20962, counter 334556\n",
      "Environment 0: Episode 4820, Score -121.67981986784315, Avg_Score -120.86031039526544\n",
      "Adding trajectory to replay buffer: step 20965, counter 334638\n",
      "Environment 13: Episode 4821, Score -112.65224351496498, Avg_Score -120.7543687360531\n",
      "Adding trajectory to replay buffer: step 20969, counter 334685\n",
      "Environment 6: Episode 4822, Score -122.34275614154339, Avg_Score -120.75243228220376\n",
      "Adding trajectory to replay buffer: step 20970, counter 334733\n",
      "Environment 15: Episode 4823, Score -120.24054670016281, Avg_Score -120.72762369450604\n",
      "Adding trajectory to replay buffer: step 20971, counter 334781\n",
      "Environment 2: Episode 4824, Score -120.144061881102, Avg_Score -120.66571364642198\n",
      "Adding trajectory to replay buffer: step 20973, counter 334828\n",
      "Environment 3: Episode 4825, Score -121.09385744803896, Avg_Score -120.6556059868068\n",
      "Adding trajectory to replay buffer: step 20977, counter 334875\n",
      "Environment 14: Episode 4826, Score -116.35599430624954, Avg_Score -120.59310062115907\n",
      "Adding trajectory to replay buffer: step 20981, counter 334924\n",
      "Environment 4: Episode 4827, Score -120.90292537758376, Avg_Score -120.57650823143112\n",
      "Adding trajectory to replay buffer: step 20983, counter 334971\n",
      "Environment 8: Episode 4828, Score -120.57517662330469, Avg_Score -120.55094850767526\n",
      "Adding trajectory to replay buffer: step 20994, counter 335039\n",
      "Environment 12: Episode 4829, Score -117.02860704515797, Avg_Score -120.50519383296297\n",
      "Adding trajectory to replay buffer: step 20996, counter 335082\n",
      "Environment 5: Episode 4830, Score -122.72597950544954, Avg_Score -120.50000782128471\n",
      "Adding trajectory to replay buffer: step 21002, counter 335127\n",
      "Environment 7: Episode 4831, Score -122.82053807855459, Avg_Score -120.4880978028555\n",
      "Adding trajectory to replay buffer: step 21002, counter 335196\n",
      "Environment 9: Episode 4832, Score -115.96457243027533, Avg_Score -120.4241581815877\n",
      "Adding trajectory to replay buffer: step 21004, counter 335238\n",
      "Environment 0: Episode 4833, Score -123.09782005606777, Avg_Score -120.42728063693481\n",
      "Adding trajectory to replay buffer: step 21007, counter 335461\n",
      "Environment 11: Episode 4834, Score -139.65679128168895, Avg_Score -120.60467443499138\n",
      "Adding trajectory to replay buffer: step 21007, counter 335503\n",
      "Environment 13: Episode 4835, Score -123.41691307969577, Avg_Score -120.60719574867012\n",
      "Adding trajectory to replay buffer: step 21013, counter 335547\n",
      "Environment 6: Episode 4836, Score -121.9561848561801, Avg_Score -120.65616023270535\n",
      "Adding trajectory to replay buffer: step 21014, counter 335591\n",
      "Environment 15: Episode 4837, Score -122.42935922419326, Avg_Score -120.64518082389674\n",
      "Adding trajectory to replay buffer: step 21016, counter 335636\n",
      "Environment 2: Episode 4838, Score -121.63648971199181, Avg_Score -120.68830459755614\n",
      "Adding trajectory to replay buffer: step 21024, counter 335683\n",
      "Environment 14: Episode 4839, Score -119.83812903968618, Avg_Score -120.66525420258338\n",
      "Adding trajectory to replay buffer: step 21027, counter 335727\n",
      "Environment 8: Episode 4840, Score -115.38016960785228, Avg_Score -120.64854873029473\n",
      "Adding trajectory to replay buffer: step 21029, counter 336043\n",
      "Environment 1: Episode 4841, Score -147.25283873907168, Avg_Score -120.95520270777934\n",
      "Adding trajectory to replay buffer: step 21036, counter 336125\n",
      "Environment 10: Episode 4842, Score -116.52543288600197, Avg_Score -120.96064393040699\n",
      "Adding trajectory to replay buffer: step 21037, counter 336166\n",
      "Environment 5: Episode 4843, Score -117.32892204928088, Avg_Score -120.90728522898519\n",
      "Adding trajectory to replay buffer: step 21053, counter 336215\n",
      "Environment 0: Episode 4844, Score -121.23755662237544, Avg_Score -120.91886967895573\n",
      "Adding trajectory to replay buffer: step 21054, counter 336256\n",
      "Environment 6: Episode 4845, Score -114.90643183820461, Avg_Score -120.84700598004456\n",
      "Adding trajectory to replay buffer: step 21058, counter 336307\n",
      "Environment 11: Episode 4846, Score -111.54568069302911, Avg_Score -120.75530660939702\n",
      "Adding trajectory to replay buffer: step 21058, counter 336351\n",
      "Environment 15: Episode 4847, Score -123.27478388432972, Avg_Score -120.75849643428671\n",
      "Adding trajectory to replay buffer: step 21060, counter 336395\n",
      "Environment 2: Episode 4848, Score -122.84363948287691, Avg_Score -120.77550606812846\n",
      "Adding trajectory to replay buffer: step 21063, counter 336451\n",
      "Environment 13: Episode 4849, Score -119.6235713288666, Avg_Score -120.75686944133474\n",
      "Adding trajectory to replay buffer: step 21064, counter 336513\n",
      "Environment 9: Episode 4850, Score -117.65339125336583, Avg_Score -120.74497986961316\n",
      "Adding trajectory to replay buffer: step 21067, counter 336556\n",
      "Environment 14: Episode 4851, Score -120.0522879086056, Avg_Score -120.79111332877196\n",
      "Adding trajectory to replay buffer: step 21068, counter 336630\n",
      "Environment 12: Episode 4852, Score -116.79557307995732, Avg_Score -120.70819501585116\n",
      "Adding trajectory to replay buffer: step 21071, counter 336674\n",
      "Environment 8: Episode 4853, Score -122.5369453229128, Avg_Score -120.71082739448164\n",
      "Adding trajectory to replay buffer: step 21074, counter 336719\n",
      "Environment 1: Episode 4854, Score -123.121214568081, Avg_Score -120.8087123096842\n",
      "Adding trajectory to replay buffer: step 21079, counter 336761\n",
      "Environment 5: Episode 4855, Score -122.6647522856264, Avg_Score -120.80403695877487\n",
      "Adding trajectory to replay buffer: step 21080, counter 336805\n",
      "Environment 10: Episode 4856, Score -122.51413331221664, Avg_Score -120.79939150879744\n",
      "Adding trajectory to replay buffer: step 21094, counter 336897\n",
      "Environment 7: Episode 4857, Score -124.45070254808975, Avg_Score -120.81688143689773\n",
      "Adding trajectory to replay buffer: step 21096, counter 336940\n",
      "Environment 0: Episode 4858, Score -122.25310650988109, Avg_Score -120.86921719715079\n",
      "Adding trajectory to replay buffer: step 21097, counter 336983\n",
      "Environment 6: Episode 4859, Score -123.01249545091764, Avg_Score -120.86898590268154\n",
      "Adding trajectory to replay buffer: step 21101, counter 337026\n",
      "Environment 15: Episode 4860, Score -122.47040129995844, Avg_Score -120.99003870522523\n",
      "Adding trajectory to replay buffer: step 21102, counter 337155\n",
      "Environment 3: Episode 4861, Score -128.1729098028677, Avg_Score -121.06077401232973\n",
      "Adding trajectory to replay buffer: step 21102, counter 337199\n",
      "Environment 11: Episode 4862, Score -123.07484271688698, Avg_Score -121.05928996274555\n",
      "Adding trajectory to replay buffer: step 21103, counter 337242\n",
      "Environment 2: Episode 4863, Score -123.25471105424067, Avg_Score -121.06616064995065\n",
      "Adding trajectory to replay buffer: step 21110, counter 337289\n",
      "Environment 13: Episode 4864, Score -120.71167158828243, Avg_Score -121.04532473432978\n",
      "Adding trajectory to replay buffer: step 21113, counter 337338\n",
      "Environment 9: Episode 4865, Score -115.54271596883176, Avg_Score -120.96550329862367\n",
      "Adding trajectory to replay buffer: step 21120, counter 337384\n",
      "Environment 1: Episode 4866, Score -120.79097811862516, Avg_Score -120.93940371252108\n",
      "Adding trajectory to replay buffer: step 21122, counter 337427\n",
      "Environment 5: Episode 4867, Score -123.05098646080307, Avg_Score -120.95710607733604\n",
      "Adding trajectory to replay buffer: step 21123, counter 337482\n",
      "Environment 12: Episode 4868, Score -121.50810298002047, Avg_Score -120.95617863081739\n",
      "Adding trajectory to replay buffer: step 21125, counter 337527\n",
      "Environment 10: Episode 4869, Score -122.04180262715556, Avg_Score -120.94856029180424\n",
      "Adding trajectory to replay buffer: step 21128, counter 337588\n",
      "Environment 14: Episode 4870, Score -113.6983185901766, Avg_Score -120.8589817068308\n",
      "Adding trajectory to replay buffer: step 21143, counter 337629\n",
      "Environment 11: Episode 4871, Score -114.60159105223852, Avg_Score -120.78643796353846\n",
      "Adding trajectory to replay buffer: step 21144, counter 337676\n",
      "Environment 6: Episode 4872, Score -115.13875437050996, Avg_Score -120.79998102188\n",
      "Adding trajectory to replay buffer: step 21147, counter 337752\n",
      "Environment 8: Episode 4873, Score -120.42983672978046, Avg_Score -120.75207873124819\n",
      "Adding trajectory to replay buffer: step 21156, counter 337812\n",
      "Environment 0: Episode 4874, Score -117.86758354904858, Avg_Score -120.71138684017957\n",
      "Adding trajectory to replay buffer: step 21159, counter 337861\n",
      "Environment 13: Episode 4875, Score -117.40555531690269, Avg_Score -120.68907408008128\n",
      "Adding trajectory to replay buffer: step 21164, counter 337905\n",
      "Environment 1: Episode 4876, Score -122.26922440132266, Avg_Score -120.71082005182626\n",
      "Adding trajectory to replay buffer: step 21164, counter 337975\n",
      "Environment 7: Episode 4877, Score -110.71780838495245, Avg_Score -120.66052204825868\n",
      "Adding trajectory to replay buffer: step 21164, counter 338026\n",
      "Environment 9: Episode 4878, Score -117.78058700139262, Avg_Score -120.6110230584785\n",
      "Adding trajectory to replay buffer: step 21165, counter 338088\n",
      "Environment 2: Episode 4879, Score -115.85575304597182, Avg_Score -120.65625624987517\n",
      "Adding trajectory to replay buffer: step 21166, counter 338132\n",
      "Environment 5: Episode 4880, Score -121.98377478491702, Avg_Score -120.6476012443282\n",
      "Adding trajectory to replay buffer: step 21168, counter 338177\n",
      "Environment 12: Episode 4881, Score -121.36397077641264, Avg_Score -120.73920490777714\n",
      "Adding trajectory to replay buffer: step 21169, counter 338365\n",
      "Environment 4: Episode 4882, Score -124.69533644034621, Avg_Score -120.75491681861294\n",
      "Adding trajectory to replay buffer: step 21169, counter 338409\n",
      "Environment 10: Episode 4883, Score -121.73569712181637, Avg_Score -120.76361199285734\n",
      "Adding trajectory to replay buffer: step 21177, counter 338458\n",
      "Environment 14: Episode 4884, Score -115.9617593583297, Avg_Score -120.70438848499616\n",
      "Adding trajectory to replay buffer: step 21185, counter 338500\n",
      "Environment 11: Episode 4885, Score -122.75598401473027, Avg_Score -120.71359893387755\n",
      "Adding trajectory to replay buffer: step 21187, counter 338543\n",
      "Environment 6: Episode 4886, Score -123.03354407352458, Avg_Score -120.72580898106442\n",
      "Adding trajectory to replay buffer: step 21188, counter 338629\n",
      "Environment 3: Episode 4887, Score -114.70535483796957, Avg_Score -120.73907263315533\n",
      "Adding trajectory to replay buffer: step 21190, counter 338672\n",
      "Environment 8: Episode 4888, Score -123.008135609497, Avg_Score -120.80800983265785\n",
      "Adding trajectory to replay buffer: step 21190, counter 338761\n",
      "Environment 15: Episode 4889, Score -117.70650951920574, Avg_Score -120.76612657763957\n",
      "Adding trajectory to replay buffer: step 21203, counter 338808\n",
      "Environment 0: Episode 4890, Score -122.60575429818903, Avg_Score -120.78547947718135\n",
      "Adding trajectory to replay buffer: step 21205, counter 338854\n",
      "Environment 13: Episode 4891, Score -118.70681708906218, Avg_Score -120.74105176682666\n",
      "Adding trajectory to replay buffer: step 21207, counter 338897\n",
      "Environment 1: Episode 4892, Score -120.51496752079204, Avg_Score -120.71496761332156\n",
      "Adding trajectory to replay buffer: step 21208, counter 338941\n",
      "Environment 9: Episode 4893, Score -122.3033701288843, Avg_Score -120.70789126388452\n",
      "Adding trajectory to replay buffer: step 21210, counter 338986\n",
      "Environment 2: Episode 4894, Score -117.96377749282246, Avg_Score -120.6525703289371\n",
      "Adding trajectory to replay buffer: step 21210, counter 339032\n",
      "Environment 7: Episode 4895, Score -121.54656522251045, Avg_Score -120.66807739895826\n",
      "Adding trajectory to replay buffer: step 21212, counter 339078\n",
      "Environment 5: Episode 4896, Score -122.12833001003973, Avg_Score -120.68872593210308\n",
      "Adding trajectory to replay buffer: step 21216, counter 339125\n",
      "Environment 4: Episode 4897, Score -117.59286231888055, Avg_Score -120.66742216216232\n",
      "Adding trajectory to replay buffer: step 21225, counter 339173\n",
      "Environment 14: Episode 4898, Score -123.53270709369704, Avg_Score -120.67500730683021\n",
      "Adding trajectory to replay buffer: step 21229, counter 339217\n",
      "Environment 11: Episode 4899, Score -121.39384515278974, Avg_Score -120.66905973751336\n",
      "Adding trajectory to replay buffer: step 21230, counter 339260\n",
      "Environment 6: Episode 4900, Score -122.52875302223737, Avg_Score -120.72450904487718\n",
      "Adding trajectory to replay buffer: step 21232, counter 339304\n",
      "Environment 3: Episode 4901, Score -120.4882623559162, Avg_Score -120.72313127862768\n",
      "Adding trajectory to replay buffer: step 21234, counter 339348\n",
      "Environment 15: Episode 4902, Score -118.5488269277507, Avg_Score -120.68510236555412\n",
      "Adding trajectory to replay buffer: step 21238, counter 339396\n",
      "Environment 8: Episode 4903, Score -120.75192436323502, Avg_Score -120.66874879106672\n",
      "Adding trajectory to replay buffer: step 21247, counter 339440\n",
      "Environment 0: Episode 4904, Score -122.78580016334604, Avg_Score -120.71097658600385\n",
      "Adding trajectory to replay buffer: step 21253, counter 339483\n",
      "Environment 2: Episode 4905, Score -116.59715583531808, Avg_Score -120.6477340870636\n",
      "Adding trajectory to replay buffer: step 21256, counter 339527\n",
      "Environment 5: Episode 4906, Score -120.46636857311552, Avg_Score -120.64639563086327\n",
      "Adding trajectory to replay buffer: step 21256, counter 339578\n",
      "Environment 13: Episode 4907, Score -120.82018218129068, Avg_Score -120.63984070835991\n",
      "Adding trajectory to replay buffer: step 21258, counter 339620\n",
      "Environment 4: Episode 4908, Score -122.96447810812606, Avg_Score -120.62996181762075\n",
      "Adding trajectory to replay buffer: step 21266, counter 339718\n",
      "Environment 12: Episode 4909, Score -115.2951477008971, Avg_Score -120.56490689376868\n",
      "Adding trajectory to replay buffer: step 21276, counter 339762\n",
      "Environment 3: Episode 4910, Score -121.81455240886038, Avg_Score -120.55769722681265\n",
      "Adding trajectory to replay buffer: step 21276, counter 339809\n",
      "Environment 11: Episode 4911, Score -120.2882100412827, Avg_Score -120.5369634528332\n",
      "Adding trajectory to replay buffer: step 21280, counter 339882\n",
      "Environment 1: Episode 4912, Score -120.29619321370436, Avg_Score -120.52213765678533\n",
      "Adding trajectory to replay buffer: step 21280, counter 339954\n",
      "Environment 9: Episode 4913, Score -119.67305109331136, Avg_Score -120.56121649122478\n",
      "Adding trajectory to replay buffer: step 21280, counter 340000\n",
      "Environment 15: Episode 4914, Score -121.6549471941072, Avg_Score -120.62867298634626\n",
      "Adding trajectory to replay buffer: step 21282, counter 340072\n",
      "Environment 7: Episode 4915, Score -117.58829643899358, Avg_Score -120.59678490561387\n",
      "Adding trajectory to replay buffer: step 21283, counter 340117\n",
      "Environment 8: Episode 4916, Score -121.97553029744017, Avg_Score -120.585547626147\n",
      "Adding trajectory to replay buffer: step 21295, counter 340165\n",
      "Environment 0: Episode 4917, Score -120.11164858156258, Avg_Score -120.55824363426849\n",
      "Adding trajectory to replay buffer: step 21295, counter 340235\n",
      "Environment 14: Episode 4918, Score -117.798195172932, Avg_Score -120.58562457645075\n",
      "Adding trajectory to replay buffer: step 21296, counter 340278\n",
      "Environment 2: Episode 4919, Score -121.77603576714732, Avg_Score -120.57379152906236\n",
      "Adding trajectory to replay buffer: step 21302, counter 340324\n",
      "Environment 5: Episode 4920, Score -122.08136972063035, Avg_Score -120.57780702759023\n",
      "Adding trajectory to replay buffer: step 21302, counter 340370\n",
      "Environment 13: Episode 4921, Score -122.1485429521793, Avg_Score -120.67277002196236\n",
      "Adding trajectory to replay buffer: step 21312, counter 340416\n",
      "Environment 12: Episode 4922, Score -117.37535589730926, Avg_Score -120.62309601952003\n",
      "Adding trajectory to replay buffer: step 21319, counter 340459\n",
      "Environment 11: Episode 4923, Score -123.3710462163873, Avg_Score -120.65440101468228\n",
      "Adding trajectory to replay buffer: step 21325, counter 340504\n",
      "Environment 1: Episode 4924, Score -121.0801029206055, Avg_Score -120.66376142507733\n",
      "Adding trajectory to replay buffer: step 21326, counter 340661\n",
      "Environment 10: Episode 4925, Score -118.72665027985784, Avg_Score -120.64008935339552\n",
      "Adding trajectory to replay buffer: step 21328, counter 340706\n",
      "Environment 8: Episode 4926, Score -120.2151648025221, Avg_Score -120.67868105835824\n",
      "Adding trajectory to replay buffer: step 21329, counter 340753\n",
      "Environment 7: Episode 4927, Score -119.89571652292895, Avg_Score -120.66860896981169\n",
      "Adding trajectory to replay buffer: step 21338, counter 340795\n",
      "Environment 2: Episode 4928, Score -118.07414448950315, Avg_Score -120.64359864847367\n",
      "Adding trajectory to replay buffer: step 21339, counter 340839\n",
      "Environment 0: Episode 4929, Score -117.53872881769638, Avg_Score -120.64869986619904\n",
      "Adding trajectory to replay buffer: step 21341, counter 340885\n",
      "Environment 14: Episode 4930, Score -121.61382399088579, Avg_Score -120.6375783110534\n",
      "Adding trajectory to replay buffer: step 21344, counter 340949\n",
      "Environment 9: Episode 4931, Score -111.69953915116525, Avg_Score -120.5263683217795\n",
      "Adding trajectory to replay buffer: step 21344, counter 340991\n",
      "Environment 13: Episode 4932, Score -117.82279469654027, Avg_Score -120.54495054444214\n",
      "Adding trajectory to replay buffer: step 21359, counter 341038\n",
      "Environment 12: Episode 4933, Score -120.44824509691199, Avg_Score -120.51845479485058\n",
      "Adding trajectory to replay buffer: step 21362, counter 341120\n",
      "Environment 15: Episode 4934, Score -114.22946564889762, Avg_Score -120.26418153852266\n",
      "Adding trajectory to replay buffer: step 21363, counter 341207\n",
      "Environment 3: Episode 4935, Score -113.20828393917562, Avg_Score -120.16209524711746\n",
      "Adding trajectory to replay buffer: step 21366, counter 341254\n",
      "Environment 11: Episode 4936, Score -121.21893550369764, Avg_Score -120.15472275359264\n",
      "Adding trajectory to replay buffer: step 21367, counter 341296\n",
      "Environment 1: Episode 4937, Score -122.98447959611937, Avg_Score -120.16027395731192\n",
      "Adding trajectory to replay buffer: step 21368, counter 341338\n",
      "Environment 10: Episode 4938, Score -123.22128267188121, Avg_Score -120.17612188691079\n",
      "Adding trajectory to replay buffer: step 21371, counter 341381\n",
      "Environment 8: Episode 4939, Score -120.6509215222622, Avg_Score -120.18424981173655\n",
      "Adding trajectory to replay buffer: step 21372, counter 341424\n",
      "Environment 7: Episode 4940, Score -121.8432468531495, Avg_Score -120.24888058418954\n",
      "Adding trajectory to replay buffer: step 21381, counter 341575\n",
      "Environment 6: Episode 4941, Score -131.67639150418495, Avg_Score -120.09311611184069\n",
      "Adding trajectory to replay buffer: step 21381, counter 341615\n",
      "Environment 14: Episode 4942, Score -117.82769923559327, Avg_Score -120.1061387753366\n",
      "Adding trajectory to replay buffer: step 21383, counter 341660\n",
      "Environment 2: Episode 4943, Score -116.78100469452578, Avg_Score -120.10065960178905\n",
      "Adding trajectory to replay buffer: step 21384, counter 341705\n",
      "Environment 0: Episode 4944, Score -123.83929492281874, Avg_Score -120.12667698479346\n",
      "Adding trajectory to replay buffer: step 21384, counter 341787\n",
      "Environment 5: Episode 4945, Score -121.1422648649079, Avg_Score -120.1890353150605\n",
      "Adding trajectory to replay buffer: step 21387, counter 341830\n",
      "Environment 13: Episode 4946, Score -123.4495494108976, Avg_Score -120.30807400223917\n",
      "Adding trajectory to replay buffer: step 21388, counter 341874\n",
      "Environment 9: Episode 4947, Score -122.5206592606126, Avg_Score -120.30053275600201\n",
      "Adding trajectory to replay buffer: step 21401, counter 341916\n",
      "Environment 12: Episode 4948, Score -122.8931541164058, Avg_Score -120.3010279023373\n",
      "Adding trajectory to replay buffer: step 21405, counter 341959\n",
      "Environment 15: Episode 4949, Score -122.49590488906576, Avg_Score -120.32975123793928\n",
      "Adding trajectory to replay buffer: step 21408, counter 342004\n",
      "Environment 3: Episode 4950, Score -122.21451484091736, Avg_Score -120.3753624738148\n",
      "Adding trajectory to replay buffer: step 21410, counter 342048\n",
      "Environment 11: Episode 4951, Score -116.3148276059162, Avg_Score -120.33798787078791\n",
      "Adding trajectory to replay buffer: step 21412, counter 342092\n",
      "Environment 10: Episode 4952, Score -116.97132340213346, Avg_Score -120.33974537400967\n",
      "Adding trajectory to replay buffer: step 21413, counter 342138\n",
      "Environment 1: Episode 4953, Score -123.40501985254139, Avg_Score -120.34842611930596\n",
      "Adding trajectory to replay buffer: step 21420, counter 342187\n",
      "Environment 8: Episode 4954, Score -118.2840013767255, Avg_Score -120.3000539873924\n",
      "Adding trajectory to replay buffer: step 21423, counter 342238\n",
      "Environment 7: Episode 4955, Score -117.2929480860519, Avg_Score -120.24633594539665\n",
      "Adding trajectory to replay buffer: step 21427, counter 342282\n",
      "Environment 2: Episode 4956, Score -120.74214831564203, Avg_Score -120.2286160954309\n",
      "Adding trajectory to replay buffer: step 21428, counter 342326\n",
      "Environment 0: Episode 4957, Score -122.35519623320425, Avg_Score -120.20766103228203\n",
      "Adding trajectory to replay buffer: step 21428, counter 342370\n",
      "Environment 5: Episode 4958, Score -119.4040464699169, Avg_Score -120.17917043188237\n",
      "Adding trajectory to replay buffer: step 21431, counter 342413\n",
      "Environment 9: Episode 4959, Score -117.87907961123065, Avg_Score -120.1278362734855\n",
      "Adding trajectory to replay buffer: step 21434, counter 342460\n",
      "Environment 13: Episode 4960, Score -120.64636128063003, Avg_Score -120.1095958732922\n",
      "Adding trajectory to replay buffer: step 21441, counter 342520\n",
      "Environment 6: Episode 4961, Score -116.72859849035305, Avg_Score -119.99515276016706\n",
      "Adding trajectory to replay buffer: step 21447, counter 342586\n",
      "Environment 14: Episode 4962, Score -118.38839715723321, Avg_Score -119.94828830457054\n",
      "Adding trajectory to replay buffer: step 21451, counter 342632\n",
      "Environment 15: Episode 4963, Score -120.89445985857523, Avg_Score -119.92468579261389\n",
      "Adding trajectory to replay buffer: step 21456, counter 342678\n",
      "Environment 11: Episode 4964, Score -121.0173689287578, Avg_Score -119.92774276601865\n",
      "Adding trajectory to replay buffer: step 21457, counter 342727\n",
      "Environment 3: Episode 4965, Score -117.10737768741262, Avg_Score -119.94338938320446\n",
      "Adding trajectory to replay buffer: step 21463, counter 342770\n",
      "Environment 8: Episode 4966, Score -122.70868650534811, Avg_Score -119.96256646707168\n",
      "Adding trajectory to replay buffer: step 21470, counter 342812\n",
      "Environment 0: Episode 4967, Score -122.70851003286614, Avg_Score -119.95914170279234\n",
      "Adding trajectory to replay buffer: step 21470, counter 342855\n",
      "Environment 2: Episode 4968, Score -123.31570432720147, Avg_Score -119.97721771626415\n",
      "Adding trajectory to replay buffer: step 21470, counter 342897\n",
      "Environment 5: Episode 4969, Score -122.69767681102081, Avg_Score -119.9837764581028\n",
      "Adding trajectory to replay buffer: step 21474, counter 342970\n",
      "Environment 12: Episode 4970, Score -116.85556085970998, Avg_Score -120.01534888079816\n",
      "Adding trajectory to replay buffer: step 21475, counter 343014\n",
      "Environment 9: Episode 4971, Score -121.57965429169747, Avg_Score -120.08512951319275\n",
      "Adding trajectory to replay buffer: step 21478, counter 343058\n",
      "Environment 13: Episode 4972, Score -122.25413137811236, Avg_Score -120.15628328326879\n",
      "Adding trajectory to replay buffer: step 21485, counter 343131\n",
      "Environment 10: Episode 4973, Score -120.24276619501536, Avg_Score -120.15441257792112\n",
      "Adding trajectory to replay buffer: step 21486, counter 343176\n",
      "Environment 6: Episode 4974, Score -119.1633682091677, Avg_Score -120.1673704245223\n",
      "Adding trajectory to replay buffer: step 21494, counter 343412\n",
      "Environment 4: Episode 4975, Score -125.01358749435775, Avg_Score -120.24345074629686\n",
      "Adding trajectory to replay buffer: step 21498, counter 343454\n",
      "Environment 11: Episode 4976, Score -123.15029560478715, Avg_Score -120.25226145833152\n",
      "Adding trajectory to replay buffer: step 21500, counter 343541\n",
      "Environment 1: Episode 4977, Score -125.73006806655414, Avg_Score -120.40238405514754\n",
      "Adding trajectory to replay buffer: step 21502, counter 343586\n",
      "Environment 3: Episode 4978, Score -121.90761386960807, Avg_Score -120.4436543238297\n",
      "Adding trajectory to replay buffer: step 21518, counter 343630\n",
      "Environment 12: Episode 4979, Score -121.44934695115685, Avg_Score -120.49959026288155\n",
      "Adding trajectory to replay buffer: step 21521, counter 343704\n",
      "Environment 14: Episode 4980, Score -115.83269680867468, Avg_Score -120.43807948311913\n",
      "Adding trajectory to replay buffer: step 21523, counter 343757\n",
      "Environment 0: Episode 4981, Score -118.25270186165845, Avg_Score -120.40696679397156\n",
      "Adding trajectory to replay buffer: step 21523, counter 343802\n",
      "Environment 13: Episode 4982, Score -116.52006146351371, Avg_Score -120.32521404420324\n",
      "Adding trajectory to replay buffer: step 21526, counter 343858\n",
      "Environment 2: Episode 4983, Score -114.30346740374652, Avg_Score -120.25089174702256\n",
      "Adding trajectory to replay buffer: step 21528, counter 343901\n",
      "Environment 10: Episode 4984, Score -122.6683112770364, Avg_Score -120.31795726620963\n",
      "Adding trajectory to replay buffer: step 21530, counter 343961\n",
      "Environment 5: Episode 4985, Score -115.23598791333102, Avg_Score -120.24275730519565\n",
      "Adding trajectory to replay buffer: step 21540, counter 344007\n",
      "Environment 4: Episode 4986, Score -121.53509120809659, Avg_Score -120.22777277654136\n",
      "Adding trajectory to replay buffer: step 21540, counter 344061\n",
      "Environment 6: Episode 4987, Score -112.89014460193924, Avg_Score -120.20962067418105\n",
      "Adding trajectory to replay buffer: step 21542, counter 344105\n",
      "Environment 11: Episode 4988, Score -119.03396831934714, Avg_Score -120.16987900127957\n",
      "Adding trajectory to replay buffer: step 21552, counter 344155\n",
      "Environment 3: Episode 4989, Score -117.73707571438638, Avg_Score -120.17018466323138\n",
      "Adding trajectory to replay buffer: step 21580, counter 344212\n",
      "Environment 0: Episode 4990, Score -118.23190363096039, Avg_Score -120.12644615655907\n",
      "Adding trajectory to replay buffer: step 21584, counter 344266\n",
      "Environment 5: Episode 4991, Score -117.42258508864914, Avg_Score -120.11360383655496\n",
      "Adding trajectory to replay buffer: step 21585, counter 344311\n",
      "Environment 4: Episode 4992, Score -121.80146221311577, Avg_Score -120.12646878347819\n",
      "Adding trajectory to replay buffer: step 21585, counter 344354\n",
      "Environment 11: Episode 4993, Score -122.42308366593844, Avg_Score -120.12766591884875\n",
      "Adding trajectory to replay buffer: step 21586, counter 344400\n",
      "Environment 6: Episode 4994, Score -120.58143867541477, Avg_Score -120.15384253067467\n",
      "Adding trajectory to replay buffer: step 21591, counter 344491\n",
      "Environment 1: Episode 4995, Score -117.34364012084963, Avg_Score -120.11181327965805\n",
      "Adding trajectory to replay buffer: step 21595, counter 344534\n",
      "Environment 3: Episode 4996, Score -117.93208739223145, Avg_Score -120.06985085347995\n",
      "Adding trajectory to replay buffer: step 21595, counter 344606\n",
      "Environment 13: Episode 4997, Score -115.32412845188068, Avg_Score -120.04716351480997\n",
      "Adding trajectory to replay buffer: step 21597, counter 344752\n",
      "Environment 15: Episode 4998, Score -118.32884813725684, Avg_Score -119.99512492524556\n",
      "Adding trajectory to replay buffer: step 21607, counter 344884\n",
      "Environment 9: Episode 4999, Score -115.17225177525341, Avg_Score -119.9329089914702\n",
      "Adding trajectory to replay buffer: step 21616, counter 345037\n",
      "Environment 8: Episode 5000, Score -120.73581239160026, Avg_Score -119.91497958516382\n",
      "Adding trajectory to replay buffer: step 21625, counter 345144\n",
      "Environment 12: Episode 5001, Score -124.09962452927108, Avg_Score -119.95109320689735\n",
      "Adding trajectory to replay buffer: step 21628, counter 345192\n",
      "Environment 0: Episode 5002, Score -122.99863844042426, Avg_Score -119.99559132202407\n",
      "Adding trajectory to replay buffer: step 21630, counter 345237\n",
      "Environment 11: Episode 5003, Score -122.15897822814435, Avg_Score -120.00966186067316\n",
      "Adding trajectory to replay buffer: step 21632, counter 345284\n",
      "Environment 4: Episode 5004, Score -122.24691473996762, Avg_Score -120.00427300643938\n",
      "Adding trajectory to replay buffer: step 21633, counter 345331\n",
      "Environment 6: Episode 5005, Score -122.72790806106354, Avg_Score -120.06558052869684\n",
      "Adding trajectory to replay buffer: step 21635, counter 345375\n",
      "Environment 1: Episode 5006, Score -121.66258142536569, Avg_Score -120.07754265721937\n",
      "Adding trajectory to replay buffer: step 21638, counter 345418\n",
      "Environment 13: Episode 5007, Score -118.26519476569631, Avg_Score -120.05199278306341\n",
      "Adding trajectory to replay buffer: step 21639, counter 345462\n",
      "Environment 3: Episode 5008, Score -121.17268135890924, Avg_Score -120.03407481557124\n",
      "Adding trajectory to replay buffer: step 21642, counter 345507\n",
      "Environment 15: Episode 5009, Score -122.46624811321684, Avg_Score -120.10578581969443\n",
      "Adding trajectory to replay buffer: step 21644, counter 345728\n",
      "Environment 7: Episode 5010, Score -123.07186274242908, Avg_Score -120.11835892303012\n",
      "Adding trajectory to replay buffer: step 21651, counter 345772\n",
      "Environment 9: Episode 5011, Score -120.29920269982642, Avg_Score -120.11846884961557\n",
      "Adding trajectory to replay buffer: step 21666, counter 345813\n",
      "Environment 12: Episode 5012, Score -114.65304572826251, Avg_Score -120.06203737476115\n",
      "Adding trajectory to replay buffer: step 21668, counter 345897\n",
      "Environment 5: Episode 5013, Score -122.32012842211253, Avg_Score -120.08850814804914\n",
      "Adding trajectory to replay buffer: step 21673, counter 345942\n",
      "Environment 0: Episode 5014, Score -117.27559528245715, Avg_Score -120.04471462893265\n",
      "Adding trajectory to replay buffer: step 21677, counter 346003\n",
      "Environment 8: Episode 5015, Score -115.95413813025505, Avg_Score -120.02837304584526\n",
      "Adding trajectory to replay buffer: step 21681, counter 346051\n",
      "Environment 6: Episode 5016, Score -119.14998194638329, Avg_Score -120.0001175623347\n",
      "Adding trajectory to replay buffer: step 21681, counter 346102\n",
      "Environment 11: Episode 5017, Score -117.62672650149464, Avg_Score -119.97526834153399\n",
      "Adding trajectory to replay buffer: step 21682, counter 346146\n",
      "Environment 13: Episode 5018, Score -121.6395856134308, Avg_Score -120.013682245939\n",
      "Adding trajectory to replay buffer: step 21683, counter 346194\n",
      "Environment 1: Episode 5019, Score -120.02291660848012, Avg_Score -119.99615105435234\n",
      "Adding trajectory to replay buffer: step 21683, counter 346356\n",
      "Environment 14: Episode 5020, Score -118.80730011108766, Avg_Score -119.9634103582569\n",
      "Adding trajectory to replay buffer: step 21684, counter 346514\n",
      "Environment 2: Episode 5021, Score -118.88502185777598, Avg_Score -119.93077514731286\n",
      "Adding trajectory to replay buffer: step 21684, counter 346556\n",
      "Environment 15: Episode 5022, Score -123.10304365916551, Avg_Score -119.98805202493143\n",
      "Adding trajectory to replay buffer: step 21686, counter 346610\n",
      "Environment 4: Episode 5023, Score -113.65528976069143, Avg_Score -119.89089446037445\n",
      "Adding trajectory to replay buffer: step 21689, counter 346655\n",
      "Environment 7: Episode 5024, Score -121.58982076899895, Avg_Score -119.89599163885839\n",
      "Adding trajectory to replay buffer: step 21702, counter 346706\n",
      "Environment 9: Episode 5025, Score -117.76536760548689, Avg_Score -119.88637881211469\n",
      "Adding trajectory to replay buffer: step 21708, counter 346775\n",
      "Environment 3: Episode 5026, Score -114.1408614525528, Avg_Score -119.82563577861501\n",
      "Adding trajectory to replay buffer: step 21708, counter 346955\n",
      "Environment 10: Episode 5027, Score -119.55851647464186, Avg_Score -119.82226377813214\n",
      "Adding trajectory to replay buffer: step 21710, counter 346997\n",
      "Environment 5: Episode 5028, Score -118.0461099535854, Avg_Score -119.82198343277298\n",
      "Adding trajectory to replay buffer: step 21710, counter 347041\n",
      "Environment 12: Episode 5029, Score -122.88033908731987, Avg_Score -119.87539953546923\n",
      "Adding trajectory to replay buffer: step 21718, counter 347086\n",
      "Environment 0: Episode 5030, Score -121.04561106008043, Avg_Score -119.86971740616116\n",
      "Adding trajectory to replay buffer: step 21725, counter 347128\n",
      "Environment 1: Episode 5031, Score -117.7533217338454, Avg_Score -119.93025523198797\n",
      "Adding trajectory to replay buffer: step 21727, counter 347171\n",
      "Environment 2: Episode 5032, Score -122.58547092114296, Avg_Score -119.977881994234\n",
      "Adding trajectory to replay buffer: step 21727, counter 347214\n",
      "Environment 15: Episode 5033, Score -122.67060105500556, Avg_Score -120.00010555381493\n",
      "Adding trajectory to replay buffer: step 21729, counter 347257\n",
      "Environment 4: Episode 5034, Score -120.41047497199911, Avg_Score -120.06191564704595\n",
      "Adding trajectory to replay buffer: step 21729, counter 347303\n",
      "Environment 14: Episode 5035, Score -120.90518304693327, Avg_Score -120.13888463812353\n",
      "Adding trajectory to replay buffer: step 21732, counter 347354\n",
      "Environment 11: Episode 5036, Score -119.01178071131744, Avg_Score -120.11681309019973\n",
      "Adding trajectory to replay buffer: step 21734, counter 347399\n",
      "Environment 7: Episode 5037, Score -121.68067416125473, Avg_Score -120.10377503585104\n",
      "Adding trajectory to replay buffer: step 21746, counter 347443\n",
      "Environment 9: Episode 5038, Score -123.1308740076491, Avg_Score -120.10287094920875\n",
      "Adding trajectory to replay buffer: step 21753, counter 347488\n",
      "Environment 3: Episode 5039, Score -121.73103314223764, Avg_Score -120.11367206540852\n",
      "Adding trajectory to replay buffer: step 21754, counter 347534\n",
      "Environment 10: Episode 5040, Score -121.35965236502204, Avg_Score -120.10883612052723\n",
      "Adding trajectory to replay buffer: step 21761, counter 347577\n",
      "Environment 0: Episode 5041, Score -122.0147554980206, Avg_Score -120.0122197604656\n",
      "Adding trajectory to replay buffer: step 21762, counter 347657\n",
      "Environment 13: Episode 5042, Score -123.73623703751156, Avg_Score -120.07130513848479\n",
      "Adding trajectory to replay buffer: step 21770, counter 347702\n",
      "Environment 1: Episode 5043, Score -122.7189703234577, Avg_Score -120.1306847947741\n",
      "Adding trajectory to replay buffer: step 21771, counter 347744\n",
      "Environment 4: Episode 5044, Score -122.7183232235033, Avg_Score -120.11947507778095\n",
      "Adding trajectory to replay buffer: step 21772, counter 347789\n",
      "Environment 2: Episode 5045, Score -121.19020301943956, Avg_Score -120.1199544593263\n",
      "Adding trajectory to replay buffer: step 21772, counter 347832\n",
      "Environment 14: Episode 5046, Score -123.30888562357798, Avg_Score -120.11854782145306\n",
      "Adding trajectory to replay buffer: step 21775, counter 347875\n",
      "Environment 11: Episode 5047, Score -121.67619704434959, Avg_Score -120.11010319929045\n",
      "Adding trajectory to replay buffer: step 21778, counter 347943\n",
      "Environment 5: Episode 5048, Score -112.39083303156495, Avg_Score -120.00507998844203\n",
      "Adding trajectory to replay buffer: step 21778, counter 347987\n",
      "Environment 7: Episode 5049, Score -122.74834463829734, Avg_Score -120.00760438593437\n",
      "Adding trajectory to replay buffer: step 21798, counter 348032\n",
      "Environment 3: Episode 5050, Score -123.26242965949079, Avg_Score -120.01808353412011\n",
      "Adding trajectory to replay buffer: step 21804, counter 348075\n",
      "Environment 0: Episode 5051, Score -122.75796207878801, Avg_Score -120.0825148788488\n",
      "Adding trajectory to replay buffer: step 21804, counter 348198\n",
      "Environment 6: Episode 5052, Score -118.20396160994719, Avg_Score -120.09484126092693\n",
      "Adding trajectory to replay buffer: step 21805, counter 348241\n",
      "Environment 13: Episode 5053, Score -122.2632047551119, Avg_Score -120.08342310995266\n",
      "Adding trajectory to replay buffer: step 21816, counter 348287\n",
      "Environment 1: Episode 5054, Score -122.55743821076366, Avg_Score -120.12615747829302\n",
      "Adding trajectory to replay buffer: step 21816, counter 348331\n",
      "Environment 14: Episode 5055, Score -123.24810984168079, Avg_Score -120.18570909584932\n",
      "Adding trajectory to replay buffer: step 21817, counter 348377\n",
      "Environment 4: Episode 5056, Score -121.72645418899569, Avg_Score -120.1955521545829\n",
      "Adding trajectory to replay buffer: step 21818, counter 348423\n",
      "Environment 2: Episode 5057, Score -121.5303272282177, Avg_Score -120.18730346453302\n",
      "Adding trajectory to replay buffer: step 21820, counter 348468\n",
      "Environment 11: Episode 5058, Score -121.92819084529827, Avg_Score -120.21254490828682\n",
      "Adding trajectory to replay buffer: step 21836, counter 348558\n",
      "Environment 9: Episode 5059, Score -115.06566821702384, Avg_Score -120.18441079434477\n",
      "Adding trajectory to replay buffer: step 21838, counter 348618\n",
      "Environment 5: Episode 5060, Score -112.71068847856795, Avg_Score -120.10505406632412\n",
      "Adding trajectory to replay buffer: step 21845, counter 348753\n",
      "Environment 12: Episode 5061, Score -117.0498721502889, Avg_Score -120.10826680292347\n",
      "Adding trajectory to replay buffer: step 21849, counter 348804\n",
      "Environment 3: Episode 5062, Score -119.8311469019546, Avg_Score -120.1226943003707\n",
      "Adding trajectory to replay buffer: step 21861, counter 348849\n",
      "Environment 1: Episode 5063, Score -123.07735324693347, Avg_Score -120.14452323425427\n",
      "Adding trajectory to replay buffer: step 21861, counter 348893\n",
      "Environment 4: Episode 5064, Score -123.06340805248048, Avg_Score -120.16498362549147\n",
      "Adding trajectory to replay buffer: step 21861, counter 348938\n",
      "Environment 14: Episode 5065, Score -122.5582755421847, Avg_Score -120.21949260403922\n",
      "Adding trajectory to replay buffer: step 21862, counter 348996\n",
      "Environment 0: Episode 5066, Score -116.61907149594278, Avg_Score -120.15859645394515\n",
      "Adding trajectory to replay buffer: step 21867, counter 349059\n",
      "Environment 6: Episode 5067, Score -113.99002688003114, Avg_Score -120.07141162241682\n",
      "Adding trajectory to replay buffer: step 21879, counter 349102\n",
      "Environment 9: Episode 5068, Score -121.88535937645287, Avg_Score -120.05710817290934\n",
      "Adding trajectory to replay buffer: step 21881, counter 349145\n",
      "Environment 5: Episode 5069, Score -122.62169485161112, Avg_Score -120.05634835331523\n",
      "Adding trajectory to replay buffer: step 21886, counter 349213\n",
      "Environment 2: Episode 5070, Score -114.2905174013935, Avg_Score -120.03069791873206\n",
      "Adding trajectory to replay buffer: step 21886, counter 349254\n",
      "Environment 12: Episode 5071, Score -114.86085510206223, Avg_Score -119.96350992683573\n",
      "Adding trajectory to replay buffer: step 21892, counter 349419\n",
      "Environment 15: Episode 5072, Score -119.67457670362977, Avg_Score -119.9377143800909\n",
      "Adding trajectory to replay buffer: step 21899, counter 349641\n",
      "Environment 8: Episode 5073, Score -122.89101689582318, Avg_Score -119.96419688709896\n",
      "Adding trajectory to replay buffer: step 21900, counter 349692\n",
      "Environment 3: Episode 5074, Score -115.3375885673243, Avg_Score -119.92593909068053\n",
      "Adding trajectory to replay buffer: step 21904, counter 349735\n",
      "Environment 14: Episode 5075, Score -121.46527610985314, Avg_Score -119.8904559768355\n",
      "Adding trajectory to replay buffer: step 21906, counter 349780\n",
      "Environment 4: Episode 5076, Score -121.32544816487531, Avg_Score -119.87220750243635\n",
      "Adding trajectory to replay buffer: step 21907, counter 349826\n",
      "Environment 1: Episode 5077, Score -121.35080542938599, Avg_Score -119.82841487606467\n",
      "Adding trajectory to replay buffer: step 21908, counter 349872\n",
      "Environment 0: Episode 5078, Score -120.07987151287621, Avg_Score -119.81013745249737\n",
      "Adding trajectory to replay buffer: step 21911, counter 349916\n",
      "Environment 6: Episode 5079, Score -122.30480069093531, Avg_Score -119.81869198989516\n",
      "Adding trajectory to replay buffer: step 21915, counter 350053\n",
      "Environment 7: Episode 5080, Score -119.30024930793172, Avg_Score -119.85336751488772\n",
      "Adding trajectory to replay buffer: step 21915, counter 350214\n",
      "Environment 10: Episode 5081, Score -120.77772012202007, Avg_Score -119.87861769749134\n",
      "Adding trajectory to replay buffer: step 21927, counter 350260\n",
      "Environment 5: Episode 5082, Score -120.02564336380674, Avg_Score -119.91367351649426\n",
      "Adding trajectory to replay buffer: step 21929, counter 350310\n",
      "Environment 9: Episode 5083, Score -120.21935355597984, Avg_Score -119.97283237801659\n",
      "Adding trajectory to replay buffer: step 21940, counter 350364\n",
      "Environment 2: Episode 5084, Score -117.09094009738291, Avg_Score -119.91705866622007\n",
      "Adding trajectory to replay buffer: step 21943, counter 350407\n",
      "Environment 3: Episode 5085, Score -122.93785779140393, Avg_Score -119.99407736500078\n",
      "Adding trajectory to replay buffer: step 21943, counter 350451\n",
      "Environment 8: Episode 5086, Score -123.20425966021853, Avg_Score -120.01076904952198\n",
      "Adding trajectory to replay buffer: step 21945, counter 350504\n",
      "Environment 15: Episode 5087, Score -118.56701632894575, Avg_Score -120.06753776679204\n",
      "Adding trajectory to replay buffer: step 21947, counter 350547\n",
      "Environment 14: Episode 5088, Score -121.29861989181427, Avg_Score -120.09018428251672\n",
      "Adding trajectory to replay buffer: step 21950, counter 350590\n",
      "Environment 1: Episode 5089, Score -122.32955382949797, Avg_Score -120.13610906366783\n",
      "Adding trajectory to replay buffer: step 21953, counter 350635\n",
      "Environment 0: Episode 5090, Score -122.93083849789573, Avg_Score -120.18309841233719\n",
      "Adding trajectory to replay buffer: step 21957, counter 350681\n",
      "Environment 6: Episode 5091, Score -119.96122619179326, Avg_Score -120.20848482336862\n",
      "Adding trajectory to replay buffer: step 21959, counter 350725\n",
      "Environment 10: Episode 5092, Score -120.61749956721502, Avg_Score -120.19664519690964\n",
      "Adding trajectory to replay buffer: step 21963, counter 350773\n",
      "Environment 7: Episode 5093, Score -120.0880286701303, Avg_Score -120.17329464695155\n",
      "Adding trajectory to replay buffer: step 21970, counter 350837\n",
      "Environment 4: Episode 5094, Score -116.11543453852397, Avg_Score -120.12863460558265\n",
      "Adding trajectory to replay buffer: step 21970, counter 350880\n",
      "Environment 5: Episode 5095, Score -122.66189460595572, Avg_Score -120.1818171504337\n",
      "Adding trajectory to replay buffer: step 21973, counter 350924\n",
      "Environment 9: Episode 5096, Score -122.08003590962664, Avg_Score -120.22329663560764\n",
      "Adding trajectory to replay buffer: step 21974, counter 351078\n",
      "Environment 11: Episode 5097, Score -120.00525003963025, Avg_Score -120.27010785148514\n",
      "Adding trajectory to replay buffer: step 21984, counter 351122\n",
      "Environment 2: Episode 5098, Score -123.34160213289906, Avg_Score -120.32023539144159\n",
      "Adding trajectory to replay buffer: step 21989, counter 351166\n",
      "Environment 15: Episode 5099, Score -122.46077151778961, Avg_Score -120.39312058886695\n",
      "Adding trajectory to replay buffer: step 21990, counter 351209\n",
      "Environment 14: Episode 5100, Score -123.19526285540437, Avg_Score -120.41771509350498\n",
      "Adding trajectory to replay buffer: step 21996, counter 351255\n",
      "Environment 1: Episode 5101, Score -120.94971456585452, Avg_Score -120.38621599387083\n",
      "Adding trajectory to replay buffer: step 22000, counter 351302\n",
      "Environment 0: Episode 5102, Score -119.41781026687411, Avg_Score -120.35040771213531\n",
      "Adding trajectory to replay buffer: step 22000, counter 351345\n",
      "Environment 6: Episode 5103, Score -123.8736301839867, Avg_Score -120.36755423169375\n",
      "Adding trajectory to replay buffer: step 22005, counter 351387\n",
      "Environment 7: Episode 5104, Score -122.75183550749968, Avg_Score -120.37260343936904\n",
      "Adding trajectory to replay buffer: step 22005, counter 351433\n",
      "Environment 10: Episode 5105, Score -121.49285824255334, Avg_Score -120.36025294118393\n",
      "Adding trajectory to replay buffer: step 22013, counter 351641\n",
      "Environment 13: Episode 5106, Score -131.59226782834233, Avg_Score -120.45954980521373\n",
      "Adding trajectory to replay buffer: step 22014, counter 351685\n",
      "Environment 4: Episode 5107, Score -122.20980629629332, Avg_Score -120.4989959205197\n",
      "Adding trajectory to replay buffer: step 22014, counter 351729\n",
      "Environment 5: Episode 5108, Score -122.30693260091978, Avg_Score -120.5103384329398\n",
      "Adding trajectory to replay buffer: step 22016, counter 351802\n",
      "Environment 8: Episode 5109, Score -109.81890504138482, Avg_Score -120.38386500222147\n",
      "Adding trajectory to replay buffer: step 22017, counter 351846\n",
      "Environment 9: Episode 5110, Score -123.65730711219521, Avg_Score -120.38971944591913\n",
      "Adding trajectory to replay buffer: step 22018, counter 351890\n",
      "Environment 11: Episode 5111, Score -123.77810487919301, Avg_Score -120.4245084677128\n",
      "Adding trajectory to replay buffer: step 22023, counter 351970\n",
      "Environment 3: Episode 5112, Score -119.81344565529562, Avg_Score -120.47611246698315\n",
      "Adding trajectory to replay buffer: step 22027, counter 352013\n",
      "Environment 2: Episode 5113, Score -123.43374698935511, Avg_Score -120.48724865265557\n",
      "Adding trajectory to replay buffer: step 22033, counter 352057\n",
      "Environment 15: Episode 5114, Score -122.60864418252311, Avg_Score -120.54057914165624\n",
      "Adding trajectory to replay buffer: step 22034, counter 352101\n",
      "Environment 14: Episode 5115, Score -122.94568583925255, Avg_Score -120.61049461874622\n",
      "Adding trajectory to replay buffer: step 22041, counter 352146\n",
      "Environment 1: Episode 5116, Score -122.62746199114558, Avg_Score -120.64526941919381\n",
      "Adding trajectory to replay buffer: step 22045, counter 352191\n",
      "Environment 0: Episode 5117, Score -123.08067885415318, Avg_Score -120.69980894272041\n",
      "Adding trajectory to replay buffer: step 22049, counter 352235\n",
      "Environment 10: Episode 5118, Score -120.11544896132375, Avg_Score -120.68456757619936\n",
      "Adding trajectory to replay buffer: step 22055, counter 352277\n",
      "Environment 13: Episode 5119, Score -123.11862617107977, Avg_Score -120.71552467182535\n",
      "Adding trajectory to replay buffer: step 22058, counter 352321\n",
      "Environment 4: Episode 5120, Score -123.28484285766756, Avg_Score -120.76030009929116\n",
      "Adding trajectory to replay buffer: step 22058, counter 352365\n",
      "Environment 5: Episode 5121, Score -123.27062838214698, Avg_Score -120.80415616453483\n",
      "Adding trajectory to replay buffer: step 22060, counter 352409\n",
      "Environment 8: Episode 5122, Score -122.379982732635, Avg_Score -120.79692555526954\n",
      "Adding trajectory to replay buffer: step 22063, counter 352472\n",
      "Environment 6: Episode 5123, Score -116.96003641327222, Avg_Score -120.82997302179534\n",
      "Adding trajectory to replay buffer: step 22063, counter 352518\n",
      "Environment 9: Episode 5124, Score -122.05354374561024, Avg_Score -120.83461025156149\n",
      "Adding trajectory to replay buffer: step 22068, counter 352563\n",
      "Environment 3: Episode 5125, Score -121.29627403315716, Avg_Score -120.86991931583817\n",
      "Adding trajectory to replay buffer: step 22071, counter 352607\n",
      "Environment 2: Episode 5126, Score -121.50487528093656, Avg_Score -120.94355945412201\n",
      "Adding trajectory to replay buffer: step 22075, counter 352796\n",
      "Environment 12: Episode 5127, Score -124.62716138241097, Avg_Score -120.9942459031997\n",
      "Adding trajectory to replay buffer: step 22077, counter 352839\n",
      "Environment 14: Episode 5128, Score -117.62717175695114, Avg_Score -120.99005652123334\n",
      "Adding trajectory to replay buffer: step 22077, counter 352883\n",
      "Environment 15: Episode 5129, Score -122.43613329601908, Avg_Score -120.98561446332033\n",
      "Adding trajectory to replay buffer: step 22088, counter 352930\n",
      "Environment 1: Episode 5130, Score -123.35296655598034, Avg_Score -121.00868801827933\n",
      "Adding trajectory to replay buffer: step 22094, counter 352975\n",
      "Environment 10: Episode 5131, Score -121.9161828926839, Avg_Score -121.05031662986772\n",
      "Adding trajectory to replay buffer: step 22098, counter 353018\n",
      "Environment 13: Episode 5132, Score -123.33978396788088, Avg_Score -121.0578597603351\n",
      "Adding trajectory to replay buffer: step 22101, counter 353061\n",
      "Environment 4: Episode 5133, Score -122.92672115232237, Avg_Score -121.06042096130824\n",
      "Adding trajectory to replay buffer: step 22101, counter 353104\n",
      "Environment 5: Episode 5134, Score -122.83705646944233, Avg_Score -121.08468677628267\n",
      "Adding trajectory to replay buffer: step 22103, counter 353147\n",
      "Environment 8: Episode 5135, Score -121.2617965713516, Avg_Score -121.08825291152685\n",
      "Adding trajectory to replay buffer: step 22108, counter 353192\n",
      "Environment 6: Episode 5136, Score -120.2244709445946, Avg_Score -121.10037981385963\n",
      "Adding trajectory to replay buffer: step 22108, counter 353237\n",
      "Environment 9: Episode 5137, Score -116.58204315471276, Avg_Score -121.04939350379419\n",
      "Adding trajectory to replay buffer: step 22111, counter 353303\n",
      "Environment 0: Episode 5138, Score -114.96761293176935, Avg_Score -120.96776089303543\n",
      "Adding trajectory to replay buffer: step 22115, counter 353350\n",
      "Environment 3: Episode 5139, Score -122.99547290190867, Avg_Score -120.98040529063213\n",
      "Adding trajectory to replay buffer: step 22117, counter 353396\n",
      "Environment 2: Episode 5140, Score -124.03081107706825, Avg_Score -121.00711687775258\n",
      "Adding trajectory to replay buffer: step 22118, counter 353439\n",
      "Environment 12: Episode 5141, Score -122.79375243730719, Avg_Score -121.01490684714543\n",
      "Adding trajectory to replay buffer: step 22120, counter 353482\n",
      "Environment 14: Episode 5142, Score -122.70041956352567, Avg_Score -121.00454867240558\n",
      "Adding trajectory to replay buffer: step 22120, counter 353525\n",
      "Environment 15: Episode 5143, Score -122.80010109359212, Avg_Score -121.00535998010692\n",
      "Adding trajectory to replay buffer: step 22131, counter 353568\n",
      "Environment 1: Episode 5144, Score -123.01387388033855, Avg_Score -121.00831548667527\n",
      "Adding trajectory to replay buffer: step 22145, counter 353615\n",
      "Environment 13: Episode 5145, Score -116.90336602628605, Avg_Score -120.96544711674375\n",
      "Adding trajectory to replay buffer: step 22146, counter 353667\n",
      "Environment 10: Episode 5146, Score -115.06898228383375, Avg_Score -120.8830480833463\n",
      "Adding trajectory to replay buffer: step 22149, counter 353715\n",
      "Environment 4: Episode 5147, Score -123.93997981412832, Avg_Score -120.90568591104407\n",
      "Adding trajectory to replay buffer: step 22155, counter 353759\n",
      "Environment 0: Episode 5148, Score -122.78254928733284, Avg_Score -121.00960307360175\n",
      "Adding trajectory to replay buffer: step 22159, counter 353803\n",
      "Environment 3: Episode 5149, Score -120.59545097906454, Avg_Score -120.9880741370094\n",
      "Adding trajectory to replay buffer: step 22161, counter 353847\n",
      "Environment 2: Episode 5150, Score -123.13570802525183, Avg_Score -120.98680692066701\n",
      "Adding trajectory to replay buffer: step 22164, counter 353993\n",
      "Environment 11: Episode 5151, Score -118.50857548478804, Avg_Score -120.94431305472699\n",
      "Adding trajectory to replay buffer: step 22165, counter 354040\n",
      "Environment 12: Episode 5152, Score -120.37416185061323, Avg_Score -120.96601505713369\n",
      "Adding trajectory to replay buffer: step 22167, counter 354099\n",
      "Environment 9: Episode 5153, Score -116.42880294709839, Avg_Score -120.90767103905355\n",
      "Adding trajectory to replay buffer: step 22170, counter 354161\n",
      "Environment 6: Episode 5154, Score -115.96447137719082, Avg_Score -120.84174137071783\n",
      "Adding trajectory to replay buffer: step 22176, counter 354206\n",
      "Environment 1: Episode 5155, Score -121.75250836688156, Avg_Score -120.82678535596985\n",
      "Adding trajectory to replay buffer: step 22189, counter 354249\n",
      "Environment 10: Episode 5156, Score -121.69873464698345, Avg_Score -120.82650816054968\n",
      "Adding trajectory to replay buffer: step 22191, counter 354295\n",
      "Environment 13: Episode 5157, Score -119.51270395129546, Avg_Score -120.80633192778045\n",
      "Adding trajectory to replay buffer: step 22192, counter 354338\n",
      "Environment 4: Episode 5158, Score -123.80818390626337, Avg_Score -120.82513185839011\n",
      "Adding trajectory to replay buffer: step 22202, counter 354385\n",
      "Environment 0: Episode 5159, Score -116.11018727946592, Avg_Score -120.83557704901453\n",
      "Adding trajectory to replay buffer: step 22203, counter 354468\n",
      "Environment 15: Episode 5160, Score -116.6256976777756, Avg_Score -120.8747271410066\n",
      "Adding trajectory to replay buffer: step 22205, counter 354514\n",
      "Environment 3: Episode 5161, Score -121.08399247652292, Avg_Score -120.91506834426892\n",
      "Adding trajectory to replay buffer: step 22206, counter 354559\n",
      "Environment 2: Episode 5162, Score -123.20190351119142, Avg_Score -120.94877591036129\n",
      "Adding trajectory to replay buffer: step 22208, counter 354762\n",
      "Environment 7: Episode 5163, Score -122.11112511728456, Avg_Score -120.93911362906482\n",
      "Adding trajectory to replay buffer: step 22211, counter 354809\n",
      "Environment 11: Episode 5164, Score -119.80301697429582, Avg_Score -120.90650971828295\n",
      "Adding trajectory to replay buffer: step 22214, counter 354903\n",
      "Environment 14: Episode 5165, Score -124.24911771615564, Avg_Score -120.92341814002265\n",
      "Adding trajectory to replay buffer: step 22235, counter 354962\n",
      "Environment 1: Episode 5166, Score -115.3917868040769, Avg_Score -120.911145293104\n",
      "Adding trajectory to replay buffer: step 22235, counter 355005\n",
      "Environment 4: Episode 5167, Score -117.77051158083634, Avg_Score -120.94895014011206\n",
      "Adding trajectory to replay buffer: step 22235, counter 355049\n",
      "Environment 13: Episode 5168, Score -120.16971755409365, Avg_Score -120.9317937218885\n",
      "Adding trajectory to replay buffer: step 22238, counter 355184\n",
      "Environment 8: Episode 5169, Score -128.98876871537354, Avg_Score -120.99546446052611\n",
      "Adding trajectory to replay buffer: step 22238, counter 355255\n",
      "Environment 9: Episode 5170, Score -117.76217407603687, Avg_Score -121.03018102727253\n",
      "Adding trajectory to replay buffer: step 22246, counter 355299\n",
      "Environment 0: Episode 5171, Score -122.04734735403272, Avg_Score -121.10204594979228\n",
      "Adding trajectory to replay buffer: step 22247, counter 355357\n",
      "Environment 10: Episode 5172, Score -115.02197579732227, Avg_Score -121.05551994072921\n",
      "Adding trajectory to replay buffer: step 22248, counter 355400\n",
      "Environment 3: Episode 5173, Score -122.91716925322078, Avg_Score -121.0557814643032\n",
      "Adding trajectory to replay buffer: step 22248, counter 355445\n",
      "Environment 15: Episode 5174, Score -118.00011454355842, Avg_Score -121.0824067240655\n",
      "Adding trajectory to replay buffer: step 22249, counter 355488\n",
      "Environment 2: Episode 5175, Score -122.94259995949145, Avg_Score -121.09717996256191\n",
      "Adding trajectory to replay buffer: step 22252, counter 355532\n",
      "Environment 7: Episode 5176, Score -121.67812960162821, Avg_Score -121.10070677692947\n",
      "Adding trajectory to replay buffer: step 22257, counter 355578\n",
      "Environment 11: Episode 5177, Score -116.56142268796079, Avg_Score -121.05281294951519\n",
      "Adding trajectory to replay buffer: step 22278, counter 355642\n",
      "Environment 14: Episode 5178, Score -115.87918206253059, Avg_Score -121.01080605501173\n",
      "Adding trajectory to replay buffer: step 22280, counter 355687\n",
      "Environment 1: Episode 5179, Score -120.34586517566319, Avg_Score -120.99121669985901\n",
      "Adding trajectory to replay buffer: step 22280, counter 355732\n",
      "Environment 13: Episode 5180, Score -121.00255572554532, Avg_Score -121.00823976403517\n",
      "Adding trajectory to replay buffer: step 22281, counter 355778\n",
      "Environment 4: Episode 5181, Score -120.65331890435269, Avg_Score -121.0069957518585\n",
      "Adding trajectory to replay buffer: step 22289, counter 355966\n",
      "Environment 5: Episode 5182, Score -121.12065373707874, Avg_Score -121.01794585559122\n",
      "Adding trajectory to replay buffer: step 22289, counter 356017\n",
      "Environment 9: Episode 5183, Score -116.65940613100119, Avg_Score -120.98234638134144\n",
      "Adding trajectory to replay buffer: step 22291, counter 356062\n",
      "Environment 0: Episode 5184, Score -120.36725838508582, Avg_Score -121.01510956421843\n",
      "Adding trajectory to replay buffer: step 22301, counter 356125\n",
      "Environment 8: Episode 5185, Score -116.59248050906324, Avg_Score -120.95165579139504\n",
      "Adding trajectory to replay buffer: step 22302, counter 356180\n",
      "Environment 10: Episode 5186, Score -115.35201622232516, Avg_Score -120.8731333570161\n",
      "Adding trajectory to replay buffer: step 22303, counter 356234\n",
      "Environment 2: Episode 5187, Score -118.90718393937311, Avg_Score -120.87653503312035\n",
      "Adding trajectory to replay buffer: step 22304, counter 356286\n",
      "Environment 7: Episode 5188, Score -117.31777062879006, Avg_Score -120.83672654049013\n",
      "Adding trajectory to replay buffer: step 22308, counter 356346\n",
      "Environment 3: Episode 5189, Score -116.23532870077715, Avg_Score -120.77578428920292\n",
      "Adding trajectory to replay buffer: step 22314, counter 356490\n",
      "Environment 6: Episode 5190, Score -115.23046408165494, Avg_Score -120.6987805450405\n",
      "Adding trajectory to replay buffer: step 22316, counter 356549\n",
      "Environment 11: Episode 5191, Score -115.85487627263677, Avg_Score -120.65771704584893\n",
      "Adding trajectory to replay buffer: step 22316, counter 356617\n",
      "Environment 15: Episode 5192, Score -119.89421902862068, Avg_Score -120.65048424046302\n",
      "Adding trajectory to replay buffer: step 22321, counter 356660\n",
      "Environment 14: Episode 5193, Score -122.61532487387211, Avg_Score -120.67575720250045\n",
      "Adding trajectory to replay buffer: step 22322, counter 356702\n",
      "Environment 1: Episode 5194, Score -122.80110745487735, Avg_Score -120.74261393166397\n",
      "Adding trajectory to replay buffer: step 22322, counter 356744\n",
      "Environment 13: Episode 5195, Score -122.71266542726073, Avg_Score -120.74312163987705\n",
      "Adding trajectory to replay buffer: step 22326, counter 356789\n",
      "Environment 4: Episode 5196, Score -121.12338698342319, Avg_Score -120.73355515061499\n",
      "Adding trajectory to replay buffer: step 22337, counter 356837\n",
      "Environment 5: Episode 5197, Score -123.60791091549335, Avg_Score -120.76958175937365\n",
      "Adding trajectory to replay buffer: step 22342, counter 356888\n",
      "Environment 0: Episode 5198, Score -117.57136721731536, Avg_Score -120.7118794102178\n",
      "Adding trajectory to replay buffer: step 22344, counter 356929\n",
      "Environment 2: Episode 5199, Score -116.58282415830902, Avg_Score -120.65309993662298\n",
      "Adding trajectory to replay buffer: step 22345, counter 356973\n",
      "Environment 8: Episode 5200, Score -122.07430515076034, Avg_Score -120.64189035957655\n",
      "Adding trajectory to replay buffer: step 22345, counter 357016\n",
      "Environment 10: Episode 5201, Score -123.07038694830176, Avg_Score -120.66309708340101\n",
      "Adding trajectory to replay buffer: step 22351, counter 357063\n",
      "Environment 7: Episode 5202, Score -116.90125177279425, Avg_Score -120.63793149846022\n",
      "Adding trajectory to replay buffer: step 22352, counter 357107\n",
      "Environment 3: Episode 5203, Score -122.26184627392314, Avg_Score -120.62181365935959\n",
      "Adding trajectory to replay buffer: step 22357, counter 357150\n",
      "Environment 6: Episode 5204, Score -122.09252429879953, Avg_Score -120.61522054727259\n",
      "Adding trajectory to replay buffer: step 22357, counter 357218\n",
      "Environment 9: Episode 5205, Score -114.83678749094966, Avg_Score -120.54865983975655\n",
      "Adding trajectory to replay buffer: step 22359, counter 357261\n",
      "Environment 15: Episode 5206, Score -122.57451928359332, Avg_Score -120.45848235430906\n",
      "Adding trajectory to replay buffer: step 22360, counter 357305\n",
      "Environment 11: Episode 5207, Score -122.91177265932286, Avg_Score -120.46550201793934\n",
      "Adding trajectory to replay buffer: step 22372, counter 357351\n",
      "Environment 4: Episode 5208, Score -120.54021752538036, Avg_Score -120.44783486718394\n",
      "Adding trajectory to replay buffer: step 22372, counter 357558\n",
      "Environment 12: Episode 5209, Score -123.78114369083599, Avg_Score -120.58745725367848\n",
      "Adding trajectory to replay buffer: step 22383, counter 357604\n",
      "Environment 5: Episode 5210, Score -117.9892112729537, Avg_Score -120.53077629528605\n",
      "Adding trajectory to replay buffer: step 22390, counter 357649\n",
      "Environment 8: Episode 5211, Score -116.90393760424541, Avg_Score -120.46203462253656\n",
      "Adding trajectory to replay buffer: step 22393, counter 357690\n",
      "Environment 3: Episode 5212, Score -116.06322511587726, Avg_Score -120.42453241714239\n",
      "Adding trajectory to replay buffer: step 22396, counter 357735\n",
      "Environment 7: Episode 5213, Score -122.93250367124071, Avg_Score -120.41951998396124\n",
      "Adding trajectory to replay buffer: step 22397, counter 357810\n",
      "Environment 1: Episode 5214, Score -113.06708202222859, Avg_Score -120.32410436235828\n",
      "Adding trajectory to replay buffer: step 22399, counter 357852\n",
      "Environment 9: Episode 5215, Score -122.7109690536093, Avg_Score -120.32175719450186\n",
      "Adding trajectory to replay buffer: step 22400, counter 357895\n",
      "Environment 6: Episode 5216, Score -123.51829737604596, Avg_Score -120.33066554835086\n",
      "Adding trajectory to replay buffer: step 22400, counter 357973\n",
      "Environment 13: Episode 5217, Score -114.12902831873173, Avg_Score -120.24114904299665\n",
      "Adding trajectory to replay buffer: step 22403, counter 358017\n",
      "Environment 15: Episode 5218, Score -121.77458915214925, Avg_Score -120.25774044490494\n",
      "Adding trajectory to replay buffer: step 22404, counter 358076\n",
      "Environment 10: Episode 5219, Score -115.33785786063038, Avg_Score -120.17993276180044\n",
      "Adding trajectory to replay buffer: step 22404, counter 358120\n",
      "Environment 11: Episode 5220, Score -121.40312640649515, Avg_Score -120.1611155972887\n",
      "Adding trajectory to replay buffer: step 22410, counter 358186\n",
      "Environment 2: Episode 5221, Score -117.9558441120293, Avg_Score -120.10796775458753\n",
      "Adding trajectory to replay buffer: step 22416, counter 358230\n",
      "Environment 4: Episode 5222, Score -121.08943268124635, Avg_Score -120.09506225407364\n",
      "Adding trajectory to replay buffer: step 22416, counter 358274\n",
      "Environment 12: Episode 5223, Score -121.63000761628648, Avg_Score -120.14176196610379\n",
      "Adding trajectory to replay buffer: step 22440, counter 358317\n",
      "Environment 1: Episode 5224, Score -122.58224320815754, Avg_Score -120.14704896072926\n",
      "Adding trajectory to replay buffer: step 22443, counter 358360\n",
      "Environment 6: Episode 5225, Score -118.53988414921673, Avg_Score -120.11948506188985\n",
      "Adding trajectory to replay buffer: step 22444, counter 358411\n",
      "Environment 3: Episode 5226, Score -122.47596521795417, Avg_Score -120.12919596126002\n",
      "Adding trajectory to replay buffer: step 22451, counter 358458\n",
      "Environment 10: Episode 5227, Score -121.29764111208792, Avg_Score -120.09590075855677\n",
      "Adding trajectory to replay buffer: step 22451, counter 358505\n",
      "Environment 11: Episode 5228, Score -121.0854003942199, Avg_Score -120.13048304492946\n",
      "Adding trajectory to replay buffer: step 22452, counter 358574\n",
      "Environment 5: Episode 5229, Score -115.41774928970324, Avg_Score -120.06029920486631\n",
      "Adding trajectory to replay buffer: step 22452, counter 358623\n",
      "Environment 15: Episode 5230, Score -117.50355589941951, Avg_Score -120.00180509830071\n",
      "Adding trajectory to replay buffer: step 22453, counter 358680\n",
      "Environment 7: Episode 5231, Score -114.81252690293702, Avg_Score -119.93076853840323\n",
      "Adding trajectory to replay buffer: step 22457, counter 358738\n",
      "Environment 9: Episode 5232, Score -115.60681337234068, Avg_Score -119.85343883244782\n",
      "Adding trajectory to replay buffer: step 22458, counter 358854\n",
      "Environment 0: Episode 5233, Score -124.7135336358777, Avg_Score -119.87130695728337\n",
      "Adding trajectory to replay buffer: step 22463, counter 358901\n",
      "Environment 4: Episode 5234, Score -122.95978018611422, Avg_Score -119.87253419445007\n",
      "Adding trajectory to replay buffer: step 22464, counter 358949\n",
      "Environment 12: Episode 5235, Score -121.92638029055234, Avg_Score -119.87918003164206\n",
      "Adding trajectory to replay buffer: step 22467, counter 359006\n",
      "Environment 2: Episode 5236, Score -115.2756104334804, Avg_Score -119.82969142653093\n",
      "Adding trajectory to replay buffer: step 22469, counter 359075\n",
      "Environment 13: Episode 5237, Score -113.17109124820679, Avg_Score -119.79558190746589\n",
      "Adding trajectory to replay buffer: step 22484, counter 359119\n",
      "Environment 1: Episode 5238, Score -122.29459260557996, Avg_Score -119.86885170420399\n",
      "Adding trajectory to replay buffer: step 22487, counter 359162\n",
      "Environment 3: Episode 5239, Score -122.94981254960597, Avg_Score -119.86839510068097\n",
      "Adding trajectory to replay buffer: step 22487, counter 359206\n",
      "Environment 6: Episode 5240, Score -122.17615253131527, Avg_Score -119.84984851522346\n",
      "Adding trajectory to replay buffer: step 22493, counter 359248\n",
      "Environment 10: Episode 5241, Score -122.93683267661494, Avg_Score -119.85127931761652\n",
      "Adding trajectory to replay buffer: step 22493, counter 359290\n",
      "Environment 11: Episode 5242, Score -123.04456677977275, Avg_Score -119.85472078977898\n",
      "Adding trajectory to replay buffer: step 22494, counter 359394\n",
      "Environment 8: Episode 5243, Score -117.13313090705311, Avg_Score -119.79805108791362\n",
      "Adding trajectory to replay buffer: step 22495, counter 359437\n",
      "Environment 5: Episode 5244, Score -122.67474393963876, Avg_Score -119.79465978850664\n",
      "Adding trajectory to replay buffer: step 22495, counter 359480\n",
      "Environment 15: Episode 5245, Score -122.57835970766159, Avg_Score -119.85140972532041\n",
      "Adding trajectory to replay buffer: step 22496, counter 359523\n",
      "Environment 7: Episode 5246, Score -122.71603074651895, Avg_Score -119.92788020994725\n",
      "Adding trajectory to replay buffer: step 22500, counter 359566\n",
      "Environment 9: Episode 5247, Score -121.95096095581042, Avg_Score -119.90799002136406\n",
      "Adding trajectory to replay buffer: step 22503, counter 359611\n",
      "Environment 0: Episode 5248, Score -121.63630594625636, Avg_Score -119.89652758795327\n",
      "Adding trajectory to replay buffer: step 22509, counter 359653\n",
      "Environment 2: Episode 5249, Score -123.26769434253126, Avg_Score -119.92325002158793\n",
      "Adding trajectory to replay buffer: step 22513, counter 359697\n",
      "Environment 13: Episode 5250, Score -122.97348156970119, Avg_Score -119.92162775703243\n",
      "Adding trajectory to replay buffer: step 22523, counter 359899\n",
      "Environment 14: Episode 5251, Score -124.77213937348748, Avg_Score -119.98426339591944\n",
      "Adding trajectory to replay buffer: step 22527, counter 359942\n",
      "Environment 1: Episode 5252, Score -123.09784640589729, Avg_Score -120.01150024147228\n",
      "Adding trajectory to replay buffer: step 22530, counter 359985\n",
      "Environment 3: Episode 5253, Score -122.40254293081227, Avg_Score -120.07123764130942\n",
      "Adding trajectory to replay buffer: step 22530, counter 360028\n",
      "Environment 6: Episode 5254, Score -122.3481737189473, Avg_Score -120.135074664727\n",
      "Adding trajectory to replay buffer: step 22535, counter 360070\n",
      "Environment 10: Episode 5255, Score -123.41164568072061, Avg_Score -120.15166603786538\n",
      "Adding trajectory to replay buffer: step 22538, counter 360144\n",
      "Environment 12: Episode 5256, Score -116.80570696207508, Avg_Score -120.1027357610163\n",
      "Adding trajectory to replay buffer: step 22539, counter 360188\n",
      "Environment 5: Episode 5257, Score -123.25711589877616, Avg_Score -120.1401798804911\n",
      "Adding trajectory to replay buffer: step 22539, counter 360233\n",
      "Environment 8: Episode 5258, Score -116.86550390281155, Avg_Score -120.0707530804566\n",
      "Adding trajectory to replay buffer: step 22539, counter 360279\n",
      "Environment 11: Episode 5259, Score -121.48825245052637, Avg_Score -120.12453373216721\n",
      "Adding trajectory to replay buffer: step 22540, counter 360324\n",
      "Environment 15: Episode 5260, Score -122.19611187256066, Avg_Score -120.18023787411505\n",
      "Adding trajectory to replay buffer: step 22548, counter 360369\n",
      "Environment 0: Episode 5261, Score -123.36346877929878, Avg_Score -120.20303263714281\n",
      "Adding trajectory to replay buffer: step 22552, counter 360412\n",
      "Environment 2: Episode 5262, Score -122.64245921425086, Avg_Score -120.19743819417339\n",
      "Adding trajectory to replay buffer: step 22556, counter 360455\n",
      "Environment 13: Episode 5263, Score -122.55078068374345, Avg_Score -120.20183474983799\n",
      "Adding trajectory to replay buffer: step 22572, counter 360500\n",
      "Environment 1: Episode 5264, Score -119.0469655261431, Avg_Score -120.19427423535646\n",
      "Adding trajectory to replay buffer: step 22574, counter 360544\n",
      "Environment 6: Episode 5265, Score -121.77140346929059, Avg_Score -120.16949709288781\n",
      "Adding trajectory to replay buffer: step 22575, counter 360589\n",
      "Environment 3: Episode 5266, Score -121.67616721464974, Avg_Score -120.23234089699353\n",
      "Adding trajectory to replay buffer: step 22578, counter 360632\n",
      "Environment 10: Episode 5267, Score -117.66769078196398, Avg_Score -120.23131268900484\n",
      "Adding trajectory to replay buffer: step 22581, counter 360674\n",
      "Environment 5: Episode 5268, Score -123.0443779107742, Avg_Score -120.26005929257164\n",
      "Adding trajectory to replay buffer: step 22581, counter 360716\n",
      "Environment 8: Episode 5269, Score -123.06357664408736, Avg_Score -120.20080737185877\n",
      "Adding trajectory to replay buffer: step 22581, counter 360758\n",
      "Environment 11: Episode 5270, Score -123.02404609927659, Avg_Score -120.25342609209117\n",
      "Adding trajectory to replay buffer: step 22583, counter 360801\n",
      "Environment 15: Episode 5271, Score -123.43141974376017, Avg_Score -120.26726681598842\n",
      "Adding trajectory to replay buffer: step 22592, counter 360845\n",
      "Environment 0: Episode 5272, Score -122.93506554148905, Avg_Score -120.34639771343011\n",
      "Adding trajectory to replay buffer: step 22597, counter 360890\n",
      "Environment 2: Episode 5273, Score -122.09513874648698, Avg_Score -120.33817740836277\n",
      "Adding trajectory to replay buffer: step 22599, counter 360933\n",
      "Environment 13: Episode 5274, Score -123.21738807320656, Avg_Score -120.39035014365925\n",
      "Adding trajectory to replay buffer: step 22612, counter 361082\n",
      "Environment 4: Episode 5275, Score -117.54362264764744, Avg_Score -120.33636037054079\n",
      "Adding trajectory to replay buffer: step 22614, counter 361124\n",
      "Environment 1: Episode 5276, Score -123.08597194797049, Avg_Score -120.3504387940042\n",
      "Adding trajectory to replay buffer: step 22617, counter 361167\n",
      "Environment 6: Episode 5277, Score -122.92533430704103, Avg_Score -120.414077910195\n",
      "Adding trajectory to replay buffer: step 22620, counter 361212\n",
      "Environment 3: Episode 5278, Score -123.01886414365967, Avg_Score -120.4854747310063\n",
      "Adding trajectory to replay buffer: step 22624, counter 361258\n",
      "Environment 10: Episode 5279, Score -121.82345835773461, Avg_Score -120.50025066282701\n",
      "Adding trajectory to replay buffer: step 22626, counter 361301\n",
      "Environment 15: Episode 5280, Score -123.53229988408337, Avg_Score -120.52554810441241\n",
      "Adding trajectory to replay buffer: step 22632, counter 361433\n",
      "Environment 9: Episode 5281, Score -117.76815113783425, Avg_Score -120.49669642674723\n",
      "Adding trajectory to replay buffer: step 22635, counter 361572\n",
      "Environment 7: Episode 5282, Score -117.27083355615537, Avg_Score -120.45819822493799\n",
      "Adding trajectory to replay buffer: step 22637, counter 361617\n",
      "Environment 0: Episode 5283, Score -122.3176133533089, Avg_Score -120.51478029716107\n",
      "Adding trajectory to replay buffer: step 22640, counter 361660\n",
      "Environment 2: Episode 5284, Score -122.33238935211983, Avg_Score -120.5344316068314\n",
      "Adding trajectory to replay buffer: step 22642, counter 361721\n",
      "Environment 5: Episode 5285, Score -116.76498676369215, Avg_Score -120.53615666937769\n",
      "Adding trajectory to replay buffer: step 22642, counter 361764\n",
      "Environment 13: Episode 5286, Score -122.2700862502201, Avg_Score -120.60533736965664\n",
      "Adding trajectory to replay buffer: step 22657, counter 361807\n",
      "Environment 1: Episode 5287, Score -119.16909217710855, Avg_Score -120.60795645203399\n",
      "Adding trajectory to replay buffer: step 22661, counter 361851\n",
      "Environment 6: Episode 5288, Score -122.80087880755836, Avg_Score -120.66278753382169\n",
      "Adding trajectory to replay buffer: step 22673, counter 361898\n",
      "Environment 15: Episode 5289, Score -120.93810916526803, Avg_Score -120.7098153384666\n",
      "Adding trajectory to replay buffer: step 22674, counter 361948\n",
      "Environment 10: Episode 5290, Score -113.79105476327179, Avg_Score -120.69542124528276\n",
      "Adding trajectory to replay buffer: step 22677, counter 362013\n",
      "Environment 4: Episode 5291, Score -114.04759165106528, Avg_Score -120.67734839906706\n",
      "Adding trajectory to replay buffer: step 22679, counter 362060\n",
      "Environment 9: Episode 5292, Score -120.17335453117266, Avg_Score -120.68013975409256\n",
      "Adding trajectory to replay buffer: step 22681, counter 362106\n",
      "Environment 7: Episode 5293, Score -120.40140124329862, Avg_Score -120.65800051778682\n",
      "Adding trajectory to replay buffer: step 22684, counter 362150\n",
      "Environment 2: Episode 5294, Score -119.03390261667221, Avg_Score -120.6203284694048\n",
      "Adding trajectory to replay buffer: step 22686, counter 362199\n",
      "Environment 0: Episode 5295, Score -116.40697535332106, Avg_Score -120.5572715686654\n",
      "Adding trajectory to replay buffer: step 22686, counter 362243\n",
      "Environment 5: Episode 5296, Score -116.2507809724044, Avg_Score -120.50854550855522\n",
      "Adding trajectory to replay buffer: step 22693, counter 362355\n",
      "Environment 8: Episode 5297, Score -125.14639200169717, Avg_Score -120.52393031941725\n",
      "Adding trajectory to replay buffer: step 22693, counter 362467\n",
      "Environment 11: Episode 5298, Score -114.24313245173978, Avg_Score -120.4906479717615\n",
      "Adding trajectory to replay buffer: step 22700, counter 362510\n",
      "Environment 1: Episode 5299, Score -123.41452343057469, Avg_Score -120.55896496448415\n",
      "Adding trajectory to replay buffer: step 22705, counter 362554\n",
      "Environment 6: Episode 5300, Score -122.51529998223423, Avg_Score -120.5633749127989\n",
      "Adding trajectory to replay buffer: step 22706, counter 362618\n",
      "Environment 13: Episode 5301, Score -117.01068548014263, Avg_Score -120.50277789811732\n",
      "Adding trajectory to replay buffer: step 22708, counter 362803\n",
      "Environment 14: Episode 5302, Score -122.00617979834168, Avg_Score -120.55382717837277\n",
      "Adding trajectory to replay buffer: step 22709, counter 362892\n",
      "Environment 3: Episode 5303, Score -114.99660599716381, Avg_Score -120.48117477560518\n",
      "Adding trajectory to replay buffer: step 22715, counter 362933\n",
      "Environment 10: Episode 5304, Score -116.55418719710363, Avg_Score -120.4257914045882\n",
      "Adding trajectory to replay buffer: step 22716, counter 362976\n",
      "Environment 15: Episode 5305, Score -121.45859012889429, Avg_Score -120.49200943096767\n",
      "Adding trajectory to replay buffer: step 22723, counter 363022\n",
      "Environment 4: Episode 5306, Score -116.256976214552, Avg_Score -120.42883400027726\n",
      "Adding trajectory to replay buffer: step 22726, counter 363067\n",
      "Environment 7: Episode 5307, Score -122.28711492314439, Avg_Score -120.42258742291547\n",
      "Adding trajectory to replay buffer: step 22737, counter 363111\n",
      "Environment 11: Episode 5308, Score -122.33220084461198, Avg_Score -120.4405072561078\n",
      "Adding trajectory to replay buffer: step 22739, counter 363157\n",
      "Environment 8: Episode 5309, Score -121.71744232535394, Avg_Score -120.41987024245299\n",
      "Adding trajectory to replay buffer: step 22744, counter 363201\n",
      "Environment 1: Episode 5310, Score -123.01798925918777, Avg_Score -120.47015802231532\n",
      "Adding trajectory to replay buffer: step 22744, counter 363407\n",
      "Environment 12: Episode 5311, Score -121.82178526487326, Avg_Score -120.5193364989216\n",
      "Adding trajectory to replay buffer: step 22748, counter 363450\n",
      "Environment 6: Episode 5312, Score -119.40891801100597, Avg_Score -120.5527934278729\n",
      "Adding trajectory to replay buffer: step 22749, counter 363513\n",
      "Environment 5: Episode 5313, Score -116.79901959449921, Avg_Score -120.49145858710546\n",
      "Adding trajectory to replay buffer: step 22749, counter 363556\n",
      "Environment 13: Episode 5314, Score -122.06306840483349, Avg_Score -120.58141845093151\n",
      "Adding trajectory to replay buffer: step 22757, counter 363605\n",
      "Environment 14: Episode 5315, Score -117.0452832475494, Avg_Score -120.52476159287093\n",
      "Adding trajectory to replay buffer: step 22760, counter 363650\n",
      "Environment 10: Episode 5316, Score -121.71994644771206, Avg_Score -120.50677808358759\n",
      "Adding trajectory to replay buffer: step 22765, counter 363731\n",
      "Environment 2: Episode 5317, Score -113.89711226989019, Avg_Score -120.50445892309918\n",
      "Adding trajectory to replay buffer: step 22765, counter 363787\n",
      "Environment 3: Episode 5318, Score -115.25910667800343, Avg_Score -120.43930409835771\n",
      "Adding trajectory to replay buffer: step 22768, counter 363832\n",
      "Environment 4: Episode 5319, Score -121.44743250882874, Avg_Score -120.5003998448397\n",
      "Adding trajectory to replay buffer: step 22783, counter 363876\n",
      "Environment 8: Episode 5320, Score -117.52425542339682, Avg_Score -120.46161113500871\n",
      "Adding trajectory to replay buffer: step 22783, counter 363922\n",
      "Environment 11: Episode 5321, Score -121.76741934851495, Avg_Score -120.49972688737357\n",
      "Adding trajectory to replay buffer: step 22787, counter 363965\n",
      "Environment 1: Episode 5322, Score -122.89640313420568, Avg_Score -120.51779659190315\n",
      "Adding trajectory to replay buffer: step 22787, counter 364008\n",
      "Environment 12: Episode 5323, Score -123.05264957428352, Avg_Score -120.53202301148312\n",
      "Adding trajectory to replay buffer: step 22788, counter 364080\n",
      "Environment 15: Episode 5324, Score -119.15017316128663, Avg_Score -120.49770231101442\n",
      "Adding trajectory to replay buffer: step 22791, counter 364123\n",
      "Environment 6: Episode 5325, Score -122.11404933713874, Avg_Score -120.53344396289363\n",
      "Adding trajectory to replay buffer: step 22793, counter 364190\n",
      "Environment 7: Episode 5326, Score -119.07419614365573, Avg_Score -120.49942627215064\n",
      "Adding trajectory to replay buffer: step 22794, counter 364235\n",
      "Environment 5: Episode 5327, Score -122.21157522452809, Avg_Score -120.50856561327505\n",
      "Adding trajectory to replay buffer: step 22794, counter 364280\n",
      "Environment 13: Episode 5328, Score -122.04762984261289, Avg_Score -120.51818790775899\n",
      "Adding trajectory to replay buffer: step 22804, counter 364324\n",
      "Environment 10: Episode 5329, Score -123.1675218084144, Avg_Score -120.5956856329461\n",
      "Adding trajectory to replay buffer: step 22809, counter 364368\n",
      "Environment 2: Episode 5330, Score -122.44222771432314, Avg_Score -120.64507235109514\n",
      "Adding trajectory to replay buffer: step 22811, counter 364411\n",
      "Environment 4: Episode 5331, Score -122.95028215720008, Avg_Score -120.72644990363779\n",
      "Adding trajectory to replay buffer: step 22813, counter 364467\n",
      "Environment 14: Episode 5332, Score -115.40815481967044, Avg_Score -120.72446331811106\n",
      "Adding trajectory to replay buffer: step 22825, counter 364509\n",
      "Environment 11: Episode 5333, Score -119.39806764824813, Avg_Score -120.67130865823475\n",
      "Adding trajectory to replay buffer: step 22826, counter 364552\n",
      "Environment 8: Episode 5334, Score -122.50749627936881, Avg_Score -120.6667858191673\n",
      "Adding trajectory to replay buffer: step 22830, counter 364595\n",
      "Environment 1: Episode 5335, Score -122.62551943677104, Avg_Score -120.67377721062951\n",
      "Adding trajectory to replay buffer: step 22830, counter 364638\n",
      "Environment 12: Episode 5336, Score -122.58855835118331, Avg_Score -120.74690668980655\n",
      "Adding trajectory to replay buffer: step 22835, counter 364787\n",
      "Environment 0: Episode 5337, Score -118.59205636535523, Avg_Score -120.80111634097803\n",
      "Adding trajectory to replay buffer: step 22837, counter 364830\n",
      "Environment 5: Episode 5338, Score -119.0134159015206, Avg_Score -120.76830457393743\n",
      "Adding trajectory to replay buffer: step 22839, counter 364875\n",
      "Environment 13: Episode 5339, Score -121.49057252385467, Avg_Score -120.75371217367994\n",
      "Adding trajectory to replay buffer: step 22840, counter 364922\n",
      "Environment 7: Episode 5340, Score -121.90924117290942, Avg_Score -120.75104306009587\n",
      "Adding trajectory to replay buffer: step 22850, counter 364968\n",
      "Environment 10: Episode 5341, Score -120.95709852920038, Avg_Score -120.73124571862172\n",
      "Adding trajectory to replay buffer: step 22859, counter 365014\n",
      "Environment 14: Episode 5342, Score -119.51794154621537, Avg_Score -120.69597946628615\n",
      "Adding trajectory to replay buffer: step 22861, counter 365110\n",
      "Environment 3: Episode 5343, Score -122.23155992942552, Avg_Score -120.74696375650984\n",
      "Adding trajectory to replay buffer: step 22867, counter 365168\n",
      "Environment 2: Episode 5344, Score -114.15032513426058, Avg_Score -120.66171956845606\n",
      "Adding trajectory to replay buffer: step 22868, counter 365211\n",
      "Environment 11: Episode 5345, Score -122.15665378208769, Avg_Score -120.65750250920031\n",
      "Adding trajectory to replay buffer: step 22870, counter 365255\n",
      "Environment 8: Episode 5346, Score -122.13233323381283, Avg_Score -120.65166553407323\n",
      "Adding trajectory to replay buffer: step 22876, counter 365301\n",
      "Environment 12: Episode 5347, Score -122.43391497884055, Avg_Score -120.65649507430356\n",
      "Adding trajectory to replay buffer: step 22882, counter 365344\n",
      "Environment 13: Episode 5348, Score -122.09007221783386, Avg_Score -120.66103273701934\n",
      "Adding trajectory to replay buffer: step 22883, counter 365397\n",
      "Environment 1: Episode 5349, Score -122.42464981769771, Avg_Score -120.652602291771\n",
      "Adding trajectory to replay buffer: step 22883, counter 365443\n",
      "Environment 5: Episode 5350, Score -121.67110354101533, Avg_Score -120.63957851148415\n",
      "Adding trajectory to replay buffer: step 22884, counter 365487\n",
      "Environment 7: Episode 5351, Score -122.22965146787382, Avg_Score -120.61415363242799\n",
      "Adding trajectory to replay buffer: step 22885, counter 365561\n",
      "Environment 4: Episode 5352, Score -119.17555073724314, Avg_Score -120.57493067574146\n",
      "Adding trajectory to replay buffer: step 22903, counter 365605\n",
      "Environment 14: Episode 5353, Score -121.40437433714854, Avg_Score -120.56494898980482\n",
      "Adding trajectory to replay buffer: step 22906, counter 365676\n",
      "Environment 0: Episode 5354, Score -118.58549278052585, Avg_Score -120.52732218042061\n",
      "Adding trajectory to replay buffer: step 22911, counter 365720\n",
      "Environment 2: Episode 5355, Score -119.03800302493572, Avg_Score -120.48358575386281\n",
      "Adding trajectory to replay buffer: step 22914, counter 365764\n",
      "Environment 8: Episode 5356, Score -117.30415812720483, Avg_Score -120.48857026551408\n",
      "Adding trajectory to replay buffer: step 22919, counter 365815\n",
      "Environment 11: Episode 5357, Score -116.89600129788245, Avg_Score -120.42495911950516\n",
      "Adding trajectory to replay buffer: step 22919, counter 365858\n",
      "Environment 12: Episode 5358, Score -122.14590086602605, Avg_Score -120.4777630891373\n",
      "Adding trajectory to replay buffer: step 22923, counter 365931\n",
      "Environment 10: Episode 5359, Score -116.29847546660652, Avg_Score -120.4258653192981\n",
      "Adding trajectory to replay buffer: step 22926, counter 366066\n",
      "Environment 6: Episode 5360, Score -113.79450053797042, Avg_Score -120.3418492059522\n",
      "Adding trajectory to replay buffer: step 22927, counter 366109\n",
      "Environment 7: Episode 5361, Score -121.0536521225404, Avg_Score -120.31875103938462\n",
      "Adding trajectory to replay buffer: step 22927, counter 366248\n",
      "Environment 15: Episode 5362, Score -118.88011759823497, Avg_Score -120.28112762322444\n",
      "Adding trajectory to replay buffer: step 22928, counter 366293\n",
      "Environment 5: Episode 5363, Score -120.88263371091149, Avg_Score -120.26444615349614\n",
      "Adding trajectory to replay buffer: step 22928, counter 366339\n",
      "Environment 13: Episode 5364, Score -120.83991541934572, Avg_Score -120.28237565242814\n",
      "Adding trajectory to replay buffer: step 22933, counter 366593\n",
      "Environment 9: Episode 5365, Score -126.6157162575734, Avg_Score -120.33081878031096\n",
      "Adding trajectory to replay buffer: step 22936, counter 366646\n",
      "Environment 1: Episode 5366, Score -116.93280763031728, Avg_Score -120.28338518446765\n",
      "Adding trajectory to replay buffer: step 22948, counter 366691\n",
      "Environment 14: Episode 5367, Score -122.37334534118014, Avg_Score -120.33044173005982\n",
      "Adding trajectory to replay buffer: step 22951, counter 366736\n",
      "Environment 0: Episode 5368, Score -121.47305706872915, Avg_Score -120.31472852163937\n",
      "Adding trajectory to replay buffer: step 22953, counter 366778\n",
      "Environment 2: Episode 5369, Score -123.05565368120992, Avg_Score -120.3146492920106\n",
      "Adding trajectory to replay buffer: step 22957, counter 366821\n",
      "Environment 8: Episode 5370, Score -122.68237326020189, Avg_Score -120.31123256361984\n",
      "Adding trajectory to replay buffer: step 22963, counter 366865\n",
      "Environment 12: Episode 5371, Score -118.59484554565512, Avg_Score -120.26286682163878\n",
      "Adding trajectory to replay buffer: step 22965, counter 366969\n",
      "Environment 3: Episode 5372, Score -125.66329950884047, Avg_Score -120.29014916131229\n",
      "Adding trajectory to replay buffer: step 22967, counter 367013\n",
      "Environment 10: Episode 5373, Score -122.20018861463294, Avg_Score -120.29119965999377\n",
      "Adding trajectory to replay buffer: step 22971, counter 367058\n",
      "Environment 6: Episode 5374, Score -116.36500505124157, Avg_Score -120.22267582977412\n",
      "Adding trajectory to replay buffer: step 22971, counter 367101\n",
      "Environment 13: Episode 5375, Score -118.50362412472069, Avg_Score -120.23227584454486\n",
      "Adding trajectory to replay buffer: step 22972, counter 367146\n",
      "Environment 7: Episode 5376, Score -122.28341229320131, Avg_Score -120.22425024799718\n",
      "Adding trajectory to replay buffer: step 22973, counter 367192\n",
      "Environment 15: Episode 5377, Score -119.5075915217561, Avg_Score -120.19007282014432\n",
      "Adding trajectory to replay buffer: step 22979, counter 367235\n",
      "Environment 1: Episode 5378, Score -122.68383284401816, Avg_Score -120.18672250714792\n",
      "Adding trajectory to replay buffer: step 22990, counter 367277\n",
      "Environment 14: Episode 5379, Score -122.6118823080212, Avg_Score -120.19460674665078\n",
      "Adding trajectory to replay buffer: step 22996, counter 367320\n",
      "Environment 2: Episode 5380, Score -122.73322815617608, Avg_Score -120.18661602937169\n",
      "Adding trajectory to replay buffer: step 23007, counter 367364\n",
      "Environment 12: Episode 5381, Score -114.85185630907118, Avg_Score -120.15745308108406\n",
      "Adding trajectory to replay buffer: step 23008, counter 367407\n",
      "Environment 3: Episode 5382, Score -117.17837374973297, Avg_Score -120.15652848301984\n",
      "Adding trajectory to replay buffer: step 23010, counter 367489\n",
      "Environment 5: Episode 5383, Score -113.2438276711249, Avg_Score -120.06579062619801\n",
      "Adding trajectory to replay buffer: step 23012, counter 367544\n",
      "Environment 8: Episode 5384, Score -114.35665734007458, Avg_Score -119.98603330607753\n",
      "Adding trajectory to replay buffer: step 23014, counter 367607\n",
      "Environment 0: Episode 5385, Score -116.76538580410494, Avg_Score -119.98603729648167\n",
      "Adding trajectory to replay buffer: step 23014, counter 367649\n",
      "Environment 7: Episode 5386, Score -123.17834866829031, Avg_Score -119.99511992066236\n",
      "Adding trajectory to replay buffer: step 23014, counter 367692\n",
      "Environment 13: Episode 5387, Score -122.82982289154269, Avg_Score -120.03172722780671\n",
      "Adding trajectory to replay buffer: step 23015, counter 367734\n",
      "Environment 15: Episode 5388, Score -122.63934386434717, Avg_Score -120.0301118783746\n",
      "Adding trajectory to replay buffer: step 23022, counter 367823\n",
      "Environment 9: Episode 5389, Score -115.40153076789404, Avg_Score -119.97474609440086\n",
      "Adding trajectory to replay buffer: step 23031, counter 367875\n",
      "Environment 1: Episode 5390, Score -116.03502629694654, Avg_Score -119.99718580973759\n",
      "Adding trajectory to replay buffer: step 23033, counter 367941\n",
      "Environment 10: Episode 5391, Score -116.36329573471224, Avg_Score -120.02034285057405\n",
      "Adding trajectory to replay buffer: step 23036, counter 367987\n",
      "Environment 14: Episode 5392, Score -117.59257625823344, Avg_Score -119.99453506784468\n",
      "Adding trajectory to replay buffer: step 23041, counter 368032\n",
      "Environment 2: Episode 5393, Score -121.89040919089257, Avg_Score -120.00942514732061\n",
      "Adding trajectory to replay buffer: step 23045, counter 368106\n",
      "Environment 6: Episode 5394, Score -116.2538353968809, Avg_Score -119.9816244751227\n",
      "Adding trajectory to replay buffer: step 23050, counter 368149\n",
      "Environment 12: Episode 5395, Score -123.23051663218997, Avg_Score -120.04985988791141\n",
      "Adding trajectory to replay buffer: step 23052, counter 368193\n",
      "Environment 3: Episode 5396, Score -121.66625827059956, Avg_Score -120.10401466089336\n",
      "Adding trajectory to replay buffer: step 23054, counter 368237\n",
      "Environment 5: Episode 5397, Score -122.10975866747089, Avg_Score -120.0736483275511\n",
      "Adding trajectory to replay buffer: step 23055, counter 368280\n",
      "Environment 8: Episode 5398, Score -120.34099236204351, Avg_Score -120.13462692665415\n",
      "Adding trajectory to replay buffer: step 23057, counter 368323\n",
      "Environment 0: Episode 5399, Score -122.71954890752397, Avg_Score -120.12767718142362\n",
      "Adding trajectory to replay buffer: step 23058, counter 368367\n",
      "Environment 13: Episode 5400, Score -122.03009446251764, Avg_Score -120.12282512622646\n",
      "Adding trajectory to replay buffer: step 23059, counter 368412\n",
      "Environment 7: Episode 5401, Score -121.9751234614905, Avg_Score -120.17246950603995\n",
      "Adding trajectory to replay buffer: step 23065, counter 368592\n",
      "Environment 4: Episode 5402, Score -132.60193062055563, Avg_Score -120.27842701426206\n",
      "Adding trajectory to replay buffer: step 23066, counter 368636\n",
      "Environment 9: Episode 5403, Score -116.10071212552116, Avg_Score -120.28946807554564\n",
      "Adding trajectory to replay buffer: step 23077, counter 368680\n",
      "Environment 10: Episode 5404, Score -116.67281984502947, Avg_Score -120.2906544020249\n",
      "Adding trajectory to replay buffer: step 23077, counter 368742\n",
      "Environment 15: Episode 5405, Score -112.30054743449949, Avg_Score -120.19907397508094\n",
      "Adding trajectory to replay buffer: step 23081, counter 368904\n",
      "Environment 11: Episode 5406, Score -116.39124027742321, Avg_Score -120.20041661570967\n",
      "Adding trajectory to replay buffer: step 23084, counter 368947\n",
      "Environment 2: Episode 5407, Score -123.06321809482512, Avg_Score -120.20817764742645\n",
      "Adding trajectory to replay buffer: step 23089, counter 368991\n",
      "Environment 6: Episode 5408, Score -120.45830715486096, Avg_Score -120.18943871052898\n",
      "Adding trajectory to replay buffer: step 23094, counter 369035\n",
      "Environment 12: Episode 5409, Score -122.80937454394623, Avg_Score -120.20035803271486\n",
      "Adding trajectory to replay buffer: step 23098, counter 369076\n",
      "Environment 0: Episode 5410, Score -114.98297167822967, Avg_Score -120.12000785690529\n",
      "Adding trajectory to replay buffer: step 23099, counter 369123\n",
      "Environment 3: Episode 5411, Score -120.05913065587046, Avg_Score -120.10238131081525\n",
      "Adding trajectory to replay buffer: step 23100, counter 369169\n",
      "Environment 5: Episode 5412, Score -121.58623405715326, Avg_Score -120.12415447127673\n",
      "Adding trajectory to replay buffer: step 23103, counter 369217\n",
      "Environment 8: Episode 5413, Score -120.39082230921649, Avg_Score -120.1600724984239\n",
      "Adding trajectory to replay buffer: step 23104, counter 369263\n",
      "Environment 13: Episode 5414, Score -122.01451511215853, Avg_Score -120.15958696549711\n",
      "Adding trajectory to replay buffer: step 23105, counter 369309\n",
      "Environment 7: Episode 5415, Score -122.5713963404124, Avg_Score -120.21484809642574\n",
      "Adding trajectory to replay buffer: step 23111, counter 369354\n",
      "Environment 9: Episode 5416, Score -116.6974493972802, Avg_Score -120.16462312592141\n",
      "Adding trajectory to replay buffer: step 23119, counter 369442\n",
      "Environment 1: Episode 5417, Score -111.02695591136317, Avg_Score -120.13592156233615\n",
      "Adding trajectory to replay buffer: step 23120, counter 369485\n",
      "Environment 10: Episode 5418, Score -120.21289738344774, Avg_Score -120.18545946939062\n",
      "Adding trajectory to replay buffer: step 23120, counter 369528\n",
      "Environment 15: Episode 5419, Score -121.36704736213572, Avg_Score -120.18465561792367\n",
      "Adding trajectory to replay buffer: step 23124, counter 369571\n",
      "Environment 11: Episode 5420, Score -122.61649792635316, Avg_Score -120.23557804295325\n",
      "Adding trajectory to replay buffer: step 23136, counter 369618\n",
      "Environment 6: Episode 5421, Score -121.40590899658763, Avg_Score -120.23196293943398\n",
      "Adding trajectory to replay buffer: step 23141, counter 369665\n",
      "Environment 12: Episode 5422, Score -116.85335674893545, Avg_Score -120.17153247558126\n",
      "Adding trajectory to replay buffer: step 23143, counter 369710\n",
      "Environment 0: Episode 5423, Score -121.83592795996307, Avg_Score -120.15936525943808\n",
      "Adding trajectory to replay buffer: step 23145, counter 369755\n",
      "Environment 5: Episode 5424, Score -120.50835249636633, Avg_Score -120.17294705278887\n",
      "Adding trajectory to replay buffer: step 23147, counter 369799\n",
      "Environment 8: Episode 5425, Score -121.87828258593939, Avg_Score -120.17058938527688\n",
      "Adding trajectory to replay buffer: step 23149, counter 369843\n",
      "Environment 7: Episode 5426, Score -121.72686222415417, Avg_Score -120.19711604608186\n",
      "Adding trajectory to replay buffer: step 23150, counter 369889\n",
      "Environment 13: Episode 5427, Score -121.14407828303489, Avg_Score -120.18644107666694\n",
      "Adding trajectory to replay buffer: step 23150, counter 370003\n",
      "Environment 14: Episode 5428, Score -117.14612918435782, Avg_Score -120.13742607008436\n",
      "Adding trajectory to replay buffer: step 23154, counter 370046\n",
      "Environment 9: Episode 5429, Score -122.76253153745893, Avg_Score -120.1333761673748\n",
      "Adding trajectory to replay buffer: step 23161, counter 370088\n",
      "Environment 1: Episode 5430, Score -115.85769944776223, Avg_Score -120.0675308847092\n",
      "Adding trajectory to replay buffer: step 23168, counter 370136\n",
      "Environment 10: Episode 5431, Score -122.73893716497783, Avg_Score -120.06541743478698\n",
      "Adding trajectory to replay buffer: step 23174, counter 370190\n",
      "Environment 15: Episode 5432, Score -119.87322948549749, Avg_Score -120.11006818144524\n",
      "Adding trajectory to replay buffer: step 23179, counter 370233\n",
      "Environment 6: Episode 5433, Score -122.91612965485763, Avg_Score -120.14524880151133\n",
      "Adding trajectory to replay buffer: step 23183, counter 370275\n",
      "Environment 12: Episode 5434, Score -123.23658754630696, Avg_Score -120.15253971418073\n",
      "Adding trajectory to replay buffer: step 23186, counter 370318\n",
      "Environment 0: Episode 5435, Score -122.97459419303263, Avg_Score -120.15603046174334\n",
      "Adding trajectory to replay buffer: step 23188, counter 370361\n",
      "Environment 5: Episode 5436, Score -123.49517598322655, Avg_Score -120.16509663806377\n",
      "Adding trajectory to replay buffer: step 23190, counter 370404\n",
      "Environment 8: Episode 5437, Score -122.20744609186923, Avg_Score -120.2012505353289\n",
      "Adding trajectory to replay buffer: step 23192, counter 370447\n",
      "Environment 7: Episode 5438, Score -122.86297801454862, Avg_Score -120.2397461564592\n",
      "Adding trajectory to replay buffer: step 23194, counter 370491\n",
      "Environment 13: Episode 5439, Score -116.25260724149645, Avg_Score -120.1873665036356\n",
      "Adding trajectory to replay buffer: step 23194, counter 370535\n",
      "Environment 14: Episode 5440, Score -123.06415111450106, Avg_Score -120.19891560305153\n",
      "Adding trajectory to replay buffer: step 23200, counter 370581\n",
      "Environment 9: Episode 5441, Score -119.75512401128562, Avg_Score -120.18689585787239\n",
      "Adding trajectory to replay buffer: step 23204, counter 370624\n",
      "Environment 1: Episode 5442, Score -122.20058519030488, Avg_Score -120.21372229431329\n",
      "Adding trajectory to replay buffer: step 23207, counter 370707\n",
      "Environment 11: Episode 5443, Score -115.00654204715043, Avg_Score -120.14147211549056\n",
      "Adding trajectory to replay buffer: step 23211, counter 370750\n",
      "Environment 10: Episode 5444, Score -122.7720084262186, Avg_Score -120.22768894841012\n",
      "Adding trajectory to replay buffer: step 23216, counter 370867\n",
      "Environment 3: Episode 5445, Score -118.86425484797142, Avg_Score -120.19476495906896\n",
      "Adding trajectory to replay buffer: step 23217, counter 370910\n",
      "Environment 15: Episode 5446, Score -118.40557199055328, Avg_Score -120.15749734663639\n",
      "Adding trajectory to replay buffer: step 23229, counter 370960\n",
      "Environment 6: Episode 5447, Score -123.64230938590566, Avg_Score -120.16958129070701\n",
      "Adding trajectory to replay buffer: step 23231, counter 371005\n",
      "Environment 0: Episode 5448, Score -122.57271537222714, Avg_Score -120.17440772225098\n",
      "Adding trajectory to replay buffer: step 23232, counter 371049\n",
      "Environment 5: Episode 5449, Score -119.97518614539379, Avg_Score -120.1499130855279\n",
      "Adding trajectory to replay buffer: step 23235, counter 371219\n",
      "Environment 4: Episode 5450, Score -118.64063523282623, Avg_Score -120.11960840244605\n",
      "Adding trajectory to replay buffer: step 23237, counter 371264\n",
      "Environment 7: Episode 5451, Score -122.46234735588232, Avg_Score -120.12193536132612\n",
      "Adding trajectory to replay buffer: step 23238, counter 371308\n",
      "Environment 14: Episode 5452, Score -122.48794709151052, Avg_Score -120.1550593248688\n",
      "Adding trajectory to replay buffer: step 23239, counter 371353\n",
      "Environment 13: Episode 5453, Score -122.48298233663166, Avg_Score -120.16584540486359\n",
      "Adding trajectory to replay buffer: step 23241, counter 371411\n",
      "Environment 12: Episode 5454, Score -116.07809399682719, Avg_Score -120.14077141702663\n",
      "Adding trajectory to replay buffer: step 23243, counter 371454\n",
      "Environment 9: Episode 5455, Score -123.71010327228717, Avg_Score -120.18749241950015\n",
      "Adding trajectory to replay buffer: step 23247, counter 371497\n",
      "Environment 1: Episode 5456, Score -122.08640168590969, Avg_Score -120.2353148550872\n",
      "Adding trajectory to replay buffer: step 23251, counter 371664\n",
      "Environment 2: Episode 5457, Score -132.05844528347254, Avg_Score -120.38693929494309\n",
      "Adding trajectory to replay buffer: step 23251, counter 371708\n",
      "Environment 11: Episode 5458, Score -120.1177876627762, Avg_Score -120.36665816291057\n",
      "Adding trajectory to replay buffer: step 23255, counter 371752\n",
      "Environment 10: Episode 5459, Score -122.38278219063085, Avg_Score -120.42750123015085\n",
      "Adding trajectory to replay buffer: step 23266, counter 371828\n",
      "Environment 8: Episode 5460, Score -115.97663466355391, Avg_Score -120.44932257140668\n",
      "Adding trajectory to replay buffer: step 23274, counter 371871\n",
      "Environment 0: Episode 5461, Score -122.22752919182305, Avg_Score -120.46106134209947\n",
      "Adding trajectory to replay buffer: step 23277, counter 371916\n",
      "Environment 5: Episode 5462, Score -121.7755083421655, Avg_Score -120.49001524953879\n",
      "Adding trajectory to replay buffer: step 23278, counter 371959\n",
      "Environment 4: Episode 5463, Score -122.55666217772911, Avg_Score -120.50675553420699\n",
      "Adding trajectory to replay buffer: step 23281, counter 372023\n",
      "Environment 15: Episode 5464, Score -116.08738805755227, Avg_Score -120.45923026058905\n",
      "Adding trajectory to replay buffer: step 23282, counter 372068\n",
      "Environment 7: Episode 5465, Score -121.2557668519672, Avg_Score -120.40563076653297\n",
      "Adding trajectory to replay buffer: step 23283, counter 372113\n",
      "Environment 14: Episode 5466, Score -122.84000816209677, Avg_Score -120.46470277185075\n",
      "Adding trajectory to replay buffer: step 23285, counter 372159\n",
      "Environment 13: Episode 5467, Score -118.28428111865, Avg_Score -120.42381212962547\n",
      "Adding trajectory to replay buffer: step 23286, counter 372204\n",
      "Environment 12: Episode 5468, Score -117.50733239164825, Avg_Score -120.38415488285467\n",
      "Adding trajectory to replay buffer: step 23289, counter 372250\n",
      "Environment 9: Episode 5469, Score -118.8877030606009, Avg_Score -120.34247537664858\n",
      "Adding trajectory to replay buffer: step 23295, counter 372298\n",
      "Environment 1: Episode 5470, Score -120.52236312662127, Avg_Score -120.32087527531277\n",
      "Adding trajectory to replay buffer: step 23303, counter 372346\n",
      "Environment 10: Episode 5471, Score -119.19871076309354, Avg_Score -120.32691392748718\n",
      "Adding trajectory to replay buffer: step 23307, counter 372402\n",
      "Environment 11: Episode 5472, Score -115.1256703978212, Avg_Score -120.221537636377\n",
      "Adding trajectory to replay buffer: step 23321, counter 372446\n",
      "Environment 5: Episode 5473, Score -122.10867630692944, Avg_Score -120.22062251329993\n",
      "Adding trajectory to replay buffer: step 23322, counter 372490\n",
      "Environment 4: Episode 5474, Score -122.33392360929152, Avg_Score -120.28031169888044\n",
      "Adding trajectory to replay buffer: step 23325, counter 372533\n",
      "Environment 7: Episode 5475, Score -122.99444592687364, Avg_Score -120.32521991690196\n",
      "Adding trajectory to replay buffer: step 23326, counter 372578\n",
      "Environment 15: Episode 5476, Score -122.71333747513903, Avg_Score -120.3295191687213\n",
      "Adding trajectory to replay buffer: step 23329, counter 372678\n",
      "Environment 6: Episode 5477, Score -120.749926949149, Avg_Score -120.34194252299527\n",
      "Adding trajectory to replay buffer: step 23329, counter 372721\n",
      "Environment 12: Episode 5478, Score -122.4235148034282, Avg_Score -120.33933934258934\n",
      "Adding trajectory to replay buffer: step 23329, counter 372767\n",
      "Environment 14: Episode 5479, Score -119.58861482197977, Avg_Score -120.30910666772893\n",
      "Adding trajectory to replay buffer: step 23333, counter 372811\n",
      "Environment 9: Episode 5480, Score -122.06381106954191, Avg_Score -120.30241249686262\n",
      "Adding trajectory to replay buffer: step 23337, counter 372853\n",
      "Environment 1: Episode 5481, Score -118.04507039969165, Avg_Score -120.33434463776885\n",
      "Adding trajectory to replay buffer: step 23347, counter 372934\n",
      "Environment 8: Episode 5482, Score -120.27643458186525, Avg_Score -120.36532524609014\n",
      "Adding trajectory to replay buffer: step 23350, counter 372981\n",
      "Environment 10: Episode 5483, Score -123.37425714192975, Avg_Score -120.46662954079818\n",
      "Adding trajectory to replay buffer: step 23360, counter 373034\n",
      "Environment 11: Episode 5484, Score -119.48865657018652, Avg_Score -120.51794953309931\n",
      "Adding trajectory to replay buffer: step 23368, counter 373077\n",
      "Environment 7: Episode 5485, Score -123.30892021871173, Avg_Score -120.58338487724536\n",
      "Adding trajectory to replay buffer: step 23368, counter 373119\n",
      "Environment 15: Episode 5486, Score -122.86862682945468, Avg_Score -120.58028765885702\n",
      "Adding trajectory to replay buffer: step 23372, counter 373162\n",
      "Environment 6: Episode 5487, Score -122.37284078970677, Avg_Score -120.57571783783865\n",
      "Adding trajectory to replay buffer: step 23372, counter 373205\n",
      "Environment 12: Episode 5488, Score -122.47445838735811, Avg_Score -120.57406898306876\n",
      "Adding trajectory to replay buffer: step 23372, counter 373248\n",
      "Environment 14: Episode 5489, Score -122.38005770089549, Avg_Score -120.64385425239878\n",
      "Adding trajectory to replay buffer: step 23377, counter 373292\n",
      "Environment 9: Episode 5490, Score -121.7659424335373, Avg_Score -120.70116341376468\n",
      "Adding trajectory to replay buffer: step 23380, counter 373350\n",
      "Environment 4: Episode 5491, Score -111.56108361783127, Avg_Score -120.65314129259588\n",
      "Adding trajectory to replay buffer: step 23381, counter 373410\n",
      "Environment 5: Episode 5492, Score -116.19051360806947, Avg_Score -120.63912066609424\n",
      "Adding trajectory to replay buffer: step 23383, counter 373456\n",
      "Environment 1: Episode 5493, Score -120.59987514154179, Avg_Score -120.62621532560071\n",
      "Adding trajectory to replay buffer: step 23390, counter 373572\n",
      "Environment 0: Episode 5494, Score -127.24540078594569, Avg_Score -120.73613097949138\n",
      "Adding trajectory to replay buffer: step 23394, counter 373619\n",
      "Environment 8: Episode 5495, Score -122.209994032003, Avg_Score -120.72592575348948\n",
      "Adding trajectory to replay buffer: step 23407, counter 373741\n",
      "Environment 13: Episode 5496, Score -116.65141956447437, Avg_Score -120.67577736642824\n",
      "Adding trajectory to replay buffer: step 23411, counter 373901\n",
      "Environment 2: Episode 5497, Score -120.82032862880266, Avg_Score -120.66288306604152\n",
      "Adding trajectory to replay buffer: step 23416, counter 373949\n",
      "Environment 7: Episode 5498, Score -118.66124819026764, Avg_Score -120.6460856243238\n",
      "Adding trajectory to replay buffer: step 23416, counter 373993\n",
      "Environment 14: Episode 5499, Score -123.19949437832241, Avg_Score -120.6508850790318\n",
      "Adding trajectory to replay buffer: step 23418, counter 374039\n",
      "Environment 6: Episode 5500, Score -118.80413628386395, Avg_Score -120.61862549724526\n",
      "Adding trajectory to replay buffer: step 23420, counter 374082\n",
      "Environment 9: Episode 5501, Score -119.87461830032586, Avg_Score -120.5976204456336\n",
      "Adding trajectory to replay buffer: step 23420, counter 374130\n",
      "Environment 12: Episode 5502, Score -122.07804752425228, Avg_Score -120.49238161467056\n",
      "Adding trajectory to replay buffer: step 23422, counter 374172\n",
      "Environment 4: Episode 5503, Score -115.56134244532201, Avg_Score -120.48698791786857\n",
      "Adding trajectory to replay buffer: step 23425, counter 374216\n",
      "Environment 5: Episode 5504, Score -124.21610329249097, Avg_Score -120.56242075234323\n",
      "Adding trajectory to replay buffer: step 23428, counter 374261\n",
      "Environment 1: Episode 5505, Score -123.69965243811667, Avg_Score -120.67641180237939\n",
      "Adding trajectory to replay buffer: step 23433, counter 374304\n",
      "Environment 0: Episode 5506, Score -122.27382319309501, Avg_Score -120.73523763153611\n",
      "Adding trajectory to replay buffer: step 23437, counter 374347\n",
      "Environment 8: Episode 5507, Score -123.61336023221537, Avg_Score -120.74073905291003\n",
      "Adding trajectory to replay buffer: step 23452, counter 374392\n",
      "Environment 13: Episode 5508, Score -121.79416414094996, Avg_Score -120.75409762277091\n",
      "Adding trajectory to replay buffer: step 23453, counter 374434\n",
      "Environment 2: Episode 5509, Score -123.04029376179662, Avg_Score -120.7564068149494\n",
      "Adding trajectory to replay buffer: step 23454, counter 374672\n",
      "Environment 3: Episode 5510, Score -127.93069194560249, Avg_Score -120.88588401762317\n",
      "Adding trajectory to replay buffer: step 23460, counter 374716\n",
      "Environment 7: Episode 5511, Score -117.83472581051973, Avg_Score -120.86363996916967\n",
      "Adding trajectory to replay buffer: step 23460, counter 374760\n",
      "Environment 14: Episode 5512, Score -122.48484040824201, Avg_Score -120.87262603268054\n",
      "Adding trajectory to replay buffer: step 23461, counter 374803\n",
      "Environment 6: Episode 5513, Score -117.24915840076098, Avg_Score -120.841209393596\n",
      "Adding trajectory to replay buffer: step 23468, counter 374846\n",
      "Environment 5: Episode 5514, Score -116.61486243055512, Avg_Score -120.78721286677994\n",
      "Adding trajectory to replay buffer: step 23468, counter 374894\n",
      "Environment 12: Episode 5515, Score -121.46808450900639, Avg_Score -120.77617974846588\n",
      "Adding trajectory to replay buffer: step 23469, counter 374941\n",
      "Environment 4: Episode 5516, Score -122.34183864575563, Avg_Score -120.83262364095063\n",
      "Adding trajectory to replay buffer: step 23477, counter 374985\n",
      "Environment 0: Episode 5517, Score -123.54553053280152, Avg_Score -120.95780938716501\n",
      "Adding trajectory to replay buffer: step 23480, counter 375105\n",
      "Environment 11: Episode 5518, Score -117.54834447374257, Avg_Score -120.93116385806796\n",
      "Adding trajectory to replay buffer: step 23487, counter 375172\n",
      "Environment 9: Episode 5519, Score -113.10730380902626, Avg_Score -120.84856642253688\n",
      "Adding trajectory to replay buffer: step 23495, counter 375215\n",
      "Environment 13: Episode 5520, Score -122.95696032928242, Avg_Score -120.85197104656618\n",
      "Adding trajectory to replay buffer: step 23497, counter 375259\n",
      "Environment 2: Episode 5521, Score -122.82257860362407, Avg_Score -120.86613774263652\n",
      "Adding trajectory to replay buffer: step 23500, counter 375305\n",
      "Environment 3: Episode 5522, Score -121.32646664873512, Avg_Score -120.91086884163452\n",
      "Adding trajectory to replay buffer: step 23503, counter 375440\n",
      "Environment 15: Episode 5523, Score -127.42791886552547, Avg_Score -120.96678875069014\n",
      "Adding trajectory to replay buffer: step 23506, counter 375485\n",
      "Environment 6: Episode 5524, Score -123.22329104280223, Avg_Score -120.9939381361545\n",
      "Adding trajectory to replay buffer: step 23506, counter 375531\n",
      "Environment 7: Episode 5525, Score -121.51091597113448, Avg_Score -120.99026447000647\n",
      "Adding trajectory to replay buffer: step 23506, counter 375577\n",
      "Environment 14: Episode 5526, Score -120.65265776238778, Avg_Score -120.97952242538881\n",
      "Adding trajectory to replay buffer: step 23510, counter 375659\n",
      "Environment 1: Episode 5527, Score -110.87942974432123, Avg_Score -120.87687594000167\n",
      "Adding trajectory to replay buffer: step 23512, counter 375703\n",
      "Environment 5: Episode 5528, Score -122.43560603534866, Avg_Score -120.92977070851161\n",
      "Adding trajectory to replay buffer: step 23512, counter 375747\n",
      "Environment 12: Episode 5529, Score -122.37663738562291, Avg_Score -120.92591176699324\n",
      "Adding trajectory to replay buffer: step 23514, counter 375792\n",
      "Environment 4: Episode 5530, Score -121.54120840250948, Avg_Score -120.9827468565407\n",
      "Adding trajectory to replay buffer: step 23523, counter 375838\n",
      "Environment 0: Episode 5531, Score -121.30208342301472, Avg_Score -120.96837831912109\n",
      "Adding trajectory to replay buffer: step 23525, counter 375883\n",
      "Environment 11: Episode 5532, Score -122.46984493429524, Avg_Score -120.99434447360908\n",
      "Adding trajectory to replay buffer: step 23530, counter 375926\n",
      "Environment 9: Episode 5533, Score -123.36363722946682, Avg_Score -120.99881954935516\n",
      "Adding trajectory to replay buffer: step 23540, counter 375971\n",
      "Environment 13: Episode 5534, Score -121.3105619659635, Avg_Score -120.97955929355174\n",
      "Adding trajectory to replay buffer: step 23541, counter 376015\n",
      "Environment 2: Episode 5535, Score -122.86427462534358, Avg_Score -120.97845609787484\n",
      "Adding trajectory to replay buffer: step 23545, counter 376060\n",
      "Environment 3: Episode 5536, Score -122.36408220675341, Avg_Score -120.96714516011012\n",
      "Adding trajectory to replay buffer: step 23546, counter 376256\n",
      "Environment 10: Episode 5537, Score -119.48480580287125, Avg_Score -120.93991875722014\n",
      "Adding trajectory to replay buffer: step 23549, counter 376299\n",
      "Environment 14: Episode 5538, Score -122.9709765953614, Avg_Score -120.94099874302827\n",
      "Adding trajectory to replay buffer: step 23550, counter 376343\n",
      "Environment 6: Episode 5539, Score -121.7748322153048, Avg_Score -120.99622099276635\n",
      "Adding trajectory to replay buffer: step 23552, counter 376389\n",
      "Environment 7: Episode 5540, Score -121.4317956020174, Avg_Score -120.9798974376415\n",
      "Adding trajectory to replay buffer: step 23554, counter 376433\n",
      "Environment 1: Episode 5541, Score -122.13637320219352, Avg_Score -121.00370992955058\n",
      "Adding trajectory to replay buffer: step 23555, counter 376476\n",
      "Environment 12: Episode 5542, Score -122.75659783108966, Avg_Score -121.00927005595842\n",
      "Adding trajectory to replay buffer: step 23556, counter 376520\n",
      "Environment 5: Episode 5543, Score -122.41552070472933, Avg_Score -121.08335984253421\n",
      "Adding trajectory to replay buffer: step 23558, counter 376564\n",
      "Environment 4: Episode 5544, Score -121.900574188279, Avg_Score -121.0746455001548\n",
      "Adding trajectory to replay buffer: step 23570, counter 376609\n",
      "Environment 11: Episode 5545, Score -122.69048974529034, Avg_Score -121.11290784912799\n",
      "Adding trajectory to replay buffer: step 23576, counter 376655\n",
      "Environment 9: Episode 5546, Score -124.15264638415724, Avg_Score -121.17037859306403\n",
      "Adding trajectory to replay buffer: step 23582, counter 376697\n",
      "Environment 13: Episode 5547, Score -122.76303403988108, Avg_Score -121.16158583960377\n",
      "Adding trajectory to replay buffer: step 23585, counter 376741\n",
      "Environment 2: Episode 5548, Score -118.69135381971734, Avg_Score -121.12277222407867\n",
      "Adding trajectory to replay buffer: step 23590, counter 376785\n",
      "Environment 10: Episode 5549, Score -122.60450163096873, Avg_Score -121.14906537893442\n",
      "Adding trajectory to replay buffer: step 23590, counter 376872\n",
      "Environment 15: Episode 5550, Score -113.44987762416588, Avg_Score -121.09715780284782\n",
      "Adding trajectory to replay buffer: step 23591, counter 376918\n",
      "Environment 3: Episode 5551, Score -121.57583009435609, Avg_Score -121.08829263023256\n",
      "Adding trajectory to replay buffer: step 23593, counter 376961\n",
      "Environment 6: Episode 5552, Score -123.2720846198905, Avg_Score -121.09613400551635\n",
      "Adding trajectory to replay buffer: step 23593, counter 377005\n",
      "Environment 14: Episode 5553, Score -123.25888552341983, Avg_Score -121.10389303738422\n",
      "Adding trajectory to replay buffer: step 23598, counter 377051\n",
      "Environment 7: Episode 5554, Score -118.25319573786544, Avg_Score -121.12564405479459\n",
      "Adding trajectory to replay buffer: step 23598, counter 377094\n",
      "Environment 12: Episode 5555, Score -122.44112746269008, Avg_Score -121.11295429669863\n",
      "Adding trajectory to replay buffer: step 23600, counter 377171\n",
      "Environment 0: Episode 5556, Score -113.62774237581141, Avg_Score -121.02836770359764\n",
      "Adding trajectory to replay buffer: step 23600, counter 377215\n",
      "Environment 5: Episode 5557, Score -122.3691756338744, Avg_Score -120.93147500710165\n",
      "Adding trajectory to replay buffer: step 23601, counter 377258\n",
      "Environment 4: Episode 5558, Score -122.42065478979052, Avg_Score -120.95450367837181\n",
      "Adding trajectory to replay buffer: step 23614, counter 377302\n",
      "Environment 11: Episode 5559, Score -123.03902392370068, Avg_Score -120.9610660957025\n",
      "Adding trajectory to replay buffer: step 23623, counter 377349\n",
      "Environment 9: Episode 5560, Score -120.48293471421303, Avg_Score -121.00612909620908\n",
      "Adding trajectory to replay buffer: step 23626, counter 377393\n",
      "Environment 13: Episode 5561, Score -119.39928707290068, Avg_Score -120.97784667501983\n",
      "Adding trajectory to replay buffer: step 23630, counter 377438\n",
      "Environment 2: Episode 5562, Score -121.49195066888906, Avg_Score -120.97501109828707\n",
      "Adding trajectory to replay buffer: step 23636, counter 377484\n",
      "Environment 10: Episode 5563, Score -122.72592795732866, Avg_Score -120.97670375608305\n",
      "Adding trajectory to replay buffer: step 23636, counter 377530\n",
      "Environment 15: Episode 5564, Score -119.69486708893254, Avg_Score -121.01277854639686\n",
      "Adding trajectory to replay buffer: step 23638, counter 377577\n",
      "Environment 3: Episode 5565, Score -122.40234314609816, Avg_Score -121.02424430933816\n",
      "Adding trajectory to replay buffer: step 23638, counter 377622\n",
      "Environment 14: Episode 5566, Score -119.6789394341993, Avg_Score -120.99263362205916\n",
      "Adding trajectory to replay buffer: step 23640, counter 377669\n",
      "Environment 6: Episode 5567, Score -120.54905961191903, Avg_Score -121.01528140699187\n",
      "Adding trajectory to replay buffer: step 23641, counter 377712\n",
      "Environment 7: Episode 5568, Score -123.18996066442443, Avg_Score -121.07210768971962\n",
      "Adding trajectory to replay buffer: step 23642, counter 377756\n",
      "Environment 12: Episode 5569, Score -121.88252839033926, Avg_Score -121.102055943017\n",
      "Adding trajectory to replay buffer: step 23644, counter 377800\n",
      "Environment 0: Episode 5570, Score -120.40653427593658, Avg_Score -121.10089765451015\n",
      "Adding trajectory to replay buffer: step 23644, counter 377844\n",
      "Environment 5: Episode 5571, Score -120.74133196892029, Avg_Score -121.11632386656842\n",
      "Adding trajectory to replay buffer: step 23651, counter 378058\n",
      "Environment 8: Episode 5572, Score -124.70100417547422, Avg_Score -121.21207720434495\n",
      "Adding trajectory to replay buffer: step 23657, counter 378101\n",
      "Environment 11: Episode 5573, Score -121.83267255777245, Avg_Score -121.20931716685338\n",
      "Adding trajectory to replay buffer: step 23665, counter 378212\n",
      "Environment 1: Episode 5574, Score -114.71983740430946, Avg_Score -121.13317630480354\n",
      "Adding trajectory to replay buffer: step 23669, counter 378258\n",
      "Environment 9: Episode 5575, Score -120.09261594157417, Avg_Score -121.10415800495055\n",
      "Adding trajectory to replay buffer: step 23670, counter 378302\n",
      "Environment 13: Episode 5576, Score -123.51490244575979, Avg_Score -121.11217365465676\n",
      "Adding trajectory to replay buffer: step 23673, counter 378345\n",
      "Environment 2: Episode 5577, Score -123.05031512232125, Avg_Score -121.13517753638851\n",
      "Adding trajectory to replay buffer: step 23682, counter 378391\n",
      "Environment 15: Episode 5578, Score -120.60048789532871, Avg_Score -121.11694726730751\n",
      "Adding trajectory to replay buffer: step 23685, counter 378435\n",
      "Environment 7: Episode 5579, Score -117.7761318263424, Avg_Score -121.09882243735113\n",
      "Adding trajectory to replay buffer: step 23686, counter 378481\n",
      "Environment 6: Episode 5580, Score -120.61458108247321, Avg_Score -121.08433013748045\n",
      "Adding trajectory to replay buffer: step 23687, counter 378526\n",
      "Environment 12: Episode 5581, Score -122.2159290246237, Avg_Score -121.12603872372976\n",
      "Adding trajectory to replay buffer: step 23688, counter 378570\n",
      "Environment 0: Episode 5582, Score -120.6551341387493, Avg_Score -121.12982571929862\n",
      "Adding trajectory to replay buffer: step 23688, counter 378614\n",
      "Environment 5: Episode 5583, Score -122.79054931641247, Avg_Score -121.12398864104345\n",
      "Adding trajectory to replay buffer: step 23693, counter 378706\n",
      "Environment 4: Episode 5584, Score -112.7251099220241, Avg_Score -121.05635317456185\n",
      "Adding trajectory to replay buffer: step 23694, counter 378749\n",
      "Environment 8: Episode 5585, Score -122.53644059653952, Avg_Score -121.04862837834013\n",
      "Adding trajectory to replay buffer: step 23701, counter 378793\n",
      "Environment 11: Episode 5586, Score -122.64158846925633, Avg_Score -121.04635799473813\n",
      "Adding trajectory to replay buffer: step 23706, counter 378861\n",
      "Environment 14: Episode 5587, Score -117.96749059809558, Avg_Score -121.00230449282202\n",
      "Adding trajectory to replay buffer: step 23707, counter 378903\n",
      "Environment 1: Episode 5588, Score -121.95265414557916, Avg_Score -120.99708645040425\n",
      "Adding trajectory to replay buffer: step 23711, counter 378978\n",
      "Environment 10: Episode 5589, Score -111.80374551182861, Avg_Score -120.8913233285136\n",
      "Adding trajectory to replay buffer: step 23713, counter 379021\n",
      "Environment 13: Episode 5590, Score -122.05717681596056, Avg_Score -120.89423567233781\n",
      "Adding trajectory to replay buffer: step 23715, counter 379067\n",
      "Environment 9: Episode 5591, Score -121.21317553745273, Avg_Score -120.99075659153402\n",
      "Adding trajectory to replay buffer: step 23718, counter 379112\n",
      "Environment 2: Episode 5592, Score -117.08016756160309, Avg_Score -120.99965313106937\n",
      "Adding trajectory to replay buffer: step 23724, counter 379154\n",
      "Environment 15: Episode 5593, Score -118.70919507100805, Avg_Score -120.98074633036404\n",
      "Adding trajectory to replay buffer: step 23731, counter 379197\n",
      "Environment 0: Episode 5594, Score -122.37496130879038, Avg_Score -120.9320419355925\n",
      "Adding trajectory to replay buffer: step 23731, counter 379290\n",
      "Environment 3: Episode 5595, Score -113.0443248192047, Avg_Score -120.84038524346451\n",
      "Adding trajectory to replay buffer: step 23733, counter 379335\n",
      "Environment 5: Episode 5596, Score -121.80249571829073, Avg_Score -120.89189600500269\n",
      "Adding trajectory to replay buffer: step 23736, counter 379378\n",
      "Environment 4: Episode 5597, Score -123.00423467001382, Avg_Score -120.9137350654148\n",
      "Adding trajectory to replay buffer: step 23737, counter 379421\n",
      "Environment 8: Episode 5598, Score -122.76915470459937, Avg_Score -120.95481413055813\n",
      "Adding trajectory to replay buffer: step 23746, counter 379482\n",
      "Environment 7: Episode 5599, Score -119.16637281875717, Avg_Score -120.91448291496248\n",
      "Adding trajectory to replay buffer: step 23746, counter 379527\n",
      "Environment 11: Episode 5600, Score -123.14603950389848, Avg_Score -120.95790194716282\n",
      "Adding trajectory to replay buffer: step 23746, counter 379586\n",
      "Environment 12: Episode 5601, Score -116.93137613975381, Avg_Score -120.92846952555712\n",
      "Adding trajectory to replay buffer: step 23755, counter 379630\n",
      "Environment 10: Episode 5602, Score -121.51003577942588, Avg_Score -120.92278940810884\n",
      "Adding trajectory to replay buffer: step 23758, counter 379675\n",
      "Environment 13: Episode 5603, Score -121.95490020186392, Avg_Score -120.98672498567426\n",
      "Adding trajectory to replay buffer: step 23759, counter 379719\n",
      "Environment 9: Episode 5604, Score -122.70251468400657, Avg_Score -120.97158909958942\n",
      "Adding trajectory to replay buffer: step 23768, counter 379763\n",
      "Environment 15: Episode 5605, Score -121.86571527670945, Avg_Score -120.95324972797535\n",
      "Adding trajectory to replay buffer: step 23776, counter 379806\n",
      "Environment 5: Episode 5606, Score -123.40640169132129, Avg_Score -120.96457551295762\n",
      "Adding trajectory to replay buffer: step 23781, counter 379851\n",
      "Environment 4: Episode 5607, Score -118.83951898612455, Avg_Score -120.9168371004967\n",
      "Adding trajectory to replay buffer: step 23781, counter 379926\n",
      "Environment 14: Episode 5608, Score -119.32748589257089, Avg_Score -120.8921703180129\n",
      "Adding trajectory to replay buffer: step 23783, counter 379991\n",
      "Environment 2: Episode 5609, Score -117.23072818563972, Avg_Score -120.83407466225131\n",
      "Adding trajectory to replay buffer: step 23783, counter 380043\n",
      "Environment 3: Episode 5610, Score -115.14305153025066, Avg_Score -120.7061982580978\n",
      "Adding trajectory to replay buffer: step 23790, counter 380147\n",
      "Environment 6: Episode 5611, Score -124.34147483652148, Avg_Score -120.7712657483578\n",
      "Adding trajectory to replay buffer: step 23793, counter 380194\n",
      "Environment 7: Episode 5612, Score -114.85993615424819, Avg_Score -120.69501670581785\n",
      "Adding trajectory to replay buffer: step 23793, counter 380241\n",
      "Environment 11: Episode 5613, Score -118.93404098157089, Avg_Score -120.71186553162597\n",
      "Adding trajectory to replay buffer: step 23794, counter 380289\n",
      "Environment 12: Episode 5614, Score -116.39353688785931, Avg_Score -120.709652276199\n",
      "Adding trajectory to replay buffer: step 23795, counter 380347\n",
      "Environment 8: Episode 5615, Score -116.80959743497831, Avg_Score -120.66306740545869\n",
      "Adding trajectory to replay buffer: step 23802, counter 380390\n",
      "Environment 9: Episode 5616, Score -123.66340730800107, Avg_Score -120.67628309208114\n",
      "Adding trajectory to replay buffer: step 23811, counter 380433\n",
      "Environment 15: Episode 5617, Score -123.8123136732659, Avg_Score -120.67895092348579\n",
      "Adding trajectory to replay buffer: step 23815, counter 380490\n",
      "Environment 13: Episode 5618, Score -115.628166153421, Avg_Score -120.65974914028259\n",
      "Adding trajectory to replay buffer: step 23818, counter 380577\n",
      "Environment 0: Episode 5619, Score -115.24217045788964, Avg_Score -120.68109780677122\n",
      "Adding trajectory to replay buffer: step 23819, counter 380620\n",
      "Environment 5: Episode 5620, Score -123.00661491950353, Avg_Score -120.6815943526734\n",
      "Adding trajectory to replay buffer: step 23821, counter 380734\n",
      "Environment 1: Episode 5621, Score -118.43274014647244, Avg_Score -120.63769596810191\n",
      "Adding trajectory to replay buffer: step 23827, counter 380778\n",
      "Environment 2: Episode 5622, Score -123.34292100881723, Avg_Score -120.65786051170272\n",
      "Adding trajectory to replay buffer: step 23827, counter 380822\n",
      "Environment 3: Episode 5623, Score -124.06606390241843, Avg_Score -120.62424196207166\n",
      "Adding trajectory to replay buffer: step 23828, counter 380869\n",
      "Environment 14: Episode 5624, Score -116.65365009665365, Avg_Score -120.5585455526102\n",
      "Adding trajectory to replay buffer: step 23836, counter 380915\n",
      "Environment 6: Episode 5625, Score -121.80178404287871, Avg_Score -120.56145423332764\n",
      "Adding trajectory to replay buffer: step 23837, counter 380958\n",
      "Environment 12: Episode 5626, Score -122.65391452639426, Avg_Score -120.5814668009677\n",
      "Adding trajectory to replay buffer: step 23838, counter 381001\n",
      "Environment 8: Episode 5627, Score -122.73126441479724, Avg_Score -120.69998514767246\n",
      "Adding trajectory to replay buffer: step 23838, counter 381046\n",
      "Environment 11: Episode 5628, Score -121.99527134837024, Avg_Score -120.69558180080269\n",
      "Adding trajectory to replay buffer: step 23846, counter 381090\n",
      "Environment 9: Episode 5629, Score -119.15871984800945, Avg_Score -120.66340262542653\n",
      "Adding trajectory to replay buffer: step 23853, counter 381188\n",
      "Environment 10: Episode 5630, Score -124.5934111992872, Avg_Score -120.69392465339432\n",
      "Adding trajectory to replay buffer: step 23856, counter 381233\n",
      "Environment 15: Episode 5631, Score -123.2725170275774, Avg_Score -120.71362898943993\n",
      "Adding trajectory to replay buffer: step 23859, counter 381277\n",
      "Environment 13: Episode 5632, Score -121.34656539473683, Avg_Score -120.70239619404434\n",
      "Adding trajectory to replay buffer: step 23860, counter 381356\n",
      "Environment 4: Episode 5633, Score -115.60170481831953, Avg_Score -120.62477686993289\n",
      "Adding trajectory to replay buffer: step 23862, counter 381400\n",
      "Environment 0: Episode 5634, Score -122.23789556675528, Avg_Score -120.63405020594081\n",
      "Adding trajectory to replay buffer: step 23864, counter 381445\n",
      "Environment 5: Episode 5635, Score -122.04941915392752, Avg_Score -120.62590165122663\n",
      "Adding trajectory to replay buffer: step 23865, counter 381489\n",
      "Environment 1: Episode 5636, Score -122.06435018648952, Avg_Score -120.62290433102399\n",
      "Adding trajectory to replay buffer: step 23874, counter 381535\n",
      "Environment 14: Episode 5637, Score -120.86143370854978, Avg_Score -120.63667061008077\n",
      "Adding trajectory to replay buffer: step 23880, counter 381579\n",
      "Environment 6: Episode 5638, Score -122.38452979485753, Avg_Score -120.63080614207573\n",
      "Adding trajectory to replay buffer: step 23881, counter 381623\n",
      "Environment 12: Episode 5639, Score -123.10237602503972, Avg_Score -120.64408158017308\n",
      "Adding trajectory to replay buffer: step 23882, counter 381667\n",
      "Environment 8: Episode 5640, Score -120.61207764934625, Avg_Score -120.63588440064638\n",
      "Adding trajectory to replay buffer: step 23882, counter 381711\n",
      "Environment 11: Episode 5641, Score -121.95558917814432, Avg_Score -120.63407656040589\n",
      "Adding trajectory to replay buffer: step 23892, counter 381757\n",
      "Environment 9: Episode 5642, Score -120.4845281346102, Avg_Score -120.61135586344108\n",
      "Adding trajectory to replay buffer: step 23904, counter 381801\n",
      "Environment 4: Episode 5643, Score -119.03858774626318, Avg_Score -120.57758653385643\n",
      "Adding trajectory to replay buffer: step 23904, counter 381849\n",
      "Environment 15: Episode 5644, Score -119.89461791040253, Avg_Score -120.55752697107766\n",
      "Adding trajectory to replay buffer: step 23911, counter 381896\n",
      "Environment 5: Episode 5645, Score -118.35542161697957, Avg_Score -120.51417628979456\n",
      "Adding trajectory to replay buffer: step 23918, counter 381940\n",
      "Environment 14: Episode 5646, Score -122.0981087412797, Avg_Score -120.49363091336579\n",
      "Adding trajectory to replay buffer: step 23924, counter 381999\n",
      "Environment 1: Episode 5647, Score -115.42691647905174, Avg_Score -120.4202697377575\n",
      "Adding trajectory to replay buffer: step 23925, counter 382044\n",
      "Environment 6: Episode 5648, Score -122.72796172810035, Avg_Score -120.46063581684132\n",
      "Adding trajectory to replay buffer: step 23926, counter 382089\n",
      "Environment 12: Episode 5649, Score -122.63921212927129, Avg_Score -120.46098292182437\n",
      "Adding trajectory to replay buffer: step 23928, counter 382135\n",
      "Environment 8: Episode 5650, Score -121.78087089746383, Avg_Score -120.54429285455736\n",
      "Adding trajectory to replay buffer: step 23928, counter 382181\n",
      "Environment 11: Episode 5651, Score -119.35335375222627, Avg_Score -120.52206809113606\n",
      "Adding trajectory to replay buffer: step 23937, counter 382265\n",
      "Environment 10: Episode 5652, Score -121.28123302681124, Avg_Score -120.50215957520528\n",
      "Adding trajectory to replay buffer: step 23938, counter 382311\n",
      "Environment 9: Episode 5653, Score -122.20452599373212, Avg_Score -120.49161597990839\n",
      "Adding trajectory to replay buffer: step 23948, counter 382355\n",
      "Environment 4: Episode 5654, Score -116.5989296978588, Avg_Score -120.47507331950833\n",
      "Adding trajectory to replay buffer: step 23963, counter 382400\n",
      "Environment 14: Episode 5655, Score -120.35830744970156, Avg_Score -120.45424511937843\n",
      "Adding trajectory to replay buffer: step 23963, counter 382459\n",
      "Environment 15: Episode 5656, Score -116.45283180507522, Avg_Score -120.48249601367108\n",
      "Adding trajectory to replay buffer: step 23969, counter 382601\n",
      "Environment 2: Episode 5657, Score -129.7064458824539, Avg_Score -120.55586871615688\n",
      "Adding trajectory to replay buffer: step 23970, counter 382647\n",
      "Environment 1: Episode 5658, Score -123.51269711870329, Avg_Score -120.56678913944599\n",
      "Adding trajectory to replay buffer: step 23976, counter 382695\n",
      "Environment 11: Episode 5659, Score -120.27302383275128, Avg_Score -120.53912913853648\n",
      "Adding trajectory to replay buffer: step 23980, counter 382848\n",
      "Environment 3: Episode 5660, Score -130.79023150817926, Avg_Score -120.64220210647616\n",
      "Adding trajectory to replay buffer: step 23986, counter 382896\n",
      "Environment 9: Episode 5661, Score -122.19393199771208, Avg_Score -120.67014855572427\n",
      "Adding trajectory to replay buffer: step 23987, counter 382946\n",
      "Environment 10: Episode 5662, Score -120.0722459053391, Avg_Score -120.65595150808876\n",
      "Adding trajectory to replay buffer: step 23991, counter 382989\n",
      "Environment 4: Episode 5663, Score -122.05377967939836, Avg_Score -120.64923002530945\n",
      "Adding trajectory to replay buffer: step 23991, counter 383069\n",
      "Environment 5: Episode 5664, Score -122.30447974858946, Avg_Score -120.675326151906\n",
      "Adding trajectory to replay buffer: step 24006, counter 383112\n",
      "Environment 14: Episode 5665, Score -123.67136075754651, Avg_Score -120.68801632802048\n",
      "Adding trajectory to replay buffer: step 24007, counter 383156\n",
      "Environment 15: Episode 5666, Score -123.60689792188009, Avg_Score -120.7272959128973\n",
      "Adding trajectory to replay buffer: step 24013, counter 383200\n",
      "Environment 2: Episode 5667, Score -122.63879427818085, Avg_Score -120.7481932595599\n",
      "Adding trajectory to replay buffer: step 24014, counter 383244\n",
      "Environment 1: Episode 5668, Score -122.78826998134517, Avg_Score -120.74417635272914\n",
      "Adding trajectory to replay buffer: step 24014, counter 383399\n",
      "Environment 13: Episode 5669, Score -118.49634537420299, Avg_Score -120.71031452256778\n",
      "Adding trajectory to replay buffer: step 24017, counter 383490\n",
      "Environment 12: Episode 5670, Score -111.24838389770191, Avg_Score -120.61873301878542\n",
      "Adding trajectory to replay buffer: step 24021, counter 383535\n",
      "Environment 11: Episode 5671, Score -122.51308910066697, Avg_Score -120.63645059010291\n",
      "Adding trajectory to replay buffer: step 24024, counter 383579\n",
      "Environment 3: Episode 5672, Score -122.29841556172818, Avg_Score -120.61242470396543\n",
      "Adding trajectory to replay buffer: step 24028, counter 383621\n",
      "Environment 9: Episode 5673, Score -122.96277151986958, Avg_Score -120.62372569358644\n",
      "Adding trajectory to replay buffer: step 24031, counter 383790\n",
      "Environment 0: Episode 5674, Score -134.12451123239754, Avg_Score -120.81777243186731\n",
      "Adding trajectory to replay buffer: step 24032, counter 383835\n",
      "Environment 10: Episode 5675, Score -121.59493168237682, Avg_Score -120.83279558927532\n",
      "Adding trajectory to replay buffer: step 24033, counter 383940\n",
      "Environment 8: Episode 5676, Score -114.01117189336662, Avg_Score -120.7377582837514\n",
      "Adding trajectory to replay buffer: step 24037, counter 383986\n",
      "Environment 4: Episode 5677, Score -119.43656876694038, Avg_Score -120.70162082019756\n",
      "Adding trajectory to replay buffer: step 24038, counter 384099\n",
      "Environment 6: Episode 5678, Score -116.47422787582502, Avg_Score -120.66035822000255\n",
      "Adding trajectory to replay buffer: step 24048, counter 384141\n",
      "Environment 14: Episode 5679, Score -117.14402539613968, Avg_Score -120.65403715570054\n",
      "Adding trajectory to replay buffer: step 24052, counter 384186\n",
      "Environment 15: Episode 5680, Score -122.02180003649183, Avg_Score -120.6681093452407\n",
      "Adding trajectory to replay buffer: step 24055, counter 384448\n",
      "Environment 7: Episode 5681, Score -129.04056217865752, Avg_Score -120.73635567678105\n",
      "Adding trajectory to replay buffer: step 24058, counter 384515\n",
      "Environment 5: Episode 5682, Score -116.70274819724223, Avg_Score -120.69683181736596\n",
      "Adding trajectory to replay buffer: step 24059, counter 384561\n",
      "Environment 2: Episode 5683, Score -121.03498656564454, Avg_Score -120.67927618985827\n",
      "Adding trajectory to replay buffer: step 24060, counter 384607\n",
      "Environment 1: Episode 5684, Score -121.93871623284804, Avg_Score -120.77141225296651\n",
      "Adding trajectory to replay buffer: step 24060, counter 384653\n",
      "Environment 13: Episode 5685, Score -122.08290726579354, Avg_Score -120.76687691965904\n",
      "Adding trajectory to replay buffer: step 24066, counter 384698\n",
      "Environment 11: Episode 5686, Score -121.83529622488966, Avg_Score -120.75881399721537\n",
      "Adding trajectory to replay buffer: step 24072, counter 384746\n",
      "Environment 3: Episode 5687, Score -118.5783417028257, Avg_Score -120.7649225082627\n",
      "Adding trajectory to replay buffer: step 24078, counter 384791\n",
      "Environment 8: Episode 5688, Score -122.69155763893264, Avg_Score -120.77231154319622\n",
      "Adding trajectory to replay buffer: step 24079, counter 384853\n",
      "Environment 12: Episode 5689, Score -114.73717808272937, Avg_Score -120.80164586890524\n",
      "Adding trajectory to replay buffer: step 24080, counter 384896\n",
      "Environment 4: Episode 5690, Score -122.0070115969833, Avg_Score -120.80114421671546\n",
      "Adding trajectory to replay buffer: step 24081, counter 384939\n",
      "Environment 6: Episode 5691, Score -121.3846683289092, Avg_Score -120.80285914463\n",
      "Adding trajectory to replay buffer: step 24081, counter 384988\n",
      "Environment 10: Episode 5692, Score -116.62156069477336, Avg_Score -120.79827307596173\n",
      "Adding trajectory to replay buffer: step 24087, counter 385044\n",
      "Environment 0: Episode 5693, Score -115.76460286717614, Avg_Score -120.76882715392343\n",
      "Adding trajectory to replay buffer: step 24093, counter 385089\n",
      "Environment 14: Episode 5694, Score -121.45661021920418, Avg_Score -120.75964364302754\n",
      "Adding trajectory to replay buffer: step 24101, counter 385138\n",
      "Environment 15: Episode 5695, Score -118.76913077492515, Avg_Score -120.81689170258477\n",
      "Adding trajectory to replay buffer: step 24103, counter 385181\n",
      "Environment 1: Episode 5696, Score -122.62170106100479, Avg_Score -120.82508375601192\n",
      "Adding trajectory to replay buffer: step 24105, counter 385226\n",
      "Environment 13: Episode 5697, Score -122.82877537319314, Avg_Score -120.8233291630437\n",
      "Adding trajectory to replay buffer: step 24106, counter 385273\n",
      "Environment 2: Episode 5698, Score -121.32323543672325, Avg_Score -120.80886997036495\n",
      "Adding trajectory to replay buffer: step 24112, counter 385319\n",
      "Environment 11: Episode 5699, Score -118.31182957211385, Avg_Score -120.80032453789852\n",
      "Adding trajectory to replay buffer: step 24122, counter 385383\n",
      "Environment 5: Episode 5700, Score -116.70922128376489, Avg_Score -120.7359563556972\n",
      "Adding trajectory to replay buffer: step 24122, counter 385426\n",
      "Environment 12: Episode 5701, Score -115.93413838524434, Avg_Score -120.72598397815209\n",
      "Adding trajectory to replay buffer: step 24127, counter 385472\n",
      "Environment 6: Episode 5702, Score -123.56493767927277, Avg_Score -120.74653299715055\n",
      "Adding trajectory to replay buffer: step 24127, counter 385518\n",
      "Environment 10: Episode 5703, Score -122.80856404047212, Avg_Score -120.75506963553664\n",
      "Adding trajectory to replay buffer: step 24130, counter 385561\n",
      "Environment 0: Episode 5704, Score -123.1949819671698, Avg_Score -120.75999430836826\n",
      "Adding trajectory to replay buffer: step 24132, counter 385613\n",
      "Environment 4: Episode 5705, Score -118.1003003305321, Avg_Score -120.72234015890648\n",
      "Adding trajectory to replay buffer: step 24136, counter 385656\n",
      "Environment 14: Episode 5706, Score -120.2529858576222, Avg_Score -120.69080600056948\n",
      "Adding trajectory to replay buffer: step 24146, counter 385699\n",
      "Environment 1: Episode 5707, Score -116.39756401634278, Avg_Score -120.66638645087167\n",
      "Adding trajectory to replay buffer: step 24148, counter 385819\n",
      "Environment 9: Episode 5708, Score -127.61892339354847, Avg_Score -120.74930082588145\n",
      "Adding trajectory to replay buffer: step 24149, counter 385863\n",
      "Environment 13: Episode 5709, Score -122.26539746912135, Avg_Score -120.79964751871628\n",
      "Adding trajectory to replay buffer: step 24152, counter 385909\n",
      "Environment 2: Episode 5710, Score -119.37988686114922, Avg_Score -120.84201587202527\n",
      "Adding trajectory to replay buffer: step 24154, counter 385951\n",
      "Environment 11: Episode 5711, Score -122.71803138480335, Avg_Score -120.82578143750808\n",
      "Adding trajectory to replay buffer: step 24170, counter 386043\n",
      "Environment 8: Episode 5712, Score -121.1220295841492, Avg_Score -120.8884023718071\n",
      "Adding trajectory to replay buffer: step 24171, counter 386087\n",
      "Environment 6: Episode 5713, Score -121.7013658325076, Avg_Score -120.91607562031646\n",
      "Adding trajectory to replay buffer: step 24171, counter 386131\n",
      "Environment 10: Episode 5714, Score -121.94400645182344, Avg_Score -120.9715803159561\n",
      "Adding trajectory to replay buffer: step 24178, counter 386187\n",
      "Environment 5: Episode 5715, Score -115.4531296678198, Avg_Score -120.95801563828452\n",
      "Adding trajectory to replay buffer: step 24180, counter 386266\n",
      "Environment 15: Episode 5716, Score -114.37116448353976, Avg_Score -120.86509321003992\n",
      "Adding trajectory to replay buffer: step 24185, counter 386379\n",
      "Environment 3: Episode 5717, Score -127.97658062885081, Avg_Score -120.90673587959576\n",
      "Adding trajectory to replay buffer: step 24187, counter 386511\n",
      "Environment 7: Episode 5718, Score -116.23669654591691, Avg_Score -120.9128211835207\n",
      "Adding trajectory to replay buffer: step 24196, counter 386561\n",
      "Environment 1: Episode 5719, Score -124.83501014792175, Avg_Score -121.00874958042102\n",
      "Adding trajectory to replay buffer: step 24198, counter 386611\n",
      "Environment 9: Episode 5720, Score -118.01414341094346, Avg_Score -120.95882486533543\n",
      "Adding trajectory to replay buffer: step 24199, counter 386658\n",
      "Environment 2: Episode 5721, Score -121.78832264473849, Avg_Score -120.9923806903181\n",
      "Adding trajectory to replay buffer: step 24200, counter 386736\n",
      "Environment 12: Episode 5722, Score -111.20548836619345, Avg_Score -120.87100636389187\n",
      "Adding trajectory to replay buffer: step 24204, counter 386810\n",
      "Environment 0: Episode 5723, Score -113.42517607545899, Avg_Score -120.76459748562228\n",
      "Adding trajectory to replay buffer: step 24209, counter 386887\n",
      "Environment 4: Episode 5724, Score -119.72969196875579, Avg_Score -120.79535790434329\n",
      "Adding trajectory to replay buffer: step 24213, counter 386964\n",
      "Environment 14: Episode 5725, Score -116.86829714509844, Avg_Score -120.74602303536548\n",
      "Adding trajectory to replay buffer: step 24214, counter 387008\n",
      "Environment 8: Episode 5726, Score -122.18162824009794, Avg_Score -120.74130017250252\n",
      "Adding trajectory to replay buffer: step 24215, counter 387052\n",
      "Environment 6: Episode 5727, Score -122.09672700283863, Avg_Score -120.73495479838293\n",
      "Adding trajectory to replay buffer: step 24215, counter 387096\n",
      "Environment 10: Episode 5728, Score -122.02070295203912, Avg_Score -120.73520911441963\n",
      "Adding trajectory to replay buffer: step 24223, counter 387141\n",
      "Environment 5: Episode 5729, Score -119.50753375537818, Avg_Score -120.7386972534933\n",
      "Adding trajectory to replay buffer: step 24227, counter 387214\n",
      "Environment 11: Episode 5730, Score -114.87831212226178, Avg_Score -120.64154626272305\n",
      "Adding trajectory to replay buffer: step 24230, counter 387264\n",
      "Environment 15: Episode 5731, Score -113.1227584951433, Avg_Score -120.54004867739872\n",
      "Adding trajectory to replay buffer: step 24242, counter 387308\n",
      "Environment 9: Episode 5732, Score -121.56962616849194, Avg_Score -120.54227928513625\n",
      "Adding trajectory to replay buffer: step 24243, counter 387352\n",
      "Environment 2: Episode 5733, Score -122.5921089164714, Avg_Score -120.61218332611779\n",
      "Adding trajectory to replay buffer: step 24246, counter 387402\n",
      "Environment 1: Episode 5734, Score -117.11870936188164, Avg_Score -120.560991464069\n",
      "Adding trajectory to replay buffer: step 24250, counter 387448\n",
      "Environment 0: Episode 5735, Score -119.01673811512688, Avg_Score -120.53066465368103\n",
      "Adding trajectory to replay buffer: step 24255, counter 387494\n",
      "Environment 4: Episode 5736, Score -121.257722288476, Avg_Score -120.52259837470088\n",
      "Adding trajectory to replay buffer: step 24256, counter 387537\n",
      "Environment 14: Episode 5737, Score -122.84196440301028, Avg_Score -120.5424036816455\n",
      "Adding trajectory to replay buffer: step 24257, counter 387580\n",
      "Environment 8: Episode 5738, Score -122.07312265337073, Avg_Score -120.53928961023067\n",
      "Adding trajectory to replay buffer: step 24259, counter 387624\n",
      "Environment 6: Episode 5739, Score -117.22484375884477, Avg_Score -120.48051428756871\n",
      "Adding trajectory to replay buffer: step 24264, counter 387703\n",
      "Environment 3: Episode 5740, Score -117.70475054749288, Avg_Score -120.45144101655016\n",
      "Adding trajectory to replay buffer: step 24268, counter 387771\n",
      "Environment 12: Episode 5741, Score -117.8130667429672, Avg_Score -120.41001579219838\n",
      "Adding trajectory to replay buffer: step 24275, counter 387816\n",
      "Environment 15: Episode 5742, Score -120.7064240086085, Avg_Score -120.41223475093837\n",
      "Adding trajectory to replay buffer: step 24286, counter 387859\n",
      "Environment 2: Episode 5743, Score -122.82495782802565, Avg_Score -120.45009845175599\n",
      "Adding trajectory to replay buffer: step 24286, counter 387903\n",
      "Environment 9: Episode 5744, Score -121.35757836388115, Avg_Score -120.46472805629077\n",
      "Adding trajectory to replay buffer: step 24289, counter 387946\n",
      "Environment 1: Episode 5745, Score -121.93556835716777, Avg_Score -120.50052952369266\n",
      "Adding trajectory to replay buffer: step 24292, counter 388015\n",
      "Environment 5: Episode 5746, Score -114.74072264032937, Avg_Score -120.4269556626832\n",
      "Adding trajectory to replay buffer: step 24296, counter 388061\n",
      "Environment 0: Episode 5747, Score -121.43421812780139, Avg_Score -120.48702867917068\n",
      "Adding trajectory to replay buffer: step 24299, counter 388105\n",
      "Environment 4: Episode 5748, Score -116.06042900127296, Avg_Score -120.42035335190238\n",
      "Adding trajectory to replay buffer: step 24302, counter 388150\n",
      "Environment 8: Episode 5749, Score -120.04423692059517, Avg_Score -120.39440359981563\n",
      "Adding trajectory to replay buffer: step 24305, counter 388196\n",
      "Environment 6: Episode 5750, Score -121.4824542756627, Avg_Score -120.3914194335976\n",
      "Adding trajectory to replay buffer: step 24311, counter 388243\n",
      "Environment 3: Episode 5751, Score -117.2454665981941, Avg_Score -120.37034056205728\n",
      "Adding trajectory to replay buffer: step 24316, counter 388303\n",
      "Environment 14: Episode 5752, Score -114.09331734307297, Avg_Score -120.29846140521991\n",
      "Adding trajectory to replay buffer: step 24319, counter 388347\n",
      "Environment 15: Episode 5753, Score -123.16422470240079, Avg_Score -120.30805839230663\n",
      "Adding trajectory to replay buffer: step 24328, counter 388389\n",
      "Environment 9: Episode 5754, Score -115.93271844600699, Avg_Score -120.30139627978808\n",
      "Adding trajectory to replay buffer: step 24330, counter 388433\n",
      "Environment 2: Episode 5755, Score -122.40987558386102, Avg_Score -120.32191196112964\n",
      "Adding trajectory to replay buffer: step 24334, counter 388478\n",
      "Environment 1: Episode 5756, Score -121.9410492023391, Avg_Score -120.37679413510232\n",
      "Adding trajectory to replay buffer: step 24335, counter 388521\n",
      "Environment 5: Episode 5757, Score -122.98798049708891, Avg_Score -120.30960948124869\n",
      "Adding trajectory to replay buffer: step 24339, counter 388564\n",
      "Environment 0: Episode 5758, Score -117.7208749954657, Avg_Score -120.25169126001629\n",
      "Adding trajectory to replay buffer: step 24342, counter 388757\n",
      "Environment 13: Episode 5759, Score -125.63831641086004, Avg_Score -120.30534418579738\n",
      "Adding trajectory to replay buffer: step 24344, counter 388802\n",
      "Environment 4: Episode 5760, Score -120.91849809059501, Avg_Score -120.20662685162154\n",
      "Adding trajectory to replay buffer: step 24349, counter 388849\n",
      "Environment 8: Episode 5761, Score -121.52418487433033, Avg_Score -120.19992938038772\n",
      "Adding trajectory to replay buffer: step 24351, counter 388895\n",
      "Environment 6: Episode 5762, Score -118.39215041709878, Avg_Score -120.18312842550532\n",
      "Adding trajectory to replay buffer: step 24352, counter 388979\n",
      "Environment 12: Episode 5763, Score -118.14006894602278, Avg_Score -120.14399131817156\n",
      "Adding trajectory to replay buffer: step 24356, counter 389024\n",
      "Environment 3: Episode 5764, Score -118.91682690186241, Avg_Score -120.1101147897043\n",
      "Adding trajectory to replay buffer: step 24361, counter 389069\n",
      "Environment 14: Episode 5765, Score -116.52922279320904, Avg_Score -120.03869341006093\n",
      "Adding trajectory to replay buffer: step 24364, counter 389114\n",
      "Environment 15: Episode 5766, Score -119.63088888152069, Avg_Score -119.99893331965731\n",
      "Adding trajectory to replay buffer: step 24371, counter 389298\n",
      "Environment 7: Episode 5767, Score -120.14710980316524, Avg_Score -119.97401647490716\n",
      "Adding trajectory to replay buffer: step 24372, counter 389342\n",
      "Environment 9: Episode 5768, Score -121.59700812866788, Avg_Score -119.96210385638038\n",
      "Adding trajectory to replay buffer: step 24377, counter 389385\n",
      "Environment 1: Episode 5769, Score -122.97020823417108, Avg_Score -120.00684248498006\n",
      "Adding trajectory to replay buffer: step 24377, counter 389427\n",
      "Environment 5: Episode 5770, Score -122.85474754347342, Avg_Score -120.1229061214378\n",
      "Adding trajectory to replay buffer: step 24383, counter 389471\n",
      "Environment 0: Episode 5771, Score -114.13985303388971, Avg_Score -120.03917376077003\n",
      "Adding trajectory to replay buffer: step 24387, counter 389516\n",
      "Environment 13: Episode 5772, Score -122.98916340180797, Avg_Score -120.04608123917082\n",
      "Adding trajectory to replay buffer: step 24391, counter 389563\n",
      "Environment 4: Episode 5773, Score -120.51697827840503, Avg_Score -120.02162330675617\n",
      "Adding trajectory to replay buffer: step 24392, counter 389728\n",
      "Environment 11: Episode 5774, Score -120.42858324945657, Avg_Score -119.88466402692677\n",
      "Adding trajectory to replay buffer: step 24396, counter 389794\n",
      "Environment 2: Episode 5775, Score -116.90158091620262, Avg_Score -119.83773051926502\n",
      "Adding trajectory to replay buffer: step 24398, counter 389841\n",
      "Environment 6: Episode 5776, Score -119.73546173197839, Avg_Score -119.89497341765113\n",
      "Adding trajectory to replay buffer: step 24399, counter 389884\n",
      "Environment 3: Episode 5777, Score -120.48819858945099, Avg_Score -119.90548971587623\n",
      "Adding trajectory to replay buffer: step 24406, counter 389941\n",
      "Environment 8: Episode 5778, Score -115.34956417182119, Avg_Score -119.89424307883618\n",
      "Adding trajectory to replay buffer: step 24413, counter 389993\n",
      "Environment 14: Episode 5779, Score -117.97813359827299, Avg_Score -119.90258416085754\n",
      "Adding trajectory to replay buffer: step 24417, counter 390058\n",
      "Environment 12: Episode 5780, Score -114.81562435324925, Avg_Score -119.8305224040251\n",
      "Adding trajectory to replay buffer: step 24418, counter 390104\n",
      "Environment 9: Episode 5781, Score -122.29342909276485, Avg_Score -119.76305107316617\n",
      "Adding trajectory to replay buffer: step 24423, counter 390150\n",
      "Environment 5: Episode 5782, Score -121.45687865891742, Avg_Score -119.81059237778292\n",
      "Adding trajectory to replay buffer: step 24425, counter 390192\n",
      "Environment 0: Episode 5783, Score -122.93151103529893, Avg_Score -119.82955762247946\n",
      "Adding trajectory to replay buffer: step 24432, counter 390237\n",
      "Environment 13: Episode 5784, Score -115.96139092395516, Avg_Score -119.76978436939052\n",
      "Adding trajectory to replay buffer: step 24438, counter 390460\n",
      "Environment 10: Episode 5785, Score -136.32496063241268, Avg_Score -119.91220490305673\n",
      "Adding trajectory to replay buffer: step 24439, counter 390522\n",
      "Environment 1: Episode 5786, Score -118.11092759612637, Avg_Score -119.8749612167691\n",
      "Adding trajectory to replay buffer: step 24440, counter 390571\n",
      "Environment 4: Episode 5787, Score -123.66631090977106, Avg_Score -119.92584090883857\n",
      "Adding trajectory to replay buffer: step 24441, counter 390616\n",
      "Environment 2: Episode 5788, Score -122.79958950930708, Avg_Score -119.9269212275423\n",
      "Adding trajectory to replay buffer: step 24443, counter 390660\n",
      "Environment 3: Episode 5789, Score -121.99590720523086, Avg_Score -119.9995085187673\n",
      "Adding trajectory to replay buffer: step 24443, counter 390705\n",
      "Environment 6: Episode 5790, Score -119.22063582956915, Avg_Score -119.97164476109319\n",
      "Adding trajectory to replay buffer: step 24448, counter 390747\n",
      "Environment 8: Episode 5791, Score -122.54234583013815, Avg_Score -119.98322153610549\n",
      "Adding trajectory to replay buffer: step 24450, counter 390805\n",
      "Environment 11: Episode 5792, Score -114.9700611667807, Avg_Score -119.96670654082556\n",
      "Adding trajectory to replay buffer: step 24451, counter 390892\n",
      "Environment 15: Episode 5793, Score -122.32914561212964, Avg_Score -120.0323519682751\n",
      "Adding trajectory to replay buffer: step 24453, counter 390974\n",
      "Environment 7: Episode 5794, Score -121.82463118281713, Avg_Score -120.03603217791122\n",
      "Adding trajectory to replay buffer: step 24457, counter 391018\n",
      "Environment 14: Episode 5795, Score -122.5783893731522, Avg_Score -120.07412476389347\n",
      "Adding trajectory to replay buffer: step 24459, counter 391060\n",
      "Environment 12: Episode 5796, Score -115.82695449728953, Avg_Score -120.00617729825633\n",
      "Adding trajectory to replay buffer: step 24460, counter 391102\n",
      "Environment 9: Episode 5797, Score -121.7103768916813, Avg_Score -119.99499331344121\n",
      "Adding trajectory to replay buffer: step 24468, counter 391147\n",
      "Environment 5: Episode 5798, Score -121.5670437445653, Avg_Score -119.99743139651966\n",
      "Adding trajectory to replay buffer: step 24471, counter 391193\n",
      "Environment 0: Episode 5799, Score -118.80772643896192, Avg_Score -120.0023903651881\n",
      "Adding trajectory to replay buffer: step 24482, counter 391243\n",
      "Environment 13: Episode 5800, Score -117.51426480525919, Avg_Score -120.01044080040305\n",
      "Adding trajectory to replay buffer: step 24487, counter 391292\n",
      "Environment 10: Episode 5801, Score -121.72139090400438, Avg_Score -120.06831332559062\n",
      "Adding trajectory to replay buffer: step 24492, counter 391336\n",
      "Environment 8: Episode 5802, Score -116.38743562429337, Avg_Score -119.99653830504084\n",
      "Adding trajectory to replay buffer: step 24493, counter 391379\n",
      "Environment 11: Episode 5803, Score -115.26070983522929, Avg_Score -119.9210597629884\n",
      "Adding trajectory to replay buffer: step 24499, counter 391421\n",
      "Environment 14: Episode 5804, Score -116.59810379078922, Avg_Score -119.8550909812246\n",
      "Adding trajectory to replay buffer: step 24499, counter 391469\n",
      "Environment 15: Episode 5805, Score -119.66816257158295, Avg_Score -119.87076960363511\n",
      "Adding trajectory to replay buffer: step 24503, counter 391512\n",
      "Environment 9: Episode 5806, Score -122.288186450035, Avg_Score -119.89112160955924\n",
      "Adding trajectory to replay buffer: step 24504, counter 391557\n",
      "Environment 12: Episode 5807, Score -121.06107080544284, Avg_Score -119.93775667745024\n",
      "Adding trajectory to replay buffer: step 24506, counter 391624\n",
      "Environment 1: Episode 5808, Score -110.10821903574373, Avg_Score -119.76264963387217\n",
      "Adding trajectory to replay buffer: step 24507, counter 391688\n",
      "Environment 3: Episode 5809, Score -114.34471849179702, Avg_Score -119.68344284409893\n",
      "Adding trajectory to replay buffer: step 24513, counter 391733\n",
      "Environment 5: Episode 5810, Score -122.88693023038469, Avg_Score -119.71851327779126\n",
      "Adding trajectory to replay buffer: step 24514, counter 391804\n",
      "Environment 6: Episode 5811, Score -115.42231393248588, Avg_Score -119.64555610326812\n",
      "Adding trajectory to replay buffer: step 24516, counter 391849\n",
      "Environment 0: Episode 5812, Score -121.58891584614788, Avg_Score -119.6502249658881\n",
      "Adding trajectory to replay buffer: step 24518, counter 391926\n",
      "Environment 2: Episode 5813, Score -119.6722839103217, Avg_Score -119.62993414666627\n",
      "Adding trajectory to replay buffer: step 24526, counter 391970\n",
      "Environment 13: Episode 5814, Score -121.62965608934607, Avg_Score -119.62679064304149\n",
      "Adding trajectory to replay buffer: step 24529, counter 392059\n",
      "Environment 4: Episode 5815, Score -119.79434558710891, Avg_Score -119.67020280223439\n",
      "Adding trajectory to replay buffer: step 24534, counter 392106\n",
      "Environment 10: Episode 5816, Score -122.38553732517114, Avg_Score -119.75034653065069\n",
      "Adding trajectory to replay buffer: step 24543, counter 392150\n",
      "Environment 15: Episode 5817, Score -122.32592871330897, Avg_Score -119.69384001149527\n",
      "Adding trajectory to replay buffer: step 24544, counter 392195\n",
      "Environment 14: Episode 5818, Score -121.82587733889619, Avg_Score -119.74973181942507\n",
      "Adding trajectory to replay buffer: step 24546, counter 392238\n",
      "Environment 9: Episode 5819, Score -117.63190653211872, Avg_Score -119.67770078326704\n",
      "Adding trajectory to replay buffer: step 24549, counter 392281\n",
      "Environment 1: Episode 5820, Score -122.60003377618827, Avg_Score -119.72355968691949\n",
      "Adding trajectory to replay buffer: step 24549, counter 392337\n",
      "Environment 11: Episode 5821, Score -115.50201497773826, Avg_Score -119.66069661024947\n",
      "Adding trajectory to replay buffer: step 24554, counter 392438\n",
      "Environment 7: Episode 5822, Score -116.06143427170502, Avg_Score -119.70925606930459\n",
      "Adding trajectory to replay buffer: step 24557, counter 392481\n",
      "Environment 6: Episode 5823, Score -123.11106236807133, Avg_Score -119.8061149322307\n",
      "Adding trajectory to replay buffer: step 24558, counter 392526\n",
      "Environment 5: Episode 5824, Score -120.69473600074586, Avg_Score -119.8157653725506\n",
      "Adding trajectory to replay buffer: step 24560, counter 392570\n",
      "Environment 0: Episode 5825, Score -122.17800099134197, Avg_Score -119.86886241101303\n",
      "Adding trajectory to replay buffer: step 24563, counter 392615\n",
      "Environment 2: Episode 5826, Score -120.57263644836408, Avg_Score -119.85277249309569\n",
      "Adding trajectory to replay buffer: step 24568, counter 392679\n",
      "Environment 12: Episode 5827, Score -117.17632396458649, Avg_Score -119.80356846271316\n",
      "Adding trajectory to replay buffer: step 24573, counter 392723\n",
      "Environment 4: Episode 5828, Score -120.78502498936777, Avg_Score -119.79121168308646\n",
      "Adding trajectory to replay buffer: step 24580, counter 392769\n",
      "Environment 10: Episode 5829, Score -122.19756455110014, Avg_Score -119.81811199104366\n",
      "Adding trajectory to replay buffer: step 24583, counter 392826\n",
      "Environment 13: Episode 5830, Score -114.20217868117926, Avg_Score -119.81135065663284\n",
      "Adding trajectory to replay buffer: step 24589, counter 392872\n",
      "Environment 15: Episode 5831, Score -122.14687833935334, Avg_Score -119.90159185507497\n",
      "Adding trajectory to replay buffer: step 24590, counter 392918\n",
      "Environment 14: Episode 5832, Score -121.32110293446358, Avg_Score -119.89910662273469\n",
      "Adding trajectory to replay buffer: step 24594, counter 392966\n",
      "Environment 9: Episode 5833, Score -122.83827705826363, Avg_Score -119.9015683041526\n",
      "Adding trajectory to replay buffer: step 24600, counter 393009\n",
      "Environment 6: Episode 5834, Score -123.81678101654663, Avg_Score -119.96854902069924\n",
      "Adding trajectory to replay buffer: step 24601, counter 393052\n",
      "Environment 5: Episode 5835, Score -123.16605721158659, Avg_Score -120.01004221166384\n",
      "Adding trajectory to replay buffer: step 24601, counter 393099\n",
      "Environment 7: Episode 5836, Score -122.79420227070992, Avg_Score -120.02540701148621\n",
      "Adding trajectory to replay buffer: step 24603, counter 393142\n",
      "Environment 0: Episode 5837, Score -123.08257619145266, Avg_Score -120.02781312937063\n",
      "Adding trajectory to replay buffer: step 24607, counter 393200\n",
      "Environment 1: Episode 5838, Score -115.23307545430089, Avg_Score -119.95941265737994\n",
      "Adding trajectory to replay buffer: step 24609, counter 393246\n",
      "Environment 2: Episode 5839, Score -122.16064805455505, Avg_Score -120.00877070033702\n",
      "Adding trajectory to replay buffer: step 24609, counter 393306\n",
      "Environment 11: Episode 5840, Score -116.47693142460722, Avg_Score -119.9964925091082\n",
      "Adding trajectory to replay buffer: step 24613, counter 393351\n",
      "Environment 12: Episode 5841, Score -121.21873936914714, Avg_Score -120.03054923536997\n",
      "Adding trajectory to replay buffer: step 24624, counter 393395\n",
      "Environment 10: Episode 5842, Score -122.16739491405772, Avg_Score -120.04515894442449\n",
      "Adding trajectory to replay buffer: step 24626, counter 393438\n",
      "Environment 13: Episode 5843, Score -122.65337166520581, Avg_Score -120.04344308279629\n",
      "Adding trajectory to replay buffer: step 24636, counter 393484\n",
      "Environment 14: Episode 5844, Score -119.4408113946244, Avg_Score -120.0242754131037\n",
      "Adding trajectory to replay buffer: step 24640, counter 393530\n",
      "Environment 9: Episode 5845, Score -121.81944347536067, Avg_Score -120.0231141642856\n",
      "Adding trajectory to replay buffer: step 24643, counter 393573\n",
      "Environment 6: Episode 5846, Score -122.07810381332163, Avg_Score -120.09648797601554\n",
      "Adding trajectory to replay buffer: step 24644, counter 393628\n",
      "Environment 15: Episode 5847, Score -119.66792709673072, Avg_Score -120.07882506570485\n",
      "Adding trajectory to replay buffer: step 24645, counter 393672\n",
      "Environment 5: Episode 5848, Score -121.7096700880515, Avg_Score -120.13531747657262\n",
      "Adding trajectory to replay buffer: step 24645, counter 393716\n",
      "Environment 7: Episode 5849, Score -121.8389016240829, Avg_Score -120.1532641236075\n",
      "Adding trajectory to replay buffer: step 24646, counter 393759\n",
      "Environment 0: Episode 5850, Score -122.80835035129986, Avg_Score -120.16652308436389\n",
      "Adding trajectory to replay buffer: step 24651, counter 393803\n",
      "Environment 1: Episode 5851, Score -116.26348147380104, Avg_Score -120.15670323311993\n",
      "Adding trajectory to replay buffer: step 24653, counter 393847\n",
      "Environment 2: Episode 5852, Score -121.54968406262, Avg_Score -120.23126690031542\n",
      "Adding trajectory to replay buffer: step 24657, counter 393895\n",
      "Environment 11: Episode 5853, Score -115.57692460979335, Avg_Score -120.15539389938932\n",
      "Adding trajectory to replay buffer: step 24662, counter 393944\n",
      "Environment 12: Episode 5854, Score -119.59298292034586, Avg_Score -120.19199654413272\n",
      "Adding trajectory to replay buffer: step 24669, counter 394121\n",
      "Environment 8: Episode 5855, Score -121.6352046283471, Avg_Score -120.18424983457759\n",
      "Adding trajectory to replay buffer: step 24682, counter 394167\n",
      "Environment 14: Episode 5856, Score -121.8616735739857, Avg_Score -120.18345607829406\n",
      "Adding trajectory to replay buffer: step 24687, counter 394211\n",
      "Environment 6: Episode 5857, Score -116.73285112080475, Avg_Score -120.12090478453123\n",
      "Adding trajectory to replay buffer: step 24694, counter 394279\n",
      "Environment 13: Episode 5858, Score -117.00950299938769, Avg_Score -120.11379106457045\n",
      "Adding trajectory to replay buffer: step 24695, counter 394329\n",
      "Environment 5: Episode 5859, Score -118.41362415078905, Avg_Score -120.04154414196975\n",
      "Adding trajectory to replay buffer: step 24699, counter 394377\n",
      "Environment 1: Episode 5860, Score -122.1777134692357, Avg_Score -120.0541362957562\n",
      "Adding trajectory to replay buffer: step 24699, counter 394423\n",
      "Environment 2: Episode 5861, Score -122.61658525069865, Avg_Score -120.06506029951987\n",
      "Adding trajectory to replay buffer: step 24699, counter 394615\n",
      "Environment 3: Episode 5862, Score -120.22339984129803, Avg_Score -120.08337279376182\n",
      "Adding trajectory to replay buffer: step 24700, counter 394670\n",
      "Environment 7: Episode 5863, Score -117.0453014272352, Avg_Score -120.07242511857397\n",
      "Adding trajectory to replay buffer: step 24704, counter 394717\n",
      "Environment 11: Episode 5864, Score -122.55892226775921, Avg_Score -120.10884607223294\n",
      "Adding trajectory to replay buffer: step 24707, counter 394762\n",
      "Environment 12: Episode 5865, Score -121.15564935188554, Avg_Score -120.15511033781972\n",
      "Adding trajectory to replay buffer: step 24710, counter 394826\n",
      "Environment 0: Episode 5866, Score -116.28904817578072, Avg_Score -120.1216919307623\n",
      "Adding trajectory to replay buffer: step 24727, counter 394871\n",
      "Environment 14: Episode 5867, Score -122.40894003067663, Avg_Score -120.14431023303742\n",
      "Adding trajectory to replay buffer: step 24738, counter 394965\n",
      "Environment 15: Episode 5868, Score -121.38180238286593, Avg_Score -120.14215817557938\n",
      "Adding trajectory to replay buffer: step 24739, counter 395009\n",
      "Environment 5: Episode 5869, Score -123.32618020886741, Avg_Score -120.14571789532634\n",
      "Adding trajectory to replay buffer: step 24740, counter 395176\n",
      "Environment 4: Episode 5870, Score -120.09618099170055, Avg_Score -120.11813222980864\n",
      "Adding trajectory to replay buffer: step 24742, counter 395219\n",
      "Environment 1: Episode 5871, Score -122.6238222850468, Avg_Score -120.20297192232019\n",
      "Adding trajectory to replay buffer: step 24742, counter 395262\n",
      "Environment 2: Episode 5872, Score -122.75586557898237, Avg_Score -120.20063894409192\n",
      "Adding trajectory to replay buffer: step 24742, counter 395305\n",
      "Environment 3: Episode 5873, Score -122.73017318637359, Avg_Score -120.2227708931716\n",
      "Adding trajectory to replay buffer: step 24744, counter 395349\n",
      "Environment 7: Episode 5874, Score -122.41458506377663, Avg_Score -120.24263091131479\n",
      "Adding trajectory to replay buffer: step 24748, counter 395393\n",
      "Environment 11: Episode 5875, Score -121.6847892345786, Avg_Score -120.29046299449855\n",
      "Adding trajectory to replay buffer: step 24751, counter 395450\n",
      "Environment 13: Episode 5876, Score -115.21258265002072, Avg_Score -120.24523420367898\n",
      "Adding trajectory to replay buffer: step 24754, counter 395494\n",
      "Environment 0: Episode 5877, Score -122.05320777499551, Avg_Score -120.26088429553444\n",
      "Adding trajectory to replay buffer: step 24754, counter 395541\n",
      "Environment 12: Episode 5878, Score -120.47983696981271, Avg_Score -120.31218702351434\n",
      "Adding trajectory to replay buffer: step 24767, counter 395668\n",
      "Environment 9: Episode 5879, Score -129.08329153460141, Avg_Score -120.42323860287763\n",
      "Adding trajectory to replay buffer: step 24781, counter 395780\n",
      "Environment 8: Episode 5880, Score -120.6299643317908, Avg_Score -120.48138200266303\n",
      "Adding trajectory to replay buffer: step 24782, counter 395824\n",
      "Environment 15: Episode 5881, Score -121.90459449073238, Avg_Score -120.47749365664272\n",
      "Adding trajectory to replay buffer: step 24784, counter 395869\n",
      "Environment 5: Episode 5882, Score -120.25286958861103, Avg_Score -120.46545356593964\n",
      "Adding trajectory to replay buffer: step 24785, counter 395914\n",
      "Environment 4: Episode 5883, Score -117.93343909494952, Avg_Score -120.41547284653616\n",
      "Adding trajectory to replay buffer: step 24786, counter 395958\n",
      "Environment 1: Episode 5884, Score -123.03417266400345, Avg_Score -120.48620066393663\n",
      "Adding trajectory to replay buffer: step 24787, counter 396003\n",
      "Environment 2: Episode 5885, Score -122.2291619119253, Avg_Score -120.3452426767318\n",
      "Adding trajectory to replay buffer: step 24788, counter 396049\n",
      "Environment 3: Episode 5886, Score -121.89284782159577, Avg_Score -120.38306187898648\n",
      "Adding trajectory to replay buffer: step 24789, counter 396151\n",
      "Environment 6: Episode 5887, Score -113.79452584174896, Avg_Score -120.2843440283063\n",
      "Adding trajectory to replay buffer: step 24794, counter 396197\n",
      "Environment 11: Episode 5888, Score -120.64014966582755, Avg_Score -120.2627496298715\n",
      "Adding trajectory to replay buffer: step 24799, counter 396242\n",
      "Environment 0: Episode 5889, Score -122.9097797522551, Avg_Score -120.27188835534174\n",
      "Adding trajectory to replay buffer: step 24800, counter 396288\n",
      "Environment 12: Episode 5890, Score -123.03305868018667, Avg_Score -120.31001258384791\n",
      "Adding trajectory to replay buffer: step 24809, counter 396353\n",
      "Environment 7: Episode 5891, Score -114.16441978324143, Avg_Score -120.22623332337895\n",
      "Adding trajectory to replay buffer: step 24813, counter 396399\n",
      "Environment 9: Episode 5892, Score -123.11070000518083, Avg_Score -120.30763971176293\n",
      "Adding trajectory to replay buffer: step 24820, counter 396468\n",
      "Environment 13: Episode 5893, Score -117.26065509184295, Avg_Score -120.25695480656006\n",
      "Adding trajectory to replay buffer: step 24827, counter 396514\n",
      "Environment 8: Episode 5894, Score -119.46106035951773, Avg_Score -120.23331909832709\n",
      "Adding trajectory to replay buffer: step 24828, counter 396557\n",
      "Environment 4: Episode 5895, Score -122.95966031673551, Avg_Score -120.2371318077629\n",
      "Adding trajectory to replay buffer: step 24829, counter 396602\n",
      "Environment 5: Episode 5896, Score -123.1488020498287, Avg_Score -120.3103502832883\n",
      "Adding trajectory to replay buffer: step 24830, counter 396646\n",
      "Environment 1: Episode 5897, Score -121.93134224762271, Avg_Score -120.3125599368477\n",
      "Adding trajectory to replay buffer: step 24831, counter 396688\n",
      "Environment 6: Episode 5898, Score -122.89235863049763, Avg_Score -120.32581308570701\n",
      "Adding trajectory to replay buffer: step 24833, counter 396734\n",
      "Environment 2: Episode 5899, Score -120.0013050957496, Avg_Score -120.3377488722749\n",
      "Adding trajectory to replay buffer: step 24836, counter 396843\n",
      "Environment 14: Episode 5900, Score -114.61625124924848, Avg_Score -120.30876873671477\n",
      "Adding trajectory to replay buffer: step 24837, counter 396886\n",
      "Environment 11: Episode 5901, Score -122.97769436764966, Avg_Score -120.32133177135124\n",
      "Adding trajectory to replay buffer: step 24842, counter 396929\n",
      "Environment 0: Episode 5902, Score -122.81233548182124, Avg_Score -120.38558076992653\n",
      "Adding trajectory to replay buffer: step 24851, counter 396992\n",
      "Environment 3: Episode 5903, Score -116.64509202759776, Avg_Score -120.3994245918502\n",
      "Adding trajectory to replay buffer: step 24852, counter 397062\n",
      "Environment 15: Episode 5904, Score -118.7433679645403, Avg_Score -120.42087723358773\n",
      "Adding trajectory to replay buffer: step 24854, counter 397107\n",
      "Environment 7: Episode 5905, Score -122.7301881591361, Avg_Score -120.45149748946325\n",
      "Adding trajectory to replay buffer: step 24856, counter 397339\n",
      "Environment 10: Episode 5906, Score -123.40818436129453, Avg_Score -120.46269746857587\n",
      "Adding trajectory to replay buffer: step 24869, counter 397381\n",
      "Environment 8: Episode 5907, Score -122.2921827750237, Avg_Score -120.47500858827166\n",
      "Adding trajectory to replay buffer: step 24872, counter 397423\n",
      "Environment 1: Episode 5908, Score -122.66015310065325, Avg_Score -120.60052792892074\n",
      "Adding trajectory to replay buffer: step 24875, counter 397467\n",
      "Environment 6: Episode 5909, Score -122.25173131976402, Avg_Score -120.67959805720042\n",
      "Adding trajectory to replay buffer: step 24877, counter 397544\n",
      "Environment 12: Episode 5910, Score -115.79853714377806, Avg_Score -120.60871412633436\n",
      "Adding trajectory to replay buffer: step 24880, counter 397587\n",
      "Environment 11: Episode 5911, Score -123.25418892596103, Avg_Score -120.68703287626911\n",
      "Adding trajectory to replay buffer: step 24889, counter 397634\n",
      "Environment 0: Episode 5912, Score -122.88924176258904, Avg_Score -120.70003613543354\n",
      "Adding trajectory to replay buffer: step 24895, counter 397701\n",
      "Environment 4: Episode 5913, Score -116.33541676323426, Avg_Score -120.66666746396265\n",
      "Adding trajectory to replay buffer: step 24896, counter 397745\n",
      "Environment 15: Episode 5914, Score -122.32608066627584, Avg_Score -120.67363170973196\n",
      "Adding trajectory to replay buffer: step 24897, counter 397791\n",
      "Environment 3: Episode 5915, Score -119.26858278157003, Avg_Score -120.66837408167657\n",
      "Adding trajectory to replay buffer: step 24899, counter 397836\n",
      "Environment 7: Episode 5916, Score -120.04907570085551, Avg_Score -120.64500946543343\n",
      "Adding trajectory to replay buffer: step 24900, counter 397880\n",
      "Environment 10: Episode 5917, Score -119.63841636529379, Avg_Score -120.61813434195328\n",
      "Adding trajectory to replay buffer: step 24901, counter 397948\n",
      "Environment 2: Episode 5918, Score -117.71068578423994, Avg_Score -120.5769824264067\n",
      "Adding trajectory to replay buffer: step 24921, counter 398040\n",
      "Environment 5: Episode 5919, Score -122.55245341470652, Avg_Score -120.62618789523258\n",
      "Adding trajectory to replay buffer: step 24923, counter 398150\n",
      "Environment 9: Episode 5920, Score -114.65695849314892, Avg_Score -120.54675714240219\n",
      "Adding trajectory to replay buffer: step 24924, counter 398202\n",
      "Environment 1: Episode 5921, Score -116.79152739713092, Avg_Score -120.55965226659612\n",
      "Adding trajectory to replay buffer: step 24925, counter 398252\n",
      "Environment 6: Episode 5922, Score -122.45248281439368, Avg_Score -120.62356275202299\n",
      "Adding trajectory to replay buffer: step 24929, counter 398345\n",
      "Environment 14: Episode 5923, Score -115.76760454413078, Avg_Score -120.55012817378358\n",
      "Adding trajectory to replay buffer: step 24933, counter 398458\n",
      "Environment 13: Episode 5924, Score -126.46268420195781, Avg_Score -120.60780765579568\n",
      "Adding trajectory to replay buffer: step 24937, counter 398506\n",
      "Environment 0: Episode 5925, Score -116.13774484741316, Avg_Score -120.5474050943564\n",
      "Adding trajectory to replay buffer: step 24942, counter 398568\n",
      "Environment 11: Episode 5926, Score -113.16362638480278, Avg_Score -120.47331499372079\n",
      "Adding trajectory to replay buffer: step 24943, counter 398615\n",
      "Environment 15: Episode 5927, Score -122.11942470816523, Avg_Score -120.52274600115658\n",
      "Adding trajectory to replay buffer: step 24944, counter 398664\n",
      "Environment 4: Episode 5928, Score -120.75246074749715, Avg_Score -120.52242035873788\n",
      "Adding trajectory to replay buffer: step 24947, counter 398712\n",
      "Environment 7: Episode 5929, Score -121.2290432164104, Avg_Score -120.51273514539096\n",
      "Adding trajectory to replay buffer: step 24947, counter 398759\n",
      "Environment 10: Episode 5930, Score -119.2135690053453, Avg_Score -120.56284904863263\n",
      "Adding trajectory to replay buffer: step 24947, counter 398829\n",
      "Environment 12: Episode 5931, Score -118.50797528587468, Avg_Score -120.52646001809784\n",
      "Adding trajectory to replay buffer: step 24965, counter 398925\n",
      "Environment 8: Episode 5932, Score -124.2743399420424, Avg_Score -120.55599238817364\n",
      "Adding trajectory to replay buffer: step 24967, counter 398968\n",
      "Environment 1: Episode 5933, Score -122.85698002959292, Avg_Score -120.55617941788692\n",
      "Adding trajectory to replay buffer: step 24968, counter 399011\n",
      "Environment 6: Episode 5934, Score -122.49604994366692, Avg_Score -120.54297210715814\n",
      "Adding trajectory to replay buffer: step 24972, counter 399054\n",
      "Environment 14: Episode 5935, Score -120.46041334252806, Avg_Score -120.51591566846753\n",
      "Adding trajectory to replay buffer: step 24978, counter 399111\n",
      "Environment 5: Episode 5936, Score -115.27819331448588, Avg_Score -120.44075557890528\n",
      "Adding trajectory to replay buffer: step 24989, counter 399158\n",
      "Environment 11: Episode 5937, Score -123.4404041995909, Avg_Score -120.44433385898667\n",
      "Adding trajectory to replay buffer: step 24990, counter 399205\n",
      "Environment 15: Episode 5938, Score -121.97347747606847, Avg_Score -120.51173787920436\n",
      "Adding trajectory to replay buffer: step 24993, counter 399251\n",
      "Environment 7: Episode 5939, Score -117.44950355594233, Avg_Score -120.46462643421823\n",
      "Adding trajectory to replay buffer: step 24993, counter 399297\n",
      "Environment 12: Episode 5940, Score -119.97836764464901, Avg_Score -120.49964079641865\n",
      "Adding trajectory to replay buffer: step 24995, counter 399348\n",
      "Environment 4: Episode 5941, Score -116.815480541775, Avg_Score -120.45560820814494\n",
      "Adding trajectory to replay buffer: step 25013, counter 399414\n",
      "Environment 10: Episode 5942, Score -117.2667686954133, Avg_Score -120.4066019459585\n",
      "Adding trajectory to replay buffer: step 25015, counter 399462\n",
      "Environment 1: Episode 5943, Score -123.15524060880642, Avg_Score -120.41162063539448\n",
      "Adding trajectory to replay buffer: step 25015, counter 399512\n",
      "Environment 8: Episode 5944, Score -114.87202803148888, Avg_Score -120.36593280176314\n",
      "Adding trajectory to replay buffer: step 25016, counter 399595\n",
      "Environment 13: Episode 5945, Score -111.70435951555373, Avg_Score -120.26478196216503\n",
      "Adding trajectory to replay buffer: step 25018, counter 399641\n",
      "Environment 14: Episode 5946, Score -120.01731625012744, Avg_Score -120.24417408653309\n",
      "Adding trajectory to replay buffer: step 25020, counter 399760\n",
      "Environment 2: Episode 5947, Score -125.84967931768918, Avg_Score -120.3059916087427\n",
      "Adding trajectory to replay buffer: step 25021, counter 399803\n",
      "Environment 5: Episode 5948, Score -122.67600578455627, Avg_Score -120.31565496570772\n",
      "Adding trajectory to replay buffer: step 25033, counter 399846\n",
      "Environment 15: Episode 5949, Score -117.14698994314112, Avg_Score -120.26873584889834\n",
      "Adding trajectory to replay buffer: step 25034, counter 399943\n",
      "Environment 0: Episode 5950, Score -123.6865454759841, Avg_Score -120.27751780014518\n",
      "Adding trajectory to replay buffer: step 25035, counter 399989\n",
      "Environment 11: Episode 5951, Score -120.01411234093706, Avg_Score -120.31502410881654\n",
      "Adding trajectory to replay buffer: step 25043, counter 400037\n",
      "Environment 4: Episode 5952, Score -121.10733685029857, Avg_Score -120.31060063669331\n",
      "Adding trajectory to replay buffer: step 25059, counter 400081\n",
      "Environment 1: Episode 5953, Score -122.23349532912238, Avg_Score -120.37716634388661\n",
      "Adding trajectory to replay buffer: step 25059, counter 400125\n",
      "Environment 8: Episode 5954, Score -122.50549661247355, Avg_Score -120.40629148080791\n",
      "Adding trajectory to replay buffer: step 25059, counter 400168\n",
      "Environment 13: Episode 5955, Score -123.13234190306292, Avg_Score -120.42126285355508\n",
      "Adding trajectory to replay buffer: step 25063, counter 400213\n",
      "Environment 14: Episode 5956, Score -120.8057213308358, Avg_Score -120.41070333112356\n",
      "Adding trajectory to replay buffer: step 25065, counter 400258\n",
      "Environment 2: Episode 5957, Score -122.43973466949352, Avg_Score -120.46777216661047\n",
      "Adding trajectory to replay buffer: step 25065, counter 400302\n",
      "Environment 5: Episode 5958, Score -122.32029968045279, Avg_Score -120.5208801334211\n",
      "Adding trajectory to replay buffer: step 25077, counter 400346\n",
      "Environment 15: Episode 5959, Score -119.47710071649402, Avg_Score -120.53151489907816\n",
      "Adding trajectory to replay buffer: step 25078, counter 400390\n",
      "Environment 0: Episode 5960, Score -121.40992101391342, Avg_Score -120.52383697452495\n",
      "Adding trajectory to replay buffer: step 25080, counter 400435\n",
      "Environment 11: Episode 5961, Score -114.65404718104378, Avg_Score -120.4442115938284\n",
      "Adding trajectory to replay buffer: step 25081, counter 400503\n",
      "Environment 10: Episode 5962, Score -118.06989779683637, Avg_Score -120.42267657338378\n",
      "Adding trajectory to replay buffer: step 25088, counter 400548\n",
      "Environment 4: Episode 5963, Score -111.0676596501184, Avg_Score -120.36290015561262\n",
      "Adding trajectory to replay buffer: step 25106, counter 400757\n",
      "Environment 3: Episode 5964, Score -120.01946899652145, Avg_Score -120.33750562290022\n",
      "Adding trajectory to replay buffer: step 25110, counter 400802\n",
      "Environment 2: Episode 5965, Score -123.42677020524256, Avg_Score -120.36021683143377\n",
      "Adding trajectory to replay buffer: step 25110, counter 400919\n",
      "Environment 7: Episode 5966, Score -127.66194140571915, Avg_Score -120.47394576373314\n",
      "Adding trajectory to replay buffer: step 25110, counter 400970\n",
      "Environment 13: Episode 5967, Score -122.4527973120194, Avg_Score -120.47438433654658\n",
      "Adding trajectory to replay buffer: step 25110, counter 401017\n",
      "Environment 14: Episode 5968, Score -121.04480997259854, Avg_Score -120.4710144124439\n",
      "Adding trajectory to replay buffer: step 25111, counter 401063\n",
      "Environment 5: Episode 5969, Score -117.3322586299597, Avg_Score -120.41107519665485\n",
      "Adding trajectory to replay buffer: step 25116, counter 401211\n",
      "Environment 6: Episode 5970, Score -128.69265322195366, Avg_Score -120.49703991895738\n",
      "Adding trajectory to replay buffer: step 25121, counter 401254\n",
      "Environment 0: Episode 5971, Score -122.78662681163351, Avg_Score -120.49866796422324\n",
      "Adding trajectory to replay buffer: step 25121, counter 401298\n",
      "Environment 15: Episode 5972, Score -122.56363428929448, Avg_Score -120.49674565132635\n",
      "Adding trajectory to replay buffer: step 25122, counter 401340\n",
      "Environment 11: Episode 5973, Score -123.10081324261117, Avg_Score -120.50045205188871\n",
      "Adding trajectory to replay buffer: step 25123, counter 401404\n",
      "Environment 1: Episode 5974, Score -116.51958182573195, Avg_Score -120.44150201950828\n",
      "Adding trajectory to replay buffer: step 25123, counter 401446\n",
      "Environment 10: Episode 5975, Score -122.85049467644345, Avg_Score -120.45315907392693\n",
      "Adding trajectory to replay buffer: step 25124, counter 401511\n",
      "Environment 8: Episode 5976, Score -118.7217882146885, Avg_Score -120.48825112957361\n",
      "Adding trajectory to replay buffer: step 25130, counter 401553\n",
      "Environment 4: Episode 5977, Score -122.81251553128536, Avg_Score -120.49584420713653\n",
      "Adding trajectory to replay buffer: step 25139, counter 401769\n",
      "Environment 9: Episode 5978, Score -138.4699117733656, Avg_Score -120.67574495517205\n",
      "Adding trajectory to replay buffer: step 25153, counter 401812\n",
      "Environment 2: Episode 5979, Score -122.42937811050626, Avg_Score -120.6092058209311\n",
      "Adding trajectory to replay buffer: step 25154, counter 401856\n",
      "Environment 7: Episode 5980, Score -121.51765949126892, Avg_Score -120.61808277252585\n",
      "Adding trajectory to replay buffer: step 25155, counter 401901\n",
      "Environment 14: Episode 5981, Score -122.67638865427433, Avg_Score -120.62580071416127\n",
      "Adding trajectory to replay buffer: step 25156, counter 401947\n",
      "Environment 13: Episode 5982, Score -122.04231050785506, Avg_Score -120.6436951233537\n",
      "Adding trajectory to replay buffer: step 25157, counter 401993\n",
      "Environment 5: Episode 5983, Score -119.99395976819501, Avg_Score -120.66430033008615\n",
      "Adding trajectory to replay buffer: step 25160, counter 402037\n",
      "Environment 6: Episode 5984, Score -121.73437954823548, Avg_Score -120.65130239892848\n",
      "Adding trajectory to replay buffer: step 25164, counter 402080\n",
      "Environment 15: Episode 5985, Score -121.83475322016949, Avg_Score -120.64735831201092\n",
      "Adding trajectory to replay buffer: step 25165, counter 402124\n",
      "Environment 0: Episode 5986, Score -122.36027851672843, Avg_Score -120.65203261896224\n",
      "Adding trajectory to replay buffer: step 25166, counter 402167\n",
      "Environment 1: Episode 5987, Score -122.44301306680589, Avg_Score -120.7385174912128\n",
      "Adding trajectory to replay buffer: step 25166, counter 402211\n",
      "Environment 11: Episode 5988, Score -122.91913287348797, Avg_Score -120.76130732328942\n",
      "Adding trajectory to replay buffer: step 25167, counter 402255\n",
      "Environment 10: Episode 5989, Score -122.81705379721834, Avg_Score -120.76038006373905\n",
      "Adding trajectory to replay buffer: step 25167, counter 402429\n",
      "Environment 12: Episode 5990, Score -121.91177822192697, Avg_Score -120.74916725915645\n",
      "Adding trajectory to replay buffer: step 25168, counter 402473\n",
      "Environment 8: Episode 5991, Score -122.65176803381927, Avg_Score -120.8340407416622\n",
      "Adding trajectory to replay buffer: step 25177, counter 402520\n",
      "Environment 4: Episode 5992, Score -120.8686120138783, Avg_Score -120.81161986174921\n",
      "Adding trajectory to replay buffer: step 25186, counter 402600\n",
      "Environment 3: Episode 5993, Score -115.18750949613191, Avg_Score -120.79088840579209\n",
      "Adding trajectory to replay buffer: step 25192, counter 402653\n",
      "Environment 9: Episode 5994, Score -117.80353739392261, Avg_Score -120.77431317613615\n",
      "Adding trajectory to replay buffer: step 25195, counter 402695\n",
      "Environment 2: Episode 5995, Score -123.05803180351046, Avg_Score -120.77529689100389\n",
      "Adding trajectory to replay buffer: step 25197, counter 402738\n",
      "Environment 7: Episode 5996, Score -122.54155498100259, Avg_Score -120.76922442031564\n",
      "Adding trajectory to replay buffer: step 25198, counter 402781\n",
      "Environment 14: Episode 5997, Score -121.8278565574872, Avg_Score -120.7681895634143\n",
      "Adding trajectory to replay buffer: step 25200, counter 402825\n",
      "Environment 13: Episode 5998, Score -119.91248134947878, Avg_Score -120.7383907906041\n",
      "Adding trajectory to replay buffer: step 25202, counter 402870\n",
      "Environment 5: Episode 5999, Score -122.32785949592913, Avg_Score -120.76165633460589\n",
      "Adding trajectory to replay buffer: step 25202, counter 402912\n",
      "Environment 6: Episode 6000, Score -117.27144025115606, Avg_Score -120.78820822462494\n",
      "Adding trajectory to replay buffer: step 25208, counter 402956\n",
      "Environment 15: Episode 6001, Score -122.93106209481756, Avg_Score -120.78774190189664\n",
      "Adding trajectory to replay buffer: step 25210, counter 403001\n",
      "Environment 0: Episode 6002, Score -123.35652228296591, Avg_Score -120.79318376990808\n",
      "Adding trajectory to replay buffer: step 25210, counter 403045\n",
      "Environment 1: Episode 6003, Score -122.8424123924828, Avg_Score -120.85515697355693\n",
      "Adding trajectory to replay buffer: step 25213, counter 403091\n",
      "Environment 10: Episode 6004, Score -121.75323944287561, Avg_Score -120.88525568834031\n",
      "Adding trajectory to replay buffer: step 25213, counter 403138\n",
      "Environment 11: Episode 6005, Score -120.85183723130699, Avg_Score -120.86647217906201\n",
      "Adding trajectory to replay buffer: step 25214, counter 403185\n",
      "Environment 12: Episode 6006, Score -120.4724260226631, Avg_Score -120.83711459567569\n",
      "Adding trajectory to replay buffer: step 25222, counter 403230\n",
      "Environment 4: Episode 6007, Score -121.92251285926004, Avg_Score -120.83341789651806\n",
      "Adding trajectory to replay buffer: step 25230, counter 403274\n",
      "Environment 3: Episode 6008, Score -120.67416132144939, Avg_Score -120.81355797872601\n",
      "Adding trajectory to replay buffer: step 25239, counter 403321\n",
      "Environment 9: Episode 6009, Score -119.46077273359025, Avg_Score -120.78564839286427\n",
      "Adding trajectory to replay buffer: step 25240, counter 403364\n",
      "Environment 7: Episode 6010, Score -122.61924729047344, Avg_Score -120.85385549433121\n",
      "Adding trajectory to replay buffer: step 25242, counter 403408\n",
      "Environment 14: Episode 6011, Score -122.8648253634603, Avg_Score -120.84996185870621\n",
      "Adding trajectory to replay buffer: step 25245, counter 403453\n",
      "Environment 13: Episode 6012, Score -122.02489603543219, Avg_Score -120.84131840143465\n",
      "Adding trajectory to replay buffer: step 25247, counter 403498\n",
      "Environment 5: Episode 6013, Score -120.32221056660575, Avg_Score -120.88118633946839\n",
      "Adding trajectory to replay buffer: step 25247, counter 403543\n",
      "Environment 6: Episode 6014, Score -121.3715382217647, Avg_Score -120.87164091502325\n",
      "Adding trajectory to replay buffer: step 25252, counter 403587\n",
      "Environment 15: Episode 6015, Score -123.37743158471523, Avg_Score -120.91272940305471\n",
      "Adding trajectory to replay buffer: step 25257, counter 403634\n",
      "Environment 0: Episode 6016, Score -123.14529909476018, Avg_Score -120.94369163699375\n",
      "Adding trajectory to replay buffer: step 25257, counter 403723\n",
      "Environment 8: Episode 6017, Score -120.54063033726811, Avg_Score -120.9527137767135\n",
      "Adding trajectory to replay buffer: step 25265, counter 403766\n",
      "Environment 4: Episode 6018, Score -122.28974316820502, Avg_Score -120.99850435055316\n",
      "Adding trajectory to replay buffer: step 25275, counter 403811\n",
      "Environment 3: Episode 6019, Score -119.98946360070693, Avg_Score -120.97287445241317\n",
      "Adding trajectory to replay buffer: step 25280, counter 403877\n",
      "Environment 12: Episode 6020, Score -118.30210888081106, Avg_Score -121.00932595628977\n",
      "Adding trajectory to replay buffer: step 25284, counter 403921\n",
      "Environment 7: Episode 6021, Score -123.23498264045517, Avg_Score -121.07376050872301\n",
      "Adding trajectory to replay buffer: step 25288, counter 403964\n",
      "Environment 13: Episode 6022, Score -123.09054738403734, Avg_Score -121.08014115441945\n",
      "Adding trajectory to replay buffer: step 25291, counter 404008\n",
      "Environment 5: Episode 6023, Score -122.57304179191652, Avg_Score -121.14819552689733\n",
      "Adding trajectory to replay buffer: step 25291, counter 404052\n",
      "Environment 6: Episode 6024, Score -122.60027716827665, Avg_Score -121.1095714565605\n",
      "Adding trajectory to replay buffer: step 25291, counter 404130\n",
      "Environment 10: Episode 6025, Score -114.52418932892988, Avg_Score -121.09343590137566\n",
      "Adding trajectory to replay buffer: step 25295, counter 404173\n",
      "Environment 15: Episode 6026, Score -123.06042967586095, Avg_Score -121.19240393428625\n",
      "Adding trajectory to replay buffer: step 25311, counter 404219\n",
      "Environment 4: Episode 6027, Score -121.05223501221386, Avg_Score -121.18173203732673\n",
      "Adding trajectory to replay buffer: step 25319, counter 404263\n",
      "Environment 3: Episode 6028, Score -121.43167004516968, Avg_Score -121.18852413030346\n",
      "Adding trajectory to replay buffer: step 25325, counter 404308\n",
      "Environment 12: Episode 6029, Score -121.52796969940762, Avg_Score -121.19151339513343\n",
      "Adding trajectory to replay buffer: step 25331, counter 404355\n",
      "Environment 7: Episode 6030, Score -121.24617672983993, Avg_Score -121.21183947237837\n",
      "Adding trajectory to replay buffer: step 25332, counter 404430\n",
      "Environment 0: Episode 6031, Score -112.50351215839758, Avg_Score -121.1517948411036\n",
      "Adding trajectory to replay buffer: step 25334, counter 404473\n",
      "Environment 5: Episode 6032, Score -122.55141287975262, Avg_Score -121.13456557048072\n",
      "Adding trajectory to replay buffer: step 25334, counter 404516\n",
      "Environment 6: Episode 6033, Score -122.3487956281677, Avg_Score -121.12948372646646\n",
      "Adding trajectory to replay buffer: step 25334, counter 404559\n",
      "Environment 10: Episode 6034, Score -123.33328483708203, Avg_Score -121.13785607540063\n",
      "Adding trajectory to replay buffer: step 25338, counter 404602\n",
      "Environment 15: Episode 6035, Score -122.58129565136134, Avg_Score -121.15906489848894\n",
      "Adding trajectory to replay buffer: step 25344, counter 404689\n",
      "Environment 8: Episode 6036, Score -115.25339649313936, Avg_Score -121.15881693027546\n",
      "Adding trajectory to replay buffer: step 25348, counter 404795\n",
      "Environment 14: Episode 6037, Score -114.88340280128084, Avg_Score -121.07324691629238\n",
      "Adding trajectory to replay buffer: step 25360, counter 404844\n",
      "Environment 4: Episode 6038, Score -115.25543288443548, Avg_Score -121.00606647037607\n",
      "Adding trajectory to replay buffer: step 25363, counter 404919\n",
      "Environment 13: Episode 6039, Score -120.58452694288269, Avg_Score -121.03741670424544\n",
      "Adding trajectory to replay buffer: step 25372, counter 404966\n",
      "Environment 12: Episode 6040, Score -124.0587379751665, Avg_Score -121.07822040755063\n",
      "Adding trajectory to replay buffer: step 25373, counter 405008\n",
      "Environment 7: Episode 6041, Score -123.69541174039742, Avg_Score -121.14701971953686\n",
      "Adding trajectory to replay buffer: step 25375, counter 405051\n",
      "Environment 0: Episode 6042, Score -123.49573416527113, Avg_Score -121.20930937423543\n",
      "Adding trajectory to replay buffer: step 25376, counter 405093\n",
      "Environment 5: Episode 6043, Score -122.80665399884742, Avg_Score -121.20582350813584\n",
      "Adding trajectory to replay buffer: step 25376, counter 405135\n",
      "Environment 10: Episode 6044, Score -123.79263173607985, Avg_Score -121.29502954518173\n",
      "Adding trajectory to replay buffer: step 25377, counter 405178\n",
      "Environment 6: Episode 6045, Score -123.50657605949417, Avg_Score -121.41305171062113\n",
      "Adding trajectory to replay buffer: step 25381, counter 405221\n",
      "Environment 15: Episode 6046, Score -123.17495339387717, Avg_Score -121.44462808205861\n",
      "Adding trajectory to replay buffer: step 25382, counter 405390\n",
      "Environment 11: Episode 6047, Score -123.09787881936069, Avg_Score -121.41711007707532\n",
      "Adding trajectory to replay buffer: step 25387, counter 405433\n",
      "Environment 8: Episode 6048, Score -123.25773213199713, Avg_Score -121.42292734054973\n",
      "Adding trajectory to replay buffer: step 25393, counter 405478\n",
      "Environment 14: Episode 6049, Score -122.08079213069256, Avg_Score -121.47226536242526\n",
      "Adding trajectory to replay buffer: step 25403, counter 405671\n",
      "Environment 1: Episode 6050, Score -123.19368987964216, Avg_Score -121.46733680646186\n",
      "Adding trajectory to replay buffer: step 25403, counter 405714\n",
      "Environment 4: Episode 6051, Score -122.44427818211106, Avg_Score -121.49163846487359\n",
      "Adding trajectory to replay buffer: step 25405, counter 405756\n",
      "Environment 13: Episode 6052, Score -123.38570627297771, Avg_Score -121.51442215910039\n",
      "Adding trajectory to replay buffer: step 25416, counter 405933\n",
      "Environment 9: Episode 6053, Score -120.33018757188319, Avg_Score -121.49538908152799\n",
      "Adding trajectory to replay buffer: step 25420, counter 405978\n",
      "Environment 0: Episode 6054, Score -123.51287822356014, Avg_Score -121.50546289763885\n",
      "Adding trajectory to replay buffer: step 25420, counter 406022\n",
      "Environment 5: Episode 6055, Score -122.34325471000932, Avg_Score -121.49757202570832\n",
      "Adding trajectory to replay buffer: step 25422, counter 406067\n",
      "Environment 6: Episode 6056, Score -121.47576323429806, Avg_Score -121.50427244474297\n",
      "Adding trajectory to replay buffer: step 25422, counter 406116\n",
      "Environment 7: Episode 6057, Score -117.37365348237691, Avg_Score -121.45361163287178\n",
      "Adding trajectory to replay buffer: step 25425, counter 406346\n",
      "Environment 2: Episode 6058, Score -125.3390117893051, Avg_Score -121.48379875396031\n",
      "Adding trajectory to replay buffer: step 25426, counter 406391\n",
      "Environment 15: Episode 6059, Score -122.34002240720454, Avg_Score -121.51242797086744\n",
      "Adding trajectory to replay buffer: step 25427, counter 406436\n",
      "Environment 11: Episode 6060, Score -120.45497900720002, Avg_Score -121.50287855080029\n",
      "Adding trajectory to replay buffer: step 25428, counter 406477\n",
      "Environment 8: Episode 6061, Score -116.88307099463356, Avg_Score -121.52516878893618\n",
      "Adding trajectory to replay buffer: step 25438, counter 406522\n",
      "Environment 14: Episode 6062, Score -120.51417064787479, Avg_Score -121.54961151744659\n",
      "Adding trajectory to replay buffer: step 25447, counter 406566\n",
      "Environment 1: Episode 6063, Score -122.32459905091363, Avg_Score -121.66218091145454\n",
      "Adding trajectory to replay buffer: step 25447, counter 406610\n",
      "Environment 4: Episode 6064, Score -121.87892779430808, Avg_Score -121.6807754994324\n",
      "Adding trajectory to replay buffer: step 25447, counter 406652\n",
      "Environment 13: Episode 6065, Score -114.9267430587889, Avg_Score -121.59577522796788\n",
      "Adding trajectory to replay buffer: step 25463, counter 406699\n",
      "Environment 9: Episode 6066, Score -119.71977255320611, Avg_Score -121.51635353944273\n",
      "Adding trajectory to replay buffer: step 25465, counter 406744\n",
      "Environment 0: Episode 6067, Score -117.69276027141078, Avg_Score -121.46875316903665\n",
      "Adding trajectory to replay buffer: step 25469, counter 406837\n",
      "Environment 10: Episode 6068, Score -112.69186129324139, Avg_Score -121.38522368224308\n",
      "Adding trajectory to replay buffer: step 25471, counter 406880\n",
      "Environment 8: Episode 6069, Score -122.77969292334282, Avg_Score -121.43969802517691\n",
      "Adding trajectory to replay buffer: step 25471, counter 406924\n",
      "Environment 11: Episode 6070, Score -123.76807521774185, Avg_Score -121.3904522451348\n",
      "Adding trajectory to replay buffer: step 25473, counter 406975\n",
      "Environment 7: Episode 6071, Score -121.74046414551528, Avg_Score -121.3799906184736\n",
      "Adding trajectory to replay buffer: step 25475, counter 407025\n",
      "Environment 2: Episode 6072, Score -117.54948425322368, Avg_Score -121.3298491181129\n",
      "Adding trajectory to replay buffer: step 25480, counter 407083\n",
      "Environment 6: Episode 6073, Score -115.2822258757328, Avg_Score -121.25166324444416\n",
      "Adding trajectory to replay buffer: step 25480, counter 407125\n",
      "Environment 14: Episode 6074, Score -122.89373132446346, Avg_Score -121.31540473943147\n",
      "Adding trajectory to replay buffer: step 25485, counter 407184\n",
      "Environment 15: Episode 6075, Score -115.95614016628514, Avg_Score -121.2464611943299\n",
      "Adding trajectory to replay buffer: step 25497, counter 407234\n",
      "Environment 4: Episode 6076, Score -116.84617171665157, Avg_Score -121.22770502934952\n",
      "Adding trajectory to replay buffer: step 25506, counter 407277\n",
      "Environment 9: Episode 6077, Score -122.004165012845, Avg_Score -121.21962152416512\n",
      "Adding trajectory to replay buffer: step 25509, counter 407366\n",
      "Environment 5: Episode 6078, Score -112.27514834098767, Avg_Score -120.95767388984137\n",
      "Adding trajectory to replay buffer: step 25513, counter 407408\n",
      "Environment 11: Episode 6079, Score -115.75141229675897, Avg_Score -120.8908942317039\n",
      "Adding trajectory to replay buffer: step 25517, counter 407452\n",
      "Environment 7: Episode 6080, Score -116.07074531393623, Avg_Score -120.83642508993056\n",
      "Adding trajectory to replay buffer: step 25523, counter 407500\n",
      "Environment 2: Episode 6081, Score -121.5323254351796, Avg_Score -120.82498445773963\n",
      "Adding trajectory to replay buffer: step 25525, counter 407556\n",
      "Environment 10: Episode 6082, Score -114.68996799577774, Avg_Score -120.75146103261882\n",
      "Adding trajectory to replay buffer: step 25528, counter 407765\n",
      "Environment 3: Episode 6083, Score -122.59808927636033, Avg_Score -120.77750232770047\n",
      "Adding trajectory to replay buffer: step 25529, counter 407809\n",
      "Environment 15: Episode 6084, Score -123.25861929033822, Avg_Score -120.79274472512151\n",
      "Adding trajectory to replay buffer: step 25534, counter 407872\n",
      "Environment 8: Episode 6085, Score -113.40419377136728, Avg_Score -120.70843913063347\n",
      "Adding trajectory to replay buffer: step 25538, counter 407963\n",
      "Environment 13: Episode 6086, Score -113.94220627572699, Avg_Score -120.62425840822347\n",
      "Adding trajectory to replay buffer: step 25540, counter 408023\n",
      "Environment 14: Episode 6087, Score -116.50242539767471, Avg_Score -120.56485253153214\n",
      "Adding trajectory to replay buffer: step 25542, counter 408068\n",
      "Environment 4: Episode 6088, Score -121.583186372693, Avg_Score -120.55149306652419\n",
      "Adding trajectory to replay buffer: step 25552, counter 408173\n",
      "Environment 1: Episode 6089, Score -117.37501392464154, Avg_Score -120.49707266779842\n",
      "Adding trajectory to replay buffer: step 25553, counter 408217\n",
      "Environment 5: Episode 6090, Score -122.63902524604214, Avg_Score -120.50434513803957\n",
      "Adding trajectory to replay buffer: step 25554, counter 408306\n",
      "Environment 0: Episode 6091, Score -117.58683244073144, Avg_Score -120.45369578210868\n",
      "Adding trajectory to replay buffer: step 25558, counter 408351\n",
      "Environment 11: Episode 6092, Score -120.26934743472748, Avg_Score -120.44770313631717\n",
      "Adding trajectory to replay buffer: step 25558, counter 408537\n",
      "Environment 12: Episode 6093, Score -118.37781351004975, Avg_Score -120.47960617645633\n",
      "Adding trajectory to replay buffer: step 25562, counter 408582\n",
      "Environment 7: Episode 6094, Score -120.27312431959746, Avg_Score -120.50430204571306\n",
      "Adding trajectory to replay buffer: step 25566, counter 408625\n",
      "Environment 2: Episode 6095, Score -114.80535245925685, Avg_Score -120.42177525227052\n",
      "Adding trajectory to replay buffer: step 25571, counter 408668\n",
      "Environment 3: Episode 6096, Score -123.83179960120222, Avg_Score -120.43467769847253\n",
      "Adding trajectory to replay buffer: step 25571, counter 408759\n",
      "Environment 6: Episode 6097, Score -122.45245951119989, Avg_Score -120.44092372800964\n",
      "Adding trajectory to replay buffer: step 25572, counter 408802\n",
      "Environment 15: Episode 6098, Score -123.79856357327662, Avg_Score -120.47978455024763\n",
      "Adding trajectory to replay buffer: step 25577, counter 408845\n",
      "Environment 8: Episode 6099, Score -123.33778342069996, Avg_Score -120.48988378949532\n",
      "Adding trajectory to replay buffer: step 25577, counter 408884\n",
      "Environment 13: Episode 6100, Score -116.7592549641927, Avg_Score -120.48476193662569\n",
      "Adding trajectory to replay buffer: step 25583, counter 408927\n",
      "Environment 14: Episode 6101, Score -123.63333071658451, Avg_Score -120.49178462284335\n",
      "Adding trajectory to replay buffer: step 25586, counter 408971\n",
      "Environment 4: Episode 6102, Score -122.88731098966394, Avg_Score -120.48709250991033\n",
      "Adding trajectory to replay buffer: step 25596, counter 409015\n",
      "Environment 1: Episode 6103, Score -121.78896260442089, Avg_Score -120.47655801202971\n",
      "Adding trajectory to replay buffer: step 25598, counter 409107\n",
      "Environment 9: Episode 6104, Score -115.1827096233337, Avg_Score -120.41085271383429\n",
      "Adding trajectory to replay buffer: step 25600, counter 409153\n",
      "Environment 0: Episode 6105, Score -121.63470544494689, Avg_Score -120.41868139597067\n",
      "Adding trajectory to replay buffer: step 25601, counter 409196\n",
      "Environment 11: Episode 6106, Score -122.56745885412086, Avg_Score -120.43963172428525\n",
      "Adding trajectory to replay buffer: step 25602, counter 409240\n",
      "Environment 12: Episode 6107, Score -121.89573456904232, Avg_Score -120.43936394138305\n",
      "Adding trajectory to replay buffer: step 25604, counter 409282\n",
      "Environment 7: Episode 6108, Score -123.3939234469384, Avg_Score -120.46656156263792\n",
      "Adding trajectory to replay buffer: step 25607, counter 409323\n",
      "Environment 2: Episode 6109, Score -116.99087313820671, Avg_Score -120.44186256668411\n",
      "Adding trajectory to replay buffer: step 25613, counter 409411\n",
      "Environment 10: Episode 6110, Score -120.46298197119063, Avg_Score -120.42029991349128\n",
      "Adding trajectory to replay buffer: step 25618, counter 409458\n",
      "Environment 3: Episode 6111, Score -120.98091260123564, Avg_Score -120.40146078586902\n",
      "Adding trajectory to replay buffer: step 25618, counter 409499\n",
      "Environment 13: Episode 6112, Score -114.9437844295067, Avg_Score -120.33064966980976\n",
      "Adding trajectory to replay buffer: step 25625, counter 409541\n",
      "Environment 14: Episode 6113, Score -116.02864062742951, Avg_Score -120.28771397041801\n",
      "Adding trajectory to replay buffer: step 25631, counter 409586\n",
      "Environment 4: Episode 6114, Score -120.51321520005354, Avg_Score -120.27913074020091\n",
      "Adding trajectory to replay buffer: step 25633, counter 409648\n",
      "Environment 6: Episode 6115, Score -110.51992429806106, Avg_Score -120.15055566733437\n",
      "Adding trajectory to replay buffer: step 25642, counter 409690\n",
      "Environment 0: Episode 6116, Score -122.5134502093587, Avg_Score -120.14423717848037\n",
      "Adding trajectory to replay buffer: step 25642, counter 409736\n",
      "Environment 1: Episode 6117, Score -121.49534884208875, Avg_Score -120.15378436352857\n",
      "Adding trajectory to replay buffer: step 25644, counter 409779\n",
      "Environment 11: Episode 6118, Score -122.529609471074, Avg_Score -120.15618302655726\n",
      "Adding trajectory to replay buffer: step 25645, counter 409871\n",
      "Environment 5: Episode 6119, Score -117.30548515620612, Avg_Score -120.12934324211224\n",
      "Adding trajectory to replay buffer: step 25645, counter 409914\n",
      "Environment 12: Episode 6120, Score -123.5555044394297, Avg_Score -120.18187719769843\n",
      "Adding trajectory to replay buffer: step 25648, counter 409958\n",
      "Environment 7: Episode 6121, Score -121.37245905297188, Avg_Score -120.1632519618236\n",
      "Adding trajectory to replay buffer: step 25652, counter 410003\n",
      "Environment 2: Episode 6122, Score -120.97387305092501, Avg_Score -120.1420852184925\n",
      "Adding trajectory to replay buffer: step 25659, counter 410090\n",
      "Environment 15: Episode 6123, Score -122.46664589791683, Avg_Score -120.1410212595525\n",
      "Adding trajectory to replay buffer: step 25660, counter 410137\n",
      "Environment 10: Episode 6124, Score -121.96320527701266, Avg_Score -120.13465054063987\n",
      "Adding trajectory to replay buffer: step 25663, counter 410182\n",
      "Environment 3: Episode 6125, Score -118.19953119380709, Avg_Score -120.17140395928863\n",
      "Adding trajectory to replay buffer: step 25664, counter 410228\n",
      "Environment 13: Episode 6126, Score -120.36773537921285, Avg_Score -120.14447701632216\n",
      "Adding trajectory to replay buffer: step 25670, counter 410273\n",
      "Environment 14: Episode 6127, Score -123.15942947450839, Avg_Score -120.16554896094513\n",
      "Adding trajectory to replay buffer: step 25676, counter 410318\n",
      "Environment 4: Episode 6128, Score -122.6200944482647, Avg_Score -120.17743320497605\n",
      "Adding trajectory to replay buffer: step 25687, counter 410363\n",
      "Environment 0: Episode 6129, Score -122.25606420366498, Avg_Score -120.1847141500186\n",
      "Adding trajectory to replay buffer: step 25689, counter 410410\n",
      "Environment 1: Episode 6130, Score -120.87756582864934, Avg_Score -120.1810280410067\n",
      "Adding trajectory to replay buffer: step 25689, counter 410454\n",
      "Environment 5: Episode 6131, Score -122.45794014913776, Avg_Score -120.2805723209141\n",
      "Adding trajectory to replay buffer: step 25691, counter 410501\n",
      "Environment 11: Episode 6132, Score -120.16518857353368, Avg_Score -120.2567100778519\n",
      "Adding trajectory to replay buffer: step 25692, counter 410545\n",
      "Environment 7: Episode 6133, Score -121.97894928614362, Avg_Score -120.25301161443167\n",
      "Adding trajectory to replay buffer: step 25699, counter 410667\n",
      "Environment 8: Episode 6134, Score -112.55488470877086, Avg_Score -120.14522761314856\n",
      "Adding trajectory to replay buffer: step 25703, counter 410711\n",
      "Environment 15: Episode 6135, Score -117.59579661563039, Avg_Score -120.09537262279126\n",
      "Adding trajectory to replay buffer: step 25704, counter 410755\n",
      "Environment 10: Episode 6136, Score -118.50256812277622, Avg_Score -120.12786433908762\n",
      "Adding trajectory to replay buffer: step 25709, counter 410801\n",
      "Environment 3: Episode 6137, Score -122.01803154067571, Avg_Score -120.19921062648159\n",
      "Adding trajectory to replay buffer: step 25710, counter 410847\n",
      "Environment 13: Episode 6138, Score -118.80567141355574, Avg_Score -120.2347130117728\n",
      "Adding trajectory to replay buffer: step 25717, counter 410919\n",
      "Environment 12: Episode 6139, Score -112.99081020054594, Avg_Score -120.15877584434946\n",
      "Adding trajectory to replay buffer: step 25726, counter 411047\n",
      "Environment 9: Episode 6140, Score -116.98701141293657, Avg_Score -120.08805857872716\n",
      "Adding trajectory to replay buffer: step 25733, counter 411091\n",
      "Environment 5: Episode 6141, Score -122.59814204048614, Avg_Score -120.07708588172804\n",
      "Adding trajectory to replay buffer: step 25734, counter 411136\n",
      "Environment 1: Episode 6142, Score -122.65496788152494, Avg_Score -120.06867821889058\n",
      "Adding trajectory to replay buffer: step 25736, counter 411196\n",
      "Environment 4: Episode 6143, Score -110.97152483763794, Avg_Score -119.9503269272785\n",
      "Adding trajectory to replay buffer: step 25736, counter 411240\n",
      "Environment 7: Episode 6144, Score -121.60021992066751, Avg_Score -119.92840280912435\n",
      "Adding trajectory to replay buffer: step 25736, counter 411306\n",
      "Environment 14: Episode 6145, Score -116.59184680070045, Avg_Score -119.85925551653644\n",
      "Adding trajectory to replay buffer: step 25739, counter 411354\n",
      "Environment 11: Episode 6146, Score -121.6006479078556, Avg_Score -119.8435124616762\n",
      "Adding trajectory to replay buffer: step 25746, counter 411413\n",
      "Environment 0: Episode 6147, Score -115.55319562044181, Avg_Score -119.768065629687\n",
      "Adding trajectory to replay buffer: step 25746, counter 411460\n",
      "Environment 8: Episode 6148, Score -120.30713124966312, Avg_Score -119.73855962086367\n",
      "Adding trajectory to replay buffer: step 25749, counter 411506\n",
      "Environment 15: Episode 6149, Score -118.01995930252163, Avg_Score -119.69795129258195\n",
      "Adding trajectory to replay buffer: step 25758, counter 411555\n",
      "Environment 3: Episode 6150, Score -118.5804620705247, Avg_Score -119.65181901449075\n",
      "Adding trajectory to replay buffer: step 25759, counter 411610\n",
      "Environment 10: Episode 6151, Score -111.95748814695763, Avg_Score -119.54695111413925\n",
      "Adding trajectory to replay buffer: step 25771, counter 411655\n",
      "Environment 9: Episode 6152, Score -118.03757612478361, Avg_Score -119.4934698126573\n",
      "Adding trajectory to replay buffer: step 25771, counter 411709\n",
      "Environment 12: Episode 6153, Score -118.86968576981562, Avg_Score -119.4788647946366\n",
      "Adding trajectory to replay buffer: step 25776, counter 411775\n",
      "Environment 13: Episode 6154, Score -119.04187127725282, Avg_Score -119.43415472517356\n",
      "Adding trajectory to replay buffer: step 25778, counter 411820\n",
      "Environment 5: Episode 6155, Score -122.49725709662339, Avg_Score -119.43569474903973\n",
      "Adding trajectory to replay buffer: step 25779, counter 411865\n",
      "Environment 1: Episode 6156, Score -122.59813732211144, Avg_Score -119.44691848991785\n",
      "Adding trajectory to replay buffer: step 25787, counter 411906\n",
      "Environment 8: Episode 6157, Score -116.30303753856569, Avg_Score -119.43621233047976\n",
      "Adding trajectory to replay buffer: step 25792, counter 411952\n",
      "Environment 0: Episode 6158, Score -118.2567683935929, Avg_Score -119.36538989652263\n",
      "Adding trajectory to replay buffer: step 25798, counter 412001\n",
      "Environment 15: Episode 6159, Score -121.04974463702737, Avg_Score -119.35248711882085\n",
      "Adding trajectory to replay buffer: step 25799, counter 412064\n",
      "Environment 7: Episode 6160, Score -113.61713735575539, Avg_Score -119.2841087023064\n",
      "Adding trajectory to replay buffer: step 25803, counter 412108\n",
      "Environment 10: Episode 6161, Score -122.87144394177757, Avg_Score -119.34399243177786\n",
      "Adding trajectory to replay buffer: step 25804, counter 412260\n",
      "Environment 2: Episode 6162, Score -118.89217268735045, Avg_Score -119.32777245217262\n",
      "Adding trajectory to replay buffer: step 25804, counter 412328\n",
      "Environment 4: Episode 6163, Score -118.32683431619219, Avg_Score -119.28779480482542\n",
      "Adding trajectory to replay buffer: step 25815, counter 412372\n",
      "Environment 9: Episode 6164, Score -123.11994636193974, Avg_Score -119.30020499050174\n",
      "Adding trajectory to replay buffer: step 25815, counter 412416\n",
      "Environment 12: Episode 6165, Score -121.88334732887522, Avg_Score -119.3697710332026\n",
      "Adding trajectory to replay buffer: step 25820, counter 412460\n",
      "Environment 13: Episode 6166, Score -116.37819123094032, Avg_Score -119.33635521997994\n",
      "Adding trajectory to replay buffer: step 25823, counter 412547\n",
      "Environment 14: Episode 6167, Score -123.9283465647151, Avg_Score -119.39871108291298\n",
      "Adding trajectory to replay buffer: step 25824, counter 412593\n",
      "Environment 5: Episode 6168, Score -123.1381585182653, Avg_Score -119.50317405516321\n",
      "Adding trajectory to replay buffer: step 25825, counter 412639\n",
      "Environment 1: Episode 6169, Score -123.16961021909614, Avg_Score -119.50707322812073\n",
      "Adding trajectory to replay buffer: step 25830, counter 412836\n",
      "Environment 6: Episode 6170, Score -120.32864896107185, Avg_Score -119.47267896555401\n",
      "Adding trajectory to replay buffer: step 25830, counter 412879\n",
      "Environment 8: Episode 6171, Score -122.88392396745832, Avg_Score -119.48411356377345\n",
      "Adding trajectory to replay buffer: step 25834, counter 412921\n",
      "Environment 0: Episode 6172, Score -123.2452370701693, Avg_Score -119.54107109194288\n",
      "Adding trajectory to replay buffer: step 25839, counter 413002\n",
      "Environment 3: Episode 6173, Score -113.94153818021778, Avg_Score -119.52766421498772\n",
      "Adding trajectory to replay buffer: step 25842, counter 413045\n",
      "Environment 7: Episode 6174, Score -122.28978938750923, Avg_Score -119.52162479561821\n",
      "Adding trajectory to replay buffer: step 25843, counter 413090\n",
      "Environment 15: Episode 6175, Score -121.53640759482732, Avg_Score -119.57742746990361\n",
      "Adding trajectory to replay buffer: step 25846, counter 413132\n",
      "Environment 2: Episode 6176, Score -123.02004051054517, Avg_Score -119.63916615784255\n",
      "Adding trajectory to replay buffer: step 25847, counter 413240\n",
      "Environment 11: Episode 6177, Score -126.13825425405551, Avg_Score -119.68050705025463\n",
      "Adding trajectory to replay buffer: step 25848, counter 413285\n",
      "Environment 10: Episode 6178, Score -122.30620190864802, Avg_Score -119.78081758593127\n",
      "Adding trajectory to replay buffer: step 25849, counter 413330\n",
      "Environment 4: Episode 6179, Score -122.15935508466822, Avg_Score -119.84489701381037\n",
      "Adding trajectory to replay buffer: step 25860, counter 413375\n",
      "Environment 9: Episode 6180, Score -121.20215169565255, Avg_Score -119.89621107762753\n",
      "Adding trajectory to replay buffer: step 25860, counter 413420\n",
      "Environment 12: Episode 6181, Score -121.07348972052522, Avg_Score -119.891622720481\n",
      "Adding trajectory to replay buffer: step 25863, counter 413463\n",
      "Environment 13: Episode 6182, Score -122.45251506214836, Avg_Score -119.96924819114469\n",
      "Adding trajectory to replay buffer: step 25867, counter 413507\n",
      "Environment 14: Episode 6183, Score -122.11692705045766, Avg_Score -119.96443656888569\n",
      "Adding trajectory to replay buffer: step 25868, counter 413550\n",
      "Environment 1: Episode 6184, Score -120.67606106293886, Avg_Score -119.93861098661168\n",
      "Adding trajectory to replay buffer: step 25871, counter 413597\n",
      "Environment 5: Episode 6185, Score -121.38248554209434, Avg_Score -120.01839390431894\n",
      "Adding trajectory to replay buffer: step 25872, counter 413639\n",
      "Environment 6: Episode 6186, Score -115.3574178787383, Avg_Score -120.03254602034907\n",
      "Adding trajectory to replay buffer: step 25874, counter 413683\n",
      "Environment 8: Episode 6187, Score -121.92418334441942, Avg_Score -120.08676359981649\n",
      "Adding trajectory to replay buffer: step 25883, counter 413727\n",
      "Environment 3: Episode 6188, Score -123.3300088423472, Avg_Score -120.10423182451305\n",
      "Adding trajectory to replay buffer: step 25890, counter 413770\n",
      "Environment 11: Episode 6189, Score -122.79977214533028, Avg_Score -120.15847940671995\n",
      "Adding trajectory to replay buffer: step 25892, counter 413814\n",
      "Environment 10: Episode 6190, Score -122.24280757006662, Avg_Score -120.15451722996016\n",
      "Adding trajectory to replay buffer: step 25893, counter 413858\n",
      "Environment 4: Episode 6191, Score -122.38627581378508, Avg_Score -120.20251166369071\n",
      "Adding trajectory to replay buffer: step 25910, counter 413926\n",
      "Environment 7: Episode 6192, Score -115.30375114122903, Avg_Score -120.15285570075572\n",
      "Adding trajectory to replay buffer: step 25910, counter 413969\n",
      "Environment 14: Episode 6193, Score -122.32991875179609, Avg_Score -120.19237675317318\n",
      "Adding trajectory to replay buffer: step 25912, counter 414013\n",
      "Environment 1: Episode 6194, Score -122.88247551574558, Avg_Score -120.21847026513467\n",
      "Adding trajectory to replay buffer: step 25914, counter 414081\n",
      "Environment 2: Episode 6195, Score -116.18733800056143, Avg_Score -120.23229012054772\n",
      "Adding trajectory to replay buffer: step 25915, counter 414125\n",
      "Environment 5: Episode 6196, Score -121.75750116098051, Avg_Score -120.21154713614551\n",
      "Adding trajectory to replay buffer: step 25917, counter 414170\n",
      "Environment 6: Episode 6197, Score -121.54484226264123, Avg_Score -120.2024709636599\n",
      "Adding trajectory to replay buffer: step 25919, counter 414229\n",
      "Environment 9: Episode 6198, Score -115.12742090563104, Avg_Score -120.11575953698346\n",
      "Adding trajectory to replay buffer: step 25927, counter 414273\n",
      "Environment 3: Episode 6199, Score -122.73399492113231, Avg_Score -120.1097216519878\n",
      "Adding trajectory to replay buffer: step 25934, counter 414347\n",
      "Environment 12: Episode 6200, Score -119.94877581744517, Avg_Score -120.14161686052033\n",
      "Adding trajectory to replay buffer: step 25936, counter 414393\n",
      "Environment 11: Episode 6201, Score -115.38020779626723, Avg_Score -120.05908563131716\n",
      "Adding trajectory to replay buffer: step 25938, counter 414439\n",
      "Environment 10: Episode 6202, Score -121.12595804966054, Avg_Score -120.04147210191712\n",
      "Adding trajectory to replay buffer: step 25954, counter 414483\n",
      "Environment 14: Episode 6203, Score -121.96114232547954, Avg_Score -120.04319389912771\n",
      "Adding trajectory to replay buffer: step 25955, counter 414575\n",
      "Environment 13: Episode 6204, Score -120.96371724624125, Avg_Score -120.1010039753568\n",
      "Adding trajectory to replay buffer: step 25956, counter 414619\n",
      "Environment 1: Episode 6205, Score -122.6053020291136, Avg_Score -120.11070994119845\n",
      "Adding trajectory to replay buffer: step 25958, counter 414662\n",
      "Environment 5: Episode 6206, Score -121.37470563314608, Avg_Score -120.09878240898871\n",
      "Adding trajectory to replay buffer: step 25959, counter 414707\n",
      "Environment 2: Episode 6207, Score -122.47761910657997, Avg_Score -120.1046012543641\n",
      "Adding trajectory to replay buffer: step 25963, counter 414753\n",
      "Environment 6: Episode 6208, Score -122.60269788240393, Avg_Score -120.09668899871876\n",
      "Adding trajectory to replay buffer: step 25966, counter 414809\n",
      "Environment 7: Episode 6209, Score -113.0290285222996, Avg_Score -120.0570705525597\n",
      "Adding trajectory to replay buffer: step 25967, counter 414857\n",
      "Environment 9: Episode 6210, Score -121.13219453061596, Avg_Score -120.06376267815395\n",
      "Adding trajectory to replay buffer: step 25971, counter 414901\n",
      "Environment 3: Episode 6211, Score -122.65272837871065, Avg_Score -120.0804808359287\n",
      "Adding trajectory to replay buffer: step 25979, counter 414946\n",
      "Environment 12: Episode 6212, Score -122.31977859115096, Avg_Score -120.15424077754514\n",
      "Adding trajectory to replay buffer: step 25980, counter 414990\n",
      "Environment 11: Episode 6213, Score -122.86634977845475, Avg_Score -120.22261786905536\n",
      "Adding trajectory to replay buffer: step 25982, counter 415034\n",
      "Environment 10: Episode 6214, Score -121.56345701338847, Avg_Score -120.2331202871887\n",
      "Adding trajectory to replay buffer: step 25983, counter 415124\n",
      "Environment 4: Episode 6215, Score -116.26659052628216, Avg_Score -120.29058694947092\n",
      "Adding trajectory to replay buffer: step 25997, counter 415167\n",
      "Environment 14: Episode 6216, Score -122.99019287453902, Avg_Score -120.29535437612275\n",
      "Adding trajectory to replay buffer: step 25999, counter 415211\n",
      "Environment 13: Episode 6217, Score -121.6500082613174, Avg_Score -120.29690097031502\n",
      "Adding trajectory to replay buffer: step 26002, counter 415255\n",
      "Environment 5: Episode 6218, Score -123.25609161818339, Avg_Score -120.3041657917861\n",
      "Adding trajectory to replay buffer: step 26004, counter 415303\n",
      "Environment 1: Episode 6219, Score -121.58587398216065, Avg_Score -120.34696968004566\n",
      "Adding trajectory to replay buffer: step 26004, counter 415348\n",
      "Environment 2: Episode 6220, Score -123.30252632345942, Avg_Score -120.34443989888595\n",
      "Adding trajectory to replay buffer: step 26009, counter 415391\n",
      "Environment 7: Episode 6221, Score -122.78022372074922, Avg_Score -120.35851754556371\n",
      "Adding trajectory to replay buffer: step 26010, counter 415434\n",
      "Environment 9: Episode 6222, Score -123.592952253215, Avg_Score -120.38470833758662\n",
      "Adding trajectory to replay buffer: step 26011, counter 415482\n",
      "Environment 6: Episode 6223, Score -124.25496822118386, Avg_Score -120.40259156081927\n",
      "Adding trajectory to replay buffer: step 26014, counter 415525\n",
      "Environment 3: Episode 6224, Score -123.05706976710633, Avg_Score -120.41353020572022\n",
      "Adding trajectory to replay buffer: step 26022, counter 415673\n",
      "Environment 8: Episode 6225, Score -120.4840478142326, Avg_Score -120.43637537192448\n",
      "Adding trajectory to replay buffer: step 26024, counter 415717\n",
      "Environment 11: Episode 6226, Score -120.12653733478268, Avg_Score -120.43396339148018\n",
      "Adding trajectory to replay buffer: step 26024, counter 415762\n",
      "Environment 12: Episode 6227, Score -121.65739053040744, Avg_Score -120.41894300203916\n",
      "Adding trajectory to replay buffer: step 26025, counter 415944\n",
      "Environment 15: Episode 6228, Score -118.97324078832317, Avg_Score -120.38247446543976\n",
      "Adding trajectory to replay buffer: step 26026, counter 415988\n",
      "Environment 10: Episode 6229, Score -121.74808098465266, Avg_Score -120.37739463324964\n",
      "Adding trajectory to replay buffer: step 26028, counter 416033\n",
      "Environment 4: Episode 6230, Score -121.87036733772482, Avg_Score -120.38732264834036\n",
      "Adding trajectory to replay buffer: step 26042, counter 416078\n",
      "Environment 14: Episode 6231, Score -121.92435868676255, Avg_Score -120.38198683371662\n",
      "Adding trajectory to replay buffer: step 26043, counter 416122\n",
      "Environment 13: Episode 6232, Score -115.04219349712568, Avg_Score -120.33075688295254\n",
      "Adding trajectory to replay buffer: step 26044, counter 416332\n",
      "Environment 0: Episode 6233, Score -123.80517744263634, Avg_Score -120.34901916451747\n",
      "Adding trajectory to replay buffer: step 26048, counter 416378\n",
      "Environment 5: Episode 6234, Score -121.81675439965042, Avg_Score -120.44163786142626\n",
      "Adding trajectory to replay buffer: step 26049, counter 416423\n",
      "Environment 1: Episode 6235, Score -121.86534681419532, Avg_Score -120.48433336341192\n",
      "Adding trajectory to replay buffer: step 26052, counter 416466\n",
      "Environment 7: Episode 6236, Score -121.94051730754614, Avg_Score -120.51871285525962\n",
      "Adding trajectory to replay buffer: step 26054, counter 416516\n",
      "Environment 2: Episode 6237, Score -119.48492155415255, Avg_Score -120.49338175539438\n",
      "Adding trajectory to replay buffer: step 26057, counter 416562\n",
      "Environment 6: Episode 6238, Score -117.4489529949644, Avg_Score -120.47981457120848\n",
      "Adding trajectory to replay buffer: step 26067, counter 416615\n",
      "Environment 3: Episode 6239, Score -117.9248709385631, Avg_Score -120.52915517858864\n",
      "Adding trajectory to replay buffer: step 26068, counter 416658\n",
      "Environment 15: Episode 6240, Score -121.71832309576807, Avg_Score -120.57646829541696\n",
      "Adding trajectory to replay buffer: step 26069, counter 416701\n",
      "Environment 10: Episode 6241, Score -118.8486710583816, Avg_Score -120.53897358559593\n",
      "Adding trajectory to replay buffer: step 26071, counter 416748\n",
      "Environment 11: Episode 6242, Score -120.79724282856844, Avg_Score -120.52039633506637\n",
      "Adding trajectory to replay buffer: step 26074, counter 416794\n",
      "Environment 4: Episode 6243, Score -120.75339654284063, Avg_Score -120.61821505211842\n",
      "Adding trajectory to replay buffer: step 26075, counter 416847\n",
      "Environment 8: Episode 6244, Score -114.979524396915, Avg_Score -120.55200809688087\n",
      "Adding trajectory to replay buffer: step 26078, counter 416915\n",
      "Environment 9: Episode 6245, Score -118.61136464032438, Avg_Score -120.57220327527712\n",
      "Adding trajectory to replay buffer: step 26089, counter 416960\n",
      "Environment 0: Episode 6246, Score -120.0302221368843, Avg_Score -120.5564990175674\n",
      "Adding trajectory to replay buffer: step 26089, counter 417006\n",
      "Environment 13: Episode 6247, Score -120.46573048677357, Avg_Score -120.60562436623073\n",
      "Adding trajectory to replay buffer: step 26089, counter 417053\n",
      "Environment 14: Episode 6248, Score -120.58920817268515, Avg_Score -120.60844513546095\n",
      "Adding trajectory to replay buffer: step 26096, counter 417101\n",
      "Environment 5: Episode 6249, Score -120.57392199858836, Avg_Score -120.63398476242165\n",
      "Adding trajectory to replay buffer: step 26098, counter 417145\n",
      "Environment 2: Episode 6250, Score -121.96234920014255, Avg_Score -120.66780363371781\n",
      "Adding trajectory to replay buffer: step 26100, counter 417188\n",
      "Environment 6: Episode 6251, Score -122.94227855462209, Avg_Score -120.77765153779444\n",
      "Adding trajectory to replay buffer: step 26101, counter 417240\n",
      "Environment 1: Episode 6252, Score -117.88384284303773, Avg_Score -120.77611420497698\n",
      "Adding trajectory to replay buffer: step 26106, counter 417322\n",
      "Environment 12: Episode 6253, Score -122.20934458593155, Avg_Score -120.80951079313814\n",
      "Adding trajectory to replay buffer: step 26111, counter 417365\n",
      "Environment 15: Episode 6254, Score -122.21869142259409, Avg_Score -120.84127899459158\n",
      "Adding trajectory to replay buffer: step 26113, counter 417411\n",
      "Environment 3: Episode 6255, Score -122.71156529719941, Avg_Score -120.8434220765973\n",
      "Adding trajectory to replay buffer: step 26114, counter 417456\n",
      "Environment 10: Episode 6256, Score -121.38256149499057, Avg_Score -120.83126631832609\n",
      "Adding trajectory to replay buffer: step 26121, counter 417506\n",
      "Environment 11: Episode 6257, Score -117.22388443379477, Avg_Score -120.84047478727842\n",
      "Adding trajectory to replay buffer: step 26122, counter 417550\n",
      "Environment 9: Episode 6258, Score -116.20784038004217, Avg_Score -120.81998550714289\n",
      "Adding trajectory to replay buffer: step 26131, counter 417592\n",
      "Environment 14: Episode 6259, Score -123.61196615372413, Avg_Score -120.84560772230985\n",
      "Adding trajectory to replay buffer: step 26132, counter 417635\n",
      "Environment 0: Episode 6260, Score -122.75690173335, Avg_Score -120.93700536608581\n",
      "Adding trajectory to replay buffer: step 26132, counter 417678\n",
      "Environment 13: Episode 6261, Score -122.95450137687723, Avg_Score -120.9378359404368\n",
      "Adding trajectory to replay buffer: step 26137, counter 417740\n",
      "Environment 8: Episode 6262, Score -116.81589215450983, Avg_Score -120.91707313510838\n",
      "Adding trajectory to replay buffer: step 26141, counter 417785\n",
      "Environment 5: Episode 6263, Score -121.53154105188077, Avg_Score -120.94912020246528\n",
      "Adding trajectory to replay buffer: step 26144, counter 417829\n",
      "Environment 6: Episode 6264, Score -117.07693977530673, Avg_Score -120.88869013659894\n",
      "Adding trajectory to replay buffer: step 26145, counter 417873\n",
      "Environment 1: Episode 6265, Score -122.74647878150083, Avg_Score -120.8973214511252\n",
      "Adding trajectory to replay buffer: step 26148, counter 417923\n",
      "Environment 2: Episode 6266, Score -113.28465713764851, Avg_Score -120.8663861101923\n",
      "Adding trajectory to replay buffer: step 26148, counter 417965\n",
      "Environment 12: Episode 6267, Score -115.88394042075922, Avg_Score -120.78594204875273\n",
      "Adding trajectory to replay buffer: step 26158, counter 418012\n",
      "Environment 15: Episode 6268, Score -121.41894353767422, Avg_Score -120.76874989894681\n",
      "Adding trajectory to replay buffer: step 26165, counter 418064\n",
      "Environment 3: Episode 6269, Score -116.05301330850398, Avg_Score -120.69758392984087\n",
      "Adding trajectory to replay buffer: step 26167, counter 418110\n",
      "Environment 11: Episode 6270, Score -122.49065303833285, Avg_Score -120.7192039706135\n",
      "Adding trajectory to replay buffer: step 26177, counter 418156\n",
      "Environment 14: Episode 6271, Score -119.60601726581439, Avg_Score -120.68642490359706\n",
      "Adding trajectory to replay buffer: step 26178, counter 418202\n",
      "Environment 0: Episode 6272, Score -120.44916291673233, Avg_Score -120.65846416206269\n",
      "Adding trajectory to replay buffer: step 26178, counter 418248\n",
      "Environment 13: Episode 6273, Score -121.72840031090254, Avg_Score -120.73633278336955\n",
      "Adding trajectory to replay buffer: step 26183, counter 418309\n",
      "Environment 9: Episode 6274, Score -113.67875781756702, Avg_Score -120.65022246767012\n",
      "Adding trajectory to replay buffer: step 26184, counter 418352\n",
      "Environment 5: Episode 6275, Score -122.70031048590938, Avg_Score -120.6618614965809\n",
      "Adding trajectory to replay buffer: step 26185, counter 418400\n",
      "Environment 8: Episode 6276, Score -120.29317547066634, Avg_Score -120.63459284618213\n",
      "Adding trajectory to replay buffer: step 26186, counter 418441\n",
      "Environment 1: Episode 6277, Score -115.72173348503497, Avg_Score -120.53042763849189\n",
      "Adding trajectory to replay buffer: step 26189, counter 418516\n",
      "Environment 10: Episode 6278, Score -117.8425349352012, Avg_Score -120.48579096875744\n",
      "Adding trajectory to replay buffer: step 26194, counter 418562\n",
      "Environment 12: Episode 6279, Score -120.50755372249645, Avg_Score -120.46927295513575\n",
      "Adding trajectory to replay buffer: step 26198, counter 418686\n",
      "Environment 4: Episode 6280, Score -116.6281741395498, Avg_Score -120.42353317957469\n",
      "Adding trajectory to replay buffer: step 26202, counter 418730\n",
      "Environment 15: Episode 6281, Score -119.71365235580814, Avg_Score -120.40993480592755\n",
      "Adding trajectory to replay buffer: step 26204, counter 418786\n",
      "Environment 2: Episode 6282, Score -116.08084450237949, Avg_Score -120.34621810032982\n",
      "Adding trajectory to replay buffer: step 26210, counter 418831\n",
      "Environment 3: Episode 6283, Score -123.4835253698, Avg_Score -120.35988408352328\n",
      "Adding trajectory to replay buffer: step 26220, counter 418874\n",
      "Environment 14: Episode 6284, Score -123.90526300265083, Avg_Score -120.3921761029204\n",
      "Adding trajectory to replay buffer: step 26223, counter 418919\n",
      "Environment 0: Episode 6285, Score -119.9329591987206, Avg_Score -120.37768083948667\n",
      "Adding trajectory to replay buffer: step 26223, counter 418964\n",
      "Environment 13: Episode 6286, Score -120.69411404026673, Avg_Score -120.43104780110193\n",
      "Adding trajectory to replay buffer: step 26225, counter 419006\n",
      "Environment 9: Episode 6287, Score -123.44063802012677, Avg_Score -120.44621234785902\n",
      "Adding trajectory to replay buffer: step 26230, counter 419050\n",
      "Environment 1: Episode 6288, Score -122.97523260608811, Avg_Score -120.44266458549644\n",
      "Adding trajectory to replay buffer: step 26230, counter 419113\n",
      "Environment 11: Episode 6289, Score -111.18415161804359, Avg_Score -120.32650838022354\n",
      "Adding trajectory to replay buffer: step 26231, counter 419159\n",
      "Environment 8: Episode 6290, Score -121.4345554674336, Avg_Score -120.31842585919722\n",
      "Adding trajectory to replay buffer: step 26233, counter 419203\n",
      "Environment 10: Episode 6291, Score -122.930279098471, Avg_Score -120.32386589204405\n",
      "Adding trajectory to replay buffer: step 26234, counter 419385\n",
      "Environment 7: Episode 6292, Score -126.21308336391238, Avg_Score -120.43295921427092\n",
      "Adding trajectory to replay buffer: step 26236, counter 419427\n",
      "Environment 12: Episode 6293, Score -122.67082350479066, Avg_Score -120.43636826180084\n",
      "Adding trajectory to replay buffer: step 26242, counter 419471\n",
      "Environment 4: Episode 6294, Score -121.78329430279942, Avg_Score -120.42537644967139\n",
      "Adding trajectory to replay buffer: step 26245, counter 419514\n",
      "Environment 15: Episode 6295, Score -122.91110429938448, Avg_Score -120.49261411265962\n",
      "Adding trajectory to replay buffer: step 26255, counter 419559\n",
      "Environment 3: Episode 6296, Score -120.7381550065397, Avg_Score -120.48242065111522\n",
      "Adding trajectory to replay buffer: step 26256, counter 419611\n",
      "Environment 2: Episode 6297, Score -118.39563027144223, Avg_Score -120.45092853120325\n",
      "Adding trajectory to replay buffer: step 26267, counter 419655\n",
      "Environment 0: Episode 6298, Score -121.76679987946463, Avg_Score -120.51732232094153\n",
      "Adding trajectory to replay buffer: step 26268, counter 419700\n",
      "Environment 13: Episode 6299, Score -122.05474798029289, Avg_Score -120.51052985153315\n",
      "Adding trajectory to replay buffer: step 26271, counter 419751\n",
      "Environment 14: Episode 6300, Score -121.89180614090836, Avg_Score -120.52996015476779\n",
      "Adding trajectory to replay buffer: step 26272, counter 419793\n",
      "Environment 11: Episode 6301, Score -116.20104484456157, Avg_Score -120.53816852525073\n",
      "Adding trajectory to replay buffer: step 26273, counter 419836\n",
      "Environment 1: Episode 6302, Score -122.84095681005095, Avg_Score -120.55531851285461\n",
      "Adding trajectory to replay buffer: step 26273, counter 419876\n",
      "Environment 10: Episode 6303, Score -116.86484510266222, Avg_Score -120.50435554062643\n",
      "Adding trajectory to replay buffer: step 26278, counter 419920\n",
      "Environment 7: Episode 6304, Score -122.37467535969243, Avg_Score -120.51846512176097\n",
      "Adding trajectory to replay buffer: step 26280, counter 419969\n",
      "Environment 8: Episode 6305, Score -116.8448921472542, Avg_Score -120.46086102294235\n",
      "Adding trajectory to replay buffer: step 26285, counter 420012\n",
      "Environment 4: Episode 6306, Score -121.93440803816728, Avg_Score -120.46645804699257\n",
      "Adding trajectory to replay buffer: step 26290, counter 420057\n",
      "Environment 15: Episode 6307, Score -123.03952884285525, Avg_Score -120.47207714435534\n",
      "Adding trajectory to replay buffer: step 26300, counter 420102\n",
      "Environment 3: Episode 6308, Score -121.9978307308567, Avg_Score -120.46602847283987\n",
      "Adding trajectory to replay buffer: step 26302, counter 420148\n",
      "Environment 2: Episode 6309, Score -121.7455166650222, Avg_Score -120.55319335426708\n",
      "Adding trajectory to replay buffer: step 26302, counter 420214\n",
      "Environment 12: Episode 6310, Score -118.25626645354988, Avg_Score -120.5244340734964\n",
      "Adding trajectory to replay buffer: step 26311, counter 420257\n",
      "Environment 13: Episode 6311, Score -117.61772380184134, Avg_Score -120.47408402772771\n",
      "Adding trajectory to replay buffer: step 26315, counter 420299\n",
      "Environment 10: Episode 6312, Score -123.50845861594503, Avg_Score -120.48597082797563\n",
      "Adding trajectory to replay buffer: step 26316, counter 420342\n",
      "Environment 1: Episode 6313, Score -120.69974660166602, Avg_Score -120.46430479620776\n",
      "Adding trajectory to replay buffer: step 26316, counter 420387\n",
      "Environment 14: Episode 6314, Score -122.62948236599689, Avg_Score -120.47496504973384\n",
      "Adding trajectory to replay buffer: step 26320, counter 420482\n",
      "Environment 9: Episode 6315, Score -124.8090643668715, Avg_Score -120.56038978813974\n",
      "Adding trajectory to replay buffer: step 26322, counter 420526\n",
      "Environment 7: Episode 6316, Score -117.57676837013041, Avg_Score -120.50625554309565\n",
      "Adding trajectory to replay buffer: step 26323, counter 420569\n",
      "Environment 8: Episode 6317, Score -122.71828374795615, Avg_Score -120.51693829796203\n",
      "Adding trajectory to replay buffer: step 26326, counter 420751\n",
      "Environment 6: Episode 6318, Score -122.61290530315702, Avg_Score -120.51050643481176\n",
      "Adding trajectory to replay buffer: step 26327, counter 420793\n",
      "Environment 4: Episode 6319, Score -123.15470218165342, Avg_Score -120.5261947168067\n",
      "Adding trajectory to replay buffer: step 26335, counter 420838\n",
      "Environment 15: Episode 6320, Score -121.09581711405517, Avg_Score -120.50412762471265\n",
      "Adding trajectory to replay buffer: step 26336, counter 420902\n",
      "Environment 11: Episode 6321, Score -114.09942518291126, Avg_Score -120.41731963933427\n",
      "Adding trajectory to replay buffer: step 26341, counter 420976\n",
      "Environment 0: Episode 6322, Score -116.74282597040245, Avg_Score -120.34881837650615\n",
      "Adding trajectory to replay buffer: step 26356, counter 421021\n",
      "Environment 13: Episode 6323, Score -122.12884404491385, Avg_Score -120.32755713474346\n",
      "Adding trajectory to replay buffer: step 26359, counter 421065\n",
      "Environment 10: Episode 6324, Score -123.03999172876279, Avg_Score -120.32738635436003\n",
      "Adding trajectory to replay buffer: step 26360, counter 421109\n",
      "Environment 1: Episode 6325, Score -122.25711378794908, Avg_Score -120.3451170140972\n",
      "Adding trajectory to replay buffer: step 26360, counter 421153\n",
      "Environment 14: Episode 6326, Score -122.42219700844089, Avg_Score -120.36807361083378\n",
      "Adding trajectory to replay buffer: step 26362, counter 421195\n",
      "Environment 9: Episode 6327, Score -122.99737666244246, Avg_Score -120.38147347215414\n",
      "Adding trajectory to replay buffer: step 26365, counter 421237\n",
      "Environment 8: Episode 6328, Score -122.57587128974373, Avg_Score -120.41749977716835\n",
      "Adding trajectory to replay buffer: step 26368, counter 421283\n",
      "Environment 7: Episode 6329, Score -122.0548550986095, Avg_Score -120.4205675183079\n",
      "Adding trajectory to replay buffer: step 26370, counter 421327\n",
      "Environment 6: Episode 6330, Score -122.62454545109533, Avg_Score -120.42810929944162\n",
      "Adding trajectory to replay buffer: step 26372, counter 421399\n",
      "Environment 3: Episode 6331, Score -112.61797514614338, Avg_Score -120.33504546403543\n",
      "Adding trajectory to replay buffer: step 26372, counter 421444\n",
      "Environment 4: Episode 6332, Score -123.66250825401892, Avg_Score -120.42124861160436\n",
      "Adding trajectory to replay buffer: step 26375, counter 421517\n",
      "Environment 12: Episode 6333, Score -119.3654660241939, Avg_Score -120.37685149741993\n",
      "Adding trajectory to replay buffer: step 26379, counter 421561\n",
      "Environment 15: Episode 6334, Score -122.87428352782689, Avg_Score -120.3874267887017\n",
      "Adding trajectory to replay buffer: step 26380, counter 421605\n",
      "Environment 11: Episode 6335, Score -121.6135124838855, Avg_Score -120.38490844539864\n",
      "Adding trajectory to replay buffer: step 26384, counter 421648\n",
      "Environment 0: Episode 6336, Score -122.3652884110088, Avg_Score -120.38915615643326\n",
      "Adding trajectory to replay buffer: step 26386, counter 421732\n",
      "Environment 2: Episode 6337, Score -118.96158327748874, Avg_Score -120.38392277366664\n",
      "Adding trajectory to replay buffer: step 26402, counter 421775\n",
      "Environment 10: Episode 6338, Score -122.4879064228336, Avg_Score -120.43431230794533\n",
      "Adding trajectory to replay buffer: step 26406, counter 421819\n",
      "Environment 9: Episode 6339, Score -122.62684325276315, Avg_Score -120.48133203108732\n",
      "Adding trajectory to replay buffer: step 26411, counter 421865\n",
      "Environment 8: Episode 6340, Score -117.49267338846438, Avg_Score -120.43907553401428\n",
      "Adding trajectory to replay buffer: step 26413, counter 421910\n",
      "Environment 7: Episode 6341, Score -122.60169769704156, Avg_Score -120.47660580040089\n",
      "Adding trajectory to replay buffer: step 26419, counter 421957\n",
      "Environment 3: Episode 6342, Score -122.40495648632572, Avg_Score -120.49268293697845\n",
      "Adding trajectory to replay buffer: step 26421, counter 422003\n",
      "Environment 12: Episode 6343, Score -121.96880326536545, Avg_Score -120.5048370042037\n",
      "Adding trajectory to replay buffer: step 26422, counter 422046\n",
      "Environment 15: Episode 6344, Score -122.56180833755246, Avg_Score -120.58065984361008\n",
      "Adding trajectory to replay buffer: step 26424, counter 422090\n",
      "Environment 11: Episode 6345, Score -122.6587878870697, Avg_Score -120.62113407607754\n",
      "Adding trajectory to replay buffer: step 26429, counter 422133\n",
      "Environment 2: Episode 6346, Score -115.61450176127566, Avg_Score -120.57697687232147\n",
      "Adding trajectory to replay buffer: step 26431, counter 422180\n",
      "Environment 0: Episode 6347, Score -121.32630951902736, Avg_Score -120.58558266264401\n",
      "Adding trajectory to replay buffer: step 26441, counter 422437\n",
      "Environment 5: Episode 6348, Score -131.11468683379204, Avg_Score -120.69083744925508\n",
      "Adding trajectory to replay buffer: step 26447, counter 422482\n",
      "Environment 10: Episode 6349, Score -121.01377019005393, Avg_Score -120.69523593116973\n",
      "Adding trajectory to replay buffer: step 26453, counter 422563\n",
      "Environment 4: Episode 6350, Score -121.82790832021048, Avg_Score -120.69389152237044\n",
      "Adding trajectory to replay buffer: step 26456, counter 422659\n",
      "Environment 1: Episode 6351, Score -115.97953840474412, Avg_Score -120.62426412087164\n",
      "Adding trajectory to replay buffer: step 26456, counter 422702\n",
      "Environment 7: Episode 6352, Score -116.18186933904886, Avg_Score -120.60724438583175\n",
      "Adding trajectory to replay buffer: step 26457, counter 422789\n",
      "Environment 6: Episode 6353, Score -124.14516595443058, Avg_Score -120.62660259951673\n",
      "Adding trajectory to replay buffer: step 26457, counter 422835\n",
      "Environment 8: Episode 6354, Score -123.30124523561696, Avg_Score -120.63742813764696\n",
      "Adding trajectory to replay buffer: step 26462, counter 422878\n",
      "Environment 3: Episode 6355, Score -119.31708820900135, Avg_Score -120.60348336676496\n",
      "Adding trajectory to replay buffer: step 26467, counter 422924\n",
      "Environment 12: Episode 6356, Score -120.1606919071867, Avg_Score -120.59126467088694\n",
      "Adding trajectory to replay buffer: step 26467, counter 423031\n",
      "Environment 14: Episode 6357, Score -117.78346607815847, Avg_Score -120.59686048733057\n",
      "Adding trajectory to replay buffer: step 26467, counter 423076\n",
      "Environment 15: Episode 6358, Score -120.64989122001765, Avg_Score -120.64128099573033\n",
      "Adding trajectory to replay buffer: step 26471, counter 423123\n",
      "Environment 11: Episode 6359, Score -122.659579432015, Avg_Score -120.63175712851321\n",
      "Adding trajectory to replay buffer: step 26475, counter 423169\n",
      "Environment 2: Episode 6360, Score -121.86891509885714, Avg_Score -120.62287726216829\n",
      "Adding trajectory to replay buffer: step 26481, counter 423244\n",
      "Environment 9: Episode 6361, Score -114.45568476780876, Avg_Score -120.53788909607762\n",
      "Adding trajectory to replay buffer: step 26499, counter 423302\n",
      "Environment 5: Episode 6362, Score -112.7607268188602, Avg_Score -120.49733744272113\n",
      "Adding trajectory to replay buffer: step 26502, counter 423348\n",
      "Environment 1: Episode 6363, Score -117.93083700711901, Avg_Score -120.4613304022735\n",
      "Adding trajectory to replay buffer: step 26504, counter 423396\n",
      "Environment 7: Episode 6364, Score -117.06357772010317, Avg_Score -120.46119678172144\n",
      "Adding trajectory to replay buffer: step 26507, counter 423441\n",
      "Environment 3: Episode 6365, Score -121.37691815673062, Avg_Score -120.44750117547375\n",
      "Adding trajectory to replay buffer: step 26511, counter 423485\n",
      "Environment 12: Episode 6366, Score -122.94935857599447, Avg_Score -120.54414818985721\n",
      "Adding trajectory to replay buffer: step 26511, counter 423529\n",
      "Environment 14: Episode 6367, Score -122.83262184022925, Avg_Score -120.6136350040519\n",
      "Adding trajectory to replay buffer: step 26511, counter 423573\n",
      "Environment 15: Episode 6368, Score -122.9714131407303, Avg_Score -120.6291597000825\n",
      "Adding trajectory to replay buffer: step 26513, counter 423629\n",
      "Environment 6: Episode 6369, Score -115.59452669340916, Avg_Score -120.62457483393153\n",
      "Adding trajectory to replay buffer: step 26514, counter 423672\n",
      "Environment 11: Episode 6370, Score -123.15646644544478, Avg_Score -120.63123296800266\n",
      "Adding trajectory to replay buffer: step 26515, counter 423730\n",
      "Environment 8: Episode 6371, Score -115.87485242926267, Avg_Score -120.59392131963713\n",
      "Adding trajectory to replay buffer: step 26521, counter 423776\n",
      "Environment 2: Episode 6372, Score -121.50686572342677, Avg_Score -120.60449834770408\n",
      "Adding trajectory to replay buffer: step 26524, counter 423944\n",
      "Environment 13: Episode 6373, Score -119.30332677661937, Avg_Score -120.58024761236123\n",
      "Adding trajectory to replay buffer: step 26525, counter 424038\n",
      "Environment 0: Episode 6374, Score -121.75212124953288, Avg_Score -120.66098124668088\n",
      "Adding trajectory to replay buffer: step 26536, counter 424127\n",
      "Environment 10: Episode 6375, Score -124.01165074461326, Avg_Score -120.6740946492679\n",
      "Adding trajectory to replay buffer: step 26539, counter 424185\n",
      "Environment 9: Episode 6376, Score -115.2940476718191, Avg_Score -120.62410337127945\n",
      "Adding trajectory to replay buffer: step 26542, counter 424228\n",
      "Environment 5: Episode 6377, Score -122.34835710948272, Avg_Score -120.69036960752392\n",
      "Adding trajectory to replay buffer: step 26547, counter 424273\n",
      "Environment 1: Episode 6378, Score -120.01047227411406, Avg_Score -120.71204898091304\n",
      "Adding trajectory to replay buffer: step 26552, counter 424321\n",
      "Environment 7: Episode 6379, Score -120.82910628039453, Avg_Score -120.71526450649202\n",
      "Adding trajectory to replay buffer: step 26553, counter 424363\n",
      "Environment 14: Episode 6380, Score -118.87107968802812, Avg_Score -120.7376935619768\n",
      "Adding trajectory to replay buffer: step 26554, counter 424406\n",
      "Environment 15: Episode 6381, Score -116.1193875718272, Avg_Score -120.70175091413698\n",
      "Adding trajectory to replay buffer: step 26556, counter 424451\n",
      "Environment 12: Episode 6382, Score -121.4940401948411, Avg_Score -120.75588287106159\n",
      "Adding trajectory to replay buffer: step 26558, counter 424556\n",
      "Environment 4: Episode 6383, Score -125.2417372154197, Avg_Score -120.77346498951779\n",
      "Adding trajectory to replay buffer: step 26558, counter 424600\n",
      "Environment 11: Episode 6384, Score -119.51220413916496, Avg_Score -120.72953440088291\n",
      "Adding trajectory to replay buffer: step 26561, counter 424646\n",
      "Environment 8: Episode 6385, Score -120.49191470180017, Avg_Score -120.7351239559137\n",
      "Adding trajectory to replay buffer: step 26569, counter 424690\n",
      "Environment 0: Episode 6386, Score -118.06625018495197, Avg_Score -120.70884531736057\n",
      "Adding trajectory to replay buffer: step 26570, counter 424736\n",
      "Environment 13: Episode 6387, Score -121.2470349038802, Avg_Score -120.68690928619807\n",
      "Adding trajectory to replay buffer: step 26572, counter 424795\n",
      "Environment 6: Episode 6388, Score -115.0996017030788, Avg_Score -120.60815297716799\n",
      "Adding trajectory to replay buffer: step 26579, counter 424853\n",
      "Environment 2: Episode 6389, Score -117.95362003897317, Avg_Score -120.6758476613773\n",
      "Adding trajectory to replay buffer: step 26579, counter 424896\n",
      "Environment 10: Episode 6390, Score -122.36728291269267, Avg_Score -120.6851749358299\n",
      "Adding trajectory to replay buffer: step 26582, counter 424939\n",
      "Environment 9: Episode 6391, Score -122.82751587986263, Avg_Score -120.6841473036438\n",
      "Adding trajectory to replay buffer: step 26586, counter 424983\n",
      "Environment 5: Episode 6392, Score -122.16345811706968, Avg_Score -120.64365105117537\n",
      "Adding trajectory to replay buffer: step 26587, counter 425063\n",
      "Environment 3: Episode 6393, Score -116.27057586122427, Avg_Score -120.57964857473974\n",
      "Adding trajectory to replay buffer: step 26598, counter 425108\n",
      "Environment 14: Episode 6394, Score -122.1170331105, Avg_Score -120.58298596281674\n",
      "Adding trajectory to replay buffer: step 26600, counter 425152\n",
      "Environment 12: Episode 6395, Score -121.92910283973316, Avg_Score -120.5731659482202\n",
      "Adding trajectory to replay buffer: step 26600, counter 425198\n",
      "Environment 15: Episode 6396, Score -121.40591734328059, Avg_Score -120.57984357158763\n",
      "Adding trajectory to replay buffer: step 26601, counter 425241\n",
      "Environment 4: Episode 6397, Score -122.55260354777302, Avg_Score -120.62141330435092\n",
      "Adding trajectory to replay buffer: step 26603, counter 425286\n",
      "Environment 11: Episode 6398, Score -120.66508988192615, Avg_Score -120.61039620437553\n",
      "Adding trajectory to replay buffer: step 26606, counter 425331\n",
      "Environment 8: Episode 6399, Score -121.52060122166016, Avg_Score -120.60505473678923\n",
      "Adding trajectory to replay buffer: step 26615, counter 425374\n",
      "Environment 6: Episode 6400, Score -122.73919151303917, Avg_Score -120.61352859051053\n",
      "Adding trajectory to replay buffer: step 26616, counter 425443\n",
      "Environment 1: Episode 6401, Score -120.2087937773491, Avg_Score -120.6536060798384\n",
      "Adding trajectory to replay buffer: step 26616, counter 425489\n",
      "Environment 13: Episode 6402, Score -121.77680328611532, Avg_Score -120.64296454459904\n",
      "Adding trajectory to replay buffer: step 26626, counter 425536\n",
      "Environment 10: Episode 6403, Score -121.13114295464071, Avg_Score -120.6856275231188\n",
      "Adding trajectory to replay buffer: step 26629, counter 425579\n",
      "Environment 5: Episode 6404, Score -115.53426123645404, Avg_Score -120.61722338188645\n",
      "Adding trajectory to replay buffer: step 26631, counter 425623\n",
      "Environment 3: Episode 6405, Score -119.86188762229837, Avg_Score -120.64739333663688\n",
      "Adding trajectory to replay buffer: step 26632, counter 425686\n",
      "Environment 0: Episode 6406, Score -114.20215020064512, Avg_Score -120.57007075826169\n",
      "Adding trajectory to replay buffer: step 26642, counter 425730\n",
      "Environment 14: Episode 6407, Score -119.50178680601965, Avg_Score -120.53469333789334\n",
      "Adding trajectory to replay buffer: step 26647, counter 425776\n",
      "Environment 4: Episode 6408, Score -121.11042510756664, Avg_Score -120.52581928166046\n",
      "Adding trajectory to replay buffer: step 26647, counter 425820\n",
      "Environment 11: Episode 6409, Score -115.67813107941052, Avg_Score -120.46514542580435\n",
      "Adding trajectory to replay buffer: step 26651, counter 425871\n",
      "Environment 12: Episode 6410, Score -120.6176594436597, Avg_Score -120.48875935570544\n",
      "Adding trajectory to replay buffer: step 26651, counter 425922\n",
      "Environment 15: Episode 6411, Score -118.39197148377697, Avg_Score -120.49650183252483\n",
      "Adding trajectory to replay buffer: step 26660, counter 425976\n",
      "Environment 8: Episode 6412, Score -111.81403406863163, Avg_Score -120.37955758705168\n",
      "Adding trajectory to replay buffer: step 26662, counter 426023\n",
      "Environment 6: Episode 6413, Score -120.57552880386697, Avg_Score -120.37831540907369\n",
      "Adding trajectory to replay buffer: step 26664, counter 426071\n",
      "Environment 1: Episode 6414, Score -121.3217190026796, Avg_Score -120.36523777544053\n",
      "Adding trajectory to replay buffer: step 26671, counter 426126\n",
      "Environment 13: Episode 6415, Score -118.1706266507587, Avg_Score -120.29885339827939\n",
      "Adding trajectory to replay buffer: step 26675, counter 426170\n",
      "Environment 3: Episode 6416, Score -122.87274098229719, Avg_Score -120.35181312440105\n",
      "Adding trajectory to replay buffer: step 26676, counter 426214\n",
      "Environment 0: Episode 6417, Score -122.3293725888071, Avg_Score -120.34792401280956\n",
      "Adding trajectory to replay buffer: step 26683, counter 426255\n",
      "Environment 14: Episode 6418, Score -117.00906616300344, Avg_Score -120.29188562140804\n",
      "Adding trajectory to replay buffer: step 26688, counter 426317\n",
      "Environment 10: Episode 6419, Score -115.3969549888447, Avg_Score -120.21430814947995\n",
      "Adding trajectory to replay buffer: step 26690, counter 426360\n",
      "Environment 4: Episode 6420, Score -119.7760748108135, Avg_Score -120.20111072644751\n",
      "Adding trajectory to replay buffer: step 26693, counter 426406\n",
      "Environment 11: Episode 6421, Score -119.6606827665673, Avg_Score -120.25672330228409\n",
      "Adding trajectory to replay buffer: step 26694, counter 426548\n",
      "Environment 7: Episode 6422, Score -129.69801854144472, Avg_Score -120.3862752279945\n",
      "Adding trajectory to replay buffer: step 26694, counter 426591\n",
      "Environment 15: Episode 6423, Score -117.6966290685876, Avg_Score -120.34195307823124\n",
      "Adding trajectory to replay buffer: step 26696, counter 426636\n",
      "Environment 12: Episode 6424, Score -119.01486388531204, Avg_Score -120.30170179979672\n",
      "Adding trajectory to replay buffer: step 26701, counter 426755\n",
      "Environment 9: Episode 6425, Score -116.67087445080125, Avg_Score -120.24583940642523\n",
      "Adding trajectory to replay buffer: step 26706, counter 426801\n",
      "Environment 8: Episode 6426, Score -120.1475759789267, Avg_Score -120.22309319613011\n",
      "Adding trajectory to replay buffer: step 26708, counter 426847\n",
      "Environment 6: Episode 6427, Score -115.62505249018346, Avg_Score -120.14936995440752\n",
      "Adding trajectory to replay buffer: step 26711, counter 426894\n",
      "Environment 1: Episode 6428, Score -115.62919046653062, Avg_Score -120.0799031461754\n",
      "Adding trajectory to replay buffer: step 26718, counter 426941\n",
      "Environment 13: Episode 6429, Score -124.04025798622581, Avg_Score -120.09975717505156\n",
      "Adding trajectory to replay buffer: step 26729, counter 426987\n",
      "Environment 14: Episode 6430, Score -121.53271102473636, Avg_Score -120.08883883078798\n",
      "Adding trajectory to replay buffer: step 26733, counter 427141\n",
      "Environment 2: Episode 6431, Score -126.25164720665789, Avg_Score -120.22517555139311\n",
      "Adding trajectory to replay buffer: step 26738, counter 427191\n",
      "Environment 10: Episode 6432, Score -116.06649374305272, Avg_Score -120.14921540628343\n",
      "Adding trajectory to replay buffer: step 26746, counter 427236\n",
      "Environment 9: Episode 6433, Score -121.14621746860321, Avg_Score -120.16702292072756\n",
      "Adding trajectory to replay buffer: step 26748, counter 427290\n",
      "Environment 7: Episode 6434, Score -116.59222867729392, Avg_Score -120.10420237222225\n",
      "Adding trajectory to replay buffer: step 26749, counter 427333\n",
      "Environment 8: Episode 6435, Score -122.96953347231387, Avg_Score -120.11776258210654\n",
      "Adding trajectory to replay buffer: step 26751, counter 427391\n",
      "Environment 11: Episode 6436, Score -120.37620904447138, Avg_Score -120.09787178844114\n",
      "Adding trajectory to replay buffer: step 26752, counter 427435\n",
      "Environment 6: Episode 6437, Score -122.18504411105565, Avg_Score -120.13010639677682\n",
      "Adding trajectory to replay buffer: step 26755, counter 427479\n",
      "Environment 1: Episode 6438, Score -121.60144365207603, Avg_Score -120.12124176906926\n",
      "Adding trajectory to replay buffer: step 26763, counter 427552\n",
      "Environment 4: Episode 6439, Score -116.47101127441165, Avg_Score -120.05968344928573\n",
      "Adding trajectory to replay buffer: step 26765, counter 427623\n",
      "Environment 15: Episode 6440, Score -115.19008591093495, Avg_Score -120.03665757451044\n",
      "Adding trajectory to replay buffer: step 26771, counter 427665\n",
      "Environment 14: Episode 6441, Score -123.105780646123, Avg_Score -120.04169840400125\n",
      "Adding trajectory to replay buffer: step 26776, counter 427708\n",
      "Environment 2: Episode 6442, Score -121.96118081438169, Avg_Score -120.0372606472818\n",
      "Adding trajectory to replay buffer: step 26783, counter 427753\n",
      "Environment 10: Episode 6443, Score -119.62275639316346, Avg_Score -120.01380017855979\n",
      "Adding trajectory to replay buffer: step 26786, counter 427793\n",
      "Environment 9: Episode 6444, Score -116.78617125066307, Avg_Score -119.95604380769088\n",
      "Adding trajectory to replay buffer: step 26792, counter 427836\n",
      "Environment 8: Episode 6445, Score -122.22926745172187, Avg_Score -119.95174860333742\n",
      "Adding trajectory to replay buffer: step 26792, counter 427910\n",
      "Environment 13: Episode 6446, Score -116.11991684898005, Avg_Score -119.95680275421446\n",
      "Adding trajectory to replay buffer: step 26794, counter 427953\n",
      "Environment 11: Episode 6447, Score -122.45551084349367, Avg_Score -119.96809476745914\n",
      "Adding trajectory to replay buffer: step 26795, counter 428073\n",
      "Environment 3: Episode 6448, Score -118.69980864226136, Avg_Score -119.84394598554381\n",
      "Adding trajectory to replay buffer: step 26798, counter 428116\n",
      "Environment 1: Episode 6449, Score -122.27285237513235, Avg_Score -119.8565368073946\n",
      "Adding trajectory to replay buffer: step 26800, counter 428164\n",
      "Environment 6: Episode 6450, Score -119.43152126498705, Avg_Score -119.83257293684237\n",
      "Adding trajectory to replay buffer: step 26807, counter 428223\n",
      "Environment 7: Episode 6451, Score -116.0613169122835, Avg_Score -119.83339072191777\n",
      "Adding trajectory to replay buffer: step 26817, counter 428269\n",
      "Environment 14: Episode 6452, Score -122.01195100971746, Avg_Score -119.89169153862446\n",
      "Adding trajectory to replay buffer: step 26818, counter 428411\n",
      "Environment 0: Episode 6453, Score -120.71953719890924, Avg_Score -119.85743525106923\n",
      "Adding trajectory to replay buffer: step 26820, counter 428455\n",
      "Environment 2: Episode 6454, Score -119.2381046291669, Avg_Score -119.81680384500474\n",
      "Adding trajectory to replay buffer: step 26827, counter 428519\n",
      "Environment 4: Episode 6455, Score -117.3145775378632, Avg_Score -119.79677873829338\n",
      "Adding trajectory to replay buffer: step 26833, counter 428569\n",
      "Environment 10: Episode 6456, Score -119.71345241935737, Avg_Score -119.79230634341508\n",
      "Adding trajectory to replay buffer: step 26836, counter 428613\n",
      "Environment 8: Episode 6457, Score -120.08102618413542, Avg_Score -119.81528194447486\n",
      "Adding trajectory to replay buffer: step 26837, counter 428664\n",
      "Environment 9: Episode 6458, Score -117.59910128043282, Avg_Score -119.78477404507902\n",
      "Adding trajectory to replay buffer: step 26838, counter 428707\n",
      "Environment 3: Episode 6459, Score -116.2113058815971, Avg_Score -119.72029130957485\n",
      "Adding trajectory to replay buffer: step 26840, counter 428753\n",
      "Environment 11: Episode 6460, Score -122.04132459028624, Avg_Score -119.72201540448913\n",
      "Adding trajectory to replay buffer: step 26844, counter 428805\n",
      "Environment 13: Episode 6461, Score -119.85073082491694, Avg_Score -119.7759658650602\n",
      "Adding trajectory to replay buffer: step 26851, counter 428891\n",
      "Environment 15: Episode 6462, Score -116.18500705565016, Avg_Score -119.81020866742809\n",
      "Adding trajectory to replay buffer: step 26860, counter 428951\n",
      "Environment 6: Episode 6463, Score -116.46044018083562, Avg_Score -119.79550469916526\n",
      "Adding trajectory to replay buffer: step 26860, counter 429115\n",
      "Environment 12: Episode 6464, Score -121.1580381789611, Avg_Score -119.83644930375385\n",
      "Adding trajectory to replay buffer: step 26862, counter 429160\n",
      "Environment 14: Episode 6465, Score -121.91788981898502, Avg_Score -119.84185902037638\n",
      "Adding trajectory to replay buffer: step 26865, counter 429205\n",
      "Environment 2: Episode 6466, Score -119.46999809508833, Avg_Score -119.80706541556732\n",
      "Adding trajectory to replay buffer: step 26867, counter 429254\n",
      "Environment 0: Episode 6467, Score -117.11332418814177, Avg_Score -119.74987243904644\n",
      "Adding trajectory to replay buffer: step 26870, counter 429326\n",
      "Environment 1: Episode 6468, Score -110.72962017202191, Avg_Score -119.62745450935935\n",
      "Adding trajectory to replay buffer: step 26870, counter 429369\n",
      "Environment 4: Episode 6469, Score -122.53943754865354, Avg_Score -119.6969036179118\n",
      "Adding trajectory to replay buffer: step 26873, counter 429409\n",
      "Environment 10: Episode 6470, Score -116.12355486562973, Avg_Score -119.62657450211364\n",
      "Adding trajectory to replay buffer: step 26880, counter 429451\n",
      "Environment 3: Episode 6471, Score -117.81826839945465, Avg_Score -119.64600866181556\n",
      "Adding trajectory to replay buffer: step 26884, counter 429498\n",
      "Environment 9: Episode 6472, Score -120.45513243951089, Avg_Score -119.63549132897639\n",
      "Adding trajectory to replay buffer: step 26887, counter 429541\n",
      "Environment 13: Episode 6473, Score -122.35437133495074, Avg_Score -119.66600177455967\n",
      "Adding trajectory to replay buffer: step 26894, counter 429584\n",
      "Environment 15: Episode 6474, Score -122.17502961372958, Avg_Score -119.67023085820165\n",
      "Adding trajectory to replay buffer: step 26900, counter 429677\n",
      "Environment 7: Episode 6475, Score -115.9970260817092, Avg_Score -119.5900846115726\n",
      "Adding trajectory to replay buffer: step 26904, counter 429721\n",
      "Environment 12: Episode 6476, Score -122.5670669788172, Avg_Score -119.66281480464261\n",
      "Adding trajectory to replay buffer: step 26906, counter 429767\n",
      "Environment 6: Episode 6477, Score -122.6438984652143, Avg_Score -119.66577021819991\n",
      "Adding trajectory to replay buffer: step 26907, counter 429834\n",
      "Environment 11: Episode 6478, Score -118.64327210910743, Avg_Score -119.65209821654985\n",
      "Adding trajectory to replay buffer: step 26911, counter 429878\n",
      "Environment 0: Episode 6479, Score -117.36366454832628, Avg_Score -119.61744379922916\n",
      "Adding trajectory to replay buffer: step 26918, counter 429923\n",
      "Environment 10: Episode 6480, Score -118.8698390130171, Avg_Score -119.61743139247905\n",
      "Adding trajectory to replay buffer: step 26921, counter 429982\n",
      "Environment 14: Episode 6481, Score -115.00038207521911, Avg_Score -119.60624133751297\n",
      "Adding trajectory to replay buffer: step 26924, counter 430026\n",
      "Environment 3: Episode 6482, Score -122.48877797501659, Avg_Score -119.61618871531473\n",
      "Adding trajectory to replay buffer: step 26927, counter 430083\n",
      "Environment 4: Episode 6483, Score -115.4897686751516, Avg_Score -119.51866902991205\n",
      "Adding trajectory to replay buffer: step 26927, counter 430126\n",
      "Environment 9: Episode 6484, Score -122.94037497722545, Avg_Score -119.55295073829264\n",
      "Adding trajectory to replay buffer: step 26930, counter 430186\n",
      "Environment 1: Episode 6485, Score -116.49534223667719, Avg_Score -119.51298501364143\n",
      "Adding trajectory to replay buffer: step 26932, counter 430231\n",
      "Environment 13: Episode 6486, Score -122.27242390263515, Avg_Score -119.55504675081824\n",
      "Adding trajectory to replay buffer: step 26945, counter 430276\n",
      "Environment 7: Episode 6487, Score -117.55732645771839, Avg_Score -119.51814966635663\n",
      "Adding trajectory to replay buffer: step 26947, counter 430319\n",
      "Environment 12: Episode 6488, Score -122.94195433869896, Avg_Score -119.59657319271282\n",
      "Adding trajectory to replay buffer: step 26948, counter 430638\n",
      "Environment 5: Episode 6489, Score -134.37421090140998, Avg_Score -119.76077910133718\n",
      "Adding trajectory to replay buffer: step 26950, counter 430682\n",
      "Environment 6: Episode 6490, Score -122.79000311599238, Avg_Score -119.76500630337014\n",
      "Adding trajectory to replay buffer: step 26951, counter 430726\n",
      "Environment 11: Episode 6491, Score -122.66453262299115, Avg_Score -119.76337647080143\n",
      "Adding trajectory to replay buffer: step 26954, counter 430769\n",
      "Environment 0: Episode 6492, Score -123.1965192140521, Avg_Score -119.77370708177128\n",
      "Adding trajectory to replay buffer: step 26955, counter 430830\n",
      "Environment 15: Episode 6493, Score -117.54845228183828, Avg_Score -119.78648584597742\n",
      "Adding trajectory to replay buffer: step 26960, counter 430954\n",
      "Environment 8: Episode 6494, Score -112.98971905072158, Avg_Score -119.69521270537966\n",
      "Adding trajectory to replay buffer: step 26961, counter 431050\n",
      "Environment 2: Episode 6495, Score -111.79277439046962, Avg_Score -119.59384942088701\n",
      "Adding trajectory to replay buffer: step 26962, counter 431094\n",
      "Environment 10: Episode 6496, Score -122.33546987693694, Avg_Score -119.60314494622355\n",
      "Adding trajectory to replay buffer: step 26969, counter 431142\n",
      "Environment 14: Episode 6497, Score -119.67736509616053, Avg_Score -119.57439256170743\n",
      "Adding trajectory to replay buffer: step 26972, counter 431187\n",
      "Environment 4: Episode 6498, Score -117.24406881366919, Avg_Score -119.54018235102485\n",
      "Adding trajectory to replay buffer: step 26977, counter 431237\n",
      "Environment 9: Episode 6499, Score -121.93714616694984, Avg_Score -119.54434780047775\n",
      "Adding trajectory to replay buffer: step 26991, counter 431283\n",
      "Environment 7: Episode 6500, Score -118.21929728603736, Avg_Score -119.49914885820773\n",
      "Adding trajectory to replay buffer: step 26993, counter 431328\n",
      "Environment 5: Episode 6501, Score -122.45410639812674, Avg_Score -119.5216019844155\n",
      "Adding trajectory to replay buffer: step 26993, counter 431371\n",
      "Environment 6: Episode 6502, Score -123.70935447636869, Avg_Score -119.54092749631803\n",
      "Adding trajectory to replay buffer: step 26993, counter 431413\n",
      "Environment 11: Episode 6503, Score -122.87852436624716, Avg_Score -119.5584013104341\n",
      "Adding trajectory to replay buffer: step 26995, counter 431476\n",
      "Environment 13: Episode 6504, Score -114.28517351016774, Avg_Score -119.54591043317124\n",
      "Adding trajectory to replay buffer: step 26997, counter 431519\n",
      "Environment 0: Episode 6505, Score -123.0057548949911, Avg_Score -119.57734910589821\n",
      "Adding trajectory to replay buffer: step 27000, counter 431564\n",
      "Environment 15: Episode 6506, Score -122.03505961118452, Avg_Score -119.65567820000359\n",
      "Adding trajectory to replay buffer: step 27004, counter 431607\n",
      "Environment 2: Episode 6507, Score -122.85492219310689, Avg_Score -119.68920955387445\n",
      "Adding trajectory to replay buffer: step 27004, counter 431651\n",
      "Environment 8: Episode 6508, Score -120.70501154624361, Avg_Score -119.68515541826122\n",
      "Adding trajectory to replay buffer: step 27007, counter 431711\n",
      "Environment 12: Episode 6509, Score -116.50404634447582, Avg_Score -119.69341457091188\n",
      "Adding trajectory to replay buffer: step 27008, counter 431757\n",
      "Environment 10: Episode 6510, Score -121.09709728207811, Avg_Score -119.69820894929606\n",
      "Adding trajectory to replay buffer: step 27012, counter 431800\n",
      "Environment 14: Episode 6511, Score -122.41466455064031, Avg_Score -119.73843587996471\n",
      "Adding trajectory to replay buffer: step 27016, counter 431844\n",
      "Environment 4: Episode 6512, Score -121.6907035698959, Avg_Score -119.83720257497733\n",
      "Adding trajectory to replay buffer: step 27031, counter 431898\n",
      "Environment 9: Episode 6513, Score -118.75668583101904, Avg_Score -119.81901414524886\n",
      "Adding trajectory to replay buffer: step 27033, counter 432007\n",
      "Environment 3: Episode 6514, Score -124.7130658263246, Avg_Score -119.85292761348532\n",
      "Adding trajectory to replay buffer: step 27034, counter 432050\n",
      "Environment 7: Episode 6515, Score -116.00968791802912, Avg_Score -119.83131822615802\n",
      "Adding trajectory to replay buffer: step 27037, counter 432094\n",
      "Environment 5: Episode 6516, Score -121.12321681678668, Avg_Score -119.8138229845029\n",
      "Adding trajectory to replay buffer: step 27037, counter 432138\n",
      "Environment 11: Episode 6517, Score -116.1803575249122, Avg_Score -119.75233283386396\n",
      "Adding trajectory to replay buffer: step 27043, counter 432184\n",
      "Environment 0: Episode 6518, Score -120.96496850306045, Avg_Score -119.79189185726455\n",
      "Adding trajectory to replay buffer: step 27045, counter 432299\n",
      "Environment 1: Episode 6519, Score -123.57897298607541, Avg_Score -119.87371203723684\n",
      "Adding trajectory to replay buffer: step 27048, counter 432343\n",
      "Environment 2: Episode 6520, Score -121.85570557349858, Avg_Score -119.8945083448637\n",
      "Adding trajectory to replay buffer: step 27054, counter 432390\n",
      "Environment 12: Episode 6521, Score -120.02134913893106, Avg_Score -119.89811500858735\n",
      "Adding trajectory to replay buffer: step 27057, counter 432435\n",
      "Environment 14: Episode 6522, Score -122.45179453377301, Avg_Score -119.82565276851065\n",
      "Adding trajectory to replay buffer: step 27059, counter 432501\n",
      "Environment 6: Episode 6523, Score -111.1084831318312, Avg_Score -119.75977130914308\n",
      "Adding trajectory to replay buffer: step 27065, counter 432566\n",
      "Environment 15: Episode 6524, Score -113.78303663949296, Avg_Score -119.7074530366849\n",
      "Adding trajectory to replay buffer: step 27067, counter 432629\n",
      "Environment 8: Episode 6525, Score -116.09015570299638, Avg_Score -119.70164584920684\n",
      "Adding trajectory to replay buffer: step 27074, counter 432672\n",
      "Environment 9: Episode 6526, Score -122.22541469964882, Avg_Score -119.72242423641406\n",
      "Adding trajectory to replay buffer: step 27077, counter 432716\n",
      "Environment 3: Episode 6527, Score -123.23142491161886, Avg_Score -119.7984879606284\n",
      "Adding trajectory to replay buffer: step 27077, counter 432759\n",
      "Environment 7: Episode 6528, Score -123.44834491627168, Avg_Score -119.87667950512584\n",
      "Adding trajectory to replay buffer: step 27080, counter 432802\n",
      "Environment 11: Episode 6529, Score -120.57192046445857, Avg_Score -119.84199612990815\n",
      "Adding trajectory to replay buffer: step 27083, counter 432848\n",
      "Environment 5: Episode 6530, Score -121.55085972947194, Avg_Score -119.84217761695552\n",
      "Adding trajectory to replay buffer: step 27085, counter 432890\n",
      "Environment 0: Episode 6531, Score -117.42291318278946, Avg_Score -119.75389027671682\n",
      "Adding trajectory to replay buffer: step 27085, counter 432980\n",
      "Environment 13: Episode 6532, Score -110.63949965205603, Avg_Score -119.69962033580683\n",
      "Adding trajectory to replay buffer: step 27091, counter 433026\n",
      "Environment 1: Episode 6533, Score -118.2233615896292, Avg_Score -119.6703917770171\n",
      "Adding trajectory to replay buffer: step 27093, counter 433071\n",
      "Environment 2: Episode 6534, Score -122.9433059208213, Avg_Score -119.73390254945235\n",
      "Adding trajectory to replay buffer: step 27099, counter 433113\n",
      "Environment 14: Episode 6535, Score -123.0592276680482, Avg_Score -119.7347994914097\n",
      "Adding trajectory to replay buffer: step 27101, counter 433155\n",
      "Environment 6: Episode 6536, Score -123.08127775383244, Avg_Score -119.7618501785033\n",
      "Adding trajectory to replay buffer: step 27103, counter 433242\n",
      "Environment 4: Episode 6537, Score -113.42340801488236, Avg_Score -119.67423381754155\n",
      "Adding trajectory to replay buffer: step 27108, counter 433296\n",
      "Environment 12: Episode 6538, Score -119.38897220516888, Avg_Score -119.65210910307249\n",
      "Adding trajectory to replay buffer: step 27108, counter 433339\n",
      "Environment 15: Episode 6539, Score -123.1631002242261, Avg_Score -119.71902999257065\n",
      "Adding trajectory to replay buffer: step 27119, counter 433384\n",
      "Environment 9: Episode 6540, Score -121.86600192435832, Avg_Score -119.78578915270488\n",
      "Adding trajectory to replay buffer: step 27119, counter 433495\n",
      "Environment 10: Episode 6541, Score -117.50397782969041, Avg_Score -119.72977112454055\n",
      "Adding trajectory to replay buffer: step 27125, counter 433540\n",
      "Environment 11: Episode 6542, Score -122.14843844022788, Avg_Score -119.73164370079901\n",
      "Adding trajectory to replay buffer: step 27129, counter 433584\n",
      "Environment 13: Episode 6543, Score -121.71849881822057, Avg_Score -119.75260112504957\n",
      "Adding trajectory to replay buffer: step 27132, counter 433631\n",
      "Environment 0: Episode 6544, Score -121.26640315169655, Avg_Score -119.79740344405991\n",
      "Adding trajectory to replay buffer: step 27143, counter 433673\n",
      "Environment 6: Episode 6545, Score -123.72016987692751, Avg_Score -119.81231246831197\n",
      "Adding trajectory to replay buffer: step 27144, counter 433718\n",
      "Environment 14: Episode 6546, Score -122.12566147183566, Avg_Score -119.87236991454053\n",
      "Adding trajectory to replay buffer: step 27145, counter 433760\n",
      "Environment 4: Episode 6547, Score -123.34660546843833, Avg_Score -119.88128086078996\n",
      "Adding trajectory to replay buffer: step 27146, counter 433823\n",
      "Environment 5: Episode 6548, Score -117.76347078593137, Avg_Score -119.87191748222665\n",
      "Adding trajectory to replay buffer: step 27152, counter 433867\n",
      "Environment 12: Episode 6549, Score -122.50211729336033, Avg_Score -119.87421013140894\n",
      "Adding trajectory to replay buffer: step 27152, counter 433911\n",
      "Environment 15: Episode 6550, Score -122.56621494099623, Avg_Score -119.90555706816906\n",
      "Adding trajectory to replay buffer: step 27162, counter 433980\n",
      "Environment 2: Episode 6551, Score -118.90932335576811, Avg_Score -119.93403713260392\n",
      "Adding trajectory to replay buffer: step 27162, counter 434023\n",
      "Environment 9: Episode 6552, Score -123.30689560236284, Avg_Score -119.94698657853036\n",
      "Adding trajectory to replay buffer: step 27162, counter 434066\n",
      "Environment 10: Episode 6553, Score -124.14327951966163, Avg_Score -119.98122400173787\n",
      "Adding trajectory to replay buffer: step 27168, counter 434109\n",
      "Environment 11: Episode 6554, Score -122.7927016336446, Avg_Score -120.01676997178265\n",
      "Adding trajectory to replay buffer: step 27175, counter 434152\n",
      "Environment 0: Episode 6555, Score -120.87371340951013, Avg_Score -120.05236133049911\n",
      "Adding trajectory to replay buffer: step 27178, counter 434253\n",
      "Environment 3: Episode 6556, Score -117.41741981493371, Avg_Score -120.02940100445488\n",
      "Adding trajectory to replay buffer: step 27185, counter 434295\n",
      "Environment 6: Episode 6557, Score -122.1469404298837, Avg_Score -120.05006014691236\n",
      "Adding trajectory to replay buffer: step 27188, counter 434339\n",
      "Environment 14: Episode 6558, Score -121.5386506245155, Avg_Score -120.0894556403532\n",
      "Adding trajectory to replay buffer: step 27189, counter 434383\n",
      "Environment 4: Episode 6559, Score -122.50161125861791, Avg_Score -120.15235869412341\n",
      "Adding trajectory to replay buffer: step 27190, counter 434427\n",
      "Environment 5: Episode 6560, Score -122.14541226928804, Avg_Score -120.15339957091342\n",
      "Adding trajectory to replay buffer: step 27190, counter 434550\n",
      "Environment 8: Episode 6561, Score -117.71125756567716, Avg_Score -120.13200483832102\n",
      "Adding trajectory to replay buffer: step 27205, counter 434593\n",
      "Environment 2: Episode 6562, Score -122.43059037353471, Avg_Score -120.1944606714999\n",
      "Adding trajectory to replay buffer: step 27205, counter 434636\n",
      "Environment 9: Episode 6563, Score -122.52741078074091, Avg_Score -120.25513037749897\n",
      "Adding trajectory to replay buffer: step 27205, counter 434679\n",
      "Environment 10: Episode 6564, Score -122.48274092774466, Avg_Score -120.26837740498677\n",
      "Adding trajectory to replay buffer: step 27207, counter 434795\n",
      "Environment 1: Episode 6565, Score -127.07942410972217, Avg_Score -120.31999274789418\n",
      "Adding trajectory to replay buffer: step 27211, counter 434838\n",
      "Environment 11: Episode 6566, Score -122.63033165730381, Avg_Score -120.35159608351633\n",
      "Adding trajectory to replay buffer: step 27226, counter 434912\n",
      "Environment 12: Episode 6567, Score -115.29497695104715, Avg_Score -120.33341261114538\n",
      "Adding trajectory to replay buffer: step 27228, counter 434955\n",
      "Environment 6: Episode 6568, Score -122.96629379062914, Avg_Score -120.45577934733144\n",
      "Adding trajectory to replay buffer: step 27233, counter 434999\n",
      "Environment 4: Episode 6569, Score -121.75097183239708, Avg_Score -120.44789469016888\n",
      "Adding trajectory to replay buffer: step 27233, counter 435042\n",
      "Environment 8: Episode 6570, Score -122.18332349773372, Avg_Score -120.50849237648993\n",
      "Adding trajectory to replay buffer: step 27234, counter 435124\n",
      "Environment 15: Episode 6571, Score -114.12474129815338, Avg_Score -120.47155710547689\n",
      "Adding trajectory to replay buffer: step 27236, counter 435170\n",
      "Environment 5: Episode 6572, Score -120.58895445158083, Avg_Score -120.4728953255976\n",
      "Adding trajectory to replay buffer: step 27248, counter 435211\n",
      "Environment 1: Episode 6573, Score -115.17294093060804, Avg_Score -120.40108102155418\n",
      "Adding trajectory to replay buffer: step 27251, counter 435257\n",
      "Environment 10: Episode 6574, Score -122.86166094269541, Avg_Score -120.40794733484385\n",
      "Adding trajectory to replay buffer: step 27252, counter 435304\n",
      "Environment 2: Episode 6575, Score -123.57475669968377, Avg_Score -120.48372464102358\n",
      "Adding trajectory to replay buffer: step 27252, counter 435378\n",
      "Environment 3: Episode 6576, Score -114.81274416032123, Avg_Score -120.40618141283862\n",
      "Adding trajectory to replay buffer: step 27252, counter 435442\n",
      "Environment 14: Episode 6577, Score -112.95120475598114, Avg_Score -120.30925447574631\n",
      "Adding trajectory to replay buffer: step 27253, counter 435490\n",
      "Environment 9: Episode 6578, Score -122.18131589848362, Avg_Score -120.34463491364008\n",
      "Adding trajectory to replay buffer: step 27256, counter 435535\n",
      "Environment 11: Episode 6579, Score -121.46264891159224, Avg_Score -120.38562475727277\n",
      "Adding trajectory to replay buffer: step 27274, counter 435576\n",
      "Environment 8: Episode 6580, Score -117.57142243971738, Avg_Score -120.37264059153976\n",
      "Adding trajectory to replay buffer: step 27278, counter 435621\n",
      "Environment 4: Episode 6581, Score -117.6500537664406, Avg_Score -120.39913730845197\n",
      "Adding trajectory to replay buffer: step 27280, counter 435772\n",
      "Environment 13: Episode 6582, Score -117.29600840872651, Avg_Score -120.34720961278907\n",
      "Adding trajectory to replay buffer: step 27282, counter 435820\n",
      "Environment 15: Episode 6583, Score -116.58948166492767, Avg_Score -120.35820674268682\n",
      "Adding trajectory to replay buffer: step 27284, counter 435868\n",
      "Environment 5: Episode 6584, Score -122.29003931154497, Avg_Score -120.35170338603001\n",
      "Adding trajectory to replay buffer: step 27300, counter 435920\n",
      "Environment 1: Episode 6585, Score -117.8259237353808, Avg_Score -120.36500920101706\n",
      "Adding trajectory to replay buffer: step 27321, counter 435989\n",
      "Environment 2: Episode 6586, Score -113.85995344664903, Avg_Score -120.28088449645716\n",
      "Adding trajectory to replay buffer: step 27322, counter 436060\n",
      "Environment 10: Episode 6587, Score -118.03114182327502, Avg_Score -120.28562265011274\n",
      "Adding trajectory to replay buffer: step 27325, counter 436101\n",
      "Environment 5: Episode 6588, Score -115.12010898837075, Avg_Score -120.20740419660946\n",
      "Adding trajectory to replay buffer: step 27325, counter 436146\n",
      "Environment 13: Episode 6589, Score -122.57566440537448, Avg_Score -120.08941873164909\n",
      "Adding trajectory to replay buffer: step 27332, counter 436252\n",
      "Environment 12: Episode 6590, Score -126.19536090087519, Avg_Score -120.12347230949794\n",
      "Adding trajectory to replay buffer: step 27334, counter 436334\n",
      "Environment 14: Episode 6591, Score -120.0446640014009, Avg_Score -120.09727362328204\n",
      "Adding trajectory to replay buffer: step 27336, counter 436495\n",
      "Environment 0: Episode 6592, Score -116.38919340803226, Avg_Score -120.02920036522184\n",
      "Adding trajectory to replay buffer: step 27339, counter 436582\n",
      "Environment 3: Episode 6593, Score -121.27669535313672, Avg_Score -120.06648279593483\n",
      "Adding trajectory to replay buffer: step 27340, counter 436644\n",
      "Environment 4: Episode 6594, Score -116.46698079147129, Avg_Score -120.10125541334233\n",
      "Adding trajectory to replay buffer: step 27343, counter 436687\n",
      "Environment 1: Episode 6595, Score -122.38101878127705, Avg_Score -120.2071378572504\n",
      "Adding trajectory to replay buffer: step 27345, counter 436955\n",
      "Environment 7: Episode 6596, Score -130.13479882238803, Avg_Score -120.28513114670491\n",
      "Adding trajectory to replay buffer: step 27353, counter 437026\n",
      "Environment 15: Episode 6597, Score -113.19758485732912, Avg_Score -120.2203333443166\n",
      "Adding trajectory to replay buffer: step 27369, counter 437070\n",
      "Environment 5: Episode 6598, Score -117.55191046713665, Avg_Score -120.22341176085126\n",
      "Adding trajectory to replay buffer: step 27369, counter 437117\n",
      "Environment 10: Episode 6599, Score -121.2885714357961, Avg_Score -120.21692601353975\n",
      "Adding trajectory to replay buffer: step 27374, counter 437159\n",
      "Environment 12: Episode 6600, Score -116.07273213114155, Avg_Score -120.1954603619908\n",
      "Adding trajectory to replay buffer: step 27382, counter 437205\n",
      "Environment 0: Episode 6601, Score -117.7233241656553, Avg_Score -120.1481525396661\n",
      "Adding trajectory to replay buffer: step 27383, counter 437249\n",
      "Environment 3: Episode 6602, Score -122.3065053211879, Avg_Score -120.1341240481143\n",
      "Adding trajectory to replay buffer: step 27387, counter 437293\n",
      "Environment 1: Episode 6603, Score -122.03690152393095, Avg_Score -120.12570781969113\n",
      "Adding trajectory to replay buffer: step 27387, counter 437406\n",
      "Environment 8: Episode 6604, Score -126.15132839485207, Avg_Score -120.24436936853797\n",
      "Adding trajectory to replay buffer: step 27388, counter 437454\n",
      "Environment 4: Episode 6605, Score -119.12810253308403, Avg_Score -120.20559284491893\n",
      "Adding trajectory to replay buffer: step 27391, counter 437500\n",
      "Environment 7: Episode 6606, Score -122.09557282403422, Avg_Score -120.20619797704741\n",
      "Adding trajectory to replay buffer: step 27391, counter 437557\n",
      "Environment 14: Episode 6607, Score -121.71710279383572, Avg_Score -120.19481978305471\n",
      "Adding trajectory to replay buffer: step 27393, counter 437625\n",
      "Environment 13: Episode 6608, Score -118.01328072548348, Avg_Score -120.16790247484712\n",
      "Adding trajectory to replay buffer: step 27413, counter 437810\n",
      "Environment 6: Episode 6609, Score -133.45812763465196, Avg_Score -120.33744328774887\n",
      "Adding trajectory to replay buffer: step 27419, counter 437855\n",
      "Environment 12: Episode 6610, Score -121.46492776125794, Avg_Score -120.34112159254066\n",
      "Adding trajectory to replay buffer: step 27426, counter 437898\n",
      "Environment 3: Episode 6611, Score -123.23733030499828, Avg_Score -120.34934825008423\n",
      "Adding trajectory to replay buffer: step 27431, counter 437942\n",
      "Environment 1: Episode 6612, Score -121.84696320283227, Avg_Score -120.35091084641357\n",
      "Adding trajectory to replay buffer: step 27431, counter 437985\n",
      "Environment 4: Episode 6613, Score -121.17761607788194, Avg_Score -120.37512014888222\n",
      "Adding trajectory to replay buffer: step 27431, counter 438029\n",
      "Environment 8: Episode 6614, Score -122.70146546862368, Avg_Score -120.35500414530522\n",
      "Adding trajectory to replay buffer: step 27433, counter 438109\n",
      "Environment 15: Episode 6615, Score -113.20912211656757, Avg_Score -120.32699848729061\n",
      "Adding trajectory to replay buffer: step 27434, counter 438174\n",
      "Environment 10: Episode 6616, Score -118.18867557773055, Avg_Score -120.29765307490004\n",
      "Adding trajectory to replay buffer: step 27436, counter 438219\n",
      "Environment 14: Episode 6617, Score -122.19496849736075, Avg_Score -120.35779918462453\n",
      "Adding trajectory to replay buffer: step 27437, counter 438265\n",
      "Environment 7: Episode 6618, Score -121.93854847735106, Avg_Score -120.36753498436742\n",
      "Adding trajectory to replay buffer: step 27444, counter 438340\n",
      "Environment 5: Episode 6619, Score -120.9057914334576, Avg_Score -120.34080316884125\n",
      "Adding trajectory to replay buffer: step 27454, counter 438473\n",
      "Environment 2: Episode 6620, Score -115.80811660309706, Avg_Score -120.28032727913724\n",
      "Adding trajectory to replay buffer: step 27459, counter 438679\n",
      "Environment 9: Episode 6621, Score -125.17686380192701, Avg_Score -120.3318824257672\n",
      "Adding trajectory to replay buffer: step 27475, counter 438772\n",
      "Environment 0: Episode 6622, Score -114.37080683325044, Avg_Score -120.25107254876195\n",
      "Adding trajectory to replay buffer: step 27476, counter 438817\n",
      "Environment 1: Episode 6623, Score -122.70975733303216, Avg_Score -120.36708529077396\n",
      "Adding trajectory to replay buffer: step 27476, counter 439037\n",
      "Environment 11: Episode 6624, Score -120.95846227036357, Avg_Score -120.43883954708264\n",
      "Adding trajectory to replay buffer: step 27477, counter 439080\n",
      "Environment 10: Episode 6625, Score -123.0151611986806, Avg_Score -120.50808960203948\n",
      "Adding trajectory to replay buffer: step 27479, counter 439123\n",
      "Environment 14: Episode 6626, Score -122.25352381841341, Avg_Score -120.50837069322712\n",
      "Adding trajectory to replay buffer: step 27480, counter 439166\n",
      "Environment 7: Episode 6627, Score -122.30597291025396, Avg_Score -120.49911617321347\n",
      "Adding trajectory to replay buffer: step 27480, counter 439215\n",
      "Environment 8: Episode 6628, Score -120.82675753111765, Avg_Score -120.47290029936192\n",
      "Adding trajectory to replay buffer: step 27480, counter 439262\n",
      "Environment 15: Episode 6629, Score -120.11883961058223, Avg_Score -120.46836949082316\n",
      "Adding trajectory to replay buffer: step 27484, counter 439327\n",
      "Environment 12: Episode 6630, Score -117.35280875798719, Avg_Score -120.42638898110832\n",
      "Adding trajectory to replay buffer: step 27492, counter 439375\n",
      "Environment 5: Episode 6631, Score -122.42596774206746, Avg_Score -120.47641952670111\n",
      "Adding trajectory to replay buffer: step 27497, counter 439441\n",
      "Environment 4: Episode 6632, Score -117.08519807636365, Avg_Score -120.54087651094416\n",
      "Adding trajectory to replay buffer: step 27499, counter 439486\n",
      "Environment 2: Episode 6633, Score -121.52835108308649, Avg_Score -120.57392640587872\n",
      "Adding trajectory to replay buffer: step 27504, counter 439531\n",
      "Environment 9: Episode 6634, Score -122.26004965539028, Avg_Score -120.56709384322441\n",
      "Adding trajectory to replay buffer: step 27519, counter 439574\n",
      "Environment 1: Episode 6635, Score -123.14511840412703, Avg_Score -120.5679527505852\n",
      "Adding trajectory to replay buffer: step 27522, counter 439619\n",
      "Environment 10: Episode 6636, Score -122.86655505453237, Avg_Score -120.56580552359219\n",
      "Adding trajectory to replay buffer: step 27526, counter 439670\n",
      "Environment 0: Episode 6637, Score -117.48718859045208, Avg_Score -120.6064433293479\n",
      "Adding trajectory to replay buffer: step 27526, counter 439717\n",
      "Environment 14: Episode 6638, Score -123.9717685368055, Avg_Score -120.65227129266424\n",
      "Adding trajectory to replay buffer: step 27527, counter 439764\n",
      "Environment 8: Episode 6639, Score -120.13039508279165, Avg_Score -120.62194424124988\n",
      "Adding trajectory to replay buffer: step 27528, counter 439808\n",
      "Environment 12: Episode 6640, Score -122.99056055909892, Avg_Score -120.6331898275973\n",
      "Adding trajectory to replay buffer: step 27534, counter 439916\n",
      "Environment 3: Episode 6641, Score -114.12047810435078, Avg_Score -120.59935483034391\n",
      "Adding trajectory to replay buffer: step 27541, counter 439960\n",
      "Environment 4: Episode 6642, Score -117.58004331516227, Avg_Score -120.55367087909325\n",
      "Adding trajectory to replay buffer: step 27546, counter 440007\n",
      "Environment 2: Episode 6643, Score -122.27464383058933, Avg_Score -120.55923232921694\n",
      "Adding trajectory to replay buffer: step 27548, counter 440075\n",
      "Environment 7: Episode 6644, Score -118.17179193452435, Avg_Score -120.5282862170452\n",
      "Adding trajectory to replay buffer: step 27549, counter 440120\n",
      "Environment 9: Episode 6645, Score -121.00960753886837, Avg_Score -120.50118059366461\n",
      "Adding trajectory to replay buffer: step 27562, counter 440163\n",
      "Environment 1: Episode 6646, Score -123.88219082844506, Avg_Score -120.5187458872307\n",
      "Adding trajectory to replay buffer: step 27565, counter 440206\n",
      "Environment 10: Episode 6647, Score -123.65866669168261, Avg_Score -120.52186649946314\n",
      "Adding trajectory to replay buffer: step 27569, counter 440249\n",
      "Environment 0: Episode 6648, Score -122.48944293296027, Avg_Score -120.56912622093341\n",
      "Adding trajectory to replay buffer: step 27570, counter 440406\n",
      "Environment 6: Episode 6649, Score -115.76459495182833, Avg_Score -120.50175099751809\n",
      "Adding trajectory to replay buffer: step 27575, counter 440505\n",
      "Environment 11: Episode 6650, Score -125.6692128529412, Avg_Score -120.53278097663755\n",
      "Adding trajectory to replay buffer: step 27575, counter 440552\n",
      "Environment 12: Episode 6651, Score -120.13264405770227, Avg_Score -120.54501418365692\n",
      "Adding trajectory to replay buffer: step 27585, counter 440611\n",
      "Environment 14: Episode 6652, Score -112.03634480704056, Avg_Score -120.43230867570367\n",
      "Adding trajectory to replay buffer: step 27587, counter 440657\n",
      "Environment 4: Episode 6653, Score -117.25825739549288, Avg_Score -120.36345845446198\n",
      "Adding trajectory to replay buffer: step 27592, counter 440703\n",
      "Environment 2: Episode 6654, Score -120.12090579487135, Avg_Score -120.33674049607426\n",
      "Adding trajectory to replay buffer: step 27592, counter 440747\n",
      "Environment 7: Episode 6655, Score -122.60287430628885, Avg_Score -120.35403210504202\n",
      "Adding trajectory to replay buffer: step 27596, counter 440794\n",
      "Environment 9: Episode 6656, Score -121.11981282905613, Avg_Score -120.39105603518324\n",
      "Adding trajectory to replay buffer: step 27600, counter 440867\n",
      "Environment 8: Episode 6657, Score -112.6783962256393, Avg_Score -120.29637059314081\n",
      "Adding trajectory to replay buffer: step 27604, counter 441078\n",
      "Environment 13: Episode 6658, Score -121.00148447299065, Avg_Score -120.29099893162557\n",
      "Adding trajectory to replay buffer: step 27608, counter 441124\n",
      "Environment 1: Episode 6659, Score -120.74611695042067, Avg_Score -120.27344398854359\n",
      "Adding trajectory to replay buffer: step 27609, counter 441168\n",
      "Environment 10: Episode 6660, Score -122.22965695731156, Avg_Score -120.27428643542382\n",
      "Adding trajectory to replay buffer: step 27620, counter 441213\n",
      "Environment 11: Episode 6661, Score -121.60560153730276, Avg_Score -120.3132298751401\n",
      "Adding trajectory to replay buffer: step 27620, counter 441258\n",
      "Environment 12: Episode 6662, Score -121.86781444290591, Avg_Score -120.3076021158338\n",
      "Adding trajectory to replay buffer: step 27629, counter 441300\n",
      "Environment 4: Episode 6663, Score -119.03399330534351, Avg_Score -120.27266794107982\n",
      "Adding trajectory to replay buffer: step 27629, counter 441344\n",
      "Environment 14: Episode 6664, Score -122.32034497544232, Avg_Score -120.27104398155682\n",
      "Adding trajectory to replay buffer: step 27631, counter 441405\n",
      "Environment 6: Episode 6665, Score -116.5860620609615, Avg_Score -120.16611036106923\n",
      "Adding trajectory to replay buffer: step 27636, counter 441449\n",
      "Environment 7: Episode 6666, Score -121.50898600886204, Avg_Score -120.15489690458479\n",
      "Adding trajectory to replay buffer: step 27640, counter 441497\n",
      "Environment 2: Episode 6667, Score -121.2448220644022, Avg_Score -120.21439535571835\n",
      "Adding trajectory to replay buffer: step 27641, counter 441542\n",
      "Environment 9: Episode 6668, Score -123.51208851430627, Avg_Score -120.21985330295513\n",
      "Adding trajectory to replay buffer: step 27645, counter 441587\n",
      "Environment 8: Episode 6669, Score -123.01731588729626, Avg_Score -120.23251674350414\n",
      "Adding trajectory to replay buffer: step 27653, counter 441631\n",
      "Environment 10: Episode 6670, Score -122.2651842108549, Avg_Score -120.23333535063533\n",
      "Adding trajectory to replay buffer: step 27653, counter 441804\n",
      "Environment 15: Episode 6671, Score -121.33386175351092, Avg_Score -120.3054265551889\n",
      "Adding trajectory to replay buffer: step 27664, counter 441848\n",
      "Environment 11: Episode 6672, Score -116.46679137762823, Avg_Score -120.2642049244494\n",
      "Adding trajectory to replay buffer: step 27665, counter 441893\n",
      "Environment 12: Episode 6673, Score -118.66925404129239, Avg_Score -120.29916805555624\n",
      "Adding trajectory to replay buffer: step 27667, counter 441956\n",
      "Environment 13: Episode 6674, Score -115.8687387032155, Avg_Score -120.22923883316145\n",
      "Adding trajectory to replay buffer: step 27675, counter 442000\n",
      "Environment 6: Episode 6675, Score -122.28761162303016, Avg_Score -120.21636738239494\n",
      "Adding trajectory to replay buffer: step 27679, counter 442043\n",
      "Environment 7: Episode 6676, Score -122.17362174392056, Avg_Score -120.28997615823093\n",
      "Adding trajectory to replay buffer: step 27681, counter 442095\n",
      "Environment 14: Episode 6677, Score -116.61685502746329, Avg_Score -120.32663266094576\n",
      "Adding trajectory to replay buffer: step 27686, counter 442141\n",
      "Environment 2: Episode 6678, Score -121.78049097199055, Avg_Score -120.32262441168082\n",
      "Adding trajectory to replay buffer: step 27696, counter 442208\n",
      "Environment 4: Episode 6679, Score -115.99028176918998, Avg_Score -120.2679007402568\n",
      "Adding trajectory to replay buffer: step 27700, counter 442255\n",
      "Environment 15: Episode 6680, Score -119.98579616134428, Avg_Score -120.29204447747308\n",
      "Adding trajectory to replay buffer: step 27707, counter 442317\n",
      "Environment 8: Episode 6681, Score -112.43205743162768, Avg_Score -120.23986451412496\n",
      "Adding trajectory to replay buffer: step 27710, counter 442363\n",
      "Environment 11: Episode 6682, Score -121.05462747354868, Avg_Score -120.27745070477316\n",
      "Adding trajectory to replay buffer: step 27713, counter 442411\n",
      "Environment 12: Episode 6683, Score -115.84676021191788, Avg_Score -120.27002349024308\n",
      "Adding trajectory to replay buffer: step 27713, counter 442457\n",
      "Environment 13: Episode 6684, Score -121.01018030745226, Avg_Score -120.25722490020215\n",
      "Adding trajectory to replay buffer: step 27717, counter 442640\n",
      "Environment 3: Episode 6685, Score -121.80909707249745, Avg_Score -120.29705663357326\n",
      "Adding trajectory to replay buffer: step 27724, counter 442795\n",
      "Environment 0: Episode 6686, Score -118.25710089382963, Avg_Score -120.34102810804508\n",
      "Adding trajectory to replay buffer: step 27725, counter 442839\n",
      "Environment 14: Episode 6687, Score -121.55970298574368, Avg_Score -120.37631371966978\n",
      "Adding trajectory to replay buffer: step 27728, counter 443075\n",
      "Environment 5: Episode 6688, Score -123.43166121742871, Avg_Score -120.45942924196036\n",
      "Adding trajectory to replay buffer: step 27732, counter 443121\n",
      "Environment 2: Episode 6689, Score -121.17127719779809, Avg_Score -120.44538536988459\n",
      "Adding trajectory to replay buffer: step 27734, counter 443202\n",
      "Environment 10: Episode 6690, Score -114.15803309960106, Avg_Score -120.32501209187186\n",
      "Adding trajectory to replay buffer: step 27735, counter 443262\n",
      "Environment 6: Episode 6691, Score -115.42103326169153, Avg_Score -120.27877578447479\n",
      "Adding trajectory to replay buffer: step 27747, counter 443368\n",
      "Environment 9: Episode 6692, Score -123.9348253153905, Avg_Score -120.35423210354834\n",
      "Adding trajectory to replay buffer: step 27749, counter 443421\n",
      "Environment 4: Episode 6693, Score -118.36307239071962, Avg_Score -120.32509587392414\n",
      "Adding trajectory to replay buffer: step 27755, counter 443469\n",
      "Environment 8: Episode 6694, Score -114.63186681082658, Avg_Score -120.30674473411773\n",
      "Adding trajectory to replay buffer: step 27757, counter 443516\n",
      "Environment 11: Episode 6695, Score -122.11872138691632, Avg_Score -120.3041217601741\n",
      "Adding trajectory to replay buffer: step 27761, counter 443564\n",
      "Environment 13: Episode 6696, Score -120.59410223802676, Avg_Score -120.2087147943305\n",
      "Adding trajectory to replay buffer: step 27762, counter 443613\n",
      "Environment 12: Episode 6697, Score -116.95436424965473, Avg_Score -120.24628258825375\n",
      "Adding trajectory to replay buffer: step 27771, counter 443660\n",
      "Environment 0: Episode 6698, Score -119.5521152061913, Avg_Score -120.2662846356443\n",
      "Adding trajectory to replay buffer: step 27771, counter 443714\n",
      "Environment 3: Episode 6699, Score -112.17995130970453, Avg_Score -120.17519843438339\n",
      "Adding trajectory to replay buffer: step 27771, counter 443760\n",
      "Environment 14: Episode 6700, Score -121.63745850539703, Avg_Score -120.23084569812592\n",
      "Adding trajectory to replay buffer: step 27781, counter 443806\n",
      "Environment 6: Episode 6701, Score -121.71486105625891, Avg_Score -120.27076106703194\n",
      "Adding trajectory to replay buffer: step 27794, counter 443853\n",
      "Environment 9: Episode 6702, Score -121.8258939709707, Avg_Score -120.26595495352979\n",
      "Adding trajectory to replay buffer: step 27795, counter 443899\n",
      "Environment 4: Episode 6703, Score -122.49677442790583, Avg_Score -120.27055368256951\n",
      "Adding trajectory to replay buffer: step 27798, counter 443942\n",
      "Environment 8: Episode 6704, Score -121.15547550168571, Avg_Score -120.22059515363786\n",
      "Adding trajectory to replay buffer: step 27799, counter 443984\n",
      "Environment 11: Episode 6705, Score -123.17021819989012, Avg_Score -120.26101631030592\n",
      "Adding trajectory to replay buffer: step 27801, counter 444051\n",
      "Environment 10: Episode 6706, Score -112.88942154685222, Avg_Score -120.1689547975341\n",
      "Adding trajectory to replay buffer: step 27805, counter 444095\n",
      "Environment 13: Episode 6707, Score -121.39157717674598, Avg_Score -120.1656995413632\n",
      "Adding trajectory to replay buffer: step 27810, counter 444297\n",
      "Environment 1: Episode 6708, Score -119.13460695896237, Avg_Score -120.17691280369796\n",
      "Adding trajectory to replay buffer: step 27816, counter 444342\n",
      "Environment 3: Episode 6709, Score -122.77811990194706, Avg_Score -120.0701127263709\n",
      "Adding trajectory to replay buffer: step 27816, counter 444387\n",
      "Environment 14: Episode 6710, Score -122.08488875350139, Avg_Score -120.07631233629334\n",
      "Adding trajectory to replay buffer: step 27817, counter 444433\n",
      "Environment 0: Episode 6711, Score -121.05862643142355, Avg_Score -120.05452529755762\n",
      "Adding trajectory to replay buffer: step 27825, counter 444477\n",
      "Environment 6: Episode 6712, Score -122.44169826146961, Avg_Score -120.06047264814397\n",
      "Adding trajectory to replay buffer: step 27839, counter 444521\n",
      "Environment 4: Episode 6713, Score -121.98350367336782, Avg_Score -120.06853152409883\n",
      "Adding trajectory to replay buffer: step 27839, counter 444566\n",
      "Environment 9: Episode 6714, Score -118.01074707044289, Avg_Score -120.02162434011701\n",
      "Adding trajectory to replay buffer: step 27841, counter 444609\n",
      "Environment 8: Episode 6715, Score -115.65090574580928, Avg_Score -120.04604217640946\n",
      "Adding trajectory to replay buffer: step 27848, counter 444778\n",
      "Environment 7: Episode 6716, Score -119.4647977960744, Avg_Score -120.05880339859289\n",
      "Adding trajectory to replay buffer: step 27852, counter 444825\n",
      "Environment 13: Episode 6717, Score -119.14856152354554, Avg_Score -120.02833932885473\n",
      "Adding trajectory to replay buffer: step 27854, counter 444869\n",
      "Environment 1: Episode 6718, Score -122.51417263483754, Avg_Score -120.03409557042959\n",
      "Adding trajectory to replay buffer: step 27855, counter 444925\n",
      "Environment 11: Episode 6719, Score -112.14485221716328, Avg_Score -119.94648617826664\n",
      "Adding trajectory to replay buffer: step 27857, counter 445054\n",
      "Environment 5: Episode 6720, Score -127.27679444448555, Avg_Score -120.06117295668051\n",
      "Adding trajectory to replay buffer: step 27862, counter 445099\n",
      "Environment 0: Episode 6721, Score -121.34643757262702, Avg_Score -120.02286869438754\n",
      "Adding trajectory to replay buffer: step 27871, counter 445154\n",
      "Environment 3: Episode 6722, Score -118.54553334016353, Avg_Score -120.06461595945667\n",
      "Adding trajectory to replay buffer: step 27873, counter 445226\n",
      "Environment 10: Episode 6723, Score -116.95857824377032, Avg_Score -120.00710416856406\n",
      "Adding trajectory to replay buffer: step 27875, counter 445276\n",
      "Environment 6: Episode 6724, Score -120.85335031445884, Avg_Score -120.00605304900502\n",
      "Adding trajectory to replay buffer: step 27877, counter 445453\n",
      "Environment 15: Episode 6725, Score -122.48699117071943, Avg_Score -120.0007713487254\n",
      "Adding trajectory to replay buffer: step 27880, counter 445517\n",
      "Environment 14: Episode 6726, Score -117.18672437717144, Avg_Score -119.95010335431297\n",
      "Adding trajectory to replay buffer: step 27891, counter 445560\n",
      "Environment 7: Episode 6727, Score -122.8105012292421, Avg_Score -119.95514863750286\n",
      "Adding trajectory to replay buffer: step 27896, counter 445604\n",
      "Environment 13: Episode 6728, Score -123.27408880348814, Avg_Score -119.97962195022657\n",
      "Adding trajectory to replay buffer: step 27900, counter 445650\n",
      "Environment 1: Episode 6729, Score -121.17394748450258, Avg_Score -119.99017302896574\n",
      "Adding trajectory to replay buffer: step 27903, counter 445698\n",
      "Environment 11: Episode 6730, Score -119.12072256102537, Avg_Score -120.00785216699613\n",
      "Adding trajectory to replay buffer: step 27906, counter 445742\n",
      "Environment 0: Episode 6731, Score -116.71734429401532, Avg_Score -119.95076593251561\n",
      "Adding trajectory to replay buffer: step 27911, counter 445814\n",
      "Environment 4: Episode 6732, Score -115.0005608906094, Avg_Score -119.92991956065808\n",
      "Adding trajectory to replay buffer: step 27915, counter 445858\n",
      "Environment 3: Episode 6733, Score -121.84259665683346, Avg_Score -119.93306201639554\n",
      "Adding trajectory to replay buffer: step 27917, counter 445902\n",
      "Environment 10: Episode 6734, Score -123.50998234513216, Avg_Score -119.94556134329298\n",
      "Adding trajectory to replay buffer: step 27919, counter 445946\n",
      "Environment 6: Episode 6735, Score -122.37983198518927, Avg_Score -119.93790847910357\n",
      "Adding trajectory to replay buffer: step 27921, counter 445990\n",
      "Environment 15: Episode 6736, Score -118.2370522619405, Avg_Score -119.89161345117765\n",
      "Adding trajectory to replay buffer: step 27923, counter 446033\n",
      "Environment 14: Episode 6737, Score -122.6090083315565, Avg_Score -119.9428316485887\n",
      "Adding trajectory to replay buffer: step 27932, counter 446126\n",
      "Environment 9: Episode 6738, Score -116.77553541506019, Avg_Score -119.87086931737123\n",
      "Adding trajectory to replay buffer: step 27937, counter 446206\n",
      "Environment 5: Episode 6739, Score -118.02691555990837, Avg_Score -119.84983452214243\n",
      "Adding trajectory to replay buffer: step 27939, counter 446383\n",
      "Environment 12: Episode 6740, Score -117.22711926340435, Avg_Score -119.7922001091855\n",
      "Adding trajectory to replay buffer: step 27943, counter 446430\n",
      "Environment 13: Episode 6741, Score -122.69029663421836, Avg_Score -119.87789829448417\n",
      "Adding trajectory to replay buffer: step 27950, counter 446474\n",
      "Environment 0: Episode 6742, Score -122.4173638435416, Avg_Score -119.92627149976795\n",
      "Adding trajectory to replay buffer: step 27952, counter 446523\n",
      "Environment 11: Episode 6743, Score -113.95092300419944, Avg_Score -119.84303429150407\n",
      "Adding trajectory to replay buffer: step 27955, counter 446567\n",
      "Environment 4: Episode 6744, Score -122.81768694816219, Avg_Score -119.88949324164045\n",
      "Adding trajectory to replay buffer: step 27958, counter 446610\n",
      "Environment 3: Episode 6745, Score -122.55923916030676, Avg_Score -119.90498955785482\n",
      "Adding trajectory to replay buffer: step 27962, counter 446655\n",
      "Environment 10: Episode 6746, Score -122.00660044203823, Avg_Score -119.88623365399077\n",
      "Adding trajectory to replay buffer: step 27964, counter 446698\n",
      "Environment 15: Episode 6747, Score -122.63375211920032, Avg_Score -119.87598450826592\n",
      "Adding trajectory to replay buffer: step 27969, counter 446744\n",
      "Environment 14: Episode 6748, Score -119.60689083978471, Avg_Score -119.84715898733418\n",
      "Adding trajectory to replay buffer: step 27977, counter 446789\n",
      "Environment 9: Episode 6749, Score -121.82036468191632, Avg_Score -119.90771668463505\n",
      "Adding trajectory to replay buffer: step 27979, counter 447036\n",
      "Environment 2: Episode 6750, Score -124.9090292019583, Avg_Score -119.90011484812526\n",
      "Adding trajectory to replay buffer: step 27994, counter 447080\n",
      "Environment 0: Episode 6751, Score -123.16721073594752, Avg_Score -119.9304605149077\n",
      "Adding trajectory to replay buffer: step 27996, counter 447124\n",
      "Environment 11: Episode 6752, Score -122.43991913676386, Avg_Score -120.03449625820492\n",
      "Adding trajectory to replay buffer: step 28000, counter 447283\n",
      "Environment 8: Episode 6753, Score -114.83127901667356, Avg_Score -120.01022647441673\n",
      "Adding trajectory to replay buffer: step 28003, counter 447331\n",
      "Environment 4: Episode 6754, Score -116.64133411353765, Avg_Score -119.9754307576034\n",
      "Adding trajectory to replay buffer: step 28005, counter 447397\n",
      "Environment 12: Episode 6755, Score -117.43088695813901, Avg_Score -119.9237108841219\n",
      "Adding trajectory to replay buffer: step 28010, counter 447445\n",
      "Environment 10: Episode 6756, Score -119.53656157598894, Avg_Score -119.90787837159124\n",
      "Adding trajectory to replay buffer: step 28012, counter 447557\n",
      "Environment 1: Episode 6757, Score -113.01579044997072, Avg_Score -119.91125231383452\n",
      "Adding trajectory to replay buffer: step 28012, counter 447611\n",
      "Environment 3: Episode 6758, Score -120.67710376891928, Avg_Score -119.90800850679383\n",
      "Adding trajectory to replay buffer: step 28012, counter 447654\n",
      "Environment 14: Episode 6759, Score -123.23331776489131, Avg_Score -119.93288051493853\n",
      "Adding trajectory to replay buffer: step 28020, counter 447697\n",
      "Environment 9: Episode 6760, Score -122.9845604543065, Avg_Score -119.94042954990847\n",
      "Adding trajectory to replay buffer: step 28023, counter 447741\n",
      "Environment 2: Episode 6761, Score -123.18791232351711, Avg_Score -119.95625265777062\n",
      "Adding trajectory to replay buffer: step 28024, counter 447828\n",
      "Environment 5: Episode 6762, Score -112.24473470850599, Avg_Score -119.8600218604266\n",
      "Adding trajectory to replay buffer: step 28038, counter 447872\n",
      "Environment 0: Episode 6763, Score -123.60746290773525, Avg_Score -119.90575655645056\n",
      "Adding trajectory to replay buffer: step 28038, counter 447914\n",
      "Environment 11: Episode 6764, Score -115.54210257037542, Avg_Score -119.83797413239989\n",
      "Adding trajectory to replay buffer: step 28042, counter 447992\n",
      "Environment 15: Episode 6765, Score -118.01163648831533, Avg_Score -119.8522298766734\n",
      "Adding trajectory to replay buffer: step 28045, counter 448037\n",
      "Environment 8: Episode 6766, Score -121.10733213674277, Avg_Score -119.84821333795225\n",
      "Adding trajectory to replay buffer: step 28047, counter 448081\n",
      "Environment 4: Episode 6767, Score -121.703339495377, Avg_Score -119.85279851226198\n",
      "Adding trajectory to replay buffer: step 28053, counter 448124\n",
      "Environment 10: Episode 6768, Score -123.08585591506213, Avg_Score -119.84853618626956\n",
      "Adding trajectory to replay buffer: step 28054, counter 448235\n",
      "Environment 13: Episode 6769, Score -128.33203385635076, Avg_Score -119.90168336596008\n",
      "Adding trajectory to replay buffer: step 28055, counter 448278\n",
      "Environment 1: Episode 6770, Score -122.93990990799976, Avg_Score -119.90843062293156\n",
      "Adding trajectory to replay buffer: step 28055, counter 448321\n",
      "Environment 3: Episode 6771, Score -123.0773161905141, Avg_Score -119.92586516730158\n",
      "Adding trajectory to replay buffer: step 28055, counter 448364\n",
      "Environment 14: Episode 6772, Score -123.00476075571419, Avg_Score -119.9912448610824\n",
      "Adding trajectory to replay buffer: step 28062, counter 448406\n",
      "Environment 9: Episode 6773, Score -122.82269156192555, Avg_Score -120.03277923628873\n",
      "Adding trajectory to replay buffer: step 28063, counter 448550\n",
      "Environment 6: Episode 6774, Score -118.01400835652308, Avg_Score -120.05423193282182\n",
      "Adding trajectory to replay buffer: step 28069, counter 448595\n",
      "Environment 5: Episode 6775, Score -121.29223115902766, Avg_Score -120.0442781281818\n",
      "Adding trajectory to replay buffer: step 28083, counter 448640\n",
      "Environment 0: Episode 6776, Score -122.4576252307923, Avg_Score -120.04711816305053\n",
      "Adding trajectory to replay buffer: step 28088, counter 448686\n",
      "Environment 15: Episode 6777, Score -115.60256639523742, Avg_Score -120.03697527672826\n",
      "Adding trajectory to replay buffer: step 28089, counter 448730\n",
      "Environment 8: Episode 6778, Score -121.5001244642095, Avg_Score -120.03417161165048\n",
      "Adding trajectory to replay buffer: step 28095, counter 448787\n",
      "Environment 11: Episode 6779, Score -113.51375177868145, Avg_Score -120.00940631174537\n",
      "Adding trajectory to replay buffer: step 28097, counter 448829\n",
      "Environment 3: Episode 6780, Score -114.87416853438008, Avg_Score -119.95829003547573\n",
      "Adding trajectory to replay buffer: step 28098, counter 448873\n",
      "Environment 13: Episode 6781, Score -120.96214297352499, Avg_Score -120.0435908908947\n",
      "Adding trajectory to replay buffer: step 28099, counter 448917\n",
      "Environment 14: Episode 6782, Score -121.48776715018165, Avg_Score -120.04792228766102\n",
      "Adding trajectory to replay buffer: step 28100, counter 448962\n",
      "Environment 1: Episode 6783, Score -121.47222954614895, Avg_Score -120.10417698100333\n",
      "Adding trajectory to replay buffer: step 28101, counter 449010\n",
      "Environment 10: Episode 6784, Score -120.5589094870879, Avg_Score -120.09966427279969\n",
      "Adding trajectory to replay buffer: step 28108, counter 449056\n",
      "Environment 9: Episode 6785, Score -119.42784149200097, Avg_Score -120.07585171699473\n",
      "Adding trajectory to replay buffer: step 28109, counter 449118\n",
      "Environment 4: Episode 6786, Score -115.85448594145042, Avg_Score -120.05182556747093\n",
      "Adding trajectory to replay buffer: step 28109, counter 449222\n",
      "Environment 12: Episode 6787, Score -118.09206110975829, Avg_Score -120.01714914871108\n",
      "Adding trajectory to replay buffer: step 28110, counter 449269\n",
      "Environment 6: Episode 6788, Score -119.6891174505013, Avg_Score -119.97972371104184\n",
      "Adding trajectory to replay buffer: step 28112, counter 449312\n",
      "Environment 5: Episode 6789, Score -122.68850263045108, Avg_Score -119.99489596536836\n",
      "Adding trajectory to replay buffer: step 28129, counter 449358\n",
      "Environment 0: Episode 6790, Score -119.54632773224947, Avg_Score -120.04877891169484\n",
      "Adding trajectory to replay buffer: step 28139, counter 449409\n",
      "Environment 15: Episode 6791, Score -117.2045870073357, Avg_Score -120.06661444915132\n",
      "Adding trajectory to replay buffer: step 28140, counter 449454\n",
      "Environment 11: Episode 6792, Score -120.77551456649415, Avg_Score -120.03502134166234\n",
      "Adding trajectory to replay buffer: step 28141, counter 449497\n",
      "Environment 13: Episode 6793, Score -116.25280924581116, Avg_Score -120.01391871021326\n",
      "Adding trajectory to replay buffer: step 28142, counter 449542\n",
      "Environment 3: Episode 6794, Score -120.13381090587067, Avg_Score -120.06893815116369\n",
      "Adding trajectory to replay buffer: step 28142, counter 449585\n",
      "Environment 14: Episode 6795, Score -121.84886771382206, Avg_Score -120.06623961443275\n",
      "Adding trajectory to replay buffer: step 28143, counter 449628\n",
      "Environment 1: Episode 6796, Score -122.05898146124743, Avg_Score -120.08088840666494\n",
      "Adding trajectory to replay buffer: step 28144, counter 449671\n",
      "Environment 10: Episode 6797, Score -121.82919494122329, Avg_Score -120.12963671358062\n",
      "Adding trajectory to replay buffer: step 28151, counter 449713\n",
      "Environment 4: Episode 6798, Score -123.48002909375789, Avg_Score -120.16891585245628\n",
      "Adding trajectory to replay buffer: step 28151, counter 449755\n",
      "Environment 12: Episode 6799, Score -122.53121837883816, Avg_Score -120.27242852314762\n",
      "Adding trajectory to replay buffer: step 28152, counter 449797\n",
      "Environment 6: Episode 6800, Score -122.64535026133868, Avg_Score -120.28250744070704\n",
      "Adding trajectory to replay buffer: step 28154, counter 450060\n",
      "Environment 7: Episode 6801, Score -130.45698451939288, Avg_Score -120.36992867533837\n",
      "Adding trajectory to replay buffer: step 28155, counter 450103\n",
      "Environment 5: Episode 6802, Score -122.87961838605628, Avg_Score -120.38046591948924\n",
      "Adding trajectory to replay buffer: step 28165, counter 450160\n",
      "Environment 9: Episode 6803, Score -111.89869548885544, Avg_Score -120.27448513009872\n",
      "Adding trajectory to replay buffer: step 28166, counter 450303\n",
      "Environment 2: Episode 6804, Score -118.60240159336726, Avg_Score -120.24895439101556\n",
      "Adding trajectory to replay buffer: step 28173, counter 450347\n",
      "Environment 0: Episode 6805, Score -121.85135958228508, Avg_Score -120.23576580483949\n",
      "Adding trajectory to replay buffer: step 28186, counter 450392\n",
      "Environment 13: Episode 6806, Score -122.89435212127678, Avg_Score -120.33581511058374\n",
      "Adding trajectory to replay buffer: step 28186, counter 450439\n",
      "Environment 15: Episode 6807, Score -122.98043866823615, Avg_Score -120.35170372549861\n",
      "Adding trajectory to replay buffer: step 28187, counter 450483\n",
      "Environment 1: Episode 6808, Score -121.93372846376649, Avg_Score -120.37969494054666\n",
      "Adding trajectory to replay buffer: step 28189, counter 450528\n",
      "Environment 10: Episode 6809, Score -122.18664668100391, Avg_Score -120.37378020833722\n",
      "Adding trajectory to replay buffer: step 28195, counter 450572\n",
      "Environment 12: Episode 6810, Score -123.10453401446986, Avg_Score -120.38397666094691\n",
      "Adding trajectory to replay buffer: step 28199, counter 450619\n",
      "Environment 6: Episode 6811, Score -118.7306920313388, Avg_Score -120.36069731694607\n",
      "Adding trajectory to replay buffer: step 28199, counter 450729\n",
      "Environment 8: Episode 6812, Score -123.72831771728583, Avg_Score -120.37356351150426\n",
      "Adding trajectory to replay buffer: step 28200, counter 450778\n",
      "Environment 4: Episode 6813, Score -120.1917384425414, Avg_Score -120.35564585919599\n",
      "Adding trajectory to replay buffer: step 28203, counter 450827\n",
      "Environment 7: Episode 6814, Score -116.7771474214637, Avg_Score -120.3433098627062\n",
      "Adding trajectory to replay buffer: step 28205, counter 450892\n",
      "Environment 11: Episode 6815, Score -115.6546081062102, Avg_Score -120.34334688631021\n",
      "Adding trajectory to replay buffer: step 28210, counter 450960\n",
      "Environment 14: Episode 6816, Score -117.35273508099715, Avg_Score -120.32222625915942\n",
      "Adding trajectory to replay buffer: step 28211, counter 451029\n",
      "Environment 3: Episode 6817, Score -113.51289734916917, Avg_Score -120.26586961741563\n",
      "Adding trajectory to replay buffer: step 28212, counter 451075\n",
      "Environment 2: Episode 6818, Score -121.1508969963522, Avg_Score -120.2522368610308\n",
      "Adding trajectory to replay buffer: step 28212, counter 451122\n",
      "Environment 9: Episode 6819, Score -122.06138889465356, Avg_Score -120.35140222780568\n",
      "Adding trajectory to replay buffer: step 28214, counter 451181\n",
      "Environment 5: Episode 6820, Score -115.85779745284407, Avg_Score -120.23721225788925\n",
      "Adding trajectory to replay buffer: step 28217, counter 451225\n",
      "Environment 0: Episode 6821, Score -119.41356022937907, Avg_Score -120.21788348445678\n",
      "Adding trajectory to replay buffer: step 28231, counter 451270\n",
      "Environment 13: Episode 6822, Score -121.41626646080552, Avg_Score -120.24659081566318\n",
      "Adding trajectory to replay buffer: step 28231, counter 451315\n",
      "Environment 15: Episode 6823, Score -122.37830850766414, Avg_Score -120.30078811830212\n",
      "Adding trajectory to replay buffer: step 28233, counter 451361\n",
      "Environment 1: Episode 6824, Score -119.43127766720951, Avg_Score -120.28656739182964\n",
      "Adding trajectory to replay buffer: step 28239, counter 451405\n",
      "Environment 12: Episode 6825, Score -123.01093496356594, Avg_Score -120.2918068297581\n",
      "Adding trajectory to replay buffer: step 28240, counter 451446\n",
      "Environment 8: Episode 6826, Score -117.17398297743802, Avg_Score -120.29167941576077\n",
      "Adding trajectory to replay buffer: step 28245, counter 451491\n",
      "Environment 4: Episode 6827, Score -122.4606205444044, Avg_Score -120.28818060891241\n",
      "Adding trajectory to replay buffer: step 28247, counter 451549\n",
      "Environment 10: Episode 6828, Score -115.16824141136308, Avg_Score -120.20712213499117\n",
      "Adding trajectory to replay buffer: step 28248, counter 451598\n",
      "Environment 6: Episode 6829, Score -115.53617411151032, Avg_Score -120.15074440126124\n",
      "Adding trajectory to replay buffer: step 28250, counter 451645\n",
      "Environment 7: Episode 6830, Score -120.55494028114464, Avg_Score -120.16508657846245\n",
      "Adding trajectory to replay buffer: step 28258, counter 451691\n",
      "Environment 2: Episode 6831, Score -121.65372676634665, Avg_Score -120.21445040318575\n",
      "Adding trajectory to replay buffer: step 28262, counter 451739\n",
      "Environment 5: Episode 6832, Score -120.42517010118999, Avg_Score -120.26869649529156\n",
      "Adding trajectory to replay buffer: step 28270, counter 451797\n",
      "Environment 9: Episode 6833, Score -115.30873196163029, Avg_Score -120.20335784833952\n",
      "Adding trajectory to replay buffer: step 28271, counter 451858\n",
      "Environment 14: Episode 6834, Score -116.40874288657618, Avg_Score -120.13234545375397\n",
      "Adding trajectory to replay buffer: step 28273, counter 451914\n",
      "Environment 0: Episode 6835, Score -114.64956191644073, Avg_Score -120.05504275306649\n",
      "Adding trajectory to replay buffer: step 28275, counter 451958\n",
      "Environment 13: Episode 6836, Score -118.82880369876325, Avg_Score -120.06096026743471\n",
      "Adding trajectory to replay buffer: step 28277, counter 452002\n",
      "Environment 1: Episode 6837, Score -118.22364332985133, Avg_Score -120.01710661741765\n",
      "Adding trajectory to replay buffer: step 28277, counter 452048\n",
      "Environment 15: Episode 6838, Score -121.10587232382426, Avg_Score -120.0604099865053\n",
      "Adding trajectory to replay buffer: step 28287, counter 452124\n",
      "Environment 3: Episode 6839, Score -112.17745995057622, Avg_Score -120.001915430412\n",
      "Adding trajectory to replay buffer: step 28293, counter 452167\n",
      "Environment 7: Episode 6840, Score -117.49130459057167, Avg_Score -120.00455728368368\n",
      "Adding trajectory to replay buffer: step 28293, counter 452213\n",
      "Environment 10: Episode 6841, Score -120.30414918091459, Avg_Score -119.98069580915066\n",
      "Adding trajectory to replay buffer: step 28316, counter 452284\n",
      "Environment 4: Episode 6842, Score -119.54256186518259, Avg_Score -119.95194778936705\n",
      "Adding trajectory to replay buffer: step 28316, counter 452329\n",
      "Environment 14: Episode 6843, Score -122.5366696519951, Avg_Score -120.03780525584506\n",
      "Adding trajectory to replay buffer: step 28318, counter 452389\n",
      "Environment 2: Episode 6844, Score -116.78845560440172, Avg_Score -119.97751294240742\n",
      "Adding trajectory to replay buffer: step 28320, counter 452436\n",
      "Environment 0: Episode 6845, Score -122.38803113319415, Avg_Score -119.9758008621363\n",
      "Adding trajectory to replay buffer: step 28320, counter 452481\n",
      "Environment 13: Episode 6846, Score -119.89882524616893, Avg_Score -119.9547231101776\n",
      "Adding trajectory to replay buffer: step 28321, counter 452554\n",
      "Environment 6: Episode 6847, Score -116.54763242071867, Avg_Score -119.89386191319278\n",
      "Adding trajectory to replay buffer: step 28323, counter 452600\n",
      "Environment 1: Episode 6848, Score -117.13006257566188, Avg_Score -119.86909363055156\n",
      "Adding trajectory to replay buffer: step 28330, counter 452643\n",
      "Environment 3: Episode 6849, Score -122.57957236152266, Avg_Score -119.87668570734765\n",
      "Adding trajectory to replay buffer: step 28336, counter 452717\n",
      "Environment 5: Episode 6850, Score -116.44609633605864, Avg_Score -119.79205637868863\n",
      "Adding trajectory to replay buffer: step 28336, counter 452760\n",
      "Environment 7: Episode 6851, Score -122.97342526238536, Avg_Score -119.79011852395303\n",
      "Adding trajectory to replay buffer: step 28337, counter 452804\n",
      "Environment 10: Episode 6852, Score -122.76452419028804, Avg_Score -119.79336457448828\n",
      "Adding trajectory to replay buffer: step 28340, counter 452867\n",
      "Environment 15: Episode 6853, Score -117.25324159601827, Avg_Score -119.81758420028173\n",
      "Adding trajectory to replay buffer: step 28342, counter 452970\n",
      "Environment 12: Episode 6854, Score -124.29830869937811, Avg_Score -119.89415394614012\n",
      "Adding trajectory to replay buffer: step 28361, counter 453091\n",
      "Environment 8: Episode 6855, Score -126.15968659412673, Avg_Score -119.98144194249998\n",
      "Adding trajectory to replay buffer: step 28361, counter 453136\n",
      "Environment 14: Episode 6856, Score -120.87412040767322, Avg_Score -119.99481753081683\n",
      "Adding trajectory to replay buffer: step 28365, counter 453181\n",
      "Environment 13: Episode 6857, Score -121.25332700515352, Avg_Score -120.07719289636866\n",
      "Adding trajectory to replay buffer: step 28366, counter 453227\n",
      "Environment 0: Episode 6858, Score -119.94711234092216, Avg_Score -120.06989298208869\n",
      "Adding trajectory to replay buffer: step 28369, counter 453273\n",
      "Environment 1: Episode 6859, Score -119.98299354194043, Avg_Score -120.03738973985915\n",
      "Adding trajectory to replay buffer: step 28373, counter 453316\n",
      "Environment 3: Episode 6860, Score -118.86465257219659, Avg_Score -119.99619066103804\n",
      "Adding trajectory to replay buffer: step 28382, counter 453362\n",
      "Environment 5: Episode 6861, Score -122.7048796692832, Avg_Score -119.99136033449571\n",
      "Adding trajectory to replay buffer: step 28382, counter 453408\n",
      "Environment 7: Episode 6862, Score -119.94084651796271, Avg_Score -120.06832145259027\n",
      "Adding trajectory to replay buffer: step 28382, counter 453453\n",
      "Environment 10: Episode 6863, Score -120.1398432050962, Avg_Score -120.03364525556388\n",
      "Adding trajectory to replay buffer: step 28386, counter 453497\n",
      "Environment 12: Episode 6864, Score -123.34590187959932, Avg_Score -120.11168324865612\n",
      "Adding trajectory to replay buffer: step 28399, counter 453691\n",
      "Environment 11: Episode 6865, Score -118.33849080009051, Avg_Score -120.11495179177388\n",
      "Adding trajectory to replay buffer: step 28403, counter 453733\n",
      "Environment 14: Episode 6866, Score -117.55059660163957, Avg_Score -120.07938443642283\n",
      "Adding trajectory to replay buffer: step 28405, counter 453777\n",
      "Environment 8: Episode 6867, Score -122.39027961851097, Avg_Score -120.08625383765413\n",
      "Adding trajectory to replay buffer: step 28408, counter 453820\n",
      "Environment 13: Episode 6868, Score -122.18566891504265, Avg_Score -120.07725196765398\n",
      "Adding trajectory to replay buffer: step 28410, counter 453864\n",
      "Environment 0: Episode 6869, Score -122.04934534500788, Avg_Score -120.01442508254054\n",
      "Adding trajectory to replay buffer: step 28412, counter 453907\n",
      "Environment 1: Episode 6870, Score -123.1494563532093, Avg_Score -120.01652054699264\n",
      "Adding trajectory to replay buffer: step 28416, counter 453950\n",
      "Environment 3: Episode 6871, Score -116.43147254852951, Avg_Score -119.95006211057276\n",
      "Adding trajectory to replay buffer: step 28419, counter 454053\n",
      "Environment 4: Episode 6872, Score -115.84258506900817, Avg_Score -119.8784403537057\n",
      "Adding trajectory to replay buffer: step 28423, counter 454136\n",
      "Environment 15: Episode 6873, Score -112.87333372433235, Avg_Score -119.77894677532976\n",
      "Adding trajectory to replay buffer: step 28425, counter 454179\n",
      "Environment 7: Episode 6874, Score -122.1480303357585, Avg_Score -119.8202869951221\n",
      "Adding trajectory to replay buffer: step 28426, counter 454223\n",
      "Environment 5: Episode 6875, Score -121.38778571380489, Avg_Score -119.82124254066984\n",
      "Adding trajectory to replay buffer: step 28427, counter 454268\n",
      "Environment 10: Episode 6876, Score -122.16947142812424, Avg_Score -119.8183610026432\n",
      "Adding trajectory to replay buffer: step 28434, counter 454381\n",
      "Environment 6: Episode 6877, Score -120.06144317058227, Avg_Score -119.86294977039663\n",
      "Adding trajectory to replay buffer: step 28443, counter 454425\n",
      "Environment 11: Episode 6878, Score -123.10333394358524, Avg_Score -119.87898186519037\n",
      "Adding trajectory to replay buffer: step 28449, counter 454466\n",
      "Environment 13: Episode 6879, Score -116.88087776407724, Avg_Score -119.9126531250443\n",
      "Adding trajectory to replay buffer: step 28450, counter 454646\n",
      "Environment 9: Episode 6880, Score -122.05813248889753, Avg_Score -119.9844927645895\n",
      "Adding trajectory to replay buffer: step 28456, counter 454692\n",
      "Environment 0: Episode 6881, Score -121.35024416262347, Avg_Score -119.98837377648051\n",
      "Adding trajectory to replay buffer: step 28457, counter 454737\n",
      "Environment 1: Episode 6882, Score -122.67831331763634, Avg_Score -120.00027923815506\n",
      "Adding trajectory to replay buffer: step 28463, counter 454781\n",
      "Environment 4: Episode 6883, Score -117.71772798494685, Avg_Score -119.96273422254302\n",
      "Adding trajectory to replay buffer: step 28467, counter 454825\n",
      "Environment 15: Episode 6884, Score -122.49755519487957, Avg_Score -119.98212067962093\n",
      "Adding trajectory to replay buffer: step 28468, counter 454868\n",
      "Environment 7: Episode 6885, Score -123.7791852142637, Avg_Score -120.02563411684356\n",
      "Adding trajectory to replay buffer: step 28473, counter 454915\n",
      "Environment 5: Episode 6886, Score -121.25489586829147, Avg_Score -120.079638216112\n",
      "Adding trajectory to replay buffer: step 28473, counter 454961\n",
      "Environment 10: Episode 6887, Score -119.68487268344872, Avg_Score -120.09556633184887\n",
      "Adding trajectory to replay buffer: step 28475, counter 455020\n",
      "Environment 3: Episode 6888, Score -114.7614951003852, Avg_Score -120.04629010834773\n",
      "Adding trajectory to replay buffer: step 28478, counter 455064\n",
      "Environment 6: Episode 6889, Score -122.92443290589813, Avg_Score -120.04864941110219\n",
      "Adding trajectory to replay buffer: step 28481, counter 455142\n",
      "Environment 14: Episode 6890, Score -121.19247908305688, Avg_Score -120.06511092461027\n",
      "Adding trajectory to replay buffer: step 28491, counter 455190\n",
      "Environment 11: Episode 6891, Score -118.60728929132782, Avg_Score -120.07913794745019\n",
      "Adding trajectory to replay buffer: step 28502, counter 455235\n",
      "Environment 1: Episode 6892, Score -121.88136861422657, Avg_Score -120.09019648792751\n",
      "Adding trajectory to replay buffer: step 28507, counter 455337\n",
      "Environment 8: Episode 6893, Score -124.14961802230961, Avg_Score -120.1691645756925\n",
      "Adding trajectory to replay buffer: step 28508, counter 455382\n",
      "Environment 4: Episode 6894, Score -122.09487931315849, Avg_Score -120.18877525976539\n",
      "Adding trajectory to replay buffer: step 28511, counter 455426\n",
      "Environment 15: Episode 6895, Score -115.41792306372027, Avg_Score -120.12446581326438\n",
      "Adding trajectory to replay buffer: step 28521, counter 455474\n",
      "Environment 5: Episode 6896, Score -121.28121089113938, Avg_Score -120.11668810756328\n",
      "Adding trajectory to replay buffer: step 28521, counter 455522\n",
      "Environment 10: Episode 6897, Score -120.9761423894316, Avg_Score -120.10815758204541\n",
      "Adding trajectory to replay buffer: step 28525, counter 455729\n",
      "Environment 2: Episode 6898, Score -123.46395047363512, Avg_Score -120.10799679584416\n",
      "Adding trajectory to replay buffer: step 28525, counter 455773\n",
      "Environment 14: Episode 6899, Score -120.68519574846762, Avg_Score -120.08953656954046\n",
      "Adding trajectory to replay buffer: step 28527, counter 455825\n",
      "Environment 3: Episode 6900, Score -117.90854001607124, Avg_Score -120.04216846708779\n",
      "Adding trajectory to replay buffer: step 28527, counter 455884\n",
      "Environment 7: Episode 6901, Score -115.82879868630444, Avg_Score -119.8958866087569\n",
      "Adding trajectory to replay buffer: step 28529, counter 456027\n",
      "Environment 12: Episode 6902, Score -116.59974759934285, Avg_Score -119.83308790088978\n",
      "Adding trajectory to replay buffer: step 28532, counter 456110\n",
      "Environment 13: Episode 6903, Score -120.45160149036224, Avg_Score -119.91861696090487\n",
      "Adding trajectory to replay buffer: step 28538, counter 456157\n",
      "Environment 11: Episode 6904, Score -117.62080480951319, Avg_Score -119.90880099306636\n",
      "Adding trajectory to replay buffer: step 28547, counter 456202\n",
      "Environment 1: Episode 6905, Score -115.54307004565187, Avg_Score -119.84571809770001\n",
      "Adding trajectory to replay buffer: step 28553, counter 456247\n",
      "Environment 4: Episode 6906, Score -121.02568629076146, Avg_Score -119.82703143939487\n",
      "Adding trajectory to replay buffer: step 28555, counter 456291\n",
      "Environment 15: Episode 6907, Score -122.14426119367282, Avg_Score -119.81866966464922\n",
      "Adding trajectory to replay buffer: step 28561, counter 456374\n",
      "Environment 6: Episode 6908, Score -117.08601272536008, Avg_Score -119.77019250726516\n",
      "Adding trajectory to replay buffer: step 28566, counter 456433\n",
      "Environment 8: Episode 6909, Score -115.89138563510589, Avg_Score -119.70723989680617\n",
      "Adding trajectory to replay buffer: step 28567, counter 456479\n",
      "Environment 5: Episode 6910, Score -116.83167352896866, Avg_Score -119.64451129195119\n",
      "Adding trajectory to replay buffer: step 28568, counter 456520\n",
      "Environment 7: Episode 6911, Score -117.24778945693126, Avg_Score -119.62968226620711\n",
      "Adding trajectory to replay buffer: step 28568, counter 456567\n",
      "Environment 10: Episode 6912, Score -121.87599680364411, Avg_Score -119.6111590570707\n",
      "Adding trajectory to replay buffer: step 28569, counter 456611\n",
      "Environment 2: Episode 6913, Score -116.5928041424919, Avg_Score -119.57516971407021\n",
      "Adding trajectory to replay buffer: step 28569, counter 456653\n",
      "Environment 3: Episode 6914, Score -118.25067257023603, Avg_Score -119.5899049655579\n",
      "Adding trajectory to replay buffer: step 28569, counter 456772\n",
      "Environment 9: Episode 6915, Score -114.56700618731479, Avg_Score -119.57902894636898\n",
      "Adding trajectory to replay buffer: step 28573, counter 456816\n",
      "Environment 12: Episode 6916, Score -122.34712415567662, Avg_Score -119.62897283711574\n",
      "Adding trajectory to replay buffer: step 28573, counter 456864\n",
      "Environment 14: Episode 6917, Score -117.11448976389008, Avg_Score -119.66498876126298\n",
      "Adding trajectory to replay buffer: step 28577, counter 456909\n",
      "Environment 13: Episode 6918, Score -118.8052767524949, Avg_Score -119.64153255882438\n",
      "Adding trajectory to replay buffer: step 28582, counter 456953\n",
      "Environment 11: Episode 6919, Score -122.65765761152096, Avg_Score -119.64749524599306\n",
      "Adding trajectory to replay buffer: step 28592, counter 456998\n",
      "Environment 1: Episode 6920, Score -118.47310412024149, Avg_Score -119.67364831266701\n",
      "Adding trajectory to replay buffer: step 28599, counter 457044\n",
      "Environment 4: Episode 6921, Score -122.49016917664619, Avg_Score -119.70441440213972\n",
      "Adding trajectory to replay buffer: step 28601, counter 457090\n",
      "Environment 15: Episode 6922, Score -123.53232382704319, Avg_Score -119.7255749758021\n",
      "Adding trajectory to replay buffer: step 28608, counter 457137\n",
      "Environment 6: Episode 6923, Score -120.37934311137163, Avg_Score -119.70558532183917\n",
      "Adding trajectory to replay buffer: step 28612, counter 457180\n",
      "Environment 3: Episode 6924, Score -122.92334038129016, Avg_Score -119.74050594897999\n",
      "Adding trajectory to replay buffer: step 28613, counter 457226\n",
      "Environment 5: Episode 6925, Score -120.09527193009481, Avg_Score -119.71134931864526\n",
      "Adding trajectory to replay buffer: step 28613, counter 457273\n",
      "Environment 8: Episode 6926, Score -121.23295050531937, Avg_Score -119.75193899392407\n",
      "Adding trajectory to replay buffer: step 28614, counter 457318\n",
      "Environment 2: Episode 6927, Score -122.33174956316128, Avg_Score -119.75065028411166\n",
      "Adding trajectory to replay buffer: step 28614, counter 457363\n",
      "Environment 9: Episode 6928, Score -122.12240298746403, Avg_Score -119.82019189987264\n",
      "Adding trajectory to replay buffer: step 28621, counter 457407\n",
      "Environment 13: Episode 6929, Score -122.70354045856558, Avg_Score -119.8918655633432\n",
      "Adding trajectory to replay buffer: step 28622, counter 457573\n",
      "Environment 0: Episode 6930, Score -120.46865120668275, Avg_Score -119.89100267259857\n",
      "Adding trajectory to replay buffer: step 28624, counter 457615\n",
      "Environment 11: Episode 6931, Score -123.26116534915442, Avg_Score -119.90707705842665\n",
      "Adding trajectory to replay buffer: step 28629, counter 457671\n",
      "Environment 14: Episode 6932, Score -114.48205003959065, Avg_Score -119.84764585781063\n",
      "Adding trajectory to replay buffer: step 28632, counter 457735\n",
      "Environment 10: Episode 6933, Score -114.80848097139287, Avg_Score -119.84264334790826\n",
      "Adding trajectory to replay buffer: step 28637, counter 457780\n",
      "Environment 1: Episode 6934, Score -121.93902488842792, Avg_Score -119.8979461679268\n",
      "Adding trajectory to replay buffer: step 28645, counter 457826\n",
      "Environment 4: Episode 6935, Score -121.29279417356848, Avg_Score -119.96437849049808\n",
      "Adding trajectory to replay buffer: step 28655, counter 457868\n",
      "Environment 8: Episode 6936, Score -117.21499838311661, Avg_Score -119.94824043734161\n",
      "Adding trajectory to replay buffer: step 28656, counter 457912\n",
      "Environment 3: Episode 6937, Score -122.1862300353938, Avg_Score -119.98786630439703\n",
      "Adding trajectory to replay buffer: step 28662, counter 457961\n",
      "Environment 5: Episode 6938, Score -119.02287405658514, Avg_Score -119.96703632172465\n",
      "Adding trajectory to replay buffer: step 28665, counter 458004\n",
      "Environment 0: Episode 6939, Score -121.71336663972525, Avg_Score -120.06239538861615\n",
      "Adding trajectory to replay buffer: step 28665, counter 458048\n",
      "Environment 13: Episode 6940, Score -120.37866208071696, Avg_Score -120.09126896351758\n",
      "Adding trajectory to replay buffer: step 28667, counter 458114\n",
      "Environment 15: Episode 6941, Score -115.00156731633484, Avg_Score -120.0382431448718\n",
      "Adding trajectory to replay buffer: step 28672, counter 458178\n",
      "Environment 6: Episode 6942, Score -116.67358466485639, Avg_Score -120.00955337286854\n",
      "Adding trajectory to replay buffer: step 28680, counter 458229\n",
      "Environment 14: Episode 6943, Score -121.41004861071272, Avg_Score -119.99828716245571\n",
      "Adding trajectory to replay buffer: step 28681, counter 458273\n",
      "Environment 1: Episode 6944, Score -121.98255743204928, Avg_Score -120.05022818073218\n",
      "Adding trajectory to replay buffer: step 28686, counter 458327\n",
      "Environment 10: Episode 6945, Score -121.19575013346474, Avg_Score -120.03830537073489\n",
      "Adding trajectory to replay buffer: step 28689, counter 458371\n",
      "Environment 4: Episode 6946, Score -122.43621576100837, Avg_Score -120.06367927588327\n",
      "Adding trajectory to replay buffer: step 28689, counter 458446\n",
      "Environment 9: Episode 6947, Score -117.93649938903191, Avg_Score -120.07756794556643\n",
      "Adding trajectory to replay buffer: step 28696, counter 458574\n",
      "Environment 7: Episode 6948, Score -120.49819880288902, Avg_Score -120.1112493078387\n",
      "Adding trajectory to replay buffer: step 28701, counter 458620\n",
      "Environment 8: Episode 6949, Score -119.79549799077772, Avg_Score -120.08340856413125\n",
      "Adding trajectory to replay buffer: step 28702, counter 458666\n",
      "Environment 3: Episode 6950, Score -122.31858177529897, Avg_Score -120.14213341852364\n",
      "Adding trajectory to replay buffer: step 28705, counter 458709\n",
      "Environment 5: Episode 6951, Score -121.42266119070305, Avg_Score -120.12662577780684\n",
      "Adding trajectory to replay buffer: step 28709, counter 458753\n",
      "Environment 13: Episode 6952, Score -121.91607848577077, Avg_Score -120.11814132076167\n",
      "Adding trajectory to replay buffer: step 28710, counter 458798\n",
      "Environment 0: Episode 6953, Score -121.94712013149137, Avg_Score -120.1650801061164\n",
      "Adding trajectory to replay buffer: step 28710, counter 458894\n",
      "Environment 2: Episode 6954, Score -115.46830955835742, Avg_Score -120.07678011470617\n",
      "Adding trajectory to replay buffer: step 28711, counter 458981\n",
      "Environment 11: Episode 6955, Score -116.73405165809704, Avg_Score -119.98252376534587\n",
      "Adding trajectory to replay buffer: step 28711, counter 459025\n",
      "Environment 15: Episode 6956, Score -121.97978899647048, Avg_Score -119.99358045123383\n",
      "Adding trajectory to replay buffer: step 28715, counter 459068\n",
      "Environment 6: Episode 6957, Score -115.76394699561595, Avg_Score -119.93868665113845\n",
      "Adding trajectory to replay buffer: step 28726, counter 459114\n",
      "Environment 14: Episode 6958, Score -117.67013646689182, Avg_Score -119.91591689239816\n",
      "Adding trajectory to replay buffer: step 28727, counter 459160\n",
      "Environment 1: Episode 6959, Score -121.09327969179884, Avg_Score -119.92701975389673\n",
      "Adding trajectory to replay buffer: step 28732, counter 459206\n",
      "Environment 10: Episode 6960, Score -122.75834841714612, Avg_Score -119.96595671234621\n",
      "Adding trajectory to replay buffer: step 28735, counter 459252\n",
      "Environment 4: Episode 6961, Score -121.13708727875849, Avg_Score -119.950278788441\n",
      "Adding trajectory to replay buffer: step 28735, counter 459298\n",
      "Environment 9: Episode 6962, Score -121.9015646049728, Avg_Score -119.9698859693111\n",
      "Adding trajectory to replay buffer: step 28738, counter 459340\n",
      "Environment 7: Episode 6963, Score -116.81954172895227, Avg_Score -119.93668295454964\n",
      "Adding trajectory to replay buffer: step 28746, counter 459385\n",
      "Environment 8: Episode 6964, Score -123.2376992364414, Avg_Score -119.93560092811805\n",
      "Adding trajectory to replay buffer: step 28755, counter 459431\n",
      "Environment 13: Episode 6965, Score -119.01127697104215, Avg_Score -119.94232878982756\n",
      "Adding trajectory to replay buffer: step 28757, counter 459615\n",
      "Environment 12: Episode 6966, Score -125.20680215165021, Avg_Score -120.01889084532766\n",
      "Adding trajectory to replay buffer: step 28759, counter 459663\n",
      "Environment 11: Episode 6967, Score -120.60236718073611, Avg_Score -120.00101172094992\n",
      "Adding trajectory to replay buffer: step 28759, counter 459711\n",
      "Environment 15: Episode 6968, Score -122.80736862979892, Avg_Score -120.00722871809748\n",
      "Adding trajectory to replay buffer: step 28764, counter 459773\n",
      "Environment 3: Episode 6969, Score -116.66683508480092, Avg_Score -119.95340361549543\n",
      "Adding trajectory to replay buffer: step 28772, counter 459819\n",
      "Environment 14: Episode 6970, Score -119.97477229829317, Avg_Score -119.92165677494626\n",
      "Adding trajectory to replay buffer: step 28773, counter 459865\n",
      "Environment 1: Episode 6971, Score -123.21852502584147, Avg_Score -119.98952729971937\n",
      "Adding trajectory to replay buffer: step 28774, counter 459929\n",
      "Environment 2: Episode 6972, Score -115.31691509675545, Avg_Score -119.98427059999685\n",
      "Adding trajectory to replay buffer: step 28774, counter 459998\n",
      "Environment 5: Episode 6973, Score -117.5049865086892, Avg_Score -120.0305871278404\n",
      "Adding trajectory to replay buffer: step 28777, counter 460043\n",
      "Environment 10: Episode 6974, Score -121.39923618998255, Avg_Score -120.02309918638267\n",
      "Adding trajectory to replay buffer: step 28778, counter 460086\n",
      "Environment 4: Episode 6975, Score -120.54679169310319, Avg_Score -120.01468924617562\n",
      "Adding trajectory to replay buffer: step 28780, counter 460131\n",
      "Environment 9: Episode 6976, Score -121.6926711911327, Avg_Score -120.0099212438057\n",
      "Adding trajectory to replay buffer: step 28782, counter 460175\n",
      "Environment 7: Episode 6977, Score -122.63858682215151, Avg_Score -120.03569268032139\n",
      "Adding trajectory to replay buffer: step 28790, counter 460219\n",
      "Environment 8: Episode 6978, Score -120.23041820002223, Avg_Score -120.00696352288577\n",
      "Adding trajectory to replay buffer: step 28792, counter 460296\n",
      "Environment 6: Episode 6979, Score -110.94309878748284, Avg_Score -119.94758573311981\n",
      "Adding trajectory to replay buffer: step 28799, counter 460340\n",
      "Environment 13: Episode 6980, Score -122.41095027807914, Avg_Score -119.95111391101163\n",
      "Adding trajectory to replay buffer: step 28802, counter 460385\n",
      "Environment 12: Episode 6981, Score -122.92372341756212, Avg_Score -119.96684870356103\n",
      "Adding trajectory to replay buffer: step 28802, counter 460428\n",
      "Environment 15: Episode 6982, Score -119.40071453021467, Avg_Score -119.93407271568684\n",
      "Adding trajectory to replay buffer: step 28805, counter 460474\n",
      "Environment 11: Episode 6983, Score -117.63285680582747, Avg_Score -119.93322400389563\n",
      "Adding trajectory to replay buffer: step 28815, counter 460516\n",
      "Environment 1: Episode 6984, Score -123.0686928086641, Avg_Score -119.93893538003347\n",
      "Adding trajectory to replay buffer: step 28817, counter 460559\n",
      "Environment 2: Episode 6985, Score -123.01115198593773, Avg_Score -119.93125504775021\n",
      "Adding trajectory to replay buffer: step 28817, counter 460602\n",
      "Environment 5: Episode 6986, Score -123.06816784868575, Avg_Score -119.94938776755414\n",
      "Adding trajectory to replay buffer: step 28820, counter 460645\n",
      "Environment 10: Episode 6987, Score -123.32296950171019, Avg_Score -119.9857687357368\n",
      "Adding trajectory to replay buffer: step 28821, counter 460688\n",
      "Environment 4: Episode 6988, Score -122.84906280533161, Avg_Score -120.06664441278626\n",
      "Adding trajectory to replay buffer: step 28823, counter 460731\n",
      "Environment 9: Episode 6989, Score -123.03932689055861, Avg_Score -120.06779335263288\n",
      "Adding trajectory to replay buffer: step 28825, counter 460774\n",
      "Environment 7: Episode 6990, Score -122.71124920857821, Avg_Score -120.08298105388808\n",
      "Adding trajectory to replay buffer: step 28835, counter 460819\n",
      "Environment 8: Episode 6991, Score -120.02279705772239, Avg_Score -120.09713613155198\n",
      "Adding trajectory to replay buffer: step 28842, counter 460862\n",
      "Environment 13: Episode 6992, Score -123.04852775552435, Avg_Score -120.10880772296497\n",
      "Adding trajectory to replay buffer: step 28848, counter 460908\n",
      "Environment 12: Episode 6993, Score -121.58233720840823, Avg_Score -120.08313491482595\n",
      "Adding trajectory to replay buffer: step 28849, counter 460952\n",
      "Environment 11: Episode 6994, Score -122.92443807353763, Avg_Score -120.09143050242977\n",
      "Adding trajectory to replay buffer: step 28850, counter 461000\n",
      "Environment 15: Episode 6995, Score -121.89656089298178, Avg_Score -120.15621688072237\n",
      "Adding trajectory to replay buffer: step 28858, counter 461043\n",
      "Environment 1: Episode 6996, Score -123.1261525072176, Avg_Score -120.17466629688317\n",
      "Adding trajectory to replay buffer: step 28863, counter 461089\n",
      "Environment 5: Episode 6997, Score -118.40032771838084, Avg_Score -120.14890815017266\n",
      "Adding trajectory to replay buffer: step 28863, counter 461132\n",
      "Environment 10: Episode 6998, Score -121.94272217679831, Avg_Score -120.1336958672043\n",
      "Adding trajectory to replay buffer: step 28865, counter 461176\n",
      "Environment 4: Episode 6999, Score -121.69790057313845, Avg_Score -120.143822915451\n",
      "Adding trajectory to replay buffer: step 28867, counter 461220\n",
      "Environment 9: Episode 7000, Score -120.50319419849788, Avg_Score -120.16976945727525\n",
      "Adding trajectory to replay buffer: step 28870, counter 461380\n",
      "Environment 0: Episode 7001, Score -120.48804808395728, Avg_Score -120.21636195125177\n",
      "Adding trajectory to replay buffer: step 28879, counter 461424\n",
      "Environment 8: Episode 7002, Score -122.80762461215072, Avg_Score -120.27844072137985\n",
      "Adding trajectory to replay buffer: step 28886, counter 461468\n",
      "Environment 13: Episode 7003, Score -122.75635631352168, Avg_Score -120.30148826961144\n",
      "Adding trajectory to replay buffer: step 28888, counter 461592\n",
      "Environment 3: Episode 7004, Score -130.1938766759746, Avg_Score -120.42721898827604\n",
      "Adding trajectory to replay buffer: step 28891, counter 461635\n",
      "Environment 12: Episode 7005, Score -120.46945527129931, Avg_Score -120.47648284053254\n",
      "Adding trajectory to replay buffer: step 28892, counter 461678\n",
      "Environment 11: Episode 7006, Score -123.0982558399352, Avg_Score -120.49720853602426\n",
      "Adding trajectory to replay buffer: step 28892, counter 461720\n",
      "Environment 15: Episode 7007, Score -123.30586601297155, Avg_Score -120.50882458421725\n",
      "Adding trajectory to replay buffer: step 28905, counter 461762\n",
      "Environment 10: Episode 7008, Score -123.15437594382776, Avg_Score -120.56950821640194\n",
      "Adding trajectory to replay buffer: step 28906, counter 461805\n",
      "Environment 5: Episode 7009, Score -123.84032907927534, Avg_Score -120.64899765084363\n",
      "Adding trajectory to replay buffer: step 28908, counter 461848\n",
      "Environment 4: Episode 7010, Score -122.51541564164, Avg_Score -120.70583507197034\n",
      "Adding trajectory to replay buffer: step 28911, counter 461892\n",
      "Environment 9: Episode 7011, Score -123.07938221888121, Avg_Score -120.76415099958984\n",
      "Adding trajectory to replay buffer: step 28914, counter 461936\n",
      "Environment 0: Episode 7012, Score -121.29672528974028, Avg_Score -120.75835828445081\n",
      "Adding trajectory to replay buffer: step 28930, counter 461980\n",
      "Environment 13: Episode 7013, Score -122.33060280984765, Avg_Score -120.81573627112435\n",
      "Adding trajectory to replay buffer: step 28931, counter 462023\n",
      "Environment 3: Episode 7014, Score -123.23451576984797, Avg_Score -120.86557470312047\n",
      "Adding trajectory to replay buffer: step 28935, counter 462067\n",
      "Environment 12: Episode 7015, Score -122.07968873395708, Avg_Score -120.94070152858689\n",
      "Adding trajectory to replay buffer: step 28937, counter 462112\n",
      "Environment 15: Episode 7016, Score -122.1541192247526, Avg_Score -120.93877147927763\n",
      "Adding trajectory to replay buffer: step 28938, counter 462158\n",
      "Environment 11: Episode 7017, Score -121.49002423139972, Avg_Score -120.98252682395274\n",
      "Adding trajectory to replay buffer: step 28948, counter 462201\n",
      "Environment 10: Episode 7018, Score -123.85648172294225, Avg_Score -121.0330388736572\n",
      "Adding trajectory to replay buffer: step 28950, counter 462245\n",
      "Environment 5: Episode 7019, Score -122.57933924410989, Avg_Score -121.0322556899831\n",
      "Adding trajectory to replay buffer: step 28951, counter 462371\n",
      "Environment 7: Episode 7020, Score -116.94669380844509, Avg_Score -121.0169915868651\n",
      "Adding trajectory to replay buffer: step 28953, counter 462416\n",
      "Environment 4: Episode 7021, Score -120.09518304586659, Avg_Score -120.99304172555732\n",
      "Adding trajectory to replay buffer: step 28953, counter 462458\n",
      "Environment 9: Episode 7022, Score -123.17052774807128, Avg_Score -120.9894237647676\n",
      "Adding trajectory to replay buffer: step 28958, counter 462502\n",
      "Environment 0: Episode 7023, Score -123.04822628142126, Avg_Score -121.01611259646812\n",
      "Adding trajectory to replay buffer: step 28969, counter 462592\n",
      "Environment 8: Episode 7024, Score -120.47675929401568, Avg_Score -120.99164678559538\n",
      "Adding trajectory to replay buffer: step 28970, counter 462745\n",
      "Environment 2: Episode 7025, Score -131.35919779034455, Avg_Score -121.10428604419786\n",
      "Adding trajectory to replay buffer: step 28974, counter 462789\n",
      "Environment 13: Episode 7026, Score -117.71295993213604, Avg_Score -121.06908613846603\n",
      "Adding trajectory to replay buffer: step 28981, counter 462835\n",
      "Environment 12: Episode 7027, Score -122.74998002545101, Avg_Score -121.07326844308895\n",
      "Adding trajectory to replay buffer: step 28983, counter 462881\n",
      "Environment 15: Episode 7028, Score -119.09163673459989, Avg_Score -121.04296078056032\n",
      "Adding trajectory to replay buffer: step 28987, counter 463076\n",
      "Environment 6: Episode 7029, Score -125.98346525125949, Avg_Score -121.07576002848725\n",
      "Adding trajectory to replay buffer: step 28988, counter 463206\n",
      "Environment 1: Episode 7030, Score -120.21435439492762, Avg_Score -121.0732170603697\n",
      "Adding trajectory to replay buffer: step 28989, counter 463257\n",
      "Environment 11: Episode 7031, Score -117.78015500824526, Avg_Score -121.0184069569606\n",
      "Adding trajectory to replay buffer: step 28991, counter 463300\n",
      "Environment 10: Episode 7032, Score -122.37483776568871, Avg_Score -121.09733483422158\n",
      "Adding trajectory to replay buffer: step 28994, counter 463344\n",
      "Environment 5: Episode 7033, Score -121.24840275242181, Avg_Score -121.1617340520319\n",
      "Adding trajectory to replay buffer: step 28995, counter 463408\n",
      "Environment 3: Episode 7034, Score -116.3014088155944, Avg_Score -121.10535789130356\n",
      "Adding trajectory to replay buffer: step 28998, counter 463455\n",
      "Environment 7: Episode 7035, Score -119.34394607547671, Avg_Score -121.08586941032264\n",
      "Adding trajectory to replay buffer: step 28999, counter 463501\n",
      "Environment 4: Episode 7036, Score -122.24465188000165, Avg_Score -121.13616594529152\n",
      "Adding trajectory to replay buffer: step 28999, counter 463547\n",
      "Environment 9: Episode 7037, Score -120.90145060506897, Avg_Score -121.12331815098828\n",
      "Adding trajectory to replay buffer: step 29002, counter 463591\n",
      "Environment 0: Episode 7038, Score -117.86009498345572, Avg_Score -121.11169036025699\n",
      "Adding trajectory to replay buffer: step 29017, counter 463638\n",
      "Environment 2: Episode 7039, Score -120.52839245769755, Avg_Score -121.09984061843672\n",
      "Adding trajectory to replay buffer: step 29026, counter 463695\n",
      "Environment 8: Episode 7040, Score -114.56405161679598, Avg_Score -121.04169451379752\n",
      "Adding trajectory to replay buffer: step 29026, counter 463740\n",
      "Environment 12: Episode 7041, Score -120.74276168000574, Avg_Score -121.09910645743423\n",
      "Adding trajectory to replay buffer: step 29027, counter 463784\n",
      "Environment 15: Episode 7042, Score -120.8948932908761, Avg_Score -121.14131954369445\n",
      "Adding trajectory to replay buffer: step 29033, counter 463830\n",
      "Environment 6: Episode 7043, Score -119.52170113532495, Avg_Score -121.12243606894057\n",
      "Adding trajectory to replay buffer: step 29033, counter 463889\n",
      "Environment 13: Episode 7044, Score -116.36680936074256, Avg_Score -121.0662785882275\n",
      "Adding trajectory to replay buffer: step 29034, counter 463934\n",
      "Environment 11: Episode 7045, Score -121.67339983835258, Avg_Score -121.07105508527638\n",
      "Adding trajectory to replay buffer: step 29035, counter 463981\n",
      "Environment 1: Episode 7046, Score -121.09921892400583, Avg_Score -121.0576851169064\n",
      "Adding trajectory to replay buffer: step 29039, counter 464026\n",
      "Environment 5: Episode 7047, Score -120.98877478364669, Avg_Score -121.08820787085253\n",
      "Adding trajectory to replay buffer: step 29039, counter 464074\n",
      "Environment 10: Episode 7048, Score -122.19640001085898, Avg_Score -121.10518988293222\n",
      "Adding trajectory to replay buffer: step 29041, counter 464120\n",
      "Environment 3: Episode 7049, Score -123.18658332775905, Avg_Score -121.13910073630204\n",
      "Adding trajectory to replay buffer: step 29047, counter 464168\n",
      "Environment 4: Episode 7050, Score -120.91562831650114, Avg_Score -121.12507120171409\n",
      "Adding trajectory to replay buffer: step 29059, counter 464225\n",
      "Environment 0: Episode 7051, Score -115.00633939564663, Avg_Score -121.06090798376351\n",
      "Adding trajectory to replay buffer: step 29059, counter 464267\n",
      "Environment 2: Episode 7052, Score -116.50586172651376, Avg_Score -121.00680581617094\n",
      "Adding trajectory to replay buffer: step 29063, counter 464332\n",
      "Environment 7: Episode 7053, Score -115.68961690281952, Avg_Score -120.94423078388421\n",
      "Adding trajectory to replay buffer: step 29068, counter 464628\n",
      "Environment 14: Episode 7054, Score -132.05841666897632, Avg_Score -121.11013185499041\n",
      "Adding trajectory to replay buffer: step 29069, counter 464671\n",
      "Environment 8: Episode 7055, Score -124.11616384491015, Avg_Score -121.18395297685856\n",
      "Adding trajectory to replay buffer: step 29069, counter 464714\n",
      "Environment 12: Episode 7056, Score -124.10372845725654, Avg_Score -121.2051923714664\n",
      "Adding trajectory to replay buffer: step 29070, counter 464757\n",
      "Environment 15: Episode 7057, Score -122.71820968983074, Avg_Score -121.27473499840855\n",
      "Adding trajectory to replay buffer: step 29077, counter 464801\n",
      "Environment 6: Episode 7058, Score -122.20118645611282, Avg_Score -121.32004549830073\n",
      "Adding trajectory to replay buffer: step 29079, counter 464846\n",
      "Environment 11: Episode 7059, Score -122.3771485534596, Avg_Score -121.33288418691734\n",
      "Adding trajectory to replay buffer: step 29079, counter 464892\n",
      "Environment 13: Episode 7060, Score -120.67724647571333, Avg_Score -121.31207316750304\n",
      "Adding trajectory to replay buffer: step 29082, counter 464933\n",
      "Environment 3: Episode 7061, Score -116.27484128484502, Avg_Score -121.26345070756389\n",
      "Adding trajectory to replay buffer: step 29084, counter 464978\n",
      "Environment 5: Episode 7062, Score -122.41122291012667, Avg_Score -121.26854729061543\n",
      "Adding trajectory to replay buffer: step 29085, counter 465024\n",
      "Environment 10: Episode 7063, Score -119.11667222875109, Avg_Score -121.2915185956134\n",
      "Adding trajectory to replay buffer: step 29088, counter 465113\n",
      "Environment 9: Episode 7064, Score -124.08647551641117, Avg_Score -121.3000063584131\n",
      "Adding trajectory to replay buffer: step 29091, counter 465157\n",
      "Environment 4: Episode 7065, Score -118.55946799593542, Avg_Score -121.29548826866203\n",
      "Adding trajectory to replay buffer: step 29103, counter 465201\n",
      "Environment 2: Episode 7066, Score -122.77841740160312, Avg_Score -121.27120442116157\n",
      "Adding trajectory to replay buffer: step 29106, counter 465244\n",
      "Environment 7: Episode 7067, Score -122.70224775387285, Avg_Score -121.29220322689291\n",
      "Adding trajectory to replay buffer: step 29111, counter 465287\n",
      "Environment 14: Episode 7068, Score -122.2222450820574, Avg_Score -121.2863519914155\n",
      "Adding trajectory to replay buffer: step 29113, counter 465330\n",
      "Environment 15: Episode 7069, Score -115.50671645782889, Avg_Score -121.27475080514577\n",
      "Adding trajectory to replay buffer: step 29114, counter 465375\n",
      "Environment 8: Episode 7070, Score -123.17165679444932, Avg_Score -121.30671965010734\n",
      "Adding trajectory to replay buffer: step 29114, counter 465420\n",
      "Environment 12: Episode 7071, Score -122.682700893494, Avg_Score -121.30136140878383\n",
      "Adding trajectory to replay buffer: step 29116, counter 465501\n",
      "Environment 1: Episode 7072, Score -114.58067905299954, Avg_Score -121.29399904834627\n",
      "Adding trajectory to replay buffer: step 29121, counter 465543\n",
      "Environment 13: Episode 7073, Score -116.03335146812381, Avg_Score -121.27928269794063\n",
      "Adding trajectory to replay buffer: step 29123, counter 465589\n",
      "Environment 6: Episode 7074, Score -120.74841176815083, Avg_Score -121.2727744537223\n",
      "Adding trajectory to replay buffer: step 29125, counter 465632\n",
      "Environment 3: Episode 7075, Score -115.00286085691428, Avg_Score -121.21733514536041\n",
      "Adding trajectory to replay buffer: step 29125, counter 465678\n",
      "Environment 11: Episode 7076, Score -122.18717102375068, Avg_Score -121.22228014368656\n",
      "Adding trajectory to replay buffer: step 29129, counter 465723\n",
      "Environment 5: Episode 7077, Score -121.76805745449911, Avg_Score -121.21357485001005\n",
      "Adding trajectory to replay buffer: step 29130, counter 465768\n",
      "Environment 10: Episode 7078, Score -118.31614958277035, Avg_Score -121.19443216383752\n",
      "Adding trajectory to replay buffer: step 29137, counter 465814\n",
      "Environment 4: Episode 7079, Score -122.36057548107578, Avg_Score -121.30860693077344\n",
      "Adding trajectory to replay buffer: step 29138, counter 465864\n",
      "Environment 9: Episode 7080, Score -115.22588104809883, Avg_Score -121.23675623847363\n",
      "Adding trajectory to replay buffer: step 29148, counter 465909\n",
      "Environment 2: Episode 7081, Score -120.94136944747841, Avg_Score -121.2169326987728\n",
      "Adding trajectory to replay buffer: step 29153, counter 466003\n",
      "Environment 0: Episode 7082, Score -121.73365143375905, Avg_Score -121.24026206780823\n",
      "Adding trajectory to replay buffer: step 29153, counter 466050\n",
      "Environment 7: Episode 7083, Score -117.70789094597598, Avg_Score -121.24101240920973\n",
      "Adding trajectory to replay buffer: step 29159, counter 466096\n",
      "Environment 15: Episode 7084, Score -122.50747164502741, Avg_Score -121.23540019757337\n",
      "Adding trajectory to replay buffer: step 29160, counter 466140\n",
      "Environment 1: Episode 7085, Score -121.66247866212763, Avg_Score -121.22191346433524\n",
      "Adding trajectory to replay buffer: step 29164, counter 466193\n",
      "Environment 14: Episode 7086, Score -116.75983790290852, Avg_Score -121.15883016487747\n",
      "Adding trajectory to replay buffer: step 29165, counter 466237\n",
      "Environment 13: Episode 7087, Score -122.962556460257, Avg_Score -121.15522603446294\n",
      "Adding trajectory to replay buffer: step 29168, counter 466282\n",
      "Environment 6: Episode 7088, Score -122.21894653305536, Avg_Score -121.14892487174018\n",
      "Adding trajectory to replay buffer: step 29168, counter 466325\n",
      "Environment 11: Episode 7089, Score -122.17334103136449, Avg_Score -121.14026501314825\n",
      "Adding trajectory to replay buffer: step 29170, counter 466370\n",
      "Environment 3: Episode 7090, Score -122.91570715482223, Avg_Score -121.14230959261069\n",
      "Adding trajectory to replay buffer: step 29172, counter 466413\n",
      "Environment 5: Episode 7091, Score -122.56294778156963, Avg_Score -121.16771109984914\n",
      "Adding trajectory to replay buffer: step 29175, counter 466474\n",
      "Environment 8: Episode 7092, Score -112.11071368134891, Avg_Score -121.05833295910742\n",
      "Adding trajectory to replay buffer: step 29175, counter 466519\n",
      "Environment 10: Episode 7093, Score -116.75730097967138, Avg_Score -121.01008259682003\n",
      "Adding trajectory to replay buffer: step 29180, counter 466562\n",
      "Environment 4: Episode 7094, Score -120.34928148564883, Avg_Score -120.98433103094114\n",
      "Adding trajectory to replay buffer: step 29180, counter 466628\n",
      "Environment 12: Episode 7095, Score -118.20818278436114, Avg_Score -120.94744724985495\n",
      "Adding trajectory to replay buffer: step 29181, counter 466671\n",
      "Environment 9: Episode 7096, Score -122.89172737776084, Avg_Score -120.94510299856036\n",
      "Adding trajectory to replay buffer: step 29193, counter 466716\n",
      "Environment 2: Episode 7097, Score -121.426814705144, Avg_Score -120.97536786842801\n",
      "Adding trajectory to replay buffer: step 29204, counter 466760\n",
      "Environment 1: Episode 7098, Score -122.33897888131378, Avg_Score -120.97933043547317\n",
      "Adding trajectory to replay buffer: step 29210, counter 466817\n",
      "Environment 7: Episode 7099, Score -115.26721653120282, Avg_Score -120.91502359505382\n",
      "Adding trajectory to replay buffer: step 29212, counter 466861\n",
      "Environment 11: Episode 7100, Score -118.39126238179642, Avg_Score -120.89390427688683\n",
      "Adding trajectory to replay buffer: step 29214, counter 466907\n",
      "Environment 6: Episode 7101, Score -118.95739486188938, Avg_Score -120.87859774466611\n",
      "Adding trajectory to replay buffer: step 29216, counter 466953\n",
      "Environment 3: Episode 7102, Score -119.63223185486719, Avg_Score -120.84684381709329\n",
      "Adding trajectory to replay buffer: step 29220, counter 467001\n",
      "Environment 5: Episode 7103, Score -117.48270194605738, Avg_Score -120.79410727341863\n",
      "Adding trajectory to replay buffer: step 29223, counter 467065\n",
      "Environment 15: Episode 7104, Score -114.38513111867383, Avg_Score -120.63601981784562\n",
      "Adding trajectory to replay buffer: step 29224, counter 467109\n",
      "Environment 12: Episode 7105, Score -115.36088384311134, Avg_Score -120.58493410356374\n",
      "Adding trajectory to replay buffer: step 29227, counter 467155\n",
      "Environment 9: Episode 7106, Score -120.89015474170385, Avg_Score -120.56285309258145\n",
      "Adding trajectory to replay buffer: step 29230, counter 467232\n",
      "Environment 0: Episode 7107, Score -118.09523216804241, Avg_Score -120.51074675413214\n",
      "Adding trajectory to replay buffer: step 29230, counter 467287\n",
      "Environment 8: Episode 7108, Score -113.6516310462722, Avg_Score -120.41571930515661\n",
      "Adding trajectory to replay buffer: step 29230, counter 467352\n",
      "Environment 13: Episode 7109, Score -117.96345898789416, Avg_Score -120.35695060424277\n",
      "Adding trajectory to replay buffer: step 29251, counter 467399\n",
      "Environment 1: Episode 7110, Score -121.0012037777671, Avg_Score -120.34180848560405\n",
      "Adding trajectory to replay buffer: step 29253, counter 467442\n",
      "Environment 7: Episode 7111, Score -116.32581982144279, Avg_Score -120.27427286162965\n",
      "Adding trajectory to replay buffer: step 29256, counter 467534\n",
      "Environment 14: Episode 7112, Score -121.64451960590108, Avg_Score -120.27775080479127\n",
      "Adding trajectory to replay buffer: step 29260, counter 467578\n",
      "Environment 3: Episode 7113, Score -121.96484371715012, Avg_Score -120.27409321386432\n",
      "Adding trajectory to replay buffer: step 29260, counter 467626\n",
      "Environment 11: Episode 7114, Score -118.992721565033, Avg_Score -120.23167527181619\n",
      "Adding trajectory to replay buffer: step 29265, counter 467671\n",
      "Environment 5: Episode 7115, Score -121.52155542905939, Avg_Score -120.2260939387672\n",
      "Adding trajectory to replay buffer: step 29268, counter 467716\n",
      "Environment 15: Episode 7116, Score -121.89510877798062, Avg_Score -120.22350383429948\n",
      "Adding trajectory to replay buffer: step 29270, counter 467762\n",
      "Environment 12: Episode 7117, Score -122.40993221467858, Avg_Score -120.23270291413228\n",
      "Adding trajectory to replay buffer: step 29271, counter 467858\n",
      "Environment 10: Episode 7118, Score -126.10134638755706, Avg_Score -120.25515156077843\n",
      "Adding trajectory to replay buffer: step 29273, counter 467904\n",
      "Environment 9: Episode 7119, Score -121.18460540436021, Avg_Score -120.24120422238092\n",
      "Adding trajectory to replay buffer: step 29275, counter 467949\n",
      "Environment 0: Episode 7120, Score -121.50967450137965, Avg_Score -120.28683402931027\n",
      "Adding trajectory to replay buffer: step 29275, counter 467994\n",
      "Environment 13: Episode 7121, Score -121.14759941551151, Avg_Score -120.29735819300669\n",
      "Adding trajectory to replay buffer: step 29277, counter 468041\n",
      "Environment 8: Episode 7122, Score -121.69433240503446, Avg_Score -120.28259623957636\n",
      "Adding trajectory to replay buffer: step 29279, counter 468106\n",
      "Environment 6: Episode 7123, Score -113.50098271188325, Avg_Score -120.18712380388101\n",
      "Adding trajectory to replay buffer: step 29280, counter 468206\n",
      "Environment 4: Episode 7124, Score -114.55419391408624, Avg_Score -120.1278981500817\n",
      "Adding trajectory to replay buffer: step 29289, counter 468302\n",
      "Environment 2: Episode 7125, Score -122.53210796405442, Avg_Score -120.0396272518188\n",
      "Adding trajectory to replay buffer: step 29307, counter 468344\n",
      "Environment 5: Episode 7126, Score -117.85629352599507, Avg_Score -120.04106058775737\n",
      "Adding trajectory to replay buffer: step 29307, counter 468395\n",
      "Environment 14: Episode 7127, Score -116.14144785248489, Avg_Score -119.97497526602774\n",
      "Adding trajectory to replay buffer: step 29312, counter 468439\n",
      "Environment 15: Episode 7128, Score -122.7565131451947, Avg_Score -120.01162403013367\n",
      "Adding trajectory to replay buffer: step 29316, counter 468495\n",
      "Environment 11: Episode 7129, Score -117.68555877570188, Avg_Score -119.9286449653781\n",
      "Adding trajectory to replay buffer: step 29317, counter 468539\n",
      "Environment 9: Episode 7130, Score -120.25398066594762, Avg_Score -119.9290412280883\n",
      "Adding trajectory to replay buffer: step 29318, counter 468582\n",
      "Environment 13: Episode 7131, Score -116.57536725556851, Avg_Score -119.91699335056154\n",
      "Adding trajectory to replay buffer: step 29320, counter 468627\n",
      "Environment 0: Episode 7132, Score -122.38595754383319, Avg_Score -119.91710454834298\n",
      "Adding trajectory to replay buffer: step 29321, counter 468697\n",
      "Environment 1: Episode 7133, Score -118.70680980741172, Avg_Score -119.89168861889287\n",
      "Adding trajectory to replay buffer: step 29321, counter 468741\n",
      "Environment 8: Episode 7134, Score -122.64013407722302, Avg_Score -119.95507587150918\n",
      "Adding trajectory to replay buffer: step 29323, counter 468804\n",
      "Environment 3: Episode 7135, Score -116.56093600244759, Avg_Score -119.9272457707789\n",
      "Adding trajectory to replay buffer: step 29323, counter 468848\n",
      "Environment 6: Episode 7136, Score -122.21919276725811, Avg_Score -119.92699117965144\n",
      "Adding trajectory to replay buffer: step 29333, counter 468910\n",
      "Environment 10: Episode 7137, Score -115.32619837491339, Avg_Score -119.87123865734988\n",
      "Adding trajectory to replay buffer: step 29336, counter 468993\n",
      "Environment 7: Episode 7138, Score -110.92457935626334, Avg_Score -119.80188350107795\n",
      "Adding trajectory to replay buffer: step 29341, counter 469064\n",
      "Environment 12: Episode 7139, Score -119.05381546130326, Avg_Score -119.78713773111402\n",
      "Adding trajectory to replay buffer: step 29350, counter 469107\n",
      "Environment 14: Episode 7140, Score -122.24762245257696, Avg_Score -119.8639734394718\n",
      "Adding trajectory to replay buffer: step 29351, counter 469151\n",
      "Environment 5: Episode 7141, Score -122.81063661830159, Avg_Score -119.8846521888548\n",
      "Adding trajectory to replay buffer: step 29352, counter 469214\n",
      "Environment 2: Episode 7142, Score -116.67954976184852, Avg_Score -119.8424987535645\n",
      "Adding trajectory to replay buffer: step 29358, counter 469260\n",
      "Environment 15: Episode 7143, Score -120.59338420700779, Avg_Score -119.85321558428132\n",
      "Adding trajectory to replay buffer: step 29361, counter 469305\n",
      "Environment 11: Episode 7144, Score -123.02672019422799, Avg_Score -119.91981469261617\n",
      "Adding trajectory to replay buffer: step 29362, counter 469350\n",
      "Environment 9: Episode 7145, Score -122.9615038895042, Avg_Score -119.93269573312767\n",
      "Adding trajectory to replay buffer: step 29362, counter 469394\n",
      "Environment 13: Episode 7146, Score -122.25880534679754, Avg_Score -119.94429159735559\n",
      "Adding trajectory to replay buffer: step 29367, counter 469441\n",
      "Environment 0: Episode 7147, Score -119.1727450535254, Avg_Score -119.92613130005438\n",
      "Adding trajectory to replay buffer: step 29367, counter 469487\n",
      "Environment 1: Episode 7148, Score -120.232181444748, Avg_Score -119.90648911439327\n",
      "Adding trajectory to replay buffer: step 29367, counter 469531\n",
      "Environment 6: Episode 7149, Score -121.81076977646785, Avg_Score -119.89273097888035\n",
      "Adding trajectory to replay buffer: step 29373, counter 469583\n",
      "Environment 8: Episode 7150, Score -118.23548424264602, Avg_Score -119.86592953814181\n",
      "Adding trajectory to replay buffer: step 29375, counter 469625\n",
      "Environment 10: Episode 7151, Score -118.32914516390116, Avg_Score -119.89915759582432\n",
      "Adding trajectory to replay buffer: step 29395, counter 469670\n",
      "Environment 14: Episode 7152, Score -121.99573423659709, Avg_Score -119.95405632092518\n",
      "Adding trajectory to replay buffer: step 29396, counter 469715\n",
      "Environment 5: Episode 7153, Score -122.65806369725925, Avg_Score -120.02374078886956\n",
      "Adding trajectory to replay buffer: step 29398, counter 469761\n",
      "Environment 2: Episode 7154, Score -121.3878356212241, Avg_Score -119.91703497839205\n",
      "Adding trajectory to replay buffer: step 29409, counter 469812\n",
      "Environment 15: Episode 7155, Score -115.6223180795374, Avg_Score -119.83209652073832\n",
      "Adding trajectory to replay buffer: step 29412, counter 469862\n",
      "Environment 9: Episode 7156, Score -116.76270077480623, Avg_Score -119.75868624391383\n",
      "Adding trajectory to replay buffer: step 29415, counter 469910\n",
      "Environment 1: Episode 7157, Score -120.1436332926005, Avg_Score -119.73294047994153\n",
      "Adding trajectory to replay buffer: step 29415, counter 469963\n",
      "Environment 13: Episode 7158, Score -118.03813309685886, Avg_Score -119.69130994634901\n",
      "Adding trajectory to replay buffer: step 29417, counter 470007\n",
      "Environment 8: Episode 7159, Score -123.34486630383009, Avg_Score -119.70098712385271\n",
      "Adding trajectory to replay buffer: step 29418, counter 470050\n",
      "Environment 10: Episode 7160, Score -123.12739769713394, Avg_Score -119.72548863606691\n",
      "Adding trajectory to replay buffer: step 29425, counter 470195\n",
      "Environment 4: Episode 7161, Score -130.84817978185703, Avg_Score -119.87122202103703\n",
      "Adding trajectory to replay buffer: step 29428, counter 470256\n",
      "Environment 0: Episode 7162, Score -116.13449411133429, Avg_Score -119.80845473304912\n",
      "Adding trajectory to replay buffer: step 29435, counter 470324\n",
      "Environment 6: Episode 7163, Score -117.72927337243222, Avg_Score -119.79458074448593\n",
      "Adding trajectory to replay buffer: step 29438, counter 470367\n",
      "Environment 14: Episode 7164, Score -117.95464744876574, Avg_Score -119.73326246380944\n",
      "Adding trajectory to replay buffer: step 29439, counter 470445\n",
      "Environment 11: Episode 7165, Score -119.71777558148528, Avg_Score -119.74484553966494\n",
      "Adding trajectory to replay buffer: step 29441, counter 470490\n",
      "Environment 5: Episode 7166, Score -122.80853959143161, Avg_Score -119.74514676156323\n",
      "Adding trajectory to replay buffer: step 29443, counter 470535\n",
      "Environment 2: Episode 7167, Score -120.99654822220778, Avg_Score -119.72808976624655\n",
      "Adding trajectory to replay buffer: step 29448, counter 470647\n",
      "Environment 7: Episode 7168, Score -121.80883130579441, Avg_Score -119.72395562848392\n",
      "Adding trajectory to replay buffer: step 29458, counter 470693\n",
      "Environment 9: Episode 7169, Score -121.59932437396733, Avg_Score -119.78488170764531\n",
      "Adding trajectory to replay buffer: step 29459, counter 470737\n",
      "Environment 1: Episode 7170, Score -122.85182265036926, Avg_Score -119.7816833662045\n",
      "Adding trajectory to replay buffer: step 29461, counter 470781\n",
      "Environment 8: Episode 7171, Score -118.58081794428205, Avg_Score -119.74066453671242\n",
      "Adding trajectory to replay buffer: step 29461, counter 470827\n",
      "Environment 13: Episode 7172, Score -120.92902595526834, Avg_Score -119.8041480057351\n",
      "Adding trajectory to replay buffer: step 29462, counter 470880\n",
      "Environment 15: Episode 7173, Score -114.97851007910322, Avg_Score -119.79359959184491\n",
      "Adding trajectory to replay buffer: step 29464, counter 470926\n",
      "Environment 10: Episode 7174, Score -120.51546502505428, Avg_Score -119.79127012441396\n",
      "Adding trajectory to replay buffer: step 29472, counter 471075\n",
      "Environment 3: Episode 7175, Score -121.58416332748004, Avg_Score -119.8570831491196\n",
      "Adding trajectory to replay buffer: step 29478, counter 471125\n",
      "Environment 0: Episode 7176, Score -119.61625693750133, Avg_Score -119.8313740082571\n",
      "Adding trajectory to replay buffer: step 29480, counter 471170\n",
      "Environment 6: Episode 7177, Score -121.31523659144281, Avg_Score -119.82684579962655\n",
      "Adding trajectory to replay buffer: step 29481, counter 471213\n",
      "Environment 14: Episode 7178, Score -122.2115557976663, Avg_Score -119.8657998617755\n",
      "Adding trajectory to replay buffer: step 29484, counter 471258\n",
      "Environment 11: Episode 7179, Score -121.27392864958166, Avg_Score -119.85493339346058\n",
      "Adding trajectory to replay buffer: step 29486, counter 471319\n",
      "Environment 4: Episode 7180, Score -116.58394993040244, Avg_Score -119.8685140822836\n",
      "Adding trajectory to replay buffer: step 29487, counter 471365\n",
      "Environment 5: Episode 7181, Score -121.52413039652444, Avg_Score -119.87434169177406\n",
      "Adding trajectory to replay buffer: step 29488, counter 471410\n",
      "Environment 2: Episode 7182, Score -123.32647238264357, Avg_Score -119.8902699012629\n",
      "Adding trajectory to replay buffer: step 29502, counter 471451\n",
      "Environment 8: Episode 7183, Score -114.80205674008963, Avg_Score -119.86121155920402\n",
      "Adding trajectory to replay buffer: step 29510, counter 471500\n",
      "Environment 13: Episode 7184, Score -121.56799124977117, Avg_Score -119.85181675525146\n",
      "Adding trajectory to replay buffer: step 29511, counter 471549\n",
      "Environment 15: Episode 7185, Score -120.12412087106456, Avg_Score -119.83643317734084\n",
      "Adding trajectory to replay buffer: step 29513, counter 471598\n",
      "Environment 10: Episode 7186, Score -117.8864316328646, Avg_Score -119.8476991146404\n",
      "Adding trajectory to replay buffer: step 29516, counter 471642\n",
      "Environment 3: Episode 7187, Score -122.22012128153008, Avg_Score -119.84027476285316\n",
      "Adding trajectory to replay buffer: step 29522, counter 471686\n",
      "Environment 0: Episode 7188, Score -122.62188247055684, Avg_Score -119.84430412222817\n",
      "Adding trajectory to replay buffer: step 29525, counter 471731\n",
      "Environment 6: Episode 7189, Score -122.41152025404077, Avg_Score -119.84668591445494\n",
      "Adding trajectory to replay buffer: step 29528, counter 471918\n",
      "Environment 12: Episode 7190, Score -115.424367092981, Avg_Score -119.7717725138365\n",
      "Adding trajectory to replay buffer: step 29530, counter 471962\n",
      "Environment 4: Episode 7191, Score -122.3972039712593, Avg_Score -119.77011507573339\n",
      "Adding trajectory to replay buffer: step 29532, counter 472006\n",
      "Environment 2: Episode 7192, Score -120.79269944855075, Avg_Score -119.85693493340544\n",
      "Adding trajectory to replay buffer: step 29534, counter 472053\n",
      "Environment 5: Episode 7193, Score -120.92355462950593, Avg_Score -119.89859746990376\n",
      "Adding trajectory to replay buffer: step 29541, counter 472146\n",
      "Environment 7: Episode 7194, Score -123.2822364689242, Avg_Score -119.92792701973653\n",
      "Adding trajectory to replay buffer: step 29546, counter 472233\n",
      "Environment 1: Episode 7195, Score -111.25783049866868, Avg_Score -119.85842349687962\n",
      "Adding trajectory to replay buffer: step 29547, counter 472278\n",
      "Environment 8: Episode 7196, Score -120.82142851872867, Avg_Score -119.83772050828928\n",
      "Adding trajectory to replay buffer: step 29555, counter 472322\n",
      "Environment 15: Episode 7197, Score -123.24853441016315, Avg_Score -119.85593770533947\n",
      "Adding trajectory to replay buffer: step 29556, counter 472365\n",
      "Environment 10: Episode 7198, Score -123.90501468819815, Avg_Score -119.87159806340833\n",
      "Adding trajectory to replay buffer: step 29560, counter 472409\n",
      "Environment 3: Episode 7199, Score -122.78956483309219, Avg_Score -119.94682154642723\n",
      "Adding trajectory to replay buffer: step 29563, counter 472514\n",
      "Environment 9: Episode 7200, Score -124.90470878100334, Avg_Score -120.01195601041931\n",
      "Adding trajectory to replay buffer: step 29565, counter 472557\n",
      "Environment 0: Episode 7201, Score -123.68268111946185, Avg_Score -120.05920887299503\n",
      "Adding trajectory to replay buffer: step 29567, counter 472599\n",
      "Environment 6: Episode 7202, Score -123.07655573833858, Avg_Score -120.09365211182974\n",
      "Adding trajectory to replay buffer: step 29570, counter 472688\n",
      "Environment 14: Episode 7203, Score -113.81845417533195, Avg_Score -120.0570096341225\n",
      "Adding trajectory to replay buffer: step 29571, counter 472731\n",
      "Environment 12: Episode 7204, Score -123.48017516949028, Avg_Score -120.14796007463066\n",
      "Adding trajectory to replay buffer: step 29574, counter 472775\n",
      "Environment 4: Episode 7205, Score -122.60674610531268, Avg_Score -120.22041869725264\n",
      "Adding trajectory to replay buffer: step 29575, counter 472818\n",
      "Environment 2: Episode 7206, Score -123.58550564395574, Avg_Score -120.24737220627517\n",
      "Adding trajectory to replay buffer: step 29577, counter 472861\n",
      "Environment 5: Episode 7207, Score -123.39698051144804, Avg_Score -120.30038968970925\n",
      "Adding trajectory to replay buffer: step 29585, counter 472905\n",
      "Environment 7: Episode 7208, Score -122.38793545879051, Avg_Score -120.3877527338344\n",
      "Adding trajectory to replay buffer: step 29590, counter 472949\n",
      "Environment 1: Episode 7209, Score -122.49264196812734, Avg_Score -120.43304456363674\n",
      "Adding trajectory to replay buffer: step 29600, counter 472994\n",
      "Environment 15: Episode 7210, Score -120.67976789871355, Avg_Score -120.4298302048462\n",
      "Adding trajectory to replay buffer: step 29602, counter 473040\n",
      "Environment 10: Episode 7211, Score -120.80797588531114, Avg_Score -120.4746517654849\n",
      "Adding trajectory to replay buffer: step 29606, counter 473086\n",
      "Environment 3: Episode 7212, Score -122.98384614507482, Avg_Score -120.48804503087662\n",
      "Adding trajectory to replay buffer: step 29612, counter 473131\n",
      "Environment 6: Episode 7213, Score -117.80256498910735, Avg_Score -120.44642224359619\n",
      "Adding trajectory to replay buffer: step 29613, counter 473179\n",
      "Environment 0: Episode 7214, Score -121.35608946942538, Avg_Score -120.47005592264011\n",
      "Adding trajectory to replay buffer: step 29613, counter 473222\n",
      "Environment 14: Episode 7215, Score -120.79211185317797, Avg_Score -120.46276148688132\n",
      "Adding trajectory to replay buffer: step 29615, counter 473266\n",
      "Environment 12: Episode 7216, Score -122.92081485558425, Avg_Score -120.47301854765733\n",
      "Adding trajectory to replay buffer: step 29617, counter 473336\n",
      "Environment 8: Episode 7217, Score -117.69861958293129, Avg_Score -120.42590542133985\n",
      "Adding trajectory to replay buffer: step 29619, counter 473381\n",
      "Environment 4: Episode 7218, Score -121.66494014245147, Avg_Score -120.38154135888877\n",
      "Adding trajectory to replay buffer: step 29620, counter 473424\n",
      "Environment 5: Episode 7219, Score -115.7433687438909, Avg_Score -120.32712899228409\n",
      "Adding trajectory to replay buffer: step 29620, counter 473560\n",
      "Environment 11: Episode 7220, Score -118.38084718929309, Avg_Score -120.29584071916321\n",
      "Adding trajectory to replay buffer: step 29629, counter 473604\n",
      "Environment 7: Episode 7221, Score -120.12060151325663, Avg_Score -120.28557074014067\n",
      "Adding trajectory to replay buffer: step 29641, counter 473645\n",
      "Environment 15: Episode 7222, Score -118.42073768974468, Avg_Score -120.25283479298776\n",
      "Adding trajectory to replay buffer: step 29648, counter 473691\n",
      "Environment 10: Episode 7223, Score -117.92126684641839, Avg_Score -120.29703763433312\n",
      "Adding trajectory to replay buffer: step 29655, counter 473734\n",
      "Environment 6: Episode 7224, Score -122.80674057326341, Avg_Score -120.3795631009249\n",
      "Adding trajectory to replay buffer: step 29656, counter 473777\n",
      "Environment 0: Episode 7225, Score -122.71333753141947, Avg_Score -120.38137539659854\n",
      "Adding trajectory to replay buffer: step 29658, counter 473822\n",
      "Environment 14: Episode 7226, Score -121.07611740914857, Avg_Score -120.4135736354301\n",
      "Adding trajectory to replay buffer: step 29659, counter 473866\n",
      "Environment 12: Episode 7227, Score -122.01495988777963, Avg_Score -120.47230875578303\n",
      "Adding trajectory to replay buffer: step 29661, counter 473952\n",
      "Environment 2: Episode 7228, Score -113.51045932176771, Avg_Score -120.37984821754877\n",
      "Adding trajectory to replay buffer: step 29661, counter 473996\n",
      "Environment 8: Episode 7229, Score -120.91818417317484, Avg_Score -120.4121744715235\n",
      "Adding trajectory to replay buffer: step 29663, counter 474039\n",
      "Environment 5: Episode 7230, Score -119.38247307512847, Avg_Score -120.40345939561531\n",
      "Adding trajectory to replay buffer: step 29663, counter 474082\n",
      "Environment 11: Episode 7231, Score -118.64258222375003, Avg_Score -120.42413154529713\n",
      "Adding trajectory to replay buffer: step 29672, counter 474125\n",
      "Environment 7: Episode 7232, Score -117.26680047589913, Avg_Score -120.37293997461781\n",
      "Adding trajectory to replay buffer: step 29686, counter 474170\n",
      "Environment 15: Episode 7233, Score -121.55207515529172, Avg_Score -120.4013926280966\n",
      "Adding trajectory to replay buffer: step 29696, counter 474218\n",
      "Environment 10: Episode 7234, Score -117.20101923919655, Avg_Score -120.34700147971633\n",
      "Adding trajectory to replay buffer: step 29697, counter 474260\n",
      "Environment 6: Episode 7235, Score -118.30057710763191, Avg_Score -120.36439789076817\n",
      "Adding trajectory to replay buffer: step 29700, counter 474304\n",
      "Environment 0: Episode 7236, Score -122.60003846122945, Avg_Score -120.3682063477079\n",
      "Adding trajectory to replay buffer: step 29703, counter 474349\n",
      "Environment 14: Episode 7237, Score -121.1844277062534, Avg_Score -120.42678864102129\n",
      "Adding trajectory to replay buffer: step 29704, counter 474392\n",
      "Environment 2: Episode 7238, Score -123.30744187677341, Avg_Score -120.55061726622637\n",
      "Adding trajectory to replay buffer: step 29704, counter 474435\n",
      "Environment 8: Episode 7239, Score -123.29724386557626, Avg_Score -120.5930515502691\n",
      "Adding trajectory to replay buffer: step 29706, counter 474478\n",
      "Environment 5: Episode 7240, Score -122.7838793454816, Avg_Score -120.59841411919817\n",
      "Adding trajectory to replay buffer: step 29706, counter 474521\n",
      "Environment 11: Episode 7241, Score -122.74396998247629, Avg_Score -120.5977474528399\n",
      "Adding trajectory to replay buffer: step 29716, counter 474565\n",
      "Environment 7: Episode 7242, Score -121.38855973263891, Avg_Score -120.64483755254778\n",
      "Adding trajectory to replay buffer: step 29724, counter 474683\n",
      "Environment 3: Episode 7243, Score -113.89338241594781, Avg_Score -120.57783753463723\n",
      "Adding trajectory to replay buffer: step 29724, counter 474748\n",
      "Environment 12: Episode 7244, Score -117.53208857891087, Avg_Score -120.52289121848403\n",
      "Adding trajectory to replay buffer: step 29725, counter 474963\n",
      "Environment 13: Episode 7245, Score -124.92233898350625, Avg_Score -120.54249956942407\n",
      "Adding trajectory to replay buffer: step 29729, counter 475102\n",
      "Environment 1: Episode 7246, Score -128.58832031424024, Avg_Score -120.60579471909848\n",
      "Adding trajectory to replay buffer: step 29739, counter 475145\n",
      "Environment 10: Episode 7247, Score -122.30149423502571, Avg_Score -120.63708221091349\n",
      "Adding trajectory to replay buffer: step 29741, counter 475189\n",
      "Environment 6: Episode 7248, Score -122.09826132697921, Avg_Score -120.65574300973577\n",
      "Adding trajectory to replay buffer: step 29745, counter 475234\n",
      "Environment 0: Episode 7249, Score -121.16462038020354, Avg_Score -120.64928151577317\n",
      "Adding trajectory to replay buffer: step 29747, counter 475278\n",
      "Environment 14: Episode 7250, Score -122.02591678286257, Avg_Score -120.68718584117532\n",
      "Adding trajectory to replay buffer: step 29749, counter 475323\n",
      "Environment 2: Episode 7251, Score -122.2267767283755, Avg_Score -120.72616215682007\n",
      "Adding trajectory to replay buffer: step 29749, counter 475368\n",
      "Environment 8: Episode 7252, Score -121.18557411755032, Avg_Score -120.71806055562956\n",
      "Adding trajectory to replay buffer: step 29749, counter 475554\n",
      "Environment 9: Episode 7253, Score -122.21451810627553, Avg_Score -120.71362509971974\n",
      "Adding trajectory to replay buffer: step 29751, counter 475599\n",
      "Environment 5: Episode 7254, Score -119.53430186805377, Avg_Score -120.69508976218805\n",
      "Adding trajectory to replay buffer: step 29752, counter 475645\n",
      "Environment 11: Episode 7255, Score -120.65516617825193, Avg_Score -120.7454182431752\n",
      "Adding trajectory to replay buffer: step 29770, counter 475691\n",
      "Environment 3: Episode 7256, Score -117.17668515231833, Avg_Score -120.74955808695033\n",
      "Adding trajectory to replay buffer: step 29770, counter 475842\n",
      "Environment 4: Episode 7257, Score -116.63658755048364, Avg_Score -120.71448762952915\n",
      "Adding trajectory to replay buffer: step 29770, counter 475888\n",
      "Environment 12: Episode 7258, Score -117.80140554487612, Avg_Score -120.71212035400934\n",
      "Adding trajectory to replay buffer: step 29775, counter 475934\n",
      "Environment 1: Episode 7259, Score -121.41546624171795, Avg_Score -120.69282635338821\n",
      "Adding trajectory to replay buffer: step 29777, counter 476025\n",
      "Environment 15: Episode 7260, Score -114.45451841197162, Avg_Score -120.60609756053658\n",
      "Adding trajectory to replay buffer: step 29782, counter 476068\n",
      "Environment 10: Episode 7261, Score -122.4036607244822, Avg_Score -120.52165236996284\n",
      "Adding trajectory to replay buffer: step 29785, counter 476112\n",
      "Environment 6: Episode 7262, Score -121.86776557294218, Avg_Score -120.5789850845789\n",
      "Adding trajectory to replay buffer: step 29789, counter 476156\n",
      "Environment 0: Episode 7263, Score -122.11298727002989, Avg_Score -120.62282222355489\n",
      "Adding trajectory to replay buffer: step 29791, counter 476200\n",
      "Environment 14: Episode 7264, Score -122.71217694149446, Avg_Score -120.67039751848216\n",
      "Adding trajectory to replay buffer: step 29793, counter 476244\n",
      "Environment 2: Episode 7265, Score -117.24271404239971, Avg_Score -120.64564690309129\n",
      "Adding trajectory to replay buffer: step 29797, counter 476289\n",
      "Environment 11: Episode 7266, Score -121.38721882931206, Avg_Score -120.6314336954701\n",
      "Adding trajectory to replay buffer: step 29798, counter 476336\n",
      "Environment 5: Episode 7267, Score -120.88993988776704, Avg_Score -120.6303676121257\n",
      "Adding trajectory to replay buffer: step 29805, counter 476392\n",
      "Environment 8: Episode 7268, Score -114.34889751695965, Avg_Score -120.55576827423738\n",
      "Adding trajectory to replay buffer: step 29814, counter 476436\n",
      "Environment 4: Episode 7269, Score -122.85003356635944, Avg_Score -120.56827536616129\n",
      "Adding trajectory to replay buffer: step 29815, counter 476481\n",
      "Environment 12: Episode 7270, Score -121.81677250962207, Avg_Score -120.55792486475383\n",
      "Adding trajectory to replay buffer: step 29821, counter 476525\n",
      "Environment 15: Episode 7271, Score -122.36931633694346, Avg_Score -120.59580984868046\n",
      "Adding trajectory to replay buffer: step 29827, counter 476567\n",
      "Environment 6: Episode 7272, Score -122.82771339575325, Avg_Score -120.61479672308529\n",
      "Adding trajectory to replay buffer: step 29828, counter 476625\n",
      "Environment 3: Episode 7273, Score -114.66085626591743, Avg_Score -120.61162018495341\n",
      "Adding trajectory to replay buffer: step 29828, counter 476671\n",
      "Environment 10: Episode 7274, Score -121.00555893750551, Avg_Score -120.61652112407795\n",
      "Adding trajectory to replay buffer: step 29832, counter 476714\n",
      "Environment 0: Episode 7275, Score -123.09402757443736, Avg_Score -120.63161976654754\n",
      "Adding trajectory to replay buffer: step 29834, counter 476757\n",
      "Environment 14: Episode 7276, Score -122.69661165177078, Avg_Score -120.6624233136902\n",
      "Adding trajectory to replay buffer: step 29836, counter 476800\n",
      "Environment 2: Episode 7277, Score -122.80868655479824, Avg_Score -120.67735781332374\n",
      "Adding trajectory to replay buffer: step 29858, counter 476942\n",
      "Environment 7: Episode 7278, Score -118.62247843496377, Avg_Score -120.64146703969672\n",
      "Adding trajectory to replay buffer: step 29860, counter 476988\n",
      "Environment 4: Episode 7279, Score -119.06051059407369, Avg_Score -120.61933285914165\n",
      "Adding trajectory to replay buffer: step 29861, counter 477034\n",
      "Environment 12: Episode 7280, Score -118.33638267537518, Avg_Score -120.63685718659139\n",
      "Adding trajectory to replay buffer: step 29864, counter 477100\n",
      "Environment 5: Episode 7281, Score -111.68106217231478, Avg_Score -120.5384265043493\n",
      "Adding trajectory to replay buffer: step 29870, counter 477143\n",
      "Environment 6: Episode 7282, Score -122.79652486223782, Avg_Score -120.53312702914525\n",
      "Adding trajectory to replay buffer: step 29871, counter 477186\n",
      "Environment 3: Episode 7283, Score -122.87462428051793, Avg_Score -120.6138527045495\n",
      "Adding trajectory to replay buffer: step 29871, counter 477229\n",
      "Environment 10: Episode 7284, Score -123.0479418067007, Avg_Score -120.62865221011882\n",
      "Adding trajectory to replay buffer: step 29875, counter 477272\n",
      "Environment 0: Episode 7285, Score -122.95092773042992, Avg_Score -120.65692027871252\n",
      "Adding trajectory to replay buffer: step 29877, counter 477315\n",
      "Environment 14: Episode 7286, Score -122.18200045461705, Avg_Score -120.69987596693\n",
      "Adding trajectory to replay buffer: step 29882, counter 477361\n",
      "Environment 2: Episode 7287, Score -121.39860785227455, Avg_Score -120.69166083263747\n",
      "Adding trajectory to replay buffer: step 29885, counter 477497\n",
      "Environment 9: Episode 7288, Score -115.07760730234658, Avg_Score -120.61621808095535\n",
      "Adding trajectory to replay buffer: step 29897, counter 477669\n",
      "Environment 13: Episode 7289, Score -120.65074250270332, Avg_Score -120.59861030344196\n",
      "Adding trajectory to replay buffer: step 29898, counter 477762\n",
      "Environment 8: Episode 7290, Score -120.81680105680849, Avg_Score -120.65253464308022\n",
      "Adding trajectory to replay buffer: step 29903, counter 477807\n",
      "Environment 7: Episode 7291, Score -120.92256310790715, Avg_Score -120.63778823444669\n",
      "Adding trajectory to replay buffer: step 29905, counter 477852\n",
      "Environment 4: Episode 7292, Score -118.60813871218078, Avg_Score -120.61594262708297\n",
      "Adding trajectory to replay buffer: step 29909, counter 477897\n",
      "Environment 5: Episode 7293, Score -123.13420560459792, Avg_Score -120.63804913683389\n",
      "Adding trajectory to replay buffer: step 29909, counter 478009\n",
      "Environment 11: Episode 7294, Score -116.52625506439185, Avg_Score -120.57048932278856\n",
      "Adding trajectory to replay buffer: step 29910, counter 478058\n",
      "Environment 12: Episode 7295, Score -117.23882043795537, Avg_Score -120.63029922218143\n",
      "Adding trajectory to replay buffer: step 29914, counter 478101\n",
      "Environment 3: Episode 7296, Score -122.65064951030911, Avg_Score -120.64859143209725\n",
      "Adding trajectory to replay buffer: step 29917, counter 478147\n",
      "Environment 10: Episode 7297, Score -122.02477278906417, Avg_Score -120.63635381588625\n",
      "Adding trajectory to replay buffer: step 29919, counter 478191\n",
      "Environment 0: Episode 7298, Score -121.97668036892885, Avg_Score -120.61707047269356\n",
      "Adding trajectory to replay buffer: step 29926, counter 478235\n",
      "Environment 2: Episode 7299, Score -122.88160317183348, Avg_Score -120.61799085608098\n",
      "Adding trajectory to replay buffer: step 29931, counter 478296\n",
      "Environment 6: Episode 7300, Score -116.19326376002593, Avg_Score -120.5308764058712\n",
      "Adding trajectory to replay buffer: step 29932, counter 478453\n",
      "Environment 1: Episode 7301, Score -116.81899877826869, Avg_Score -120.46223958245926\n",
      "Adding trajectory to replay buffer: step 29941, counter 478496\n",
      "Environment 8: Episode 7302, Score -122.0955702005184, Avg_Score -120.45242972708104\n",
      "Adding trajectory to replay buffer: step 29943, counter 478542\n",
      "Environment 13: Episode 7303, Score -121.06214185037899, Avg_Score -120.52486660383151\n",
      "Adding trajectory to replay buffer: step 29947, counter 478604\n",
      "Environment 9: Episode 7304, Score -117.2330007789374, Avg_Score -120.462394859926\n",
      "Adding trajectory to replay buffer: step 29948, counter 478649\n",
      "Environment 7: Episode 7305, Score -122.0243018491245, Avg_Score -120.4565704173641\n",
      "Adding trajectory to replay buffer: step 29951, counter 478695\n",
      "Environment 4: Episode 7306, Score -120.7275514930139, Avg_Score -120.42799087585469\n",
      "Adding trajectory to replay buffer: step 29958, counter 478739\n",
      "Environment 3: Episode 7307, Score -121.82135289264284, Avg_Score -120.41223459966663\n",
      "Adding trajectory to replay buffer: step 29962, counter 478784\n",
      "Environment 10: Episode 7308, Score -121.33561296580794, Avg_Score -120.40171137473679\n",
      "Adding trajectory to replay buffer: step 29964, counter 478829\n",
      "Environment 0: Episode 7309, Score -122.11022587431843, Avg_Score -120.3978872137987\n",
      "Adding trajectory to replay buffer: step 29968, counter 478888\n",
      "Environment 11: Episode 7310, Score -113.63280761754389, Avg_Score -120.327417610987\n",
      "Adding trajectory to replay buffer: step 29971, counter 478933\n",
      "Environment 2: Episode 7311, Score -120.86597743436259, Avg_Score -120.32799762647751\n",
      "Adding trajectory to replay buffer: step 29972, counter 478995\n",
      "Environment 12: Episode 7312, Score -115.8818320011037, Avg_Score -120.25697748503781\n",
      "Adding trajectory to replay buffer: step 29974, counter 479038\n",
      "Environment 6: Episode 7313, Score -122.83146999643627, Avg_Score -120.30726653511108\n",
      "Adding trajectory to replay buffer: step 29986, counter 479147\n",
      "Environment 14: Episode 7314, Score -125.55696481120214, Avg_Score -120.34927528852884\n",
      "Adding trajectory to replay buffer: step 29989, counter 479189\n",
      "Environment 9: Episode 7315, Score -121.76869612187457, Avg_Score -120.35904113121582\n",
      "Adding trajectory to replay buffer: step 29991, counter 479359\n",
      "Environment 15: Episode 7316, Score -121.0930779731137, Avg_Score -120.3407637623911\n",
      "Adding trajectory to replay buffer: step 29994, counter 479405\n",
      "Environment 7: Episode 7317, Score -119.8508690496441, Avg_Score -120.36228625705822\n",
      "Adding trajectory to replay buffer: step 29994, counter 479456\n",
      "Environment 13: Episode 7318, Score -117.57347347163409, Avg_Score -120.32137159035004\n",
      "Adding trajectory to replay buffer: step 29995, counter 479500\n",
      "Environment 4: Episode 7319, Score -120.39555766827056, Avg_Score -120.36789347959386\n",
      "Adding trajectory to replay buffer: step 29996, counter 479564\n",
      "Environment 1: Episode 7320, Score -117.56664264798042, Avg_Score -120.35975143418074\n",
      "Adding trajectory to replay buffer: step 30009, counter 479611\n",
      "Environment 10: Episode 7321, Score -121.83885136364898, Avg_Score -120.37693393268466\n",
      "Adding trajectory to replay buffer: step 30011, counter 479654\n",
      "Environment 11: Episode 7322, Score -122.32354039545831, Avg_Score -120.4159619597418\n",
      "Adding trajectory to replay buffer: step 30014, counter 479697\n",
      "Environment 2: Episode 7323, Score -123.24847996924507, Avg_Score -120.46923409097008\n",
      "Adding trajectory to replay buffer: step 30018, counter 479741\n",
      "Environment 6: Episode 7324, Score -121.51918148037916, Avg_Score -120.4563585000412\n",
      "Adding trajectory to replay buffer: step 30018, counter 479787\n",
      "Environment 12: Episode 7325, Score -121.52800090060694, Avg_Score -120.44450513373309\n",
      "Adding trajectory to replay buffer: step 30021, counter 479867\n",
      "Environment 8: Episode 7326, Score -115.18989439152709, Avg_Score -120.38564290355689\n",
      "Adding trajectory to replay buffer: step 30036, counter 479909\n",
      "Environment 7: Episode 7327, Score -122.16849865300767, Avg_Score -120.38717829120915\n",
      "Adding trajectory to replay buffer: step 30037, counter 479957\n",
      "Environment 9: Episode 7328, Score -120.09131844102902, Avg_Score -120.45298688240176\n",
      "Adding trajectory to replay buffer: step 30037, counter 480003\n",
      "Environment 15: Episode 7329, Score -120.16684959209834, Avg_Score -120.44547353659101\n",
      "Adding trajectory to replay buffer: step 30038, counter 480047\n",
      "Environment 13: Episode 7330, Score -121.39963983972308, Avg_Score -120.46564520423695\n",
      "Adding trajectory to replay buffer: step 30039, counter 480091\n",
      "Environment 4: Episode 7331, Score -122.18723502381145, Avg_Score -120.50109173223757\n",
      "Adding trajectory to replay buffer: step 30044, counter 480177\n",
      "Environment 3: Episode 7332, Score -114.62264539512658, Avg_Score -120.47465018142984\n",
      "Adding trajectory to replay buffer: step 30044, counter 480312\n",
      "Environment 5: Episode 7333, Score -117.79224361352932, Avg_Score -120.43705186601221\n",
      "Adding trajectory to replay buffer: step 30051, counter 480377\n",
      "Environment 14: Episode 7334, Score -117.63373510551787, Avg_Score -120.44137902467543\n",
      "Adding trajectory to replay buffer: step 30053, counter 480421\n",
      "Environment 10: Episode 7335, Score -122.84767602226945, Avg_Score -120.48685001382178\n",
      "Adding trajectory to replay buffer: step 30054, counter 480464\n",
      "Environment 11: Episode 7336, Score -123.3989067279746, Avg_Score -120.49483869648924\n",
      "Adding trajectory to replay buffer: step 30061, counter 480507\n",
      "Environment 6: Episode 7337, Score -122.74910901000041, Avg_Score -120.51048550952673\n",
      "Adding trajectory to replay buffer: step 30061, counter 480550\n",
      "Environment 12: Episode 7338, Score -122.81939722924432, Avg_Score -120.50560506305146\n",
      "Adding trajectory to replay buffer: step 30064, counter 480600\n",
      "Environment 2: Episode 7339, Score -120.11949716558928, Avg_Score -120.47382759605159\n",
      "Adding trajectory to replay buffer: step 30066, counter 480670\n",
      "Environment 1: Episode 7340, Score -118.88434005763133, Avg_Score -120.4348322031731\n",
      "Adding trajectory to replay buffer: step 30068, counter 480717\n",
      "Environment 8: Episode 7341, Score -118.95106766077546, Avg_Score -120.3969031799561\n",
      "Adding trajectory to replay buffer: step 30086, counter 480764\n",
      "Environment 4: Episode 7342, Score -119.20528375667365, Avg_Score -120.37507042019644\n",
      "Adding trajectory to replay buffer: step 30089, counter 480809\n",
      "Environment 3: Episode 7343, Score -121.1971650718376, Avg_Score -120.44810824675533\n",
      "Adding trajectory to replay buffer: step 30090, counter 480855\n",
      "Environment 5: Episode 7344, Score -121.26157466636971, Avg_Score -120.48540310762993\n",
      "Adding trajectory to replay buffer: step 30095, counter 480899\n",
      "Environment 14: Episode 7345, Score -123.3725619933363, Avg_Score -120.46990533772825\n",
      "Adding trajectory to replay buffer: step 30096, counter 480941\n",
      "Environment 11: Episode 7346, Score -116.48825266138775, Avg_Score -120.34890466119973\n",
      "Adding trajectory to replay buffer: step 30105, counter 480985\n",
      "Environment 12: Episode 7347, Score -122.64543603179729, Avg_Score -120.35234407916744\n",
      "Adding trajectory to replay buffer: step 30106, counter 481053\n",
      "Environment 13: Episode 7348, Score -111.55299432517278, Avg_Score -120.24689140914937\n",
      "Adding trajectory to replay buffer: step 30109, counter 481101\n",
      "Environment 6: Episode 7349, Score -122.4521216735132, Avg_Score -120.25976642208246\n",
      "Adding trajectory to replay buffer: step 30112, counter 481147\n",
      "Environment 1: Episode 7350, Score -123.29439417649743, Avg_Score -120.2724511960188\n",
      "Adding trajectory to replay buffer: step 30113, counter 481192\n",
      "Environment 8: Episode 7351, Score -117.73014060266811, Avg_Score -120.22748483476173\n",
      "Adding trajectory to replay buffer: step 30120, counter 481259\n",
      "Environment 10: Episode 7352, Score -117.74449425226263, Avg_Score -120.19307403610884\n",
      "Adding trajectory to replay buffer: step 30121, counter 481416\n",
      "Environment 0: Episode 7353, Score -130.77092670217064, Avg_Score -120.2786381220678\n",
      "Adding trajectory to replay buffer: step 30127, counter 481506\n",
      "Environment 9: Episode 7354, Score -114.4072665965793, Avg_Score -120.22736776935308\n",
      "Adding trajectory to replay buffer: step 30129, counter 481549\n",
      "Environment 4: Episode 7355, Score -122.63306042535044, Avg_Score -120.24714671182409\n",
      "Adding trajectory to replay buffer: step 30134, counter 481593\n",
      "Environment 5: Episode 7356, Score -122.43356479717853, Avg_Score -120.29971550827267\n",
      "Adding trajectory to replay buffer: step 30136, counter 481640\n",
      "Environment 3: Episode 7357, Score -119.53104069922429, Avg_Score -120.3286600397601\n",
      "Adding trajectory to replay buffer: step 30141, counter 481745\n",
      "Environment 7: Episode 7358, Score -114.27857533236748, Avg_Score -120.293431737635\n",
      "Adding trajectory to replay buffer: step 30156, counter 481806\n",
      "Environment 14: Episode 7359, Score -115.99556398052536, Avg_Score -120.23923271502306\n",
      "Adding trajectory to replay buffer: step 30159, counter 481852\n",
      "Environment 8: Episode 7360, Score -118.75793916008311, Avg_Score -120.28226692250419\n",
      "Adding trajectory to replay buffer: step 30162, counter 481894\n",
      "Environment 10: Episode 7361, Score -117.15078316764037, Avg_Score -120.22973814693577\n",
      "Adding trajectory to replay buffer: step 30164, counter 481946\n",
      "Environment 1: Episode 7362, Score -119.96712294378443, Avg_Score -120.2107317206442\n",
      "Adding trajectory to replay buffer: step 30164, counter 482005\n",
      "Environment 12: Episode 7363, Score -115.97554174094151, Avg_Score -120.14935726535332\n",
      "Adding trajectory to replay buffer: step 30170, counter 482054\n",
      "Environment 0: Episode 7364, Score -118.7107695174537, Avg_Score -120.10934319111294\n",
      "Adding trajectory to replay buffer: step 30171, counter 482098\n",
      "Environment 9: Episode 7365, Score -122.45605302319987, Avg_Score -120.16147658092093\n",
      "Adding trajectory to replay buffer: step 30171, counter 482173\n",
      "Environment 11: Episode 7366, Score -115.40518312963009, Avg_Score -120.10165622392412\n",
      "Adding trajectory to replay buffer: step 30176, counter 482215\n",
      "Environment 5: Episode 7367, Score -122.53567728286734, Avg_Score -120.11811359787512\n",
      "Adding trajectory to replay buffer: step 30179, counter 482258\n",
      "Environment 3: Episode 7368, Score -122.14533051458373, Avg_Score -120.19607792785136\n",
      "Adding trajectory to replay buffer: step 30201, counter 482303\n",
      "Environment 14: Episode 7369, Score -123.45645229459492, Avg_Score -120.2021421151337\n",
      "Adding trajectory to replay buffer: step 30206, counter 482345\n",
      "Environment 1: Episode 7370, Score -123.04924385552046, Avg_Score -120.21446682859269\n",
      "Adding trajectory to replay buffer: step 30206, counter 482387\n",
      "Environment 12: Episode 7371, Score -123.06736032679615, Avg_Score -120.22144726849123\n",
      "Adding trajectory to replay buffer: step 30207, counter 482453\n",
      "Environment 7: Episode 7372, Score -116.17234887984023, Avg_Score -120.15489362333209\n",
      "Adding trajectory to replay buffer: step 30212, counter 482559\n",
      "Environment 13: Episode 7373, Score -127.74147261580296, Avg_Score -120.28569978683093\n",
      "Adding trajectory to replay buffer: step 30213, counter 482602\n",
      "Environment 0: Episode 7374, Score -122.38788395555441, Avg_Score -120.29952303701143\n",
      "Adding trajectory to replay buffer: step 30214, counter 482687\n",
      "Environment 4: Episode 7375, Score -120.59357862584541, Avg_Score -120.27451854752552\n",
      "Adding trajectory to replay buffer: step 30214, counter 482730\n",
      "Environment 9: Episode 7376, Score -120.26043722966128, Avg_Score -120.25015680330439\n",
      "Adding trajectory to replay buffer: step 30214, counter 482773\n",
      "Environment 11: Episode 7377, Score -120.24716012005695, Avg_Score -120.22454153895701\n",
      "Adding trajectory to replay buffer: step 30217, counter 482953\n",
      "Environment 15: Episode 7378, Score -125.82663063812781, Avg_Score -120.29658306098865\n",
      "Adding trajectory to replay buffer: step 30227, counter 483001\n",
      "Environment 3: Episode 7379, Score -121.84302463868198, Avg_Score -120.32440820143475\n",
      "Adding trajectory to replay buffer: step 30228, counter 483053\n",
      "Environment 5: Episode 7380, Score -115.38038334952543, Avg_Score -120.29484820817625\n",
      "Adding trajectory to replay buffer: step 30238, counter 483132\n",
      "Environment 8: Episode 7381, Score -121.28828355195063, Avg_Score -120.39092042197261\n",
      "Adding trajectory to replay buffer: step 30243, counter 483266\n",
      "Environment 6: Episode 7382, Score -118.61888267872334, Avg_Score -120.34914400013747\n",
      "Adding trajectory to replay buffer: step 30246, counter 483311\n",
      "Environment 14: Episode 7383, Score -121.64767479445165, Avg_Score -120.33687450527681\n",
      "Adding trajectory to replay buffer: step 30249, counter 483398\n",
      "Environment 10: Episode 7384, Score -117.9108752776999, Avg_Score -120.28550383998677\n",
      "Adding trajectory to replay buffer: step 30251, counter 483443\n",
      "Environment 1: Episode 7385, Score -121.21560097361605, Avg_Score -120.26815057241865\n",
      "Adding trajectory to replay buffer: step 30251, counter 483488\n",
      "Environment 12: Episode 7386, Score -117.69780151212278, Avg_Score -120.22330858299371\n",
      "Adding trajectory to replay buffer: step 30256, counter 483531\n",
      "Environment 0: Episode 7387, Score -115.32590901143662, Avg_Score -120.16258159458533\n",
      "Adding trajectory to replay buffer: step 30258, counter 483577\n",
      "Environment 13: Episode 7388, Score -119.14867387649107, Avg_Score -120.20329226032676\n",
      "Adding trajectory to replay buffer: step 30262, counter 483622\n",
      "Environment 15: Episode 7389, Score -122.61451403711737, Avg_Score -120.22292997567091\n",
      "Adding trajectory to replay buffer: step 30269, counter 483677\n",
      "Environment 11: Episode 7390, Score -112.8766323766845, Avg_Score -120.14352828886967\n",
      "Adding trajectory to replay buffer: step 30273, counter 483723\n",
      "Environment 3: Episode 7391, Score -115.29230424906004, Avg_Score -120.08722570028121\n",
      "Adding trajectory to replay buffer: step 30287, counter 483767\n",
      "Environment 6: Episode 7392, Score -123.2253323156362, Avg_Score -120.13339763631576\n",
      "Adding trajectory to replay buffer: step 30290, counter 483811\n",
      "Environment 14: Episode 7393, Score -123.91683652211215, Avg_Score -120.14122394549089\n",
      "Adding trajectory to replay buffer: step 30292, counter 483896\n",
      "Environment 7: Episode 7394, Score -120.31262788158345, Avg_Score -120.17908767366282\n",
      "Adding trajectory to replay buffer: step 30293, counter 483940\n",
      "Environment 10: Episode 7395, Score -122.91397585026363, Avg_Score -120.23583922778589\n",
      "Adding trajectory to replay buffer: step 30295, counter 483984\n",
      "Environment 1: Episode 7396, Score -123.85694231503022, Avg_Score -120.2479021558331\n",
      "Adding trajectory to replay buffer: step 30295, counter 484028\n",
      "Environment 12: Episode 7397, Score -116.60942818667306, Avg_Score -120.1937487098092\n",
      "Adding trajectory to replay buffer: step 30298, counter 484112\n",
      "Environment 4: Episode 7398, Score -120.47015083686696, Avg_Score -120.1786834144886\n",
      "Adding trajectory to replay buffer: step 30299, counter 484155\n",
      "Environment 0: Episode 7399, Score -123.90006269679715, Avg_Score -120.18886800973823\n",
      "Adding trajectory to replay buffer: step 30302, counter 484199\n",
      "Environment 13: Episode 7400, Score -122.59725431456665, Avg_Score -120.25290791528364\n",
      "Adding trajectory to replay buffer: step 30306, counter 484243\n",
      "Environment 15: Episode 7401, Score -116.65685052431003, Avg_Score -120.25128643274405\n",
      "Adding trajectory to replay buffer: step 30313, counter 484287\n",
      "Environment 11: Episode 7402, Score -122.07620092753011, Avg_Score -120.25109274001416\n",
      "Adding trajectory to replay buffer: step 30316, counter 484330\n",
      "Environment 3: Episode 7403, Score -123.25270003711805, Avg_Score -120.27299832188153\n",
      "Adding trajectory to replay buffer: step 30316, counter 484432\n",
      "Environment 9: Episode 7404, Score -124.5827513265821, Avg_Score -120.34649582735797\n",
      "Adding trajectory to replay buffer: step 30321, counter 484515\n",
      "Environment 8: Episode 7405, Score -117.88371029451241, Avg_Score -120.30508991181185\n",
      "Adding trajectory to replay buffer: step 30330, counter 484558\n",
      "Environment 6: Episode 7406, Score -123.71563434106173, Avg_Score -120.33497074029236\n",
      "Adding trajectory to replay buffer: step 30334, counter 484602\n",
      "Environment 14: Episode 7407, Score -122.45229146965903, Avg_Score -120.34128012606251\n",
      "Adding trajectory to replay buffer: step 30336, counter 484646\n",
      "Environment 7: Episode 7408, Score -122.31555574480507, Avg_Score -120.35107955385246\n",
      "Adding trajectory to replay buffer: step 30337, counter 484688\n",
      "Environment 12: Episode 7409, Score -123.45438736118004, Avg_Score -120.36452116872107\n",
      "Adding trajectory to replay buffer: step 30338, counter 484731\n",
      "Environment 1: Episode 7410, Score -123.45851372268827, Avg_Score -120.4627782297725\n",
      "Adding trajectory to replay buffer: step 30339, counter 484777\n",
      "Environment 10: Episode 7411, Score -120.99530265321458, Avg_Score -120.46407148196104\n",
      "Adding trajectory to replay buffer: step 30342, counter 484820\n",
      "Environment 0: Episode 7412, Score -122.33423147617776, Avg_Score -120.52859547671176\n",
      "Adding trajectory to replay buffer: step 30342, counter 484864\n",
      "Environment 4: Episode 7413, Score -121.35468717833547, Avg_Score -120.51382764853075\n",
      "Adding trajectory to replay buffer: step 30343, counter 484979\n",
      "Environment 5: Episode 7414, Score -128.859229420263, Avg_Score -120.54685029462132\n",
      "Adding trajectory to replay buffer: step 30345, counter 485022\n",
      "Environment 13: Episode 7415, Score -122.72274772624112, Avg_Score -120.556390810665\n",
      "Adding trajectory to replay buffer: step 30359, counter 485068\n",
      "Environment 11: Episode 7416, Score -116.82350090074291, Avg_Score -120.51369503994128\n",
      "Adding trajectory to replay buffer: step 30362, counter 485114\n",
      "Environment 3: Episode 7417, Score -121.8015356231009, Avg_Score -120.53320170567585\n",
      "Adding trajectory to replay buffer: step 30362, counter 485160\n",
      "Environment 9: Episode 7418, Score -120.7619329965096, Avg_Score -120.56508630092462\n",
      "Adding trajectory to replay buffer: step 30368, counter 485207\n",
      "Environment 8: Episode 7419, Score -121.08910647966496, Avg_Score -120.57202178903857\n",
      "Adding trajectory to replay buffer: step 30376, counter 485253\n",
      "Environment 6: Episode 7420, Score -121.79164458776887, Avg_Score -120.61427180843646\n",
      "Adding trajectory to replay buffer: step 30379, counter 485296\n",
      "Environment 7: Episode 7421, Score -123.30895840695686, Avg_Score -120.62897287886952\n",
      "Adding trajectory to replay buffer: step 30379, counter 485341\n",
      "Environment 14: Episode 7422, Score -121.70244068984034, Avg_Score -120.62276188181333\n",
      "Adding trajectory to replay buffer: step 30380, counter 485384\n",
      "Environment 12: Episode 7423, Score -123.30238307704963, Avg_Score -120.62330091289137\n",
      "Adding trajectory to replay buffer: step 30381, counter 485427\n",
      "Environment 1: Episode 7424, Score -122.45782502871627, Avg_Score -120.63268734837474\n",
      "Adding trajectory to replay buffer: step 30381, counter 485469\n",
      "Environment 10: Episode 7425, Score -123.33601673967112, Avg_Score -120.65076750676539\n",
      "Adding trajectory to replay buffer: step 30385, counter 485512\n",
      "Environment 0: Episode 7426, Score -122.37660209869655, Avg_Score -120.72263458383708\n",
      "Adding trajectory to replay buffer: step 30385, counter 485555\n",
      "Environment 4: Episode 7427, Score -122.8007614273016, Avg_Score -120.72895721158002\n",
      "Adding trajectory to replay buffer: step 30387, counter 485599\n",
      "Environment 5: Episode 7428, Score -121.8715318219401, Avg_Score -120.74675934538911\n",
      "Adding trajectory to replay buffer: step 30388, counter 485642\n",
      "Environment 13: Episode 7429, Score -122.20655421190958, Avg_Score -120.7671563915872\n",
      "Adding trajectory to replay buffer: step 30395, counter 485973\n",
      "Environment 2: Episode 7430, Score -143.95167665044434, Avg_Score -120.99267675969442\n",
      "Adding trajectory to replay buffer: step 30412, counter 486017\n",
      "Environment 8: Episode 7431, Score -120.65169999403071, Avg_Score -120.97732140939661\n",
      "Adding trajectory to replay buffer: step 30412, counter 486070\n",
      "Environment 11: Episode 7432, Score -118.02229901537486, Avg_Score -121.01131794559907\n",
      "Adding trajectory to replay buffer: step 30423, counter 486131\n",
      "Environment 3: Episode 7433, Score -115.14660893706295, Avg_Score -120.98486159883441\n",
      "Adding trajectory to replay buffer: step 30423, counter 486178\n",
      "Environment 6: Episode 7434, Score -118.47913291583458, Avg_Score -120.99331557693759\n",
      "Adding trajectory to replay buffer: step 30423, counter 486239\n",
      "Environment 9: Episode 7435, Score -116.30656070809624, Avg_Score -120.92790442379588\n",
      "Adding trajectory to replay buffer: step 30424, counter 486282\n",
      "Environment 1: Episode 7436, Score -123.55471264939258, Avg_Score -120.92946248301008\n",
      "Adding trajectory to replay buffer: step 30424, counter 486325\n",
      "Environment 10: Episode 7437, Score -123.40086220780947, Avg_Score -120.93598001498816\n",
      "Adding trajectory to replay buffer: step 30427, counter 486373\n",
      "Environment 14: Episode 7438, Score -122.61775037329768, Avg_Score -120.9339635464287\n",
      "Adding trajectory to replay buffer: step 30428, counter 486416\n",
      "Environment 0: Episode 7439, Score -122.93338806505066, Avg_Score -120.9621024554233\n",
      "Adding trajectory to replay buffer: step 30428, counter 486459\n",
      "Environment 4: Episode 7440, Score -123.26870052797129, Avg_Score -121.00594606012672\n",
      "Adding trajectory to replay buffer: step 30430, counter 486502\n",
      "Environment 5: Episode 7441, Score -123.03527648931555, Avg_Score -121.04678814841212\n",
      "Adding trajectory to replay buffer: step 30432, counter 486546\n",
      "Environment 13: Episode 7442, Score -123.18761445869505, Avg_Score -121.08661145543233\n",
      "Adding trajectory to replay buffer: step 30440, counter 486591\n",
      "Environment 2: Episode 7443, Score -121.47868629556027, Avg_Score -121.08942666766957\n",
      "Adding trajectory to replay buffer: step 30441, counter 486652\n",
      "Environment 12: Episode 7444, Score -112.60471250530829, Avg_Score -121.00285804605898\n",
      "Adding trajectory to replay buffer: step 30456, counter 486696\n",
      "Environment 8: Episode 7445, Score -116.0913614567512, Avg_Score -120.93004604069313\n",
      "Adding trajectory to replay buffer: step 30457, counter 486741\n",
      "Environment 11: Episode 7446, Score -120.91641282925568, Avg_Score -120.9743276423718\n",
      "Adding trajectory to replay buffer: step 30467, counter 486785\n",
      "Environment 6: Episode 7447, Score -116.65660379269967, Avg_Score -120.91443931998083\n",
      "Adding trajectory to replay buffer: step 30470, counter 486831\n",
      "Environment 10: Episode 7448, Score -117.93045691193454, Avg_Score -120.97821394584845\n",
      "Adding trajectory to replay buffer: step 30476, counter 486877\n",
      "Environment 5: Episode 7449, Score -121.35791379195638, Avg_Score -120.9672718670329\n",
      "Adding trajectory to replay buffer: step 30478, counter 486931\n",
      "Environment 1: Episode 7450, Score -117.5651594367406, Avg_Score -120.90997951963533\n",
      "Adding trajectory to replay buffer: step 30478, counter 486986\n",
      "Environment 3: Episode 7451, Score -117.76483235807841, Avg_Score -120.91032643718944\n",
      "Adding trajectory to replay buffer: step 30478, counter 487036\n",
      "Environment 4: Episode 7452, Score -120.42436125310635, Avg_Score -120.93712510719786\n",
      "Adding trajectory to replay buffer: step 30486, counter 487099\n",
      "Environment 9: Episode 7453, Score -112.46959584563349, Avg_Score -120.75411179863251\n",
      "Adding trajectory to replay buffer: step 30486, counter 487144\n",
      "Environment 12: Episode 7454, Score -123.6097170660893, Avg_Score -120.8461363033276\n",
      "Adding trajectory to replay buffer: step 30489, counter 487201\n",
      "Environment 13: Episode 7455, Score -115.06644199930442, Avg_Score -120.77047011906714\n",
      "Adding trajectory to replay buffer: step 30496, counter 487391\n",
      "Environment 15: Episode 7456, Score -130.97656928187266, Avg_Score -120.85590016391409\n",
      "Adding trajectory to replay buffer: step 30502, counter 487465\n",
      "Environment 0: Episode 7457, Score -120.04928887131126, Avg_Score -120.86108264563494\n",
      "Adding trajectory to replay buffer: step 30502, counter 487511\n",
      "Environment 8: Episode 7458, Score -121.67859688333299, Avg_Score -120.93508286114461\n",
      "Adding trajectory to replay buffer: step 30503, counter 487587\n",
      "Environment 14: Episode 7459, Score -113.61480316261449, Avg_Score -120.9112752529655\n",
      "Adding trajectory to replay buffer: step 30507, counter 487637\n",
      "Environment 11: Episode 7460, Score -118.61477105067173, Avg_Score -120.90984357187139\n",
      "Adding trajectory to replay buffer: step 30511, counter 487681\n",
      "Environment 6: Episode 7461, Score -122.39786098223243, Avg_Score -120.96231435001731\n",
      "Adding trajectory to replay buffer: step 30521, counter 487724\n",
      "Environment 3: Episode 7462, Score -122.90997981992922, Avg_Score -120.99174291877877\n",
      "Adding trajectory to replay buffer: step 30522, counter 487768\n",
      "Environment 4: Episode 7463, Score -121.89386641858952, Avg_Score -121.05092616555525\n",
      "Adding trajectory to replay buffer: step 30523, counter 487813\n",
      "Environment 1: Episode 7464, Score -120.73464983388905, Avg_Score -121.0711649687196\n",
      "Adding trajectory to replay buffer: step 30529, counter 487856\n",
      "Environment 9: Episode 7465, Score -116.29539585768742, Avg_Score -121.00955839706448\n",
      "Adding trajectory to replay buffer: step 30530, counter 487900\n",
      "Environment 12: Episode 7466, Score -123.02350731584181, Avg_Score -121.0857416389266\n",
      "Adding trajectory to replay buffer: step 30537, counter 488058\n",
      "Environment 7: Episode 7467, Score -116.26517609602585, Avg_Score -121.02303662705819\n",
      "Adding trajectory to replay buffer: step 30543, counter 488105\n",
      "Environment 15: Episode 7468, Score -120.42070863326268, Avg_Score -121.00579040824498\n",
      "Adding trajectory to replay buffer: step 30546, counter 488149\n",
      "Environment 0: Episode 7469, Score -117.46387003730548, Avg_Score -120.94586458567208\n",
      "Adding trajectory to replay buffer: step 30546, counter 488206\n",
      "Environment 13: Episode 7470, Score -121.81813547549521, Avg_Score -120.93355350187181\n",
      "Adding trajectory to replay buffer: step 30547, counter 488251\n",
      "Environment 8: Episode 7471, Score -120.35672380488117, Avg_Score -120.90644713665267\n",
      "Adding trajectory to replay buffer: step 30551, counter 488295\n",
      "Environment 11: Episode 7472, Score -122.34577611768182, Avg_Score -120.96818140903109\n",
      "Adding trajectory to replay buffer: step 30552, counter 488344\n",
      "Environment 14: Episode 7473, Score -118.03672252066991, Avg_Score -120.87113390807977\n",
      "Adding trajectory to replay buffer: step 30563, counter 488386\n",
      "Environment 3: Episode 7474, Score -119.94603151198415, Avg_Score -120.84671538364408\n",
      "Adding trajectory to replay buffer: step 30564, counter 488439\n",
      "Environment 6: Episode 7475, Score -117.98274640995201, Avg_Score -120.82060706148513\n",
      "Adding trajectory to replay buffer: step 30566, counter 488483\n",
      "Environment 4: Episode 7476, Score -122.96690741285806, Avg_Score -120.84767176331711\n",
      "Adding trajectory to replay buffer: step 30567, counter 488527\n",
      "Environment 1: Episode 7477, Score -122.56861602259055, Avg_Score -120.87088632234244\n",
      "Adding trajectory to replay buffer: step 30574, counter 488572\n",
      "Environment 9: Episode 7478, Score -123.03565203986814, Avg_Score -120.84297653635984\n",
      "Adding trajectory to replay buffer: step 30575, counter 488617\n",
      "Environment 12: Episode 7479, Score -122.92712316696904, Avg_Score -120.85381752164268\n",
      "Adding trajectory to replay buffer: step 30577, counter 488718\n",
      "Environment 5: Episode 7480, Score -114.50183112837561, Avg_Score -120.8450319994312\n",
      "Adding trajectory to replay buffer: step 30580, counter 488761\n",
      "Environment 7: Episode 7481, Score -122.71904500546958, Avg_Score -120.85933961396638\n",
      "Adding trajectory to replay buffer: step 30584, counter 488905\n",
      "Environment 2: Episode 7482, Score -118.18104698880512, Avg_Score -120.85496125706719\n",
      "Adding trajectory to replay buffer: step 30591, counter 489026\n",
      "Environment 10: Episode 7483, Score -114.76045546251815, Avg_Score -120.78608906374788\n",
      "Adding trajectory to replay buffer: step 30591, counter 489066\n",
      "Environment 11: Episode 7484, Score -114.97473200874528, Avg_Score -120.75672763105831\n",
      "Adding trajectory to replay buffer: step 30592, counter 489115\n",
      "Environment 15: Episode 7485, Score -119.98276825799763, Avg_Score -120.74439930390214\n",
      "Adding trajectory to replay buffer: step 30599, counter 489162\n",
      "Environment 14: Episode 7486, Score -117.20779430500419, Avg_Score -120.73949923183095\n",
      "Adding trajectory to replay buffer: step 30602, counter 489217\n",
      "Environment 8: Episode 7487, Score -114.68389909063218, Avg_Score -120.73307913262292\n",
      "Adding trajectory to replay buffer: step 30607, counter 489261\n",
      "Environment 3: Episode 7488, Score -121.74037721735611, Avg_Score -120.75899616603158\n",
      "Adding trajectory to replay buffer: step 30610, counter 489307\n",
      "Environment 6: Episode 7489, Score -119.13111955918993, Avg_Score -120.7241622212523\n",
      "Adding trajectory to replay buffer: step 30614, counter 489354\n",
      "Environment 1: Episode 7490, Score -121.31150994555838, Avg_Score -120.80851099694104\n",
      "Adding trajectory to replay buffer: step 30615, counter 489423\n",
      "Environment 0: Episode 7491, Score -118.0528207412536, Avg_Score -120.83611616186298\n",
      "Adding trajectory to replay buffer: step 30620, counter 489466\n",
      "Environment 5: Episode 7492, Score -122.65449141870378, Avg_Score -120.83040775289365\n",
      "Adding trajectory to replay buffer: step 30624, counter 489510\n",
      "Environment 7: Episode 7493, Score -123.15367393142172, Avg_Score -120.82277612698674\n",
      "Adding trajectory to replay buffer: step 30634, counter 489553\n",
      "Environment 10: Episode 7494, Score -116.05606813594264, Avg_Score -120.78021052953032\n",
      "Adding trajectory to replay buffer: step 30636, counter 489643\n",
      "Environment 13: Episode 7495, Score -113.48719302192579, Avg_Score -120.68594270124693\n",
      "Adding trajectory to replay buffer: step 30638, counter 489689\n",
      "Environment 15: Episode 7496, Score -121.17453934997506, Avg_Score -120.6591186715964\n",
      "Adding trajectory to replay buffer: step 30643, counter 489733\n",
      "Environment 14: Episode 7497, Score -120.0870726792626, Avg_Score -120.69389511652228\n",
      "Adding trajectory to replay buffer: step 30649, counter 489808\n",
      "Environment 9: Episode 7498, Score -114.24638029966627, Avg_Score -120.6316574111503\n",
      "Adding trajectory to replay buffer: step 30650, counter 489874\n",
      "Environment 2: Episode 7499, Score -116.25022923177542, Avg_Score -120.55515907650006\n",
      "Adding trajectory to replay buffer: step 30651, counter 489918\n",
      "Environment 3: Episode 7500, Score -122.47019915993636, Avg_Score -120.55388852495376\n",
      "Adding trajectory to replay buffer: step 30655, counter 489963\n",
      "Environment 6: Episode 7501, Score -119.90353691914119, Avg_Score -120.58635538890208\n",
      "Adding trajectory to replay buffer: step 30661, counter 490009\n",
      "Environment 0: Episode 7502, Score -122.26016493809014, Avg_Score -120.58819502900765\n",
      "Adding trajectory to replay buffer: step 30662, counter 490080\n",
      "Environment 11: Episode 7503, Score -113.40410188266512, Avg_Score -120.48970904746311\n",
      "Adding trajectory to replay buffer: step 30667, counter 490123\n",
      "Environment 7: Episode 7504, Score -120.81249202692695, Avg_Score -120.45200645446657\n",
      "Adding trajectory to replay buffer: step 30683, counter 490168\n",
      "Environment 15: Episode 7505, Score -120.65763973798542, Avg_Score -120.47974574890128\n",
      "Adding trajectory to replay buffer: step 30685, counter 490278\n",
      "Environment 12: Episode 7506, Score -122.71311049052515, Avg_Score -120.46972051039592\n",
      "Adding trajectory to replay buffer: step 30686, counter 490350\n",
      "Environment 1: Episode 7507, Score -112.99902701758563, Avg_Score -120.37518786587518\n",
      "Adding trajectory to replay buffer: step 30688, counter 490404\n",
      "Environment 10: Episode 7508, Score -122.14125753280521, Avg_Score -120.37344488375518\n",
      "Adding trajectory to replay buffer: step 30688, counter 490449\n",
      "Environment 14: Episode 7509, Score -123.58093018683854, Avg_Score -120.37471031201177\n",
      "Adding trajectory to replay buffer: step 30691, counter 490574\n",
      "Environment 4: Episode 7510, Score -124.41476257125974, Avg_Score -120.3842728004975\n",
      "Adding trajectory to replay buffer: step 30693, counter 490617\n",
      "Environment 2: Episode 7511, Score -120.23063815508411, Avg_Score -120.37662615551619\n",
      "Adding trajectory to replay buffer: step 30695, counter 490676\n",
      "Environment 13: Episode 7512, Score -113.85069853733665, Avg_Score -120.29179082612778\n",
      "Adding trajectory to replay buffer: step 30697, counter 490722\n",
      "Environment 3: Episode 7513, Score -119.24681296677701, Avg_Score -120.27071208401222\n",
      "Adding trajectory to replay buffer: step 30699, counter 490766\n",
      "Environment 6: Episode 7514, Score -117.58551902666377, Avg_Score -120.15797498007619\n",
      "Adding trajectory to replay buffer: step 30703, counter 490808\n",
      "Environment 0: Episode 7515, Score -114.8550921843021, Avg_Score -120.07929842465681\n",
      "Adding trajectory to replay buffer: step 30706, counter 490865\n",
      "Environment 9: Episode 7516, Score -119.25541700444992, Avg_Score -120.10361758569388\n",
      "Adding trajectory to replay buffer: step 30708, counter 490911\n",
      "Environment 11: Episode 7517, Score -119.67023192612392, Avg_Score -120.0823045487241\n",
      "Adding trajectory to replay buffer: step 30714, counter 490958\n",
      "Environment 7: Episode 7518, Score -117.17155734540522, Avg_Score -120.04640079221306\n",
      "Adding trajectory to replay buffer: step 30726, counter 490999\n",
      "Environment 12: Episode 7519, Score -115.84361656615138, Avg_Score -119.99394589307792\n",
      "Adding trajectory to replay buffer: step 30726, counter 491042\n",
      "Environment 15: Episode 7520, Score -122.27183402452803, Avg_Score -119.99874778744552\n",
      "Adding trajectory to replay buffer: step 30731, counter 491087\n",
      "Environment 1: Episode 7521, Score -118.01192386860711, Avg_Score -119.94577744206201\n",
      "Adding trajectory to replay buffer: step 30734, counter 491133\n",
      "Environment 10: Episode 7522, Score -120.40640769064986, Avg_Score -119.93281711207011\n",
      "Adding trajectory to replay buffer: step 30736, counter 491181\n",
      "Environment 14: Episode 7523, Score -120.07469211927676, Avg_Score -119.90054020249238\n",
      "Adding trajectory to replay buffer: step 30739, counter 491227\n",
      "Environment 2: Episode 7524, Score -123.22669238661726, Avg_Score -119.90822887607139\n",
      "Adding trajectory to replay buffer: step 30742, counter 491272\n",
      "Environment 3: Episode 7525, Score -122.55064076438919, Avg_Score -119.90037511631857\n",
      "Adding trajectory to replay buffer: step 30744, counter 491317\n",
      "Environment 6: Episode 7526, Score -122.03951085638566, Avg_Score -119.89700420389549\n",
      "Adding trajectory to replay buffer: step 30748, counter 491445\n",
      "Environment 5: Episode 7527, Score -117.96909078729836, Avg_Score -119.84868749749546\n",
      "Adding trajectory to replay buffer: step 30750, counter 491492\n",
      "Environment 0: Episode 7528, Score -120.42694198757349, Avg_Score -119.8342415991518\n",
      "Adding trajectory to replay buffer: step 30750, counter 491536\n",
      "Environment 9: Episode 7529, Score -122.16804780272899, Avg_Score -119.83385653505998\n",
      "Adding trajectory to replay buffer: step 30750, counter 491578\n",
      "Environment 11: Episode 7530, Score -122.30741358345064, Avg_Score -119.61741390439005\n",
      "Adding trajectory to replay buffer: step 30757, counter 491621\n",
      "Environment 7: Episode 7531, Score -122.36677865389797, Avg_Score -119.6345646909887\n",
      "Adding trajectory to replay buffer: step 30774, counter 491664\n",
      "Environment 1: Episode 7532, Score -122.18124701788216, Avg_Score -119.6761541710138\n",
      "Adding trajectory to replay buffer: step 30776, counter 491838\n",
      "Environment 8: Episode 7533, Score -119.90587938613197, Avg_Score -119.72374687550447\n",
      "Adding trajectory to replay buffer: step 30777, counter 491881\n",
      "Environment 10: Episode 7534, Score -122.00226628308806, Avg_Score -119.75897820917704\n",
      "Adding trajectory to replay buffer: step 30780, counter 491925\n",
      "Environment 14: Episode 7535, Score -122.6511597865969, Avg_Score -119.82242419996204\n",
      "Adding trajectory to replay buffer: step 30780, counter 491979\n",
      "Environment 15: Episode 7536, Score -114.54502257058708, Avg_Score -119.73232729917397\n",
      "Adding trajectory to replay buffer: step 30781, counter 492034\n",
      "Environment 12: Episode 7537, Score -119.76954555018494, Avg_Score -119.69601413259774\n",
      "Adding trajectory to replay buffer: step 30792, counter 492082\n",
      "Environment 6: Episode 7538, Score -121.44485859947838, Avg_Score -119.68428521485954\n",
      "Adding trajectory to replay buffer: step 30793, counter 492184\n",
      "Environment 4: Episode 7539, Score -115.47777733448333, Avg_Score -119.60972910755389\n",
      "Adding trajectory to replay buffer: step 30793, counter 492227\n",
      "Environment 11: Episode 7540, Score -121.16181747738116, Avg_Score -119.58866027704796\n",
      "Adding trajectory to replay buffer: step 30794, counter 492271\n",
      "Environment 0: Episode 7541, Score -122.2219016733002, Avg_Score -119.58052652888782\n",
      "Adding trajectory to replay buffer: step 30796, counter 492317\n",
      "Environment 9: Episode 7542, Score -121.57102782500226, Avg_Score -119.56436066255088\n",
      "Adding trajectory to replay buffer: step 30802, counter 492362\n",
      "Environment 7: Episode 7543, Score -116.48704364568368, Avg_Score -119.51444423605211\n",
      "Adding trajectory to replay buffer: step 30808, counter 492475\n",
      "Environment 13: Episode 7544, Score -124.25267028385711, Avg_Score -119.6309238138376\n",
      "Adding trajectory to replay buffer: step 30826, counter 492562\n",
      "Environment 2: Episode 7545, Score -111.33329131624724, Avg_Score -119.58334311243256\n",
      "Adding trajectory to replay buffer: step 30828, counter 492610\n",
      "Environment 14: Episode 7546, Score -120.11470887192277, Avg_Score -119.57532607285924\n",
      "Adding trajectory to replay buffer: step 30832, counter 492666\n",
      "Environment 8: Episode 7547, Score -116.62838689125702, Avg_Score -119.57504390384483\n",
      "Adding trajectory to replay buffer: step 30840, counter 492712\n",
      "Environment 0: Episode 7548, Score -121.66882904613266, Avg_Score -119.61242762518681\n",
      "Adding trajectory to replay buffer: step 30840, counter 492760\n",
      "Environment 6: Episode 7549, Score -117.47263180307672, Avg_Score -119.57357480529804\n",
      "Adding trajectory to replay buffer: step 30840, counter 492807\n",
      "Environment 11: Episode 7550, Score -120.87489037192861, Avg_Score -119.6066721146499\n",
      "Adding trajectory to replay buffer: step 30841, counter 492855\n",
      "Environment 4: Episode 7551, Score -114.0905289761573, Avg_Score -119.56992908083068\n",
      "Adding trajectory to replay buffer: step 30842, counter 492901\n",
      "Environment 9: Episode 7552, Score -120.99603719320893, Avg_Score -119.57564584023169\n",
      "Adding trajectory to replay buffer: step 30843, counter 492963\n",
      "Environment 12: Episode 7553, Score -114.61256438692783, Avg_Score -119.59707552564463\n",
      "Adding trajectory to replay buffer: step 30845, counter 493034\n",
      "Environment 1: Episode 7554, Score -118.97418050184845, Avg_Score -119.55072016000224\n",
      "Adding trajectory to replay buffer: step 30848, counter 493080\n",
      "Environment 7: Episode 7555, Score -121.49420376060903, Avg_Score -119.61499777761526\n",
      "Adding trajectory to replay buffer: step 30853, counter 493125\n",
      "Environment 13: Episode 7556, Score -121.8319335216439, Avg_Score -119.52355142001298\n",
      "Adding trajectory to replay buffer: step 30869, counter 493168\n",
      "Environment 2: Episode 7557, Score -121.8552860449087, Avg_Score -119.54161139174894\n",
      "Adding trajectory to replay buffer: step 30872, counter 493212\n",
      "Environment 14: Episode 7558, Score -122.4691494549221, Avg_Score -119.54951691746484\n",
      "Adding trajectory to replay buffer: step 30884, counter 493256\n",
      "Environment 0: Episode 7559, Score -122.3715514964483, Avg_Score -119.63708440080316\n",
      "Adding trajectory to replay buffer: step 30887, counter 493300\n",
      "Environment 12: Episode 7560, Score -119.41439441480783, Avg_Score -119.64508063444453\n",
      "Adding trajectory to replay buffer: step 30888, counter 493346\n",
      "Environment 9: Episode 7561, Score -121.1644414474337, Avg_Score -119.63274643909655\n",
      "Adding trajectory to replay buffer: step 30890, counter 493388\n",
      "Environment 7: Episode 7562, Score -117.06194203722718, Avg_Score -119.57426606126954\n",
      "Adding trajectory to replay buffer: step 30891, counter 493434\n",
      "Environment 1: Episode 7563, Score -121.73244161472532, Avg_Score -119.57265181323089\n",
      "Adding trajectory to replay buffer: step 30899, counter 493480\n",
      "Environment 13: Episode 7564, Score -120.5910504884217, Avg_Score -119.57121581977623\n",
      "Adding trajectory to replay buffer: step 30904, counter 493552\n",
      "Environment 8: Episode 7565, Score -118.56281751689625, Avg_Score -119.59389003636831\n",
      "Adding trajectory to replay buffer: step 30909, counter 493684\n",
      "Environment 10: Episode 7566, Score -124.59112468622314, Avg_Score -119.6095662100721\n",
      "Adding trajectory to replay buffer: step 30912, counter 493727\n",
      "Environment 2: Episode 7567, Score -122.98621346280724, Avg_Score -119.67677658373992\n",
      "Adding trajectory to replay buffer: step 30928, counter 493771\n",
      "Environment 0: Episode 7568, Score -123.43392784788553, Avg_Score -119.70690877588615\n",
      "Adding trajectory to replay buffer: step 30930, counter 493861\n",
      "Environment 6: Episode 7569, Score -124.88500074500654, Avg_Score -119.78112008296318\n",
      "Adding trajectory to replay buffer: step 30931, counter 493905\n",
      "Environment 12: Episode 7570, Score -122.4150658181527, Avg_Score -119.78708938638978\n",
      "Adding trajectory to replay buffer: step 30932, counter 493949\n",
      "Environment 9: Episode 7571, Score -120.87247530387592, Avg_Score -119.79224690137971\n",
      "Adding trajectory to replay buffer: step 30933, counter 493992\n",
      "Environment 7: Episode 7572, Score -120.39238357030848, Avg_Score -119.77271297590597\n",
      "Adding trajectory to replay buffer: step 30935, counter 494185\n",
      "Environment 3: Episode 7573, Score -119.73143954629451, Avg_Score -119.78966014616222\n",
      "Adding trajectory to replay buffer: step 30937, counter 494231\n",
      "Environment 1: Episode 7574, Score -119.58047255761673, Avg_Score -119.78600455661855\n",
      "Adding trajectory to replay buffer: step 30942, counter 494274\n",
      "Environment 13: Episode 7575, Score -122.50047031833728, Avg_Score -119.83118179570242\n",
      "Adding trajectory to replay buffer: step 30951, counter 494321\n",
      "Environment 8: Episode 7576, Score -120.03081069069853, Avg_Score -119.80182082848081\n",
      "Adding trajectory to replay buffer: step 30954, counter 494527\n",
      "Environment 5: Episode 7577, Score -122.33888685793205, Avg_Score -119.79952353683422\n",
      "Adding trajectory to replay buffer: step 30973, counter 494659\n",
      "Environment 4: Episode 7578, Score -117.78004054560823, Avg_Score -119.74696742189161\n",
      "Adding trajectory to replay buffer: step 30974, counter 494724\n",
      "Environment 10: Episode 7579, Score -118.0060385177868, Avg_Score -119.69775657539978\n",
      "Adding trajectory to replay buffer: step 30975, counter 494769\n",
      "Environment 6: Episode 7580, Score -120.27895455181661, Avg_Score -119.75552780963419\n",
      "Adding trajectory to replay buffer: step 30975, counter 494811\n",
      "Environment 7: Episode 7581, Score -120.026572189585, Avg_Score -119.72860308147536\n",
      "Adding trajectory to replay buffer: step 30976, counter 494856\n",
      "Environment 12: Episode 7582, Score -123.3219468568936, Avg_Score -119.78001208015623\n",
      "Adding trajectory to replay buffer: step 30980, counter 494899\n",
      "Environment 1: Episode 7583, Score -120.55462612428673, Avg_Score -119.83795378677392\n",
      "Adding trajectory to replay buffer: step 30985, counter 495012\n",
      "Environment 14: Episode 7584, Score -114.31235761067643, Avg_Score -119.83133004279327\n",
      "Adding trajectory to replay buffer: step 30989, counter 495059\n",
      "Environment 13: Episode 7585, Score -115.81974291528762, Avg_Score -119.78969978936617\n",
      "Adding trajectory to replay buffer: step 30995, counter 495100\n",
      "Environment 5: Episode 7586, Score -114.59413906688305, Avg_Score -119.76356323698495\n",
      "Adding trajectory to replay buffer: step 30995, counter 495144\n",
      "Environment 8: Episode 7587, Score -123.26409179540848, Avg_Score -119.84936516403272\n",
      "Adding trajectory to replay buffer: step 31016, counter 495187\n",
      "Environment 4: Episode 7588, Score -123.13473039363885, Avg_Score -119.86330869579552\n",
      "Adding trajectory to replay buffer: step 31017, counter 495230\n",
      "Environment 10: Episode 7589, Score -121.90745459731156, Avg_Score -119.89107204617675\n",
      "Adding trajectory to replay buffer: step 31020, counter 495275\n",
      "Environment 6: Episode 7590, Score -121.69426561562594, Avg_Score -119.89489960287742\n",
      "Adding trajectory to replay buffer: step 31021, counter 495361\n",
      "Environment 3: Episode 7591, Score -119.09176753587958, Avg_Score -119.9052890708237\n",
      "Adding trajectory to replay buffer: step 31021, counter 495407\n",
      "Environment 7: Episode 7592, Score -120.7696982540389, Avg_Score -119.88644113917704\n",
      "Adding trajectory to replay buffer: step 31023, counter 495454\n",
      "Environment 12: Episode 7593, Score -118.4750244810904, Avg_Score -119.83965464467371\n",
      "Adding trajectory to replay buffer: step 31027, counter 495501\n",
      "Environment 1: Episode 7594, Score -121.6065497263968, Avg_Score -119.89515946057826\n",
      "Adding trajectory to replay buffer: step 31028, counter 495601\n",
      "Environment 0: Episode 7595, Score -114.05108596214342, Avg_Score -119.90079838998042\n",
      "Adding trajectory to replay buffer: step 31030, counter 495646\n",
      "Environment 14: Episode 7596, Score -122.14298979485905, Avg_Score -119.91048289442926\n",
      "Adding trajectory to replay buffer: step 31037, counter 495694\n",
      "Environment 13: Episode 7597, Score -119.45184839828188, Avg_Score -119.90413065161944\n",
      "Adding trajectory to replay buffer: step 31039, counter 495738\n",
      "Environment 8: Episode 7598, Score -116.32565624929872, Avg_Score -119.92492341111576\n",
      "Adding trajectory to replay buffer: step 31044, counter 495787\n",
      "Environment 5: Episode 7599, Score -116.24102126691893, Avg_Score -119.92483133146719\n",
      "Adding trajectory to replay buffer: step 31060, counter 495831\n",
      "Environment 4: Episode 7600, Score -121.45809371897704, Avg_Score -119.91471027705761\n",
      "Adding trajectory to replay buffer: step 31065, counter 495875\n",
      "Environment 7: Episode 7601, Score -122.76520295205526, Avg_Score -119.94332693738674\n",
      "Adding trajectory to replay buffer: step 31066, counter 495920\n",
      "Environment 3: Episode 7602, Score -123.3340656279903, Avg_Score -119.95406594428574\n",
      "Adding trajectory to replay buffer: step 31068, counter 496148\n",
      "Environment 11: Episode 7603, Score -129.33701854118394, Avg_Score -120.11339511087094\n",
      "Adding trajectory to replay buffer: step 31068, counter 496193\n",
      "Environment 12: Episode 7604, Score -121.61144461247449, Avg_Score -120.1213846367264\n",
      "Adding trajectory to replay buffer: step 31070, counter 496236\n",
      "Environment 1: Episode 7605, Score -122.71011395178363, Avg_Score -120.14190937886438\n",
      "Adding trajectory to replay buffer: step 31071, counter 496279\n",
      "Environment 0: Episode 7606, Score -122.68868400346054, Avg_Score -120.14166511399372\n",
      "Adding trajectory to replay buffer: step 31072, counter 496321\n",
      "Environment 14: Episode 7607, Score -122.84661689541488, Avg_Score -120.24014101277201\n",
      "Adding trajectory to replay buffer: step 31075, counter 496616\n",
      "Environment 15: Episode 7608, Score -129.75165200772665, Avg_Score -120.31624495752125\n",
      "Adding trajectory to replay buffer: step 31079, counter 496763\n",
      "Environment 9: Episode 7609, Score -118.00932240023246, Avg_Score -120.2605288796552\n",
      "Adding trajectory to replay buffer: step 31082, counter 496808\n",
      "Environment 13: Episode 7610, Score -119.74872658664856, Avg_Score -120.21386851980907\n",
      "Adding trajectory to replay buffer: step 31085, counter 496854\n",
      "Environment 8: Episode 7611, Score -123.2037836976219, Avg_Score -120.24359997523445\n",
      "Adding trajectory to replay buffer: step 31092, counter 496902\n",
      "Environment 5: Episode 7612, Score -118.09899672202704, Avg_Score -120.28608295708135\n",
      "Adding trajectory to replay buffer: step 31103, counter 496945\n",
      "Environment 4: Episode 7613, Score -123.15707126688957, Avg_Score -120.32518554008247\n",
      "Adding trajectory to replay buffer: step 31104, counter 497137\n",
      "Environment 2: Episode 7614, Score -119.43785098066616, Avg_Score -120.34370885962248\n",
      "Adding trajectory to replay buffer: step 31112, counter 497183\n",
      "Environment 3: Episode 7615, Score -122.70941472761147, Avg_Score -120.42225208505558\n",
      "Adding trajectory to replay buffer: step 31112, counter 497230\n",
      "Environment 7: Episode 7616, Score -120.67708869662881, Avg_Score -120.43646880197736\n",
      "Adding trajectory to replay buffer: step 31112, counter 497274\n",
      "Environment 12: Episode 7617, Score -123.5690371952237, Avg_Score -120.47545685466835\n",
      "Adding trajectory to replay buffer: step 31113, counter 497319\n",
      "Environment 11: Episode 7618, Score -123.39157065213223, Avg_Score -120.53765698773563\n",
      "Adding trajectory to replay buffer: step 31115, counter 497364\n",
      "Environment 1: Episode 7619, Score -118.23906545378269, Avg_Score -120.56161147661196\n",
      "Adding trajectory to replay buffer: step 31115, counter 497407\n",
      "Environment 14: Episode 7620, Score -122.80310958617801, Avg_Score -120.56692423222847\n",
      "Adding trajectory to replay buffer: step 31123, counter 497451\n",
      "Environment 9: Episode 7621, Score -122.3973155007468, Avg_Score -120.61077814854987\n",
      "Adding trajectory to replay buffer: step 31128, counter 497497\n",
      "Environment 13: Episode 7622, Score -123.41514406393965, Avg_Score -120.64086551228276\n",
      "Adding trajectory to replay buffer: step 31132, counter 497544\n",
      "Environment 8: Episode 7623, Score -122.50631365161948, Avg_Score -120.6651817276062\n",
      "Adding trajectory to replay buffer: step 31142, counter 497594\n",
      "Environment 5: Episode 7624, Score -119.58085198783192, Avg_Score -120.62872332361835\n",
      "Adding trajectory to replay buffer: step 31146, counter 497637\n",
      "Environment 4: Episode 7625, Score -123.15700897142105, Avg_Score -120.63478700568865\n",
      "Adding trajectory to replay buffer: step 31147, counter 497680\n",
      "Environment 2: Episode 7626, Score -123.49037363148419, Avg_Score -120.64929563343964\n",
      "Adding trajectory to replay buffer: step 31156, counter 497724\n",
      "Environment 3: Episode 7627, Score -117.9538747132818, Avg_Score -120.64914347269946\n",
      "Adding trajectory to replay buffer: step 31157, counter 497769\n",
      "Environment 12: Episode 7628, Score -122.32239002060517, Avg_Score -120.66809795302981\n",
      "Adding trajectory to replay buffer: step 31158, counter 497812\n",
      "Environment 1: Episode 7629, Score -123.14136983326884, Avg_Score -120.67783117333519\n",
      "Adding trajectory to replay buffer: step 31158, counter 497855\n",
      "Environment 14: Episode 7630, Score -122.9755240178903, Avg_Score -120.6845122776796\n",
      "Adding trajectory to replay buffer: step 31159, counter 497901\n",
      "Environment 11: Episode 7631, Score -120.02129307839274, Avg_Score -120.66105742192458\n",
      "Adding trajectory to replay buffer: step 31161, counter 497987\n",
      "Environment 15: Episode 7632, Score -111.57584152991993, Avg_Score -120.55500336704496\n",
      "Adding trajectory to replay buffer: step 31168, counter 498032\n",
      "Environment 9: Episode 7633, Score -121.43294770523534, Avg_Score -120.57027405023598\n",
      "Adding trajectory to replay buffer: step 31170, counter 498074\n",
      "Environment 13: Episode 7634, Score -122.81542319804555, Avg_Score -120.57840561938558\n",
      "Adding trajectory to replay buffer: step 31176, counter 498118\n",
      "Environment 8: Episode 7635, Score -122.31802254145282, Avg_Score -120.57507424693415\n",
      "Adding trajectory to replay buffer: step 31185, counter 498191\n",
      "Environment 7: Episode 7636, Score -117.39819710666313, Avg_Score -120.6036059922949\n",
      "Adding trajectory to replay buffer: step 31189, counter 498238\n",
      "Environment 5: Episode 7637, Score -119.10102896112825, Avg_Score -120.59692082640433\n",
      "Adding trajectory to replay buffer: step 31200, counter 498280\n",
      "Environment 1: Episode 7638, Score -123.0035881545879, Avg_Score -120.61250812195543\n",
      "Adding trajectory to replay buffer: step 31200, counter 498460\n",
      "Environment 6: Episode 7639, Score -120.50491813026679, Avg_Score -120.66277952991328\n",
      "Adding trajectory to replay buffer: step 31201, counter 498515\n",
      "Environment 4: Episode 7640, Score -120.15574033267859, Avg_Score -120.65271875846625\n",
      "Adding trajectory to replay buffer: step 31202, counter 498559\n",
      "Environment 14: Episode 7641, Score -121.8294329937057, Avg_Score -120.6487940716703\n",
      "Adding trajectory to replay buffer: step 31203, counter 498603\n",
      "Environment 11: Episode 7642, Score -121.82685887009899, Avg_Score -120.65135238212126\n",
      "Adding trajectory to replay buffer: step 31203, counter 498649\n",
      "Environment 12: Episode 7643, Score -121.96469757231698, Avg_Score -120.7061289213876\n",
      "Adding trajectory to replay buffer: step 31204, counter 498692\n",
      "Environment 15: Episode 7644, Score -122.97803052664858, Avg_Score -120.69338252381553\n",
      "Adding trajectory to replay buffer: step 31211, counter 498747\n",
      "Environment 3: Episode 7645, Score -115.997324619703, Avg_Score -120.74002285685009\n",
      "Adding trajectory to replay buffer: step 31211, counter 498941\n",
      "Environment 10: Episode 7646, Score -120.57381335307161, Avg_Score -120.74461390166158\n",
      "Adding trajectory to replay buffer: step 31212, counter 498985\n",
      "Environment 9: Episode 7647, Score -122.88676891295427, Avg_Score -120.80719772187857\n",
      "Adding trajectory to replay buffer: step 31214, counter 499052\n",
      "Environment 2: Episode 7648, Score -110.91262204072127, Avg_Score -120.69963565182445\n",
      "Adding trajectory to replay buffer: step 31214, counter 499096\n",
      "Environment 13: Episode 7649, Score -121.98289738017196, Avg_Score -120.74473830759538\n",
      "Adding trajectory to replay buffer: step 31221, counter 499141\n",
      "Environment 8: Episode 7650, Score -122.77141101833931, Avg_Score -120.76370351405949\n",
      "Adding trajectory to replay buffer: step 31230, counter 499186\n",
      "Environment 7: Episode 7651, Score -122.43051662143134, Avg_Score -120.8471033905122\n",
      "Adding trajectory to replay buffer: step 31233, counter 499230\n",
      "Environment 5: Episode 7652, Score -122.86248828901405, Avg_Score -120.86576790147025\n",
      "Adding trajectory to replay buffer: step 31243, counter 499273\n",
      "Environment 1: Episode 7653, Score -121.81659119863747, Avg_Score -120.93780816958736\n",
      "Adding trajectory to replay buffer: step 31243, counter 499316\n",
      "Environment 6: Episode 7654, Score -121.42338465185713, Avg_Score -120.96230021108742\n",
      "Adding trajectory to replay buffer: step 31247, counter 499361\n",
      "Environment 14: Episode 7655, Score -120.97867163167956, Avg_Score -120.95714488979813\n",
      "Adding trajectory to replay buffer: step 31251, counter 499408\n",
      "Environment 15: Episode 7656, Score -119.98361194477044, Avg_Score -120.93866167402939\n",
      "Adding trajectory to replay buffer: step 31253, counter 499450\n",
      "Environment 10: Episode 7657, Score -118.1663720040489, Avg_Score -120.90177253362079\n",
      "Adding trajectory to replay buffer: step 31258, counter 499494\n",
      "Environment 2: Episode 7658, Score -122.20798566808367, Avg_Score -120.89916089575242\n",
      "Adding trajectory to replay buffer: step 31258, counter 499549\n",
      "Environment 12: Episode 7659, Score -112.54165226763084, Avg_Score -120.80086190346425\n",
      "Adding trajectory to replay buffer: step 31259, counter 499594\n",
      "Environment 13: Episode 7660, Score -121.32547973373285, Avg_Score -120.81997275665348\n",
      "Adding trajectory to replay buffer: step 31265, counter 499638\n",
      "Environment 8: Episode 7661, Score -122.05606026340214, Avg_Score -120.82888894481316\n",
      "Adding trajectory to replay buffer: step 31270, counter 499707\n",
      "Environment 4: Episode 7662, Score -116.14419654111067, Avg_Score -120.819711489852\n",
      "Adding trajectory to replay buffer: step 31277, counter 499754\n",
      "Environment 7: Episode 7663, Score -117.7962089130475, Avg_Score -120.7803491628352\n",
      "Adding trajectory to replay buffer: step 31287, counter 499838\n",
      "Environment 11: Episode 7664, Score -119.19658754013582, Avg_Score -120.76640453335234\n",
      "Adding trajectory to replay buffer: step 31297, counter 499923\n",
      "Environment 9: Episode 7665, Score -121.98128478492238, Avg_Score -120.8005892060326\n",
      "Adding trajectory to replay buffer: step 31298, counter 499970\n",
      "Environment 15: Episode 7666, Score -121.60764174608265, Avg_Score -120.77075437663117\n",
      "Adding trajectory to replay buffer: step 31299, counter 500011\n",
      "Environment 2: Episode 7667, Score -115.43873066063101, Avg_Score -120.69527954860943\n",
      "Adding trajectory to replay buffer: step 31302, counter 500070\n",
      "Environment 1: Episode 7668, Score -116.32491351375481, Avg_Score -120.62418940526811\n",
      "Adding trajectory to replay buffer: step 31304, counter 500116\n",
      "Environment 12: Episode 7669, Score -119.19324631712462, Avg_Score -120.56727186098931\n",
      "Adding trajectory to replay buffer: step 31313, counter 500159\n",
      "Environment 4: Episode 7670, Score -122.94971438683694, Avg_Score -120.57261834667615\n",
      "Adding trajectory to replay buffer: step 31313, counter 500207\n",
      "Environment 8: Episode 7671, Score -118.58760583540487, Avg_Score -120.54976965199144\n",
      "Adding trajectory to replay buffer: step 31318, counter 500266\n",
      "Environment 13: Episode 7672, Score -116.16632219534306, Avg_Score -120.50750903824175\n",
      "Adding trajectory to replay buffer: step 31321, counter 500310\n",
      "Environment 7: Episode 7673, Score -122.54931613102772, Avg_Score -120.53568780408911\n",
      "Adding trajectory to replay buffer: step 31327, counter 500566\n",
      "Environment 0: Episode 7674, Score -129.95025689940863, Avg_Score -120.63938564750701\n",
      "Adding trajectory to replay buffer: step 31328, counter 500661\n",
      "Environment 5: Episode 7675, Score -116.03180198752725, Avg_Score -120.5746989641989\n",
      "Adding trajectory to replay buffer: step 31333, counter 500783\n",
      "Environment 3: Episode 7676, Score -119.61450586867907, Avg_Score -120.57053591597872\n",
      "Adding trajectory to replay buffer: step 31339, counter 500835\n",
      "Environment 11: Episode 7677, Score -117.6303092887147, Avg_Score -120.52345014028657\n",
      "Adding trajectory to replay buffer: step 31341, counter 500879\n",
      "Environment 9: Episode 7678, Score -121.45026148683702, Avg_Score -120.56015234969887\n",
      "Adding trajectory to replay buffer: step 31343, counter 500979\n",
      "Environment 6: Episode 7679, Score -115.51500802204153, Avg_Score -120.5352420447414\n",
      "Adding trajectory to replay buffer: step 31343, counter 501024\n",
      "Environment 15: Episode 7680, Score -117.66674827797277, Avg_Score -120.50911998200294\n",
      "Adding trajectory to replay buffer: step 31344, counter 501069\n",
      "Environment 2: Episode 7681, Score -123.12920112305095, Avg_Score -120.5401462713376\n",
      "Adding trajectory to replay buffer: step 31347, counter 501114\n",
      "Environment 1: Episode 7682, Score -122.3257071575957, Avg_Score -120.53018387434462\n",
      "Adding trajectory to replay buffer: step 31348, counter 501158\n",
      "Environment 12: Episode 7683, Score -122.18390786807798, Avg_Score -120.54647669178257\n",
      "Adding trajectory to replay buffer: step 31355, counter 501260\n",
      "Environment 10: Episode 7684, Score -116.76999241712876, Avg_Score -120.57105303984709\n",
      "Adding trajectory to replay buffer: step 31359, counter 501372\n",
      "Environment 14: Episode 7685, Score -127.09787447696552, Avg_Score -120.68383435546386\n",
      "Adding trajectory to replay buffer: step 31367, counter 501426\n",
      "Environment 8: Episode 7686, Score -115.70165632209431, Avg_Score -120.69490952801598\n",
      "Adding trajectory to replay buffer: step 31370, counter 501483\n",
      "Environment 4: Episode 7687, Score -116.62075589902078, Avg_Score -120.6284761690521\n",
      "Adding trajectory to replay buffer: step 31372, counter 501534\n",
      "Environment 7: Episode 7688, Score -121.0595265018766, Avg_Score -120.60772413013449\n",
      "Adding trajectory to replay buffer: step 31374, counter 501590\n",
      "Environment 13: Episode 7689, Score -113.89375666536267, Avg_Score -120.52758715081498\n",
      "Adding trajectory to replay buffer: step 31377, counter 501634\n",
      "Environment 3: Episode 7690, Score -117.12146567206582, Avg_Score -120.48185915137938\n",
      "Adding trajectory to replay buffer: step 31385, counter 501680\n",
      "Environment 11: Episode 7691, Score -123.0547209430219, Avg_Score -120.5214886854508\n",
      "Adding trajectory to replay buffer: step 31389, counter 501726\n",
      "Environment 15: Episode 7692, Score -121.9400980556688, Avg_Score -120.53319268346712\n",
      "Adding trajectory to replay buffer: step 31390, counter 501769\n",
      "Environment 1: Episode 7693, Score -122.96600153166241, Avg_Score -120.57810245397285\n",
      "Adding trajectory to replay buffer: step 31390, counter 501815\n",
      "Environment 2: Episode 7694, Score -122.04515939229728, Avg_Score -120.58248855063187\n",
      "Adding trajectory to replay buffer: step 31391, counter 501858\n",
      "Environment 12: Episode 7695, Score -123.38669489446329, Avg_Score -120.67584463995505\n",
      "Adding trajectory to replay buffer: step 31397, counter 501912\n",
      "Environment 6: Episode 7696, Score -113.65892442336616, Avg_Score -120.59100398624012\n",
      "Adding trajectory to replay buffer: step 31399, counter 501956\n",
      "Environment 10: Episode 7697, Score -122.42513480359369, Avg_Score -120.62073685029323\n",
      "Adding trajectory to replay buffer: step 31403, counter 502000\n",
      "Environment 14: Episode 7698, Score -121.86737997355624, Avg_Score -120.67615408753582\n",
      "Adding trajectory to replay buffer: step 31412, counter 502084\n",
      "Environment 5: Episode 7699, Score -120.8646336914301, Avg_Score -120.72239021178095\n",
      "Adding trajectory to replay buffer: step 31421, counter 502128\n",
      "Environment 3: Episode 7700, Score -122.76147535145344, Avg_Score -120.7354240281057\n",
      "Adding trajectory to replay buffer: step 31421, counter 502179\n",
      "Environment 4: Episode 7701, Score -117.95944675657091, Avg_Score -120.68736646615088\n",
      "Adding trajectory to replay buffer: step 31430, counter 502224\n",
      "Environment 11: Episode 7702, Score -122.06757462608442, Avg_Score -120.67470155613181\n",
      "Adding trajectory to replay buffer: step 31434, counter 502268\n",
      "Environment 1: Episode 7703, Score -116.41151878199167, Avg_Score -120.54544655853988\n",
      "Adding trajectory to replay buffer: step 31434, counter 502311\n",
      "Environment 12: Episode 7704, Score -117.54401649211222, Avg_Score -120.50477227733626\n",
      "Adding trajectory to replay buffer: step 31444, counter 502428\n",
      "Environment 0: Episode 7705, Score -123.1872910362035, Avg_Score -120.50954404818044\n",
      "Adding trajectory to replay buffer: step 31446, counter 502475\n",
      "Environment 10: Episode 7706, Score -117.90158074010226, Avg_Score -120.46167301554689\n",
      "Adding trajectory to replay buffer: step 31449, counter 502557\n",
      "Environment 8: Episode 7707, Score -119.67551523904565, Avg_Score -120.4299619989832\n",
      "Adding trajectory to replay buffer: step 31449, counter 502665\n",
      "Environment 9: Episode 7708, Score -125.142228064524, Avg_Score -120.38386775955118\n",
      "Adding trajectory to replay buffer: step 31449, counter 502711\n",
      "Environment 14: Episode 7709, Score -121.47473139313361, Avg_Score -120.41852184948017\n",
      "Adding trajectory to replay buffer: step 31452, counter 502751\n",
      "Environment 5: Episode 7710, Score -114.53920874732609, Avg_Score -120.36642667108694\n",
      "Adding trajectory to replay buffer: step 31460, counter 502814\n",
      "Environment 6: Episode 7711, Score -121.63279491788583, Avg_Score -120.35071678328958\n",
      "Adding trajectory to replay buffer: step 31464, counter 502857\n",
      "Environment 3: Episode 7712, Score -121.68299103688138, Avg_Score -120.38655672643814\n",
      "Adding trajectory to replay buffer: step 31464, counter 502900\n",
      "Environment 4: Episode 7713, Score -121.63903651292622, Avg_Score -120.37137637889852\n",
      "Adding trajectory to replay buffer: step 31468, counter 502979\n",
      "Environment 15: Episode 7714, Score -119.73865081515598, Avg_Score -120.37438437724339\n",
      "Adding trajectory to replay buffer: step 31479, counter 503024\n",
      "Environment 1: Episode 7715, Score -121.36939474250485, Avg_Score -120.36098417739233\n",
      "Adding trajectory to replay buffer: step 31480, counter 503070\n",
      "Environment 12: Episode 7716, Score -120.36386086250779, Avg_Score -120.35785189905111\n",
      "Adding trajectory to replay buffer: step 31481, counter 503121\n",
      "Environment 11: Episode 7717, Score -118.13840846899028, Avg_Score -120.30354561178878\n",
      "Adding trajectory to replay buffer: step 31491, counter 503166\n",
      "Environment 10: Episode 7718, Score -118.24626922685901, Avg_Score -120.252092597536\n",
      "Adding trajectory to replay buffer: step 31496, counter 503210\n",
      "Environment 5: Episode 7719, Score -122.41804150465939, Avg_Score -120.2938823580448\n",
      "Adding trajectory to replay buffer: step 31497, counter 503317\n",
      "Environment 2: Episode 7720, Score -124.94600710694243, Avg_Score -120.31531133325245\n",
      "Adding trajectory to replay buffer: step 31497, counter 503365\n",
      "Environment 14: Episode 7721, Score -117.09721812193966, Avg_Score -120.2623103594644\n",
      "Adding trajectory to replay buffer: step 31501, counter 503492\n",
      "Environment 13: Episode 7722, Score -117.53474759870039, Avg_Score -120.203506394812\n",
      "Adding trajectory to replay buffer: step 31503, counter 503535\n",
      "Environment 6: Episode 7723, Score -123.54421812601387, Avg_Score -120.21388543955592\n",
      "Adding trajectory to replay buffer: step 31507, counter 503598\n",
      "Environment 0: Episode 7724, Score -116.07912762093173, Avg_Score -120.17886819588689\n",
      "Adding trajectory to replay buffer: step 31508, counter 503642\n",
      "Environment 3: Episode 7725, Score -121.53666381137757, Avg_Score -120.16266474428647\n",
      "Adding trajectory to replay buffer: step 31508, counter 503686\n",
      "Environment 4: Episode 7726, Score -122.24446635525426, Avg_Score -120.15020567152416\n",
      "Adding trajectory to replay buffer: step 31510, counter 503747\n",
      "Environment 9: Episode 7727, Score -116.86683520836321, Avg_Score -120.13933527647498\n",
      "Adding trajectory to replay buffer: step 31511, counter 503790\n",
      "Environment 15: Episode 7728, Score -123.57953863918657, Avg_Score -120.1519067626608\n",
      "Adding trajectory to replay buffer: step 31523, counter 503834\n",
      "Environment 1: Episode 7729, Score -122.99993448497216, Avg_Score -120.15049240917784\n",
      "Adding trajectory to replay buffer: step 31524, counter 503877\n",
      "Environment 11: Episode 7730, Score -122.52690738239389, Avg_Score -120.14600624282288\n",
      "Adding trajectory to replay buffer: step 31525, counter 503922\n",
      "Environment 12: Episode 7731, Score -122.49717832665519, Avg_Score -120.17076509530551\n",
      "Adding trajectory to replay buffer: step 31529, counter 504002\n",
      "Environment 8: Episode 7732, Score -116.44368886045552, Avg_Score -120.21944356861086\n",
      "Adding trajectory to replay buffer: step 31536, counter 504047\n",
      "Environment 10: Episode 7733, Score -120.17904546654721, Avg_Score -120.206904546224\n",
      "Adding trajectory to replay buffer: step 31541, counter 504091\n",
      "Environment 14: Episode 7734, Score -122.62978510177818, Avg_Score -120.20504816526135\n",
      "Adding trajectory to replay buffer: step 31542, counter 504136\n",
      "Environment 2: Episode 7735, Score -122.40953351785305, Avg_Score -120.20596327502534\n",
      "Adding trajectory to replay buffer: step 31543, counter 504183\n",
      "Environment 5: Episode 7736, Score -119.78549450242954, Avg_Score -120.22983624898299\n",
      "Adding trajectory to replay buffer: step 31546, counter 504228\n",
      "Environment 13: Episode 7737, Score -121.1831027824202, Avg_Score -120.25065698719592\n",
      "Adding trajectory to replay buffer: step 31547, counter 504272\n",
      "Environment 6: Episode 7738, Score -122.97169636461668, Avg_Score -120.25033806929622\n",
      "Adding trajectory to replay buffer: step 31553, counter 504317\n",
      "Environment 3: Episode 7739, Score -122.58934867304936, Avg_Score -120.27118237472405\n",
      "Adding trajectory to replay buffer: step 31553, counter 504362\n",
      "Environment 4: Episode 7740, Score -122.15543024632956, Avg_Score -120.29117927386056\n",
      "Adding trajectory to replay buffer: step 31557, counter 504408\n",
      "Environment 15: Episode 7741, Score -121.97959091921462, Avg_Score -120.29268085311566\n",
      "Adding trajectory to replay buffer: step 31559, counter 504457\n",
      "Environment 9: Episode 7742, Score -118.06433769759411, Avg_Score -120.25505564139061\n",
      "Adding trajectory to replay buffer: step 31562, counter 504512\n",
      "Environment 0: Episode 7743, Score -114.82921964023635, Avg_Score -120.18370086206978\n",
      "Adding trajectory to replay buffer: step 31562, counter 504702\n",
      "Environment 7: Episode 7744, Score -125.15402950987901, Avg_Score -120.20546085190207\n",
      "Adding trajectory to replay buffer: step 31566, counter 504744\n",
      "Environment 11: Episode 7745, Score -118.10648283055673, Avg_Score -120.22655243401063\n",
      "Adding trajectory to replay buffer: step 31568, counter 504789\n",
      "Environment 1: Episode 7746, Score -122.46698041039333, Avg_Score -120.24548410458385\n",
      "Adding trajectory to replay buffer: step 31571, counter 504835\n",
      "Environment 12: Episode 7747, Score -121.30549713238949, Avg_Score -120.22967138677818\n",
      "Adding trajectory to replay buffer: step 31575, counter 504881\n",
      "Environment 8: Episode 7748, Score -119.50619234147607, Avg_Score -120.31560708978573\n",
      "Adding trajectory to replay buffer: step 31581, counter 504926\n",
      "Environment 10: Episode 7749, Score -123.1912693521368, Avg_Score -120.32769080950538\n",
      "Adding trajectory to replay buffer: step 31584, counter 504969\n",
      "Environment 14: Episode 7750, Score -123.20913027694013, Avg_Score -120.3320680020914\n",
      "Adding trajectory to replay buffer: step 31589, counter 505015\n",
      "Environment 5: Episode 7751, Score -122.18903357777248, Avg_Score -120.32965317165481\n",
      "Adding trajectory to replay buffer: step 31590, counter 505059\n",
      "Environment 13: Episode 7752, Score -121.81092756390261, Avg_Score -120.31913756440368\n",
      "Adding trajectory to replay buffer: step 31594, counter 505100\n",
      "Environment 4: Episode 7753, Score -114.95905481031227, Avg_Score -120.25056220052042\n",
      "Adding trajectory to replay buffer: step 31600, counter 505147\n",
      "Environment 3: Episode 7754, Score -121.30566183268279, Avg_Score -120.24938497232868\n",
      "Adding trajectory to replay buffer: step 31602, counter 505190\n",
      "Environment 9: Episode 7755, Score -120.00354422952546, Avg_Score -120.23963369830713\n",
      "Adding trajectory to replay buffer: step 31603, counter 505236\n",
      "Environment 15: Episode 7756, Score -122.5730583166269, Avg_Score -120.26552816202573\n",
      "Adding trajectory to replay buffer: step 31605, counter 505299\n",
      "Environment 2: Episode 7757, Score -112.28916488913944, Avg_Score -120.20675609087664\n",
      "Adding trajectory to replay buffer: step 31607, counter 505344\n",
      "Environment 0: Episode 7758, Score -121.1275944528884, Avg_Score -120.1959521787247\n",
      "Adding trajectory to replay buffer: step 31609, counter 505387\n",
      "Environment 11: Episode 7759, Score -121.66270532076123, Avg_Score -120.287162709256\n",
      "Adding trajectory to replay buffer: step 31612, counter 505431\n",
      "Environment 1: Episode 7760, Score -116.97196419039183, Avg_Score -120.24362755382258\n",
      "Adding trajectory to replay buffer: step 31616, counter 505476\n",
      "Environment 12: Episode 7761, Score -122.82709886663345, Avg_Score -120.25133793985488\n",
      "Adding trajectory to replay buffer: step 31617, counter 505546\n",
      "Environment 6: Episode 7762, Score -118.4929481070141, Avg_Score -120.27482545551389\n",
      "Adding trajectory to replay buffer: step 31619, counter 505590\n",
      "Environment 8: Episode 7763, Score -122.37035886463181, Avg_Score -120.32056695502973\n",
      "Adding trajectory to replay buffer: step 31625, counter 505634\n",
      "Environment 10: Episode 7764, Score -122.71322573867316, Avg_Score -120.3557333370151\n",
      "Adding trajectory to replay buffer: step 31632, counter 505677\n",
      "Environment 5: Episode 7765, Score -122.38435320324638, Avg_Score -120.35976402119833\n",
      "Adding trajectory to replay buffer: step 31635, counter 505722\n",
      "Environment 13: Episode 7766, Score -120.85655870437002, Avg_Score -120.3522531907812\n",
      "Adding trajectory to replay buffer: step 31639, counter 505767\n",
      "Environment 4: Episode 7767, Score -121.39024912116366, Avg_Score -120.41176837538652\n",
      "Adding trajectory to replay buffer: step 31645, counter 505810\n",
      "Environment 9: Episode 7768, Score -122.229671056645, Avg_Score -120.4708159508154\n",
      "Adding trajectory to replay buffer: step 31646, counter 505856\n",
      "Environment 3: Episode 7769, Score -120.7466569382331, Avg_Score -120.48635005702647\n",
      "Adding trajectory to replay buffer: step 31649, counter 505902\n",
      "Environment 15: Episode 7770, Score -121.41342755961296, Avg_Score -120.47098718875424\n",
      "Adding trajectory to replay buffer: step 31650, counter 505947\n",
      "Environment 2: Episode 7771, Score -122.12203216647171, Avg_Score -120.50633145206491\n",
      "Adding trajectory to replay buffer: step 31651, counter 506014\n",
      "Environment 14: Episode 7772, Score -114.43164001429516, Avg_Score -120.48898463025442\n",
      "Adding trajectory to replay buffer: step 31652, counter 506059\n",
      "Environment 0: Episode 7773, Score -121.43300646942606, Avg_Score -120.4778215336384\n",
      "Adding trajectory to replay buffer: step 31656, counter 506103\n",
      "Environment 1: Episode 7774, Score -122.3045099273858, Avg_Score -120.40136406391817\n",
      "Adding trajectory to replay buffer: step 31659, counter 506145\n",
      "Environment 6: Episode 7775, Score -115.95893442003802, Avg_Score -120.40063538824326\n",
      "Adding trajectory to replay buffer: step 31660, counter 506189\n",
      "Environment 12: Episode 7776, Score -123.27053191541073, Avg_Score -120.43719564871057\n",
      "Adding trajectory to replay buffer: step 31663, counter 506233\n",
      "Environment 8: Episode 7777, Score -121.36971526853306, Avg_Score -120.47458970850876\n",
      "Adding trajectory to replay buffer: step 31676, counter 506277\n",
      "Environment 5: Episode 7778, Score -122.89450243512356, Avg_Score -120.48903211799161\n",
      "Adding trajectory to replay buffer: step 31680, counter 506322\n",
      "Environment 13: Episode 7779, Score -118.15206071388721, Avg_Score -120.51540264491007\n",
      "Adding trajectory to replay buffer: step 31682, counter 506379\n",
      "Environment 10: Episode 7780, Score -114.72618677956238, Avg_Score -120.48599702992597\n",
      "Adding trajectory to replay buffer: step 31683, counter 506453\n",
      "Environment 11: Episode 7781, Score -114.68109847315712, Avg_Score -120.401516003427\n",
      "Adding trajectory to replay buffer: step 31684, counter 506498\n",
      "Environment 4: Episode 7782, Score -120.50460600731645, Avg_Score -120.38330499192422\n",
      "Adding trajectory to replay buffer: step 31689, counter 506542\n",
      "Environment 9: Episode 7783, Score -120.26900011089444, Avg_Score -120.3641559143524\n",
      "Adding trajectory to replay buffer: step 31692, counter 506585\n",
      "Environment 15: Episode 7784, Score -119.57310951194415, Avg_Score -120.39218708530056\n",
      "Adding trajectory to replay buffer: step 31693, counter 506632\n",
      "Environment 3: Episode 7785, Score -119.23655356486142, Avg_Score -120.31357387617955\n",
      "Adding trajectory to replay buffer: step 31697, counter 506767\n",
      "Environment 7: Episode 7786, Score -117.36221033092838, Avg_Score -120.3301794162679\n",
      "Adding trajectory to replay buffer: step 31705, counter 506812\n",
      "Environment 12: Episode 7787, Score -121.52987133853571, Avg_Score -120.37927057066305\n",
      "Adding trajectory to replay buffer: step 31705, counter 506866\n",
      "Environment 14: Episode 7788, Score -117.73076591727957, Avg_Score -120.34598296481707\n",
      "Adding trajectory to replay buffer: step 31706, counter 506920\n",
      "Environment 0: Episode 7789, Score -121.31559054469194, Avg_Score -120.42020130361036\n",
      "Adding trajectory to replay buffer: step 31707, counter 506964\n",
      "Environment 8: Episode 7790, Score -117.54541608790743, Avg_Score -120.42444080776878\n",
      "Adding trajectory to replay buffer: step 31714, counter 507022\n",
      "Environment 1: Episode 7791, Score -115.83056875242076, Avg_Score -120.35219928586275\n",
      "Adding trajectory to replay buffer: step 31718, counter 507081\n",
      "Environment 6: Episode 7792, Score -115.19807674378436, Avg_Score -120.28477907274387\n",
      "Adding trajectory to replay buffer: step 31722, counter 507127\n",
      "Environment 5: Episode 7793, Score -121.47630970227782, Avg_Score -120.26988215445003\n",
      "Adding trajectory to replay buffer: step 31725, counter 507170\n",
      "Environment 10: Episode 7794, Score -122.87908785669569, Avg_Score -120.27822143909403\n",
      "Adding trajectory to replay buffer: step 31725, counter 507215\n",
      "Environment 13: Episode 7795, Score -121.46276341456672, Avg_Score -120.25898212429507\n",
      "Adding trajectory to replay buffer: step 31728, counter 507259\n",
      "Environment 4: Episode 7796, Score -121.61404416568018, Avg_Score -120.33853332171822\n",
      "Adding trajectory to replay buffer: step 31735, counter 507305\n",
      "Environment 9: Episode 7797, Score -120.1407485522665, Avg_Score -120.31568945920495\n",
      "Adding trajectory to replay buffer: step 31746, counter 507358\n",
      "Environment 3: Episode 7798, Score -116.85658762210173, Avg_Score -120.26558153569039\n",
      "Adding trajectory to replay buffer: step 31749, counter 507457\n",
      "Environment 2: Episode 7799, Score -116.48230595057085, Avg_Score -120.22175825828181\n",
      "Adding trajectory to replay buffer: step 31749, counter 507499\n",
      "Environment 8: Episode 7800, Score -123.20304924324341, Avg_Score -120.22617399719971\n",
      "Adding trajectory to replay buffer: step 31749, counter 507565\n",
      "Environment 11: Episode 7801, Score -118.38182001545778, Avg_Score -120.2303977297886\n",
      "Adding trajectory to replay buffer: step 31751, counter 507610\n",
      "Environment 0: Episode 7802, Score -122.2958851749506, Avg_Score -120.23268083527728\n",
      "Adding trajectory to replay buffer: step 31759, counter 507677\n",
      "Environment 15: Episode 7803, Score -112.77705782067558, Avg_Score -120.19633622566413\n",
      "Adding trajectory to replay buffer: step 31762, counter 507725\n",
      "Environment 1: Episode 7804, Score -115.82967730735801, Avg_Score -120.17919283381657\n",
      "Adding trajectory to replay buffer: step 31767, counter 507770\n",
      "Environment 5: Episode 7805, Score -122.82209735534278, Avg_Score -120.17554089700802\n",
      "Adding trajectory to replay buffer: step 31768, counter 507813\n",
      "Environment 13: Episode 7806, Score -123.03899191205886, Avg_Score -120.22691500872756\n",
      "Adding trajectory to replay buffer: step 31769, counter 507877\n",
      "Environment 12: Episode 7807, Score -115.8477944501092, Avg_Score -120.18863780083822\n",
      "Adding trajectory to replay buffer: step 31770, counter 507922\n",
      "Environment 10: Episode 7808, Score -122.43977349731078, Avg_Score -120.16161325516609\n",
      "Adding trajectory to replay buffer: step 31777, counter 508002\n",
      "Environment 7: Episode 7809, Score -115.66310146528792, Avg_Score -120.10349695588764\n",
      "Adding trajectory to replay buffer: step 31781, counter 508048\n",
      "Environment 9: Episode 7810, Score -121.06031884909422, Avg_Score -120.16870805690533\n",
      "Adding trajectory to replay buffer: step 31786, counter 508106\n",
      "Environment 4: Episode 7811, Score -115.51075880659123, Avg_Score -120.10748769579236\n",
      "Adding trajectory to replay buffer: step 31788, counter 508176\n",
      "Environment 6: Episode 7812, Score -112.69141720191823, Avg_Score -120.01757195744274\n",
      "Adding trajectory to replay buffer: step 31793, counter 508220\n",
      "Environment 2: Episode 7813, Score -122.56590822069222, Avg_Score -120.02684067452041\n",
      "Adding trajectory to replay buffer: step 31793, counter 508264\n",
      "Environment 8: Episode 7814, Score -121.84557592398238, Avg_Score -120.04790992560869\n",
      "Adding trajectory to replay buffer: step 31794, counter 508309\n",
      "Environment 11: Episode 7815, Score -121.39873921910859, Avg_Score -120.04820337037468\n",
      "Adding trajectory to replay buffer: step 31796, counter 508354\n",
      "Environment 0: Episode 7816, Score -121.86715377725784, Avg_Score -120.0632362995222\n",
      "Adding trajectory to replay buffer: step 31806, counter 508401\n",
      "Environment 15: Episode 7817, Score -117.51409961626058, Avg_Score -120.0569932109949\n",
      "Adding trajectory to replay buffer: step 31807, counter 508446\n",
      "Environment 1: Episode 7818, Score -119.45585443061341, Avg_Score -120.06908906303244\n",
      "Adding trajectory to replay buffer: step 31821, counter 508500\n",
      "Environment 5: Episode 7819, Score -112.5758416358692, Avg_Score -119.97066706434452\n",
      "Adding trajectory to replay buffer: step 31821, counter 508553\n",
      "Environment 13: Episode 7820, Score -118.71315766389542, Avg_Score -119.90833856991405\n",
      "Adding trajectory to replay buffer: step 31822, counter 508598\n",
      "Environment 7: Episode 7821, Score -122.74615682964834, Avg_Score -119.96482795699116\n",
      "Adding trajectory to replay buffer: step 31826, counter 508654\n",
      "Environment 10: Episode 7822, Score -115.30538483948881, Avg_Score -119.94253432939902\n",
      "Adding trajectory to replay buffer: step 31827, counter 508700\n",
      "Environment 9: Episode 7823, Score -122.96937389358382, Avg_Score -119.93678588707473\n",
      "Adding trajectory to replay buffer: step 31829, counter 508783\n",
      "Environment 3: Episode 7824, Score -115.35581195355269, Avg_Score -119.92955273040094\n",
      "Adding trajectory to replay buffer: step 31833, counter 508830\n",
      "Environment 4: Episode 7825, Score -118.4460507160093, Avg_Score -119.89864659944726\n",
      "Adding trajectory to replay buffer: step 31837, counter 508873\n",
      "Environment 11: Episode 7826, Score -123.54718901049222, Avg_Score -119.91167382599964\n",
      "Adding trajectory to replay buffer: step 31838, counter 508918\n",
      "Environment 2: Episode 7827, Score -123.51149923864565, Avg_Score -119.97812046630246\n",
      "Adding trajectory to replay buffer: step 31839, counter 508961\n",
      "Environment 0: Episode 7828, Score -117.89398913796433, Avg_Score -119.92126497129024\n",
      "Adding trajectory to replay buffer: step 31846, counter 509038\n",
      "Environment 12: Episode 7829, Score -118.6517510121061, Avg_Score -119.87778313656158\n",
      "Adding trajectory to replay buffer: step 31856, counter 509087\n",
      "Environment 1: Episode 7830, Score -117.20975235293868, Avg_Score -119.82461158626704\n",
      "Adding trajectory to replay buffer: step 31862, counter 509156\n",
      "Environment 8: Episode 7831, Score -117.80153024692461, Avg_Score -119.77765510546973\n",
      "Adding trajectory to replay buffer: step 31864, counter 509199\n",
      "Environment 5: Episode 7832, Score -119.86625061614129, Avg_Score -119.81188072302659\n",
      "Adding trajectory to replay buffer: step 31864, counter 509242\n",
      "Environment 13: Episode 7833, Score -119.83746279814156, Avg_Score -119.80846489634251\n",
      "Adding trajectory to replay buffer: step 31867, counter 509287\n",
      "Environment 7: Episode 7834, Score -122.32464163842488, Avg_Score -119.80541346170898\n",
      "Adding trajectory to replay buffer: step 31867, counter 509449\n",
      "Environment 14: Episode 7835, Score -119.5507607728963, Avg_Score -119.77682573425942\n",
      "Adding trajectory to replay buffer: step 31868, counter 509511\n",
      "Environment 15: Episode 7836, Score -113.94787229327531, Avg_Score -119.71844951216785\n",
      "Adding trajectory to replay buffer: step 31870, counter 509555\n",
      "Environment 10: Episode 7837, Score -122.47563077471095, Avg_Score -119.73137479209073\n",
      "Adding trajectory to replay buffer: step 31873, counter 509601\n",
      "Environment 9: Episode 7838, Score -122.05336952748151, Avg_Score -119.72219152371939\n",
      "Adding trajectory to replay buffer: step 31874, counter 509687\n",
      "Environment 6: Episode 7839, Score -117.15296483042091, Avg_Score -119.6678276852931\n",
      "Adding trajectory to replay buffer: step 31883, counter 509733\n",
      "Environment 11: Episode 7840, Score -120.27424483789255, Avg_Score -119.64901583120873\n",
      "Adding trajectory to replay buffer: step 31884, counter 509779\n",
      "Environment 2: Episode 7841, Score -122.94023639226333, Avg_Score -119.65862228593922\n",
      "Adding trajectory to replay buffer: step 31885, counter 509825\n",
      "Environment 0: Episode 7842, Score -122.2175870993603, Avg_Score -119.70015477995686\n",
      "Adding trajectory to replay buffer: step 31892, counter 509888\n",
      "Environment 3: Episode 7843, Score -117.06294327040885, Avg_Score -119.72249201625857\n",
      "Adding trajectory to replay buffer: step 31898, counter 509930\n",
      "Environment 1: Episode 7844, Score -122.610277511904, Avg_Score -119.69705449627881\n",
      "Adding trajectory to replay buffer: step 31907, counter 509973\n",
      "Environment 5: Episode 7845, Score -122.8226650654506, Avg_Score -119.74421631862775\n",
      "Adding trajectory to replay buffer: step 31907, counter 510016\n",
      "Environment 13: Episode 7846, Score -122.3457304502887, Avg_Score -119.74300381902671\n",
      "Adding trajectory to replay buffer: step 31908, counter 510062\n",
      "Environment 8: Episode 7847, Score -121.50136645686378, Avg_Score -119.74496251227143\n",
      "Adding trajectory to replay buffer: step 31911, counter 510105\n",
      "Environment 15: Episode 7848, Score -122.60331815606231, Avg_Score -119.77593377041728\n",
      "Adding trajectory to replay buffer: step 31912, counter 510150\n",
      "Environment 7: Episode 7849, Score -118.06588256996787, Avg_Score -119.72467990259561\n",
      "Adding trajectory to replay buffer: step 31912, counter 510216\n",
      "Environment 12: Episode 7850, Score -117.84566251847806, Avg_Score -119.67104522501099\n",
      "Adding trajectory to replay buffer: step 31912, counter 510261\n",
      "Environment 14: Episode 7851, Score -118.48214388165437, Avg_Score -119.63397632804981\n",
      "Adding trajectory to replay buffer: step 31918, counter 510306\n",
      "Environment 9: Episode 7852, Score -116.00811337581091, Avg_Score -119.5759481861689\n",
      "Adding trajectory to replay buffer: step 31918, counter 510354\n",
      "Environment 10: Episode 7853, Score -122.73104898832405, Avg_Score -119.65366812794902\n",
      "Adding trajectory to replay buffer: step 31928, counter 510398\n",
      "Environment 2: Episode 7854, Score -119.7119081635618, Avg_Score -119.6377305912578\n",
      "Adding trajectory to replay buffer: step 31928, counter 510443\n",
      "Environment 11: Episode 7855, Score -120.68789546064846, Avg_Score -119.64457410356904\n",
      "Adding trajectory to replay buffer: step 31937, counter 510488\n",
      "Environment 3: Episode 7856, Score -122.63043278255128, Avg_Score -119.6451478482283\n",
      "Adding trajectory to replay buffer: step 31943, counter 510533\n",
      "Environment 1: Episode 7857, Score -120.62544152388784, Avg_Score -119.72851061457577\n",
      "Adding trajectory to replay buffer: step 31954, counter 510654\n",
      "Environment 4: Episode 7858, Score -116.99726522034904, Avg_Score -119.68720732225036\n",
      "Adding trajectory to replay buffer: step 31954, counter 510700\n",
      "Environment 8: Episode 7859, Score -120.36942019257322, Avg_Score -119.67427447096848\n",
      "Adding trajectory to replay buffer: step 31958, counter 510751\n",
      "Environment 13: Episode 7860, Score -117.80735226034807, Avg_Score -119.68262835166804\n",
      "Adding trajectory to replay buffer: step 31961, counter 510794\n",
      "Environment 10: Episode 7861, Score -114.60292778204878, Avg_Score -119.60038664082218\n",
      "Adding trajectory to replay buffer: step 31963, counter 510839\n",
      "Environment 9: Episode 7862, Score -122.59872230200283, Avg_Score -119.64144438277204\n",
      "Adding trajectory to replay buffer: step 31965, counter 510919\n",
      "Environment 0: Episode 7863, Score -119.40223214122716, Avg_Score -119.611763115538\n",
      "Adding trajectory to replay buffer: step 31971, counter 510978\n",
      "Environment 14: Episode 7864, Score -115.3016207195893, Avg_Score -119.53764706534716\n",
      "Adding trajectory to replay buffer: step 31973, counter 511039\n",
      "Environment 7: Episode 7865, Score -117.6933133042031, Avg_Score -119.49073666635674\n",
      "Adding trajectory to replay buffer: step 31975, counter 511103\n",
      "Environment 15: Episode 7866, Score -116.21827431358645, Avg_Score -119.4443538224489\n",
      "Adding trajectory to replay buffer: step 31979, counter 511154\n",
      "Environment 11: Episode 7867, Score -112.95528397451466, Avg_Score -119.3600041709824\n",
      "Adding trajectory to replay buffer: step 31982, counter 511224\n",
      "Environment 12: Episode 7868, Score -115.07934725985801, Avg_Score -119.28850093301455\n",
      "Adding trajectory to replay buffer: step 31983, counter 511270\n",
      "Environment 3: Episode 7869, Score -121.25854458981256, Avg_Score -119.29361980953036\n",
      "Adding trajectory to replay buffer: step 31985, counter 511381\n",
      "Environment 6: Episode 7870, Score -125.93845945459853, Avg_Score -119.33887012848022\n",
      "Adding trajectory to replay buffer: step 31987, counter 511425\n",
      "Environment 1: Episode 7871, Score -122.53466563008291, Avg_Score -119.34299646311634\n",
      "Adding trajectory to replay buffer: step 31991, counter 511488\n",
      "Environment 2: Episode 7872, Score -117.33251719238484, Avg_Score -119.37200523489723\n",
      "Adding trajectory to replay buffer: step 31998, counter 511532\n",
      "Environment 4: Episode 7873, Score -122.78872015564082, Avg_Score -119.38556237175936\n",
      "Adding trajectory to replay buffer: step 31998, counter 511576\n",
      "Environment 8: Episode 7874, Score -122.09220898882175, Avg_Score -119.38343936237374\n",
      "Adding trajectory to replay buffer: step 32004, counter 511619\n",
      "Environment 10: Episode 7875, Score -122.75465287034089, Avg_Score -119.45139654687677\n",
      "Adding trajectory to replay buffer: step 32008, counter 511664\n",
      "Environment 9: Episode 7876, Score -121.39169099407705, Avg_Score -119.43260813766345\n",
      "Adding trajectory to replay buffer: step 32009, counter 511708\n",
      "Environment 0: Episode 7877, Score -122.63978752892837, Avg_Score -119.44530886026737\n",
      "Adding trajectory to replay buffer: step 32020, counter 511770\n",
      "Environment 13: Episode 7878, Score -116.48340693369136, Avg_Score -119.38119790525309\n",
      "Adding trajectory to replay buffer: step 32020, counter 511815\n",
      "Environment 15: Episode 7879, Score -122.09567145893226, Avg_Score -119.42063401270354\n",
      "Adding trajectory to replay buffer: step 32021, counter 511863\n",
      "Environment 7: Episode 7880, Score -120.20778425290622, Avg_Score -119.47544998743699\n",
      "Adding trajectory to replay buffer: step 32022, counter 511906\n",
      "Environment 11: Episode 7881, Score -122.82316324146899, Avg_Score -119.55687063512009\n",
      "Adding trajectory to replay buffer: step 32024, counter 511948\n",
      "Environment 12: Episode 7882, Score -122.72997280913219, Avg_Score -119.57912430313827\n",
      "Adding trajectory to replay buffer: step 32024, counter 512001\n",
      "Environment 14: Episode 7883, Score -112.17073495625021, Avg_Score -119.49814165159181\n",
      "Adding trajectory to replay buffer: step 32028, counter 512044\n",
      "Environment 6: Episode 7884, Score -122.98924897376261, Avg_Score -119.53230304621002\n",
      "Adding trajectory to replay buffer: step 32030, counter 512087\n",
      "Environment 1: Episode 7885, Score -122.21623341817907, Avg_Score -119.56209984474319\n",
      "Adding trajectory to replay buffer: step 32041, counter 512130\n",
      "Environment 8: Episode 7886, Score -116.29836379606525, Avg_Score -119.55146137939457\n",
      "Adding trajectory to replay buffer: step 32043, counter 512175\n",
      "Environment 4: Episode 7887, Score -120.37140751465837, Avg_Score -119.53987674115578\n",
      "Adding trajectory to replay buffer: step 32050, counter 512221\n",
      "Environment 10: Episode 7888, Score -122.74904605348905, Avg_Score -119.59005954251786\n",
      "Adding trajectory to replay buffer: step 32053, counter 512266\n",
      "Environment 9: Episode 7889, Score -122.15179993038811, Avg_Score -119.59842163637484\n",
      "Adding trajectory to replay buffer: step 32063, counter 512320\n",
      "Environment 0: Episode 7890, Score -117.73365303233142, Avg_Score -119.60030400581908\n",
      "Adding trajectory to replay buffer: step 32065, counter 512365\n",
      "Environment 15: Episode 7891, Score -122.98686805752416, Avg_Score -119.67186699887012\n",
      "Adding trajectory to replay buffer: step 32068, counter 512442\n",
      "Environment 2: Episode 7892, Score -114.72573789006844, Avg_Score -119.66714361033297\n",
      "Adding trajectory to replay buffer: step 32069, counter 512487\n",
      "Environment 12: Episode 7893, Score -121.26932265151478, Avg_Score -119.66507373982535\n",
      "Adding trajectory to replay buffer: step 32070, counter 512533\n",
      "Environment 14: Episode 7894, Score -121.21338449142668, Avg_Score -119.64841670617267\n",
      "Adding trajectory to replay buffer: step 32071, counter 512576\n",
      "Environment 6: Episode 7895, Score -123.8699201930364, Avg_Score -119.67248827395737\n",
      "Adding trajectory to replay buffer: step 32071, counter 512625\n",
      "Environment 11: Episode 7896, Score -116.46803281166714, Avg_Score -119.62102816041723\n",
      "Adding trajectory to replay buffer: step 32075, counter 512670\n",
      "Environment 1: Episode 7897, Score -120.67216346745131, Avg_Score -119.62634230956907\n",
      "Adding trajectory to replay buffer: step 32077, counter 512764\n",
      "Environment 3: Episode 7898, Score -112.44180304197812, Avg_Score -119.58219446376785\n",
      "Adding trajectory to replay buffer: step 32083, counter 512806\n",
      "Environment 8: Episode 7899, Score -122.86125484797607, Avg_Score -119.6459839527419\n",
      "Adding trajectory to replay buffer: step 32086, counter 512849\n",
      "Environment 4: Episode 7900, Score -119.91187607219132, Avg_Score -119.61307222103137\n",
      "Adding trajectory to replay buffer: step 32089, counter 512917\n",
      "Environment 7: Episode 7901, Score -118.12639162042831, Avg_Score -119.61051793708107\n",
      "Adding trajectory to replay buffer: step 32090, counter 512987\n",
      "Environment 13: Episode 7902, Score -116.59066358695861, Avg_Score -119.55346572120115\n",
      "Adding trajectory to replay buffer: step 32096, counter 513033\n",
      "Environment 10: Episode 7903, Score -120.58127853688597, Avg_Score -119.63150792836325\n",
      "Adding trajectory to replay buffer: step 32106, counter 513074\n",
      "Environment 15: Episode 7904, Score -114.82739320845157, Avg_Score -119.6214850873742\n",
      "Adding trajectory to replay buffer: step 32108, counter 513275\n",
      "Environment 5: Episode 7905, Score -131.45149804939766, Avg_Score -119.70777909431473\n",
      "Adding trajectory to replay buffer: step 32111, counter 513323\n",
      "Environment 0: Episode 7906, Score -118.53681250266793, Avg_Score -119.66275730022082\n",
      "Adding trajectory to replay buffer: step 32114, counter 513369\n",
      "Environment 2: Episode 7907, Score -120.28856041627502, Avg_Score -119.70716495988249\n",
      "Adding trajectory to replay buffer: step 32114, counter 513412\n",
      "Environment 6: Episode 7908, Score -122.82060246062154, Avg_Score -119.71097324951559\n",
      "Adding trajectory to replay buffer: step 32114, counter 513457\n",
      "Environment 12: Episode 7909, Score -118.97541245403762, Avg_Score -119.74409635940309\n",
      "Adding trajectory to replay buffer: step 32116, counter 513503\n",
      "Environment 14: Episode 7910, Score -121.2278474622375, Avg_Score -119.74577164553452\n",
      "Adding trajectory to replay buffer: step 32118, counter 513550\n",
      "Environment 11: Episode 7911, Score -119.43172058595408, Avg_Score -119.78498126332812\n",
      "Adding trajectory to replay buffer: step 32120, counter 513595\n",
      "Environment 1: Episode 7912, Score -120.54191448061479, Avg_Score -119.86348623611508\n",
      "Adding trajectory to replay buffer: step 32126, counter 513638\n",
      "Environment 8: Episode 7913, Score -122.87474114516998, Avg_Score -119.86657456535984\n",
      "Adding trajectory to replay buffer: step 32126, counter 513711\n",
      "Environment 9: Episode 7914, Score -114.94108896572888, Avg_Score -119.79752969577731\n",
      "Adding trajectory to replay buffer: step 32131, counter 513765\n",
      "Environment 3: Episode 7915, Score -114.16734519285399, Avg_Score -119.72521575551477\n",
      "Adding trajectory to replay buffer: step 32132, counter 513811\n",
      "Environment 4: Episode 7916, Score -121.14178546451032, Avg_Score -119.7179620723873\n",
      "Adding trajectory to replay buffer: step 32132, counter 513854\n",
      "Environment 7: Episode 7917, Score -117.0108663013056, Avg_Score -119.71292973923774\n",
      "Adding trajectory to replay buffer: step 32151, counter 513899\n",
      "Environment 15: Episode 7918, Score -122.58918224054638, Avg_Score -119.74426301733708\n",
      "Adding trajectory to replay buffer: step 32153, counter 513941\n",
      "Environment 0: Episode 7919, Score -123.0542140322257, Avg_Score -119.84904674130063\n",
      "Adding trajectory to replay buffer: step 32157, counter 513984\n",
      "Environment 2: Episode 7920, Score -122.56456557705377, Avg_Score -119.88756082043218\n",
      "Adding trajectory to replay buffer: step 32158, counter 514028\n",
      "Environment 12: Episode 7921, Score -122.43513989682248, Avg_Score -119.88445065110392\n",
      "Adding trajectory to replay buffer: step 32159, counter 514073\n",
      "Environment 6: Episode 7922, Score -122.09089491167292, Avg_Score -119.95230575182579\n",
      "Adding trajectory to replay buffer: step 32160, counter 514137\n",
      "Environment 10: Episode 7923, Score -115.84872643902929, Avg_Score -119.88109927728024\n",
      "Adding trajectory to replay buffer: step 32161, counter 514182\n",
      "Environment 14: Episode 7924, Score -121.61395290655798, Avg_Score -119.94368068681031\n",
      "Adding trajectory to replay buffer: step 32162, counter 514226\n",
      "Environment 11: Episode 7925, Score -118.0956344587685, Avg_Score -119.94017652423788\n",
      "Adding trajectory to replay buffer: step 32173, counter 514273\n",
      "Environment 8: Episode 7926, Score -118.88829632331245, Avg_Score -119.8935875973661\n",
      "Adding trajectory to replay buffer: step 32174, counter 514315\n",
      "Environment 7: Episode 7927, Score -116.40876973767766, Avg_Score -119.82256030235644\n",
      "Adding trajectory to replay buffer: step 32177, counter 514361\n",
      "Environment 3: Episode 7928, Score -122.24963595421488, Avg_Score -119.86611677051894\n",
      "Adding trajectory to replay buffer: step 32191, counter 514444\n",
      "Environment 5: Episode 7929, Score -110.2374482262507, Avg_Score -119.78197374266041\n",
      "Adding trajectory to replay buffer: step 32194, counter 514487\n",
      "Environment 15: Episode 7930, Score -123.32044792244521, Avg_Score -119.84308069835548\n",
      "Adding trajectory to replay buffer: step 32196, counter 514530\n",
      "Environment 0: Episode 7931, Score -123.17612700732921, Avg_Score -119.89682666595952\n",
      "Adding trajectory to replay buffer: step 32200, counter 514573\n",
      "Environment 2: Episode 7932, Score -122.29504324757929, Avg_Score -119.9211145922739\n",
      "Adding trajectory to replay buffer: step 32202, counter 514616\n",
      "Environment 6: Episode 7933, Score -122.77356324589998, Avg_Score -119.95047559675149\n",
      "Adding trajectory to replay buffer: step 32202, counter 514692\n",
      "Environment 9: Episode 7934, Score -117.44826407767025, Avg_Score -119.90171182114395\n",
      "Adding trajectory to replay buffer: step 32203, counter 514735\n",
      "Environment 10: Episode 7935, Score -122.82580992724934, Avg_Score -119.93446231268749\n",
      "Adding trajectory to replay buffer: step 32204, counter 514807\n",
      "Environment 4: Episode 7936, Score -112.83769033870924, Avg_Score -119.92336049314183\n",
      "Adding trajectory to replay buffer: step 32204, counter 514850\n",
      "Environment 14: Episode 7937, Score -120.66072243927543, Avg_Score -119.90521140978748\n",
      "Adding trajectory to replay buffer: step 32213, counter 514973\n",
      "Environment 13: Episode 7938, Score -115.14086622373667, Avg_Score -119.83608637675005\n",
      "Adding trajectory to replay buffer: step 32215, counter 515015\n",
      "Environment 8: Episode 7939, Score -123.28133375804623, Avg_Score -119.89737006602631\n",
      "Adding trajectory to replay buffer: step 32218, counter 515059\n",
      "Environment 7: Episode 7940, Score -121.84649078657168, Avg_Score -119.9130925255131\n",
      "Adding trajectory to replay buffer: step 32220, counter 515102\n",
      "Environment 3: Episode 7941, Score -122.64386234563403, Avg_Score -119.9101287850468\n",
      "Adding trajectory to replay buffer: step 32230, counter 515174\n",
      "Environment 12: Episode 7942, Score -114.48980313602587, Avg_Score -119.83285094541344\n",
      "Adding trajectory to replay buffer: step 32235, counter 515218\n",
      "Environment 5: Episode 7943, Score -122.21372610245055, Avg_Score -119.8843587737339\n",
      "Adding trajectory to replay buffer: step 32238, counter 515262\n",
      "Environment 15: Episode 7944, Score -122.1730486509384, Avg_Score -119.87998648512422\n",
      "Adding trajectory to replay buffer: step 32242, counter 515308\n",
      "Environment 0: Episode 7945, Score -116.8587605238178, Avg_Score -119.82034743970789\n",
      "Adding trajectory to replay buffer: step 32244, counter 515352\n",
      "Environment 2: Episode 7946, Score -117.36629313081379, Avg_Score -119.77055306651314\n",
      "Adding trajectory to replay buffer: step 32247, counter 515396\n",
      "Environment 10: Episode 7947, Score -123.19332184566372, Avg_Score -119.78747262040115\n",
      "Adding trajectory to replay buffer: step 32248, counter 515440\n",
      "Environment 14: Episode 7948, Score -117.1642569718218, Avg_Score -119.73308200855875\n",
      "Adding trajectory to replay buffer: step 32249, counter 515487\n",
      "Environment 9: Episode 7949, Score -121.6251779646532, Avg_Score -119.76867496250561\n",
      "Adding trajectory to replay buffer: step 32251, counter 515534\n",
      "Environment 4: Episode 7950, Score -120.53720811701007, Avg_Score -119.79559041849093\n",
      "Adding trajectory to replay buffer: step 32256, counter 515577\n",
      "Environment 13: Episode 7951, Score -122.39049060642782, Avg_Score -119.83467388573867\n",
      "Adding trajectory to replay buffer: step 32261, counter 515676\n",
      "Environment 11: Episode 7952, Score -114.84400549315754, Avg_Score -119.82303280691214\n",
      "Adding trajectory to replay buffer: step 32262, counter 515720\n",
      "Environment 7: Episode 7953, Score -122.43858073592497, Avg_Score -119.82010812438817\n",
      "Adding trajectory to replay buffer: step 32265, counter 515783\n",
      "Environment 6: Episode 7954, Score -113.36514421215732, Avg_Score -119.75664048487411\n",
      "Adding trajectory to replay buffer: step 32266, counter 515829\n",
      "Environment 3: Episode 7955, Score -120.25217152864859, Avg_Score -119.75228324555411\n",
      "Adding trajectory to replay buffer: step 32276, counter 515875\n",
      "Environment 12: Episode 7956, Score -121.81243548451395, Avg_Score -119.74410327257374\n",
      "Adding trajectory to replay buffer: step 32290, counter 515921\n",
      "Environment 2: Episode 7957, Score -121.50678246314389, Avg_Score -119.75291668196628\n",
      "Adding trajectory to replay buffer: step 32293, counter 515967\n",
      "Environment 10: Episode 7958, Score -120.36626462521839, Avg_Score -119.78660667601498\n",
      "Adding trajectory to replay buffer: step 32293, counter 516012\n",
      "Environment 14: Episode 7959, Score -121.32595351706259, Avg_Score -119.79617200925986\n",
      "Adding trajectory to replay buffer: step 32294, counter 516055\n",
      "Environment 4: Episode 7960, Score -116.81286468222201, Avg_Score -119.7862271334786\n",
      "Adding trajectory to replay buffer: step 32296, counter 516102\n",
      "Environment 9: Episode 7961, Score -116.34476279309827, Avg_Score -119.80364548358908\n",
      "Adding trajectory to replay buffer: step 32301, counter 516168\n",
      "Environment 5: Episode 7962, Score -117.86012273595979, Avg_Score -119.75625948792863\n",
      "Adding trajectory to replay buffer: step 32305, counter 516231\n",
      "Environment 0: Episode 7963, Score -114.69443476148012, Avg_Score -119.70918151413117\n",
      "Adding trajectory to replay buffer: step 32305, counter 516298\n",
      "Environment 15: Episode 7964, Score -114.86090907580032, Avg_Score -119.70477439769327\n",
      "Adding trajectory to replay buffer: step 32310, counter 516342\n",
      "Environment 3: Episode 7965, Score -121.26808642451962, Avg_Score -119.74052212889644\n",
      "Adding trajectory to replay buffer: step 32320, counter 516386\n",
      "Environment 12: Episode 7966, Score -115.95993155552226, Avg_Score -119.7379387013158\n",
      "Adding trajectory to replay buffer: step 32327, counter 516452\n",
      "Environment 11: Episode 7967, Score -118.61453976310167, Avg_Score -119.79453125920169\n",
      "Adding trajectory to replay buffer: step 32329, counter 516525\n",
      "Environment 13: Episode 7968, Score -118.50081907594762, Avg_Score -119.82874597736256\n",
      "Adding trajectory to replay buffer: step 32333, counter 516568\n",
      "Environment 2: Episode 7969, Score -122.19587629661461, Avg_Score -119.8381192944306\n",
      "Adding trajectory to replay buffer: step 32335, counter 516610\n",
      "Environment 10: Episode 7970, Score -114.79643030157871, Avg_Score -119.72669900290039\n",
      "Adding trajectory to replay buffer: step 32337, counter 516654\n",
      "Environment 14: Episode 7971, Score -123.37617491371743, Avg_Score -119.73511409573673\n",
      "Adding trajectory to replay buffer: step 32338, counter 516872\n",
      "Environment 1: Episode 7972, Score -123.58302866405818, Avg_Score -119.79761921045345\n",
      "Adding trajectory to replay buffer: step 32341, counter 516919\n",
      "Environment 4: Episode 7973, Score -119.861691852261, Avg_Score -119.76834892741965\n",
      "Adding trajectory to replay buffer: step 32341, counter 516964\n",
      "Environment 9: Episode 7974, Score -122.87568583875026, Avg_Score -119.77618369591896\n",
      "Adding trajectory to replay buffer: step 32347, counter 517010\n",
      "Environment 5: Episode 7975, Score -121.81094797305141, Avg_Score -119.76674664694605\n",
      "Adding trajectory to replay buffer: step 32348, counter 517053\n",
      "Environment 0: Episode 7976, Score -122.19989848858677, Avg_Score -119.77482872189114\n",
      "Adding trajectory to replay buffer: step 32348, counter 517096\n",
      "Environment 15: Episode 7977, Score -122.30434368592626, Avg_Score -119.77147428346115\n",
      "Adding trajectory to replay buffer: step 32356, counter 517142\n",
      "Environment 3: Episode 7978, Score -123.167358456914, Avg_Score -119.83831379869338\n",
      "Adding trajectory to replay buffer: step 32356, counter 517233\n",
      "Environment 6: Episode 7979, Score -114.79451009446632, Avg_Score -119.76530218504871\n",
      "Adding trajectory to replay buffer: step 32365, counter 517278\n",
      "Environment 12: Episode 7980, Score -122.24988453430186, Avg_Score -119.78572318786266\n",
      "Adding trajectory to replay buffer: step 32370, counter 517321\n",
      "Environment 11: Episode 7981, Score -122.88027488593943, Avg_Score -119.78629430430738\n",
      "Adding trajectory to replay buffer: step 32373, counter 517365\n",
      "Environment 13: Episode 7982, Score -123.4305537200862, Avg_Score -119.79330011341692\n",
      "Adding trajectory to replay buffer: step 32376, counter 517408\n",
      "Environment 2: Episode 7983, Score -123.10470817516811, Avg_Score -119.9026398456061\n",
      "Adding trajectory to replay buffer: step 32376, counter 517522\n",
      "Environment 7: Episode 7984, Score -116.04597326623337, Avg_Score -119.83320708853081\n",
      "Adding trajectory to replay buffer: step 32378, counter 517565\n",
      "Environment 10: Episode 7985, Score -122.94323746261684, Avg_Score -119.84047712897518\n",
      "Adding trajectory to replay buffer: step 32383, counter 517611\n",
      "Environment 14: Episode 7986, Score -120.71886301051391, Avg_Score -119.88468212111964\n",
      "Adding trajectory to replay buffer: step 32384, counter 517657\n",
      "Environment 1: Episode 7987, Score -119.98772326752791, Avg_Score -119.88084527864835\n",
      "Adding trajectory to replay buffer: step 32387, counter 517703\n",
      "Environment 4: Episode 7988, Score -121.17094908630227, Avg_Score -119.86506430897649\n",
      "Adding trajectory to replay buffer: step 32391, counter 517747\n",
      "Environment 5: Episode 7989, Score -120.43947117037015, Avg_Score -119.84794102137631\n",
      "Adding trajectory to replay buffer: step 32393, counter 517792\n",
      "Environment 15: Episode 7990, Score -122.31764152794425, Avg_Score -119.89378090633247\n",
      "Adding trajectory to replay buffer: step 32395, counter 517839\n",
      "Environment 0: Episode 7991, Score -122.1233029744594, Avg_Score -119.88514525550183\n",
      "Adding trajectory to replay buffer: step 32395, counter 517893\n",
      "Environment 9: Episode 7992, Score -118.3990943484294, Avg_Score -119.92187882008544\n",
      "Adding trajectory to replay buffer: step 32399, counter 517936\n",
      "Environment 6: Episode 7993, Score -119.40448800876054, Avg_Score -119.9032304736579\n",
      "Adding trajectory to replay buffer: step 32410, counter 518131\n",
      "Environment 8: Episode 7994, Score -121.25907888057765, Avg_Score -119.90368741754939\n",
      "Adding trajectory to replay buffer: step 32410, counter 518176\n",
      "Environment 12: Episode 7995, Score -121.07186616803955, Avg_Score -119.87570687729944\n",
      "Adding trajectory to replay buffer: step 32415, counter 518221\n",
      "Environment 11: Episode 7996, Score -120.50140017553171, Avg_Score -119.91604055093805\n",
      "Adding trajectory to replay buffer: step 32418, counter 518266\n",
      "Environment 13: Episode 7997, Score -122.73059069022537, Avg_Score -119.9366248231658\n",
      "Adding trajectory to replay buffer: step 32419, counter 518309\n",
      "Environment 7: Episode 7998, Score -123.47208302266401, Avg_Score -120.04692762297267\n",
      "Adding trajectory to replay buffer: step 32420, counter 518353\n",
      "Environment 2: Episode 7999, Score -123.66991842070284, Avg_Score -120.05501425869994\n",
      "Adding trajectory to replay buffer: step 32424, counter 518399\n",
      "Environment 10: Episode 8000, Score -121.38664782980084, Avg_Score -120.069761976276\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>PER/beta</td><td>▁▁▁▂▂▂▃▃▁▁▁▄▄▄▅▅▅▅▅▅▂▅▆▆▆▆▆▆▆▇▇▇▇▇▂▇███▂</td></tr><tr><td>PER/max_buffer_priority</td><td>▃▁▃▃▃█▃▃▂▃▂▁▁▂▄▂▅▅▁▁▁▆▁▂▂▄▅▂▅▃▁▃▃▃▁▅▃▂▃▅</td></tr><tr><td>PER/max_sampled_priority</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>PER/mean_buffer_priority</td><td>▃▁▆▅▅▇█▄▁▃▂▅▂▂▄▂▆▂▃▃▂▂▅▄▃▁▃▄▃▁▂▃▂▂▄▄▃▃▁▃</td></tr><tr><td>PER/mean_sampled_priority</td><td>▃██▇██▇▄▄▁▇▇▃██▅█▇█▄▅█▇▂▄▇▄█▂▇▇█▇▇██▄▇▇▇</td></tr><tr><td>PER/weight_mean</td><td>██▄▄▄▂▂▂██▅▄▁▃▄███▃█▃█▃▃█▂▂▃▃▃▃▂▂▃█▂▂▃▂▄</td></tr><tr><td>PER/weight_std</td><td>▆█▁▆▄▅▅▇▁▁▁▅▆▅▁▅▅▄▇▁█▆▇▅▁▅▅▅▅▅▁▅▁▁▇▅▆▁▅▆</td></tr><tr><td>action_0</td><td>▁▅▁▁▅▁▁▂▁████▇▄██▇██▄█▇▄█▄█▇▅█▅▇█▇████▇▅</td></tr><tr><td>action_0_noise</td><td>▁▄▄▆▄▄▇▂▄▅▄▅█▂▄▁▄▄▅▅▅▄▄▄▄▅▅▅▆▄▂▃▄▅▄▅▄▄▅▄</td></tr><tr><td>action_1</td><td>█▁██▃▃██▇▇█▃█▇▇██▄████▇▆████▇██▇█▄█▄▃▇▃█</td></tr><tr><td>action_1_noise</td><td>▆▃▄▇▄▄▅▅▇▅▆▃▄▅▁▅▇▅▅▅█▅▆▄▁▇▁▄▆▄▅▄▆▅▂█▅▅▂▅</td></tr><tr><td>action_2</td><td>▂▁▂▄▅▄▁▂▂▄▂▁▂▇▇█▅▄███▇█▄▇▇█▇▇███▇▅████▄▄</td></tr><tr><td>action_2_noise</td><td>▄█▄▂▅▇▄▄▄▇▇█▅▃▄▅▅▃▅▄▁▄▄▄▄▄▄▄▁▃▇▅▅▄▄▄▄▆▃▃</td></tr><tr><td>action_3</td><td>▇█▅▁▁▁▁▁▁▁▁▅▁▂▂▁▅▂▁▅▆▄▂▁▁▁▁▁▆▁▁▁▁▁▁▆▁▁▁▄</td></tr><tr><td>action_3_noise</td><td>▆▆▇▂▆▅▅▄▄█▅▃▃▆▆▅▆▆▅▆▂▅▄▅▅▆▅▃▅▅▅▇▅▄▆▁▅▃▃▅</td></tr><tr><td>actor_loss</td><td>▁▁▂▃▂▂▂▄▄▅▆▆▆▅▆▆▆▆▅▆▇▆█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>actor_predictions</td><td>▄▃▁▄▄███████████████████████████████████</td></tr><tr><td>best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>critic_loss</td><td>▃▃█▁▁▁▁▂▁▁▂█▂▁▃▂▂▁▁▁▁▁▂▁▁▂▁▁▁▂▂▁▂▂▁▁▁▂▁▁</td></tr><tr><td>critic_predictions</td><td>███▇▆▆▅█▇▇█▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>██████▁▁▂▃▅▃▃▂▂▂▂▄▂▂▂▂▂▂▄▂▂▄▃▂▂▂▂▄▂▅▂▄▂▂</td></tr><tr><td>step_reward</td><td>█▅██████████▄█████████████████▄███▁███▄█</td></tr><tr><td>target_actor_predictions</td><td>▆▄▁▁▁▁▁▁▅▅▅▅▅▅██████████████████████████</td></tr><tr><td>target_critic_predictions</td><td>████▆▇▇█▇▇▄▄▄▄▃▂▂▂▃▁▂▁▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>PER/beta</td><td>0.448</td></tr><tr><td>PER/max_buffer_priority</td><td>6732693.5</td></tr><tr><td>PER/max_sampled_priority</td><td>15.78906</td></tr><tr><td>PER/mean_buffer_priority</td><td>686.04865</td></tr><tr><td>PER/mean_sampled_priority</td><td>15.13139</td></tr><tr><td>PER/weight_mean</td><td>0.35405</td></tr><tr><td>PER/weight_std</td><td>0.11949</td></tr><tr><td>action_0</td><td>0.95368</td></tr><tr><td>action_0_noise</td><td>-0.04632</td></tr><tr><td>action_1</td><td>0.81793</td></tr><tr><td>action_1_noise</td><td>-0.18207</td></tr><tr><td>action_2</td><td>1</td></tr><tr><td>action_2_noise</td><td>0.39677</td></tr><tr><td>action_3</td><td>-1</td></tr><tr><td>action_3_noise</td><td>-0.01451</td></tr><tr><td>actor_loss</td><td>90.38164</td></tr><tr><td>actor_predictions</td><td>0.5</td></tr><tr><td>best</td><td>0</td></tr><tr><td>critic_loss</td><td>2.55705</td></tr><tr><td>critic_predictions</td><td>-90.38164</td></tr><tr><td>episode</td><td>8000</td></tr><tr><td>episode_reward</td><td>-121.38665</td></tr><tr><td>step_reward</td><td>-6.66008</td></tr><tr><td>target_actor_predictions</td><td>0.5</td></tr><tr><td>target_critic_predictions</td><td>-89.11299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train-148</strong> at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/jxp9log3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/jxp9log3</a><br> View project at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a><br>Synced 5 W&B file(s), 2 media file(s), 36 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250322_221545-jxp9log3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddpg_agent.train(16*500, 16, 42, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.replay_buffer.sum_tree.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/models/ddpg/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.test(10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BipedalWalker-v3')\n",
    "env = gym.make('Pendulum-v1')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(env_wrap, 100000, device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3(\n",
    "    env=env_wrap,\n",
    "    actor_model=actor,\n",
    "    critic_model=critic,\n",
    "    discount=0.99,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    noise=noise,\n",
    "    normalize_inputs=True,\n",
    "    callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')],\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.target_noise.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.train(200, 8, 42, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(td3.env.action_space.low[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/test/td3/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.state_normalizer.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER/DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPickAndPlace-v4')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL SHAPE\n",
    "goal_shape = env.observation_space['desired_goal'].shape\n",
    "print(f'goal_shape: {goal_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'uniform', 'kernel params':{'a':-3e-3, 'b':3e-3}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_buffer = ReplayBuffer(env_wrap, 100000, goal_shape=env.observation_space['desired_goal'].shape, device=device)\n",
    "replay_buffer = PrioritizedReplayBuffer(env_wrap, 10000, goal_shape=goal_shape, update_freq=1, device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, mean=0.0, stddev=0.1, device=device)\n",
    "# schedule_config = {'type':'Linear', 'params':{'start_factor':1.0, 'end_factor':0.1, 'total_iters':5000}}\n",
    "# noise_schedule = ScheduleWrapper(schedule_config)\n",
    "noise_schedule = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = DDPG(env=env_wrap,\n",
    "                actor_model=actor,\n",
    "                critic_model=critic,\n",
    "                replay_buffer=replay_buffer,\n",
    "                discount=0.95,\n",
    "                tau=0.05,\n",
    "                action_epsilon=0.2,\n",
    "                batch_size=128,\n",
    "                noise=noise,\n",
    "                noise_schedule=noise_schedule,\n",
    "                warmup=100,\n",
    "                callbacks=[rl_callbacks.WandbCallback('FetchPickAndPlace-v4')],\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "num_cycles = 50\n",
    "num_episodes = 1\n",
    "num_updates = 20\n",
    "render_freq = 50\n",
    "num_envs = 16\n",
    "seed = 42\n",
    "\n",
    "her.train(num_epochs, num_cycles, num_episodes, num_updates, render_freq, num_envs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in her.agent.env.env.envs:\n",
    "    e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/FetchPickAndPlace_HER_DDPG_PER_b/her/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = HER.load(config, load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.replay_buffer.sum_tree.tree[her.agent.replay_buffer.sum_tree.capacity-1:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "num_cycles = 50\n",
    "num_episodes = 1\n",
    "num_updates = 40\n",
    "render_freq = 100\n",
    "num_envs = 16\n",
    "seed = 42\n",
    "\n",
    "her.train(num_epochs, num_cycles, num_episodes, num_updates, render_freq, num_envs, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v4')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrap.env_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v3\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "env_id = 'Humanoid-v5'\n",
    "# env_id = \"Reacher-v5\"\n",
    "# env_id = \"Walker2d-v5\"\n",
    "# env_id = 'ALE/SpaceInvaders-ram-v5'\n",
    "# env_id = \"CarRacing-v2\"\n",
    "# env_id = \"BipedalWalkerHardcore-v3\"\n",
    "\n",
    "timesteps = 1_000_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 16\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "policy_clip = 0.2\n",
    "entropy_coeff = 0.001\n",
    "loss = 'hybrid'\n",
    "kl_coeff = 0.0\n",
    "normalize_advantages = True\n",
    "normalize_values = False\n",
    "norm_clip = np.inf\n",
    "grad_clip = 40.0\n",
    "reward_clip = 1.0\n",
    "lambda_ = 0.0\n",
    "distribution = 'beta'\n",
    "device = 'cuda'\n",
    "\n",
    "# Render Settings\n",
    "render_freq = 100\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'Humanoid-v5'\n",
    "run_name = None\n",
    "callbacks = [WandbCallback(project_name, run_name)]\n",
    "# callbacks = []\n",
    "\n",
    "seed = 42\n",
    "env = gym.make(env_id)\n",
    "\n",
    "save_dir = 'Humanoid'\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "\n",
    "# Build policy model\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "layer_config = [\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 32, 'kernel_size': (8, 8), 'stride': 4, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (4, 4), 'stride': 2, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (3, 3), 'stride': 1, 'padding': 0}},\n",
    "    # {'type': 'flatten'},\n",
    "    {'type': 'dense', 'params': {'units': 128, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "]\n",
    "output_layer_kernel = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "policy = StochasticContinuousPolicy(env, layer_config, output_layer_kernel, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, layer_config, output_layer_kernel, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=policy_clip, entropy_coefficient=entropy_coeff,\n",
    "          loss=loss, kl_coefficient=kl_coeff, normalize_advantages=normalize_advantages, normalize_values=normalize_values, value_normalizer_clip=norm_clip, policy_grad_clip=grad_clip,\n",
    "          reward_clip=reward_clip, lambda_=lambda_, callbacks=callbacks, save_dir=save_dir,device=device)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs, seed=seed, render_freq=render_freq)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/pong_v5_3/ppo/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['wrappers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = PPO.load(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.env = pong.env._initialize_env(num_envs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 2\n",
    "action_shape = (3,1)\n",
    "obs_shape = (3,)\n",
    "\n",
    "observation_space = gym.spaces.Box(low=0, high=1, shape=(num_envs, *obs_shape))\n",
    "action_space = gym.spaces.Box(low=0, high=1, shape=(num_envs, *action_shape)) if len(action_shape) > 1 else gym.spaces.MultiDiscrete([action_shape[0] for n in range(num_envs)])\n",
    "single_observation_space = gym.spaces.Box(low=0, high=1, shape=obs_shape)\n",
    "single_action_space = gym.spaces.Box(low=0, high=1, shape=action_shape) if len(action_shape) > 1 else gym.spaces.Discrete(action_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_obs = T.tensor(single_observation_space.sample())\n",
    "state, info = (T.stack([single_obs for _ in range(observation_space.shape[0])]), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = T.stack([single_obs for _ in range(observation_space.shape[0])])\n",
    "reward = T.zeros(observation_space.shape[0])\n",
    "terminated = T.zeros(observation_space.shape[0], dtype=T.bool)\n",
    "truncated = T.zeros(observation_space.shape[0], dtype=T.bool)\n",
    "info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = gym.make_vec(\"LunarLanderContinuous-v3\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ones(vec_env.single_action_space.shape).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "num_envs = 2\n",
    "expected_mu = T.stack([T.tensor([1.65, 1.65, 1.65]) for t in range(num_envs)])\n",
    "expected_sigma = T.stack([T.tensor([3.9, 3.9, 3.9]) for t in range(num_envs)])\n",
    "expected_dist = Normal(expected_mu, expected_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_dist.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.train(2000000, 128, 32, 3, 12, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[1] = 1\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium.wrappers as base_wrappers\n",
    "\n",
    "WRAPPER_REGISTRY = {\n",
    "    \"AtariPreprocessing\": {\n",
    "        \"cls\": base_wrappers.AtariPreprocessing,\n",
    "        \"default_params\": {\n",
    "            \"frame_skip\": 1,\n",
    "            \"grayscale_obs\": True,\n",
    "            \"scale_obs\": True\n",
    "        }\n",
    "    },\n",
    "    \"TimeLimit\": {\n",
    "        \"cls\": base_wrappers.TimeLimit,\n",
    "        \"default_params\": {\n",
    "            \"max_episode_steps\": 1000\n",
    "        }\n",
    "    },\n",
    "    \"TimeAwareObservation\": {\n",
    "        \"cls\": base_wrappers.TimeAwareObservation,\n",
    "        \"default_params\": {\n",
    "            \"flatten\": False,\n",
    "            \"normalize_time\": False\n",
    "        }\n",
    "    },\n",
    "    \"FrameStackObservation\": {\n",
    "        \"cls\": base_wrappers.FrameStackObservation,\n",
    "        \"default_params\": {\n",
    "            \"stack_size\": 4\n",
    "        }\n",
    "    },\n",
    "    \"ResizeObservation\": {\n",
    "        \"cls\": base_wrappers.ResizeObservation,\n",
    "        \"default_params\": {\n",
    "            \"shape\": 84\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrappers = [\n",
    "    {'type': \"AtariPreprocessing\", 'params': {'frame_skip':1, 'grayscale_obs':True, 'scale_obs':True}},\n",
    "    {'type': \"FrameStackObservation\", 'params': {'stack_size':4}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_env(vec_env, wrappers):\n",
    "    wrapper_list = []\n",
    "    for wrapper in wrappers:\n",
    "        if wrapper['type'] in WRAPPER_REGISTRY:\n",
    "            print(f'wrapper type:{wrapper[\"type\"]}')\n",
    "            # Use a copy of default_params to avoid modifying the registry\n",
    "            default_params = WRAPPER_REGISTRY[wrapper['type']][\"default_params\"].copy()\n",
    "            \n",
    "            if wrapper['type'] == \"ResizeObservation\":\n",
    "                # Ensure shape is a tuple for ResizeObservation\n",
    "                default_params['shape'] = (default_params['shape'], default_params['shape']) if isinstance(default_params['shape'], int) else default_params['shape']\n",
    "            \n",
    "            print(f'default params:{default_params}')\n",
    "            override_params = wrapper.get(\"params\", {})\n",
    "            \n",
    "            if wrapper['type'] == \"ResizeObservation\":\n",
    "                # Ensure override_params shape is a tuple\n",
    "                if 'shape' in override_params:\n",
    "                    override_params['shape'] = (override_params['shape'], override_params['shape']) if isinstance(override_params['shape'], int) else override_params['shape']\n",
    "            \n",
    "            print(f'override params:{override_params}')\n",
    "            final_params = {**default_params, **override_params}\n",
    "            print(f'final params:{final_params}')\n",
    "            \n",
    "            def wrapper_factory(env, cls=WRAPPER_REGISTRY[wrapper['type']][\"cls\"], params=final_params):\n",
    "                return cls(env, **params)\n",
    "            \n",
    "            wrapper_list.append(wrapper_factory)\n",
    "    \n",
    "    # Define apply_wrappers outside the loop\n",
    "    def apply_wrappers(env):\n",
    "        for wrapper in wrapper_list:\n",
    "            env = wrapper(env)\n",
    "            print(f'length of obs space:{len(env.observation_space.shape)}')\n",
    "            print(f'env obs space shape:{env.observation_space.shape}')\n",
    "        return env\n",
    "    \n",
    "    print(f'wrapper list:{wrapper_list}')\n",
    "    envs = [lambda: apply_wrappers(gym.make(vec_env.spec.id, render_mode=\"rgb_array\")) for _ in range(vec_env.num_envs)]    \n",
    "    return SyncVectorEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = gym.make_vec(\"ALE/Pong-v5\", render_mode=\"rgb_array\", num_envs=8)\n",
    "wrapped_vec = wrap_env(vec_env, wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_vec.single_observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in wrapped_vec.envs:\n",
    "    print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_wrappers(wrapper_store):\n",
    "    wrappers_dict = {}\n",
    "    for key, value in wrapper_store.items():\n",
    "        # Split the key into wrapper type and parameter name\n",
    "        parts = key.split('_param:')\n",
    "        print(f'parts:{parts}')\n",
    "        wrapper_type = parts[0].split('wrapper:')[1]\n",
    "        print(f'wrapper_type:{wrapper_type}')\n",
    "        param_name = parts[1]\n",
    "        print(f'param name:{param_name}')\n",
    "        \n",
    "        # If the wrapper type already exists in the dictionary, append to its params\n",
    "        if wrapper_type not in wrappers_dict:\n",
    "            wrappers_dict[wrapper_type] = {'type': wrapper_type, 'params': {}}\n",
    "        \n",
    "        wrappers_dict[wrapper_type]['params'][param_name] = value\n",
    "    \n",
    "    # Convert the dictionary to a list of dictionaries\n",
    "    formatted_wrappers = list(wrappers_dict.values())\n",
    "    \n",
    "    return formatted_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_params = {'wrapper:AtariPreprocessing_param:frame_skip': 1, 'wrapper:AtariPreprocessing_param:grayscale_obs': True, 'wrapper:AtariPreprocessing_param:scale_obs': True, 'wrapper:FrameStackObservation_param:stack_size': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_wrappers = format_wrappers(wrapper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_params = {'wrapper:AtariPreprocessing_param:frame_skip': 1, 'wrapper:AtariPreprocessing_param:grayscale_obs': True, 'wrapper:AtariPreprocessing_param:scale_obs': True, 'wrapper:FrameStackObservation_param:stack_size': 4}\n",
    "formatted_wrappers = dash_utils.format_wrappers(wrapper_params)\n",
    "#DEBUG\n",
    "print(f'formatted wrappers:{formatted_wrappers}')\n",
    "env = dash_utils.instantiate_envwrapper_obj(\"gymnasium\", \"ALE/Pong-v5\", formatted_wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/humanoid_v5_2/ppo/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "ppo = PPO.load(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.env.env = ppo.env._initialize_env(0, 8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in ppo.env.env.envs:\n",
    "    print(env.spec.pprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.train(2_000_000, 128, 64, 10, 8, 42, render_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, _ = ppo.env.reset()\n",
    "steps = 10\n",
    "all_states = []\n",
    "all_next_states = []\n",
    "for step in range(steps):\n",
    "    actions, log_probs = ppo.get_action(states)\n",
    "    next_states, rewards, terms, truncs, infos = ppo.env.step(actions)\n",
    "    all_states.append(states)\n",
    "    all_next_states.append(next_states)\n",
    "    states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, step_states in enumerate(all_states):\n",
    "    print(f'step states shape:{step_states.shape}')\n",
    "    for i in range(len(step_states)):\n",
    "        for j in range(i + 1, len(step_states)):  # Compare each environment with others\n",
    "            print(f'step state {i} shape:{step_states[i].shape}')\n",
    "            print(f'step state {j} shape:{step_states[j].shape}')\n",
    "            assert np.allclose(step_states[i], step_states[j]), f\"Environments {i} and {j} differ at step {step}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_states)):\n",
    "    for j in range(i + 1, len(all_states)):  # Note the change here\n",
    "        print(np.allclose(all_states[i], all_states[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = []\n",
    "obs = np.ones((8,1,84,84))\n",
    "for _ in range(10):\n",
    "    all_obs.append(obs)\n",
    "# all_obs = np.array(all_obs)\n",
    "all_obs = T.stack([T.tensor(s, dtype=T.float32) for s in all_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Box(low=0, high=1, shape=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_advantages = []\n",
    "all_returns = []\n",
    "all_values = []\n",
    "advantage = T.ones(128)\n",
    "return_ = T.ones(128)\n",
    "value = T.ones(128)\n",
    "num_envs = 2\n",
    "\n",
    "for _ in range(num_envs):\n",
    "    all_advantages.append(advantage)\n",
    "    all_returns.append(return_)\n",
    "    all_values.append(value)\n",
    "\n",
    "advantages = T.stack(all_advantages, dim=1)\n",
    "returns = T.stack(all_returns, dim=1)\n",
    "values = T.stack(all_values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = pong.env.reset()\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns, r, term, trunc, _ = pong.env.step(pong.env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.env.envs[0].spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = pong.env.reset()\n",
    "states = T.tensor(states)\n",
    "dist, _ = pong.policy_model(states)\n",
    "sample = dist.sample()\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_reward(reward):\n",
    "    \"\"\"\n",
    "    Clip rewards to the specified range.\n",
    "\n",
    "    Args:\n",
    "        reward (float): Reward to clip.\n",
    "\n",
    "    Returns:\n",
    "        float: Clipped reward.\n",
    "    \"\"\"\n",
    "    if reward > 1:\n",
    "        return 1\n",
    "    elif reward < -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make_vec(\"ALE/Pong-v5\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "all_dones = []\n",
    "for _ in range(10):\n",
    "    next_states, rewards, terms, truncs, infos = env.step(env.action_space.sample())\n",
    "    all_rewards.append(rewards)\n",
    "    all_dones.append(np.logical_or(terms, truncs))\n",
    "rewards = T.stack([T.tensor(r, dtype=T.float32) for r in all_rewards])\n",
    "dones = T.stack([T.tensor(d, dtype=T.float32) for d in all_dones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[clip_reward(reward) for reward in rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 6000  # Total steps\n",
    "eta_max = 1.0  # Initial noise stddev\n",
    "eta_min = 0.1  # Minimum noise stddev\n",
    "\n",
    "t = np.linspace(0, T_max, 1000)  # Sample points\n",
    "value = eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(t * np.pi / T_max))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t, value, 'b-', label='Cosine Annealing (stddev)')\n",
    "plt.axhline(y=eta_max, color='r', linestyle='--', label='Initial (1.0)')\n",
    "plt.axhline(y=eta_min, color='g', linestyle='--', label='Minimum (0.1)')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Noise StdDev')\n",
    "plt.title('Cosine Annealing Curve for Noise (stddev)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)  # Shows all directories Python checks for imports\n",
    "\n",
    "# Try to find mcp specifically\n",
    "try:\n",
    "    import mcp\n",
    "    print(f\"MCP found at: {mcp.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"MCP not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcp\n",
    "print(dir(mcp))  # This will show all available attributes/modules in mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
