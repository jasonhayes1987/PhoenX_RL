{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import umap\n",
    "import pynndescent\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Numba version:\", numba.__version__)\n",
    "print(\"UMAP version:\", umap.__version__)\n",
    "print(\"PyNNDescent version:\", pynndescent.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ale_py\n",
    "\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium.vector import VectorEnv, SyncVectorEnv\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, CriticModel, StochasticDiscretePolicy\n",
    "from rl_agents import PPO, DDPG, Reinforce, ActorCritic, TD3, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "# from helper import Normalizer\n",
    "from buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from noise import NormalNoise\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "import dash_utils\n",
    "from env_wrapper import EnvWrapper, GymnasiumWrapper\n",
    "from schedulers import ScheduleWrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mujoco version: {mujoco.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v4')\n",
    "env_spec = env.spec\n",
    "wrap_env = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.env.env.initial_qpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_env.env = wrap_env._initialize_env(num_envs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = wrap_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if T.cuda.is_available():\n",
    "        return T.device('cuda')\n",
    "    else:\n",
    "        return T.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BipedalWalker-v3')\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in env_wrap.env.envs:\n",
    "    print(e.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_buffer = ReplayBuffer(env_wrap, 100000, device=device)\n",
    "replay_buffer = PrioritizedReplayBuffer(env_wrap, 200000, alpha=0.6, beta_start=0.4, beta_iter=300, beta_update_freq=1, normalize=True, epsilon=0.01,device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, stddev=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = DDPG(env=env_wrap,\n",
    "                actor_model=actor,\n",
    "                critic_model=critic,\n",
    "                replay_buffer=replay_buffer,\n",
    "                discount=0.99,\n",
    "                tau=0.05,\n",
    "                action_epsilon=0.2,\n",
    "                batch_size=128,\n",
    "                noise=noise,\n",
    "                warmup=500,\n",
    "                callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')],\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(2000, 16, 42, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.unique(ddpg_agent.replay_buffer.states).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/models/ddpg/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.test(10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BipedalWalker-v3')\n",
    "env = gym.make('Pendulum-v1')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 400, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 300, 'kernel': 'variance_scaling', 'kernel params':{\"scale\": 1.0, \"mode\": \"fan_in\", \"distribution\": \"uniform\"}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(env_wrap, 100000, device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, stddev=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3(\n",
    "    env=env_wrap,\n",
    "    actor_model=actor,\n",
    "    critic_model=critic,\n",
    "    discount=0.99,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    noise=noise,\n",
    "    target_noise=noise,\n",
    "    actor_update_delay = 2,\n",
    "    normalize_inputs=True,\n",
    "    warmup=200,\n",
    "    # callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')],\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.target_noise.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.train(200, 8, 42, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(td3.env.action_space.low[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/test/td3/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.state_normalizer.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER/DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v4')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal_shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "# GOAL SHAPE\n",
    "goal_shape = env.observation_space['desired_goal'].shape\n",
    "print(f'goal_shape: {goal_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "device = 'cuda'\n",
    "optimizer = {'type': 'Adam','params': { 'lr': 0.001 }}\n",
    "\n",
    "layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "output_layer_config = [{'type': 'dense', 'params': {'kernel': 'uniform', 'kernel params':{'a':-3e-3, 'b':3e-3}}}]\n",
    "\n",
    "actor = ActorModel(env_wrap, layer_config, output_layer_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layer_config = [\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'},\n",
    "]\n",
    "\n",
    "merged_layer_config = [\n",
    "    \n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'xavier_uniform', 'kernel params':{\"gain\": 1.0}}},\n",
    "    {'type': 'relu'}\n",
    "]\n",
    "# output_layer_config = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "\n",
    "critic = CriticModel(env_wrap, state_layers=state_layer_config, merged_layers=merged_layer_config,\n",
    "                    output_layer_kernel=output_layer_config, optimizer_params=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# replay_buffer = ReplayBuffer(env_wrap, 100000, goal_shape=env.observation_space['desired_goal'].shape, device=device)\n",
    "replay_buffer = PrioritizedReplayBuffer(env_wrap, 10000, beta_start=0.4, beta_iter=3000, beta_update_freq=1, normalize=False, goal_shape=goal_shape, epsilon=0.01, device=device)\n",
    "noise = NormalNoise(shape=env_wrap.action_space.shape, mean=0.0, stddev=0.1, device=device)\n",
    "# schedule_config = {'type':'Linear', 'params':{'start_factor':1.0, 'end_factor':0.1, 'total_iters':5000}}\n",
    "# noise_schedule = ScheduleWrapper(schedule_config)\n",
    "noise_schedule = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'PrioritizedReplayBuffer',\n",
       " 'config': {'env': '{\"type\": \"GymnasiumWrapper\", \"env\": \"{\\\\\"id\\\\\": \\\\\"FetchReach-v4\\\\\", \\\\\"entry_point\\\\\": \\\\\"gymnasium_robotics.envs.fetch.reach:MujocoFetchReachEnv\\\\\", \\\\\"reward_threshold\\\\\": null, \\\\\"nondeterministic\\\\\": false, \\\\\"max_episode_steps\\\\\": 50, \\\\\"order_enforce\\\\\": true, \\\\\"disable_env_checker\\\\\": false, \\\\\"kwargs\\\\\": {\\\\\"reward_type\\\\\": \\\\\"sparse\\\\\"}, \\\\\"additional_wrappers\\\\\": [], \\\\\"vector_entry_point\\\\\": null}\", \"wrappers\": null}',\n",
       "  'buffer_size': 10000,\n",
       "  'alpha': 0.6,\n",
       "  'beta_start': 0.4,\n",
       "  'beta_iter': 3000,\n",
       "  'beta_update_freq': 1,\n",
       "  'priority': 'proportional',\n",
       "  'normalize': False,\n",
       "  'goal_shape': (3,),\n",
       "  'epsilon': 0.01,\n",
       "  'device': 'cuda'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = DDPG(env=env_wrap,\n",
    "                actor_model=actor,\n",
    "                critic_model=critic,\n",
    "                replay_buffer=replay_buffer,\n",
    "                discount=0.95,\n",
    "                tau=0.05,\n",
    "                action_epsilon=0.2,\n",
    "                batch_size=128,\n",
    "                noise=noise,\n",
    "                noise_schedule=noise_schedule,\n",
    "                normalize_inputs=True,\n",
    "                warmup=0,\n",
    "                callbacks=[rl_callbacks.WandbCallback('FetchReach-v4')],\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonhayes1987\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/src/app/wandb/run-20250401_211239-ubpwp0wn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/FetchReach-v4/runs/ubpwp0wn' target=\"_blank\">train-72</a></strong> to <a href='https://wandb.ai/jasonhayes1987/FetchReach-v4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/FetchReach-v4' target=\"_blank\">https://wandb.ai/jasonhayes1987/FetchReach-v4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/FetchReach-v4/runs/ubpwp0wn' target=\"_blank\">https://wandb.ai/jasonhayes1987/FetchReach-v4/runs/ubpwp0wn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 1: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 2: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 3: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 4: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 5: episode 1, score -50.0, avg_score -50.0\n",
      "Environment 6: episode 1, score -32.0, avg_score -47.42857142857143\n",
      "Environment 7: episode 1, score -22.0, avg_score -44.25\n",
      "Environment 8: episode 1, score -50.0, avg_score -44.888888888888886\n",
      "Environment 9: episode 1, score -50.0, avg_score -45.4\n",
      "Environment 10: episode 1, score -28.0, avg_score -43.81818181818182\n",
      "Environment 11: episode 1, score -50.0, avg_score -44.333333333333336\n",
      "Environment 12: episode 1, score -50.0, avg_score -44.76923076923077\n",
      "Environment 13: episode 1, score -50.0, avg_score -45.142857142857146\n",
      "Environment 14: episode 1, score -50.0, avg_score -45.46666666666667\n",
      "Environment 15: episode 1, score -50.0, avg_score -45.75\n",
      "Environment 0: episode 2, score -50.0, avg_score -46.0\n",
      "Environment 1: episode 2, score -38.0, avg_score -45.55555555555556\n",
      "Environment 2: episode 2, score -50.0, avg_score -45.78947368421053\n",
      "Environment 3: episode 2, score -50.0, avg_score -46.0\n",
      "Environment 4: episode 2, score -50.0, avg_score -46.19047619047619\n",
      "Environment 5: episode 2, score -50.0, avg_score -46.36363636363637\n",
      "Environment 6: episode 2, score -50.0, avg_score -46.52173913043478\n",
      "Environment 7: episode 2, score -50.0, avg_score -46.666666666666664\n",
      "Environment 8: episode 2, score -50.0, avg_score -46.8\n",
      "Environment 9: episode 2, score -50.0, avg_score -46.92307692307692\n",
      "Environment 10: episode 2, score -50.0, avg_score -47.03703703703704\n",
      "Environment 11: episode 2, score -50.0, avg_score -47.142857142857146\n",
      "Environment 12: episode 2, score -50.0, avg_score -47.241379310344826\n",
      "Environment 13: episode 2, score -50.0, avg_score -47.333333333333336\n",
      "Environment 14: episode 2, score -50.0, avg_score -47.41935483870968\n",
      "Environment 15: episode 2, score -50.0, avg_score -47.5\n",
      "Environment 0: episode 3, score -50.0, avg_score -47.57575757575758\n",
      "Environment 1: episode 3, score -50.0, avg_score -47.64705882352941\n",
      "Environment 2: episode 3, score -50.0, avg_score -47.714285714285715\n",
      "Environment 3: episode 3, score -50.0, avg_score -47.77777777777778\n",
      "Environment 4: episode 3, score -50.0, avg_score -47.83783783783784\n",
      "Environment 5: episode 3, score -50.0, avg_score -47.89473684210526\n",
      "Environment 6: episode 3, score -50.0, avg_score -47.94871794871795\n",
      "Environment 7: episode 3, score -50.0, avg_score -48.0\n",
      "Environment 8: episode 3, score -50.0, avg_score -48.048780487804876\n",
      "Environment 9: episode 3, score -50.0, avg_score -48.095238095238095\n",
      "Environment 10: episode 3, score -50.0, avg_score -48.13953488372093\n",
      "Environment 11: episode 3, score -50.0, avg_score -48.18181818181818\n",
      "Environment 12: episode 3, score -50.0, avg_score -48.22222222222222\n",
      "Environment 13: episode 3, score -50.0, avg_score -48.26086956521739\n",
      "Environment 14: episode 3, score -41.0, avg_score -48.1063829787234\n",
      "Environment 15: episode 3, score -50.0, avg_score -48.145833333333336\n",
      "Environment 0: episode 4, score -43.0, avg_score -48.04081632653061\n",
      "Environment 1: episode 4, score -50.0, avg_score -48.08\n",
      "Environment 2: episode 4, score -50.0, avg_score -48.11764705882353\n",
      "Environment 3: episode 4, score -50.0, avg_score -48.15384615384615\n",
      "Environment 4: episode 4, score -50.0, avg_score -48.18867924528302\n",
      "Environment 5: episode 4, score -50.0, avg_score -48.22222222222222\n",
      "Environment 6: episode 4, score -50.0, avg_score -48.25454545454546\n",
      "Environment 7: episode 4, score -50.0, avg_score -48.285714285714285\n",
      "Environment 8: episode 4, score -50.0, avg_score -48.31578947368421\n",
      "Environment 9: episode 4, score -50.0, avg_score -48.3448275862069\n",
      "Environment 10: episode 4, score -50.0, avg_score -48.3728813559322\n",
      "Environment 11: episode 4, score -50.0, avg_score -48.4\n",
      "Environment 12: episode 4, score -50.0, avg_score -48.42622950819672\n",
      "Environment 13: episode 4, score -48.0, avg_score -48.41935483870968\n",
      "Environment 14: episode 4, score -50.0, avg_score -48.44444444444444\n",
      "Environment 15: episode 4, score -50.0, avg_score -48.46875\n",
      "Environment 0: episode 5, score -50.0, avg_score -48.49230769230769\n",
      "Environment 1: episode 5, score -45.0, avg_score -48.43939393939394\n",
      "Environment 2: episode 5, score -50.0, avg_score -48.46268656716418\n",
      "Environment 3: episode 5, score -50.0, avg_score -48.48529411764706\n",
      "Environment 4: episode 5, score -50.0, avg_score -48.507246376811594\n",
      "Environment 5: episode 5, score -50.0, avg_score -48.52857142857143\n",
      "Environment 6: episode 5, score -50.0, avg_score -48.54929577464789\n",
      "Environment 7: episode 5, score -50.0, avg_score -48.56944444444444\n",
      "Environment 8: episode 5, score -50.0, avg_score -48.58904109589041\n",
      "Environment 9: episode 5, score -50.0, avg_score -48.608108108108105\n",
      "Environment 10: episode 5, score -50.0, avg_score -48.626666666666665\n",
      "Environment 11: episode 5, score -50.0, avg_score -48.64473684210526\n",
      "Environment 12: episode 5, score -50.0, avg_score -48.66233766233766\n",
      "Environment 13: episode 5, score -50.0, avg_score -48.67948717948718\n",
      "Environment 14: episode 5, score -50.0, avg_score -48.69620253164557\n",
      "Environment 15: episode 5, score -50.0, avg_score -48.7125\n",
      "Environment 0: episode 6, score -50.0, avg_score -48.72839506172839\n",
      "Environment 1: episode 6, score -50.0, avg_score -48.74390243902439\n",
      "Environment 2: episode 6, score -50.0, avg_score -48.75903614457831\n",
      "Environment 3: episode 6, score -50.0, avg_score -48.773809523809526\n",
      "Environment 4: episode 6, score -50.0, avg_score -48.78823529411765\n",
      "Environment 5: episode 6, score -50.0, avg_score -48.80232558139535\n",
      "Environment 6: episode 6, score -50.0, avg_score -48.81609195402299\n",
      "Environment 7: episode 6, score -50.0, avg_score -48.82954545454545\n",
      "Environment 8: episode 6, score -50.0, avg_score -48.842696629213485\n",
      "Environment 9: episode 6, score -50.0, avg_score -48.855555555555554\n",
      "Environment 10: episode 6, score -50.0, avg_score -48.86813186813187\n",
      "Environment 11: episode 6, score -50.0, avg_score -48.880434782608695\n",
      "Environment 12: episode 6, score -50.0, avg_score -48.89247311827957\n",
      "Environment 13: episode 6, score -50.0, avg_score -48.90425531914894\n",
      "Environment 14: episode 6, score -50.0, avg_score -48.915789473684214\n",
      "Environment 15: episode 6, score -50.0, avg_score -48.927083333333336\n",
      "Environment 0: episode 7, score -50.0, avg_score -48.93814432989691\n",
      "Environment 1: episode 7, score -50.0, avg_score -48.94897959183673\n",
      "Environment 2: episode 7, score -50.0, avg_score -48.95959595959596\n",
      "Rendering episode 100.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_100.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_100.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 4: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 5: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 6: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 7: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 8: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 9: episode 7, score -50.0, avg_score -48.97\n",
      "Environment 10: episode 7, score -50.0, avg_score -49.15\n",
      "Environment 11: episode 7, score -50.0, avg_score -49.43\n",
      "Environment 12: episode 7, score -50.0, avg_score -49.43\n",
      "Environment 13: episode 7, score -50.0, avg_score -49.43\n",
      "Environment 14: episode 7, score -50.0, avg_score -49.65\n",
      "Environment 15: episode 7, score -50.0, avg_score -49.65\n",
      "Environment 0: episode 8, score -50.0, avg_score -49.65\n",
      "Environment 1: episode 8, score -50.0, avg_score -49.65\n",
      "Environment 2: episode 8, score -50.0, avg_score -49.65\n",
      "Environment 3: episode 8, score -50.0, avg_score -49.65\n",
      "Environment 4: episode 8, score -50.0, avg_score -49.65\n",
      "Environment 5: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 6: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 7: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 8: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 9: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 10: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 11: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 12: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 13: episode 8, score -50.0, avg_score -49.77\n",
      "Environment 14: episode 8, score -14.0, avg_score -49.41\n",
      "Environment 15: episode 8, score -50.0, avg_score -49.41\n",
      "Environment 0: episode 9, score -50.0, avg_score -49.41\n",
      "Environment 1: episode 9, score -50.0, avg_score -49.41\n",
      "Environment 2: episode 9, score -50.0, avg_score -49.41\n",
      "Environment 3: episode 9, score -50.0, avg_score -49.41\n",
      "Environment 4: episode 9, score -48.0, avg_score -49.39\n",
      "Environment 5: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 6: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 7: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 8: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 9: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 10: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 11: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 12: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 13: episode 9, score -50.0, avg_score -49.39\n",
      "Environment 14: episode 9, score -47.0, avg_score -49.36\n",
      "Environment 15: episode 9, score -50.0, avg_score -49.36\n",
      "Environment 0: episode 10, score -50.0, avg_score -49.36\n",
      "Environment 1: episode 10, score -50.0, avg_score -49.36\n",
      "Environment 2: episode 10, score -50.0, avg_score -49.45\n",
      "Environment 3: episode 10, score -50.0, avg_score -49.45\n",
      "Environment 4: episode 10, score -50.0, avg_score -49.52\n",
      "Environment 5: episode 10, score -49.0, avg_score -49.51\n",
      "Environment 6: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 7: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 8: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 9: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 10: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 11: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 12: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 13: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 14: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 15: episode 10, score -50.0, avg_score -49.51\n",
      "Environment 0: episode 11, score -50.0, avg_score -49.51\n",
      "Environment 1: episode 11, score -50.0, avg_score -49.53\n",
      "Environment 2: episode 11, score -50.0, avg_score -49.53\n",
      "Environment 3: episode 11, score -50.0, avg_score -49.53\n",
      "Environment 4: episode 11, score -50.0, avg_score -49.53\n",
      "Environment 5: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 6: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 7: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 8: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 9: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 10: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 11: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 12: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 13: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 14: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 15: episode 11, score -50.0, avg_score -49.58\n",
      "Environment 0: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 1: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 2: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 3: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 4: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 5: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 6: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 7: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 8: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 9: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 10: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 11: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 12: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 13: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 14: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 15: episode 12, score -50.0, avg_score -49.58\n",
      "Environment 0: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 1: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 2: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 3: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 4: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 5: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 6: episode 13, score -50.0, avg_score -49.58\n",
      "Rendering episode 200.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_200.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_200.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 8: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 9: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 10: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 11: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 12: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 13: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 14: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 15: episode 13, score -50.0, avg_score -49.58\n",
      "Environment 0: episode 14, score -50.0, avg_score -49.58\n",
      "Environment 1: episode 14, score -50.0, avg_score -49.58\n",
      "Environment 2: episode 14, score -50.0, avg_score -49.58\n",
      "Environment 3: episode 14, score -50.0, avg_score -49.58\n",
      "Environment 4: episode 14, score -34.0, avg_score -49.42\n",
      "Environment 5: episode 14, score -50.0, avg_score -49.42\n",
      "Environment 6: episode 14, score -30.0, avg_score -49.22\n",
      "Environment 7: episode 14, score -50.0, avg_score -49.22\n",
      "Environment 8: episode 14, score -50.0, avg_score -49.22\n",
      "Environment 9: episode 14, score -50.0, avg_score -49.22\n",
      "Environment 10: episode 14, score -50.0, avg_score -49.22\n",
      "Environment 11: episode 14, score -44.0, avg_score -49.16\n",
      "Environment 12: episode 14, score -50.0, avg_score -49.16\n",
      "Environment 13: episode 14, score -50.0, avg_score -49.16\n",
      "Environment 14: episode 14, score -46.0, avg_score -49.12\n",
      "Environment 15: episode 14, score -42.0, avg_score -49.04\n",
      "Environment 0: episode 15, score -50.0, avg_score -49.04\n",
      "Environment 1: episode 15, score -50.0, avg_score -49.04\n",
      "Environment 2: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 3: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 4: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 5: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 6: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 7: episode 15, score -50.0, avg_score -49.4\n",
      "Environment 8: episode 15, score -50.0, avg_score -49.42\n",
      "Environment 9: episode 15, score -50.0, avg_score -49.42\n",
      "Environment 10: episode 15, score -39.0, avg_score -49.31\n",
      "Environment 11: episode 15, score -50.0, avg_score -49.31\n",
      "Environment 12: episode 15, score -50.0, avg_score -49.31\n",
      "Environment 13: episode 15, score -50.0, avg_score -49.31\n",
      "Environment 14: episode 15, score -50.0, avg_score -49.31\n",
      "Environment 15: episode 15, score -50.0, avg_score -49.31\n",
      "Environment 0: episode 16, score -50.0, avg_score -49.31\n",
      "Environment 1: episode 16, score -50.0, avg_score -49.31\n",
      "Environment 2: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 3: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 4: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 5: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 6: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 7: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 8: episode 16, score -50.0, avg_score -49.34\n",
      "Environment 9: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 10: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 11: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 12: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 13: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 14: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 15: episode 16, score -50.0, avg_score -49.35\n",
      "Environment 0: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 1: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 2: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 3: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 4: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 5: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 6: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 7: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 8: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 9: episode 17, score -50.0, avg_score -49.35\n",
      "Environment 10: episode 17, score -24.0, avg_score -49.09\n",
      "Environment 11: episode 17, score -34.0, avg_score -48.93\n",
      "Environment 12: episode 17, score -50.0, avg_score -48.93\n",
      "Environment 13: episode 17, score -50.0, avg_score -48.93\n",
      "Environment 14: episode 17, score -24.0, avg_score -48.67\n",
      "Environment 15: episode 17, score -50.0, avg_score -48.67\n",
      "Environment 0: episode 18, score -50.0, avg_score -48.67\n",
      "Environment 1: episode 18, score -50.0, avg_score -48.67\n",
      "Environment 2: episode 18, score -50.0, avg_score -48.67\n",
      "Environment 3: episode 18, score -50.0, avg_score -48.67\n",
      "Environment 4: episode 18, score -50.0, avg_score -48.67\n",
      "Environment 5: episode 18, score -20.0, avg_score -48.37\n",
      "Environment 6: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 7: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 8: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 9: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 10: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 11: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 12: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 13: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 14: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 15: episode 18, score -50.0, avg_score -48.37\n",
      "Environment 0: episode 19, score -50.0, avg_score -48.37\n",
      "Environment 1: episode 19, score -50.0, avg_score -48.37\n",
      "Environment 2: episode 19, score -50.0, avg_score -48.37\n",
      "Environment 3: episode 19, score -50.0, avg_score -48.37\n",
      "Environment 4: episode 19, score -50.0, avg_score -48.37\n",
      "Environment 5: episode 19, score -11.0, avg_score -47.98\n",
      "Environment 6: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 7: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 8: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 9: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 10: episode 19, score -50.0, avg_score -47.98\n",
      "Rendering episode 300.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_300.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_300.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 12: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 13: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 14: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 15: episode 19, score -50.0, avg_score -47.98\n",
      "Environment 0: episode 20, score -50.0, avg_score -47.98\n",
      "Environment 1: episode 20, score -50.0, avg_score -47.98\n",
      "Environment 2: episode 20, score -41.0, avg_score -47.89\n",
      "Environment 3: episode 20, score -50.0, avg_score -47.89\n",
      "Environment 4: episode 20, score -41.0, avg_score -47.8\n",
      "Environment 5: episode 20, score -50.0, avg_score -47.8\n",
      "Environment 6: episode 20, score -50.0, avg_score -47.8\n",
      "Environment 7: episode 20, score -50.0, avg_score -47.8\n",
      "Environment 8: episode 20, score -50.0, avg_score -47.96\n",
      "Environment 9: episode 20, score -50.0, avg_score -47.96\n",
      "Environment 10: episode 20, score -50.0, avg_score -48.16\n",
      "Environment 11: episode 20, score -31.0, avg_score -47.97\n",
      "Environment 12: episode 20, score -50.0, avg_score -47.97\n",
      "Environment 13: episode 20, score -50.0, avg_score -47.97\n",
      "Environment 14: episode 20, score -50.0, avg_score -47.97\n",
      "Environment 15: episode 20, score -50.0, avg_score -48.03\n",
      "Environment 0: episode 21, score -15.0, avg_score -47.68\n",
      "Environment 1: episode 21, score -50.0, avg_score -47.68\n",
      "Environment 2: episode 21, score -50.0, avg_score -47.72\n",
      "Environment 3: episode 21, score -50.0, avg_score -47.8\n",
      "Environment 4: episode 21, score -50.0, avg_score -47.8\n",
      "Environment 5: episode 21, score -48.0, avg_score -47.78\n",
      "Environment 6: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 7: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 8: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 9: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 10: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 11: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 12: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 13: episode 21, score -50.0, avg_score -47.78\n",
      "Environment 14: episode 21, score -50.0, avg_score -47.89\n",
      "Environment 15: episode 21, score -50.0, avg_score -47.89\n",
      "Environment 0: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 1: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 2: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 3: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 4: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 5: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 6: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 7: episode 22, score -50.0, avg_score -47.89\n",
      "Environment 8: episode 22, score -39.0, avg_score -47.78\n",
      "Environment 9: episode 22, score -45.0, avg_score -47.73\n",
      "Environment 10: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 11: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 12: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 13: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 14: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 15: episode 22, score -50.0, avg_score -47.73\n",
      "Environment 0: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 1: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 2: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 3: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 4: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 5: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 6: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 7: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 8: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 9: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 10: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 11: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 12: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 13: episode 23, score -50.0, avg_score -47.73\n",
      "Environment 14: episode 23, score -50.0, avg_score -47.99\n",
      "Environment 15: episode 23, score -50.0, avg_score -48.15\n",
      "Environment 0: episode 24, score -50.0, avg_score -48.15\n",
      "Environment 1: episode 24, score -50.0, avg_score -48.15\n",
      "Environment 2: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 3: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 4: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 5: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 6: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 7: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 8: episode 24, score -50.0, avg_score -48.41\n",
      "Environment 9: episode 24, score -50.0, avg_score -48.71\n",
      "Environment 10: episode 24, score -50.0, avg_score -48.71\n",
      "Environment 11: episode 24, score -49.0, avg_score -48.7\n",
      "Environment 12: episode 24, score -50.0, avg_score -48.7\n",
      "Environment 13: episode 24, score -47.0, avg_score -48.67\n",
      "Environment 14: episode 24, score -50.0, avg_score -48.67\n",
      "Environment 15: episode 24, score -50.0, avg_score -48.67\n",
      "Environment 0: episode 25, score -50.0, avg_score -48.67\n",
      "Environment 1: episode 25, score -50.0, avg_score -48.67\n",
      "Environment 2: episode 25, score -50.0, avg_score -48.67\n",
      "Environment 3: episode 25, score -50.0, avg_score -48.67\n",
      "Environment 4: episode 25, score -50.0, avg_score -48.67\n",
      "Environment 5: episode 25, score -46.0, avg_score -48.63\n",
      "Environment 6: episode 25, score -50.0, avg_score -48.63\n",
      "Environment 7: episode 25, score -50.0, avg_score -48.63\n",
      "Environment 8: episode 25, score -50.0, avg_score -48.63\n",
      "Environment 9: episode 25, score -50.0, avg_score -49.02\n",
      "Environment 10: episode 25, score -21.0, avg_score -48.73\n",
      "Environment 11: episode 25, score -50.0, avg_score -48.73\n",
      "Environment 12: episode 25, score -50.0, avg_score -48.73\n",
      "Environment 13: episode 25, score -50.0, avg_score -48.73\n",
      "Environment 14: episode 25, score -50.0, avg_score -48.73\n",
      "Rendering episode 400.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_400.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_400.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 25, score -50.0, avg_score -48.73\n",
      "Environment 0: episode 26, score -50.0, avg_score -48.73\n",
      "Environment 1: episode 26, score -50.0, avg_score -48.73\n",
      "Environment 2: episode 26, score -50.0, avg_score -48.73\n",
      "Environment 3: episode 26, score -50.0, avg_score -48.73\n",
      "Environment 4: episode 26, score -47.0, avg_score -48.7\n",
      "Environment 5: episode 26, score -50.0, avg_score -48.7\n",
      "Environment 6: episode 26, score -50.0, avg_score -48.79\n",
      "Environment 7: episode 26, score -50.0, avg_score -48.79\n",
      "Environment 8: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 9: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 10: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 11: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 12: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 13: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 14: episode 26, score -50.0, avg_score -48.88\n",
      "Environment 15: episode 26, score -50.0, avg_score -49.07\n",
      "Environment 0: episode 27, score -50.0, avg_score -49.07\n",
      "Environment 1: episode 27, score -49.0, avg_score -49.06\n",
      "Environment 2: episode 27, score -50.0, avg_score -49.06\n",
      "Environment 3: episode 27, score -50.0, avg_score -49.06\n",
      "Environment 4: episode 27, score -50.0, avg_score -49.41\n",
      "Environment 5: episode 27, score -50.0, avg_score -49.41\n",
      "Environment 6: episode 27, score -50.0, avg_score -49.41\n",
      "Environment 7: episode 27, score -50.0, avg_score -49.41\n",
      "Environment 8: episode 27, score -50.0, avg_score -49.41\n",
      "Environment 9: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 10: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 11: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 12: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 13: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 14: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 15: episode 27, score -50.0, avg_score -49.43\n",
      "Environment 0: episode 28, score -50.0, avg_score -49.43\n",
      "Environment 1: episode 28, score -47.0, avg_score -49.4\n",
      "Environment 2: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 3: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 4: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 5: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 6: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 7: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 8: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 9: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 10: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 11: episode 28, score -50.0, avg_score -49.4\n",
      "Environment 12: episode 28, score -47.0, avg_score -49.48\n",
      "Environment 13: episode 28, score -50.0, avg_score -49.53\n",
      "Environment 14: episode 28, score -50.0, avg_score -49.53\n",
      "Environment 15: episode 28, score -50.0, avg_score -49.53\n",
      "Environment 0: episode 29, score -50.0, avg_score -49.53\n",
      "Environment 1: episode 29, score -50.0, avg_score -49.53\n",
      "Environment 2: episode 29, score -50.0, avg_score -49.53\n",
      "Environment 3: episode 29, score -50.0, avg_score -49.53\n",
      "Environment 4: episode 29, score -49.0, avg_score -49.52\n",
      "Environment 5: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 6: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 7: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 8: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 9: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 10: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 11: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 12: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 13: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 14: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 15: episode 29, score -50.0, avg_score -49.52\n",
      "Environment 0: episode 30, score -50.0, avg_score -49.52\n",
      "Environment 1: episode 30, score -48.0, avg_score -49.5\n",
      "Environment 2: episode 30, score -50.0, avg_score -49.5\n",
      "Environment 3: episode 30, score -50.0, avg_score -49.5\n",
      "Environment 4: episode 30, score -50.0, avg_score -49.5\n",
      "Environment 5: episode 30, score -50.0, avg_score -49.5\n",
      "Environment 6: episode 30, score -41.0, avg_score -49.41\n",
      "Environment 7: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 8: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 9: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 10: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 11: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 12: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 13: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 14: episode 30, score -50.0, avg_score -49.41\n",
      "Environment 15: episode 30, score -50.0, avg_score -49.42\n",
      "Environment 0: episode 31, score -49.0, avg_score -49.41\n",
      "Environment 1: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 2: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 3: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 4: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 5: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 6: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 7: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 8: episode 31, score -50.0, avg_score -49.44\n",
      "Environment 9: episode 31, score -50.0, avg_score -49.48\n",
      "Environment 10: episode 31, score -50.0, avg_score -49.48\n",
      "Environment 11: episode 31, score -50.0, avg_score -49.48\n",
      "Environment 12: episode 31, score -50.0, avg_score -49.48\n",
      "Environment 13: episode 31, score -17.0, avg_score -49.15\n",
      "Environment 14: episode 31, score -21.0, avg_score -49.15\n",
      "Environment 15: episode 31, score -50.0, avg_score -49.15\n",
      "Environment 0: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 1: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 2: episode 32, score -50.0, avg_score -49.15\n",
      "Rendering episode 500.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_500.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_500.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 4: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 5: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 6: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 7: episode 32, score -50.0, avg_score -49.15\n",
      "Environment 8: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 9: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 10: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 11: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 12: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 13: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 14: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 15: episode 32, score -50.0, avg_score -49.18\n",
      "Environment 0: episode 33, score -50.0, avg_score -49.18\n",
      "Environment 1: episode 33, score -50.0, avg_score -49.18\n",
      "Environment 2: episode 33, score -50.0, avg_score -49.18\n",
      "Environment 3: episode 33, score -43.0, avg_score -49.11\n",
      "Environment 4: episode 33, score -35.0, avg_score -48.96\n",
      "Environment 5: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 6: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 7: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 8: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 9: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 10: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 11: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 12: episode 33, score -50.0, avg_score -48.97\n",
      "Environment 13: episode 33, score -26.0, avg_score -48.73\n",
      "Environment 14: episode 33, score -50.0, avg_score -48.73\n",
      "Environment 15: episode 33, score -50.0, avg_score -48.73\n",
      "Environment 0: episode 34, score -50.0, avg_score -48.73\n",
      "Environment 1: episode 34, score -50.0, avg_score -48.73\n",
      "Environment 2: episode 34, score -50.0, avg_score -48.73\n",
      "Environment 3: episode 34, score -50.0, avg_score -48.73\n",
      "Environment 4: episode 34, score -50.0, avg_score -48.73\n",
      "Environment 5: episode 34, score -46.0, avg_score -48.72\n",
      "Environment 6: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 7: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 8: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 9: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 10: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 11: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 12: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 13: episode 34, score -50.0, avg_score -48.72\n",
      "Environment 14: episode 34, score -48.0, avg_score -48.7\n",
      "Environment 15: episode 34, score -50.0, avg_score -48.7\n",
      "Environment 0: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 1: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 2: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 3: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 4: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 5: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 6: episode 35, score -50.0, avg_score -48.73\n",
      "Environment 7: episode 35, score -33.0, avg_score -48.56\n",
      "Environment 8: episode 35, score -50.0, avg_score -48.57\n",
      "Environment 9: episode 35, score -43.0, avg_score -48.5\n",
      "Environment 10: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 11: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 12: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 13: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 14: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 15: episode 35, score -50.0, avg_score -48.5\n",
      "Environment 0: episode 36, score -50.0, avg_score -48.5\n",
      "Environment 1: episode 36, score -50.0, avg_score -48.5\n",
      "Environment 2: episode 36, score -50.0, avg_score -48.5\n",
      "Environment 3: episode 36, score -50.0, avg_score -48.5\n",
      "Environment 4: episode 36, score -50.0, avg_score -48.5\n",
      "Environment 5: episode 36, score -50.0, avg_score -48.52\n",
      "Environment 6: episode 36, score -50.0, avg_score -48.52\n",
      "Environment 7: episode 36, score -50.0, avg_score -48.52\n",
      "Environment 8: episode 36, score -50.0, avg_score -48.52\n",
      "Environment 9: episode 36, score -50.0, avg_score -48.52\n",
      "Environment 10: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 11: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 12: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 13: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 14: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 15: episode 36, score -50.0, avg_score -48.61\n",
      "Environment 0: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 1: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 2: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 3: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 4: episode 37, score -49.0, avg_score -48.61\n",
      "Environment 5: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 6: episode 37, score -50.0, avg_score -48.61\n",
      "Environment 7: episode 37, score -35.0, avg_score -48.46\n",
      "Environment 8: episode 37, score -50.0, avg_score -48.46\n",
      "Environment 9: episode 37, score -50.0, avg_score -48.46\n",
      "Environment 10: episode 37, score -46.0, avg_score -48.42\n",
      "Environment 11: episode 37, score -26.0, avg_score -48.18\n",
      "Environment 12: episode 37, score -50.0, avg_score -48.18\n",
      "Environment 13: episode 37, score -50.0, avg_score -48.18\n",
      "Environment 14: episode 37, score -50.0, avg_score -48.18\n",
      "Environment 15: episode 37, score -25.0, avg_score -47.93\n",
      "Environment 0: episode 38, score -50.0, avg_score -47.93\n",
      "Environment 1: episode 38, score -50.0, avg_score -48.26\n",
      "Environment 2: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 3: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 4: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 5: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 6: episode 38, score -50.0, avg_score -48.55\n",
      "Rendering episode 600.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_600.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_600.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 8: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 9: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 10: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 11: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 12: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 13: episode 38, score -50.0, avg_score -48.55\n",
      "Environment 14: episode 38, score -49.0, avg_score -48.54\n",
      "Environment 15: episode 38, score -50.0, avg_score -48.54\n",
      "Environment 0: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 1: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 2: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 3: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 4: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 5: episode 39, score -50.0, avg_score -48.54\n",
      "Environment 6: episode 39, score -31.0, avg_score -48.35\n",
      "Environment 7: episode 39, score -50.0, avg_score -48.42\n",
      "Environment 8: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 9: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 10: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 11: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 12: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 13: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 14: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 15: episode 39, score -50.0, avg_score -48.57\n",
      "Environment 0: episode 40, score -46.0, avg_score -48.53\n",
      "Environment 1: episode 40, score -50.0, avg_score -48.77\n",
      "Environment 2: episode 40, score -50.0, avg_score -48.77\n",
      "Environment 3: episode 40, score -50.0, avg_score -48.77\n",
      "Environment 4: episode 40, score -33.0, avg_score -48.6\n",
      "Environment 5: episode 40, score -50.0, avg_score -48.6\n",
      "Environment 6: episode 40, score -50.0, avg_score -48.6\n",
      "Environment 7: episode 40, score -49.0, avg_score -48.59\n",
      "Environment 8: episode 40, score -50.0, avg_score -48.59\n",
      "Environment 9: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 10: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 11: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 12: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 13: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 14: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 15: episode 40, score -50.0, avg_score -48.63\n",
      "Environment 0: episode 41, score -50.0, avg_score -48.63\n",
      "Environment 1: episode 41, score -50.0, avg_score -48.63\n",
      "Environment 2: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 3: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 4: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 5: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 6: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 7: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 8: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 9: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 10: episode 41, score -50.0, avg_score -48.65\n",
      "Environment 11: episode 41, score -50.0, avg_score -48.82\n",
      "Environment 12: episode 41, score -50.0, avg_score -48.82\n",
      "Environment 13: episode 41, score -50.0, avg_score -48.89\n",
      "Environment 14: episode 41, score -50.0, avg_score -48.89\n",
      "Environment 15: episode 41, score -50.0, avg_score -48.89\n",
      "Environment 0: episode 42, score -50.0, avg_score -48.89\n",
      "Environment 1: episode 42, score -50.0, avg_score -48.89\n",
      "Environment 2: episode 42, score -50.0, avg_score -48.89\n",
      "Environment 3: episode 42, score -50.0, avg_score -48.89\n",
      "Environment 4: episode 42, score -50.0, avg_score -48.89\n",
      "Environment 5: episode 42, score -48.0, avg_score -48.87\n",
      "Environment 6: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 7: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 8: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 9: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 10: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 11: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 12: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 13: episode 42, score -50.0, avg_score -48.87\n",
      "Environment 14: episode 42, score -45.0, avg_score -48.82\n",
      "Environment 15: episode 42, score -50.0, avg_score -48.82\n",
      "Environment 0: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 1: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 2: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 3: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 4: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 5: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 6: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 7: episode 43, score -50.0, avg_score -48.82\n",
      "Environment 8: episode 43, score -50.0, avg_score -48.83\n",
      "Environment 9: episode 43, score -50.0, avg_score -48.83\n",
      "Environment 10: episode 43, score -50.0, avg_score -48.83\n",
      "Environment 11: episode 43, score -50.0, avg_score -48.98\n",
      "Environment 12: episode 43, score -50.0, avg_score -48.98\n",
      "Environment 13: episode 43, score -27.0, avg_score -48.75\n",
      "Environment 14: episode 43, score -50.0, avg_score -48.79\n",
      "Environment 15: episode 43, score -50.0, avg_score -49.03\n",
      "Environment 0: episode 44, score -50.0, avg_score -49.03\n",
      "Environment 1: episode 44, score -50.0, avg_score -49.03\n",
      "Environment 2: episode 44, score -50.0, avg_score -49.03\n",
      "Environment 3: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 4: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 5: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 6: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 7: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 8: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 9: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 10: episode 44, score -50.0, avg_score -49.28\n",
      "Rendering episode 700.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_700.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_700.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 12: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 13: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 14: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 15: episode 44, score -50.0, avg_score -49.28\n",
      "Environment 0: episode 45, score -50.0, avg_score -49.28\n",
      "Environment 1: episode 45, score -50.0, avg_score -49.28\n",
      "Environment 2: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 3: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 4: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 5: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 6: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 7: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 8: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 9: episode 45, score -50.0, avg_score -49.29\n",
      "Environment 10: episode 45, score -50.0, avg_score -49.48\n",
      "Environment 11: episode 45, score -50.0, avg_score -49.48\n",
      "Environment 12: episode 45, score -50.0, avg_score -49.48\n",
      "Environment 13: episode 45, score -50.0, avg_score -49.48\n",
      "Environment 14: episode 45, score -38.0, avg_score -49.36\n",
      "Environment 15: episode 45, score -50.0, avg_score -49.36\n",
      "Environment 0: episode 46, score -50.0, avg_score -49.36\n",
      "Environment 1: episode 46, score -50.0, avg_score -49.36\n",
      "Environment 2: episode 46, score -50.0, avg_score -49.36\n",
      "Environment 3: episode 46, score -50.0, avg_score -49.36\n",
      "Environment 4: episode 46, score -50.0, avg_score -49.4\n",
      "Environment 5: episode 46, score -25.0, avg_score -49.15\n",
      "Environment 6: episode 46, score -50.0, avg_score -49.15\n",
      "Environment 7: episode 46, score -50.0, avg_score -49.15\n",
      "Environment 8: episode 46, score -50.0, avg_score -49.32\n",
      "Environment 9: episode 46, score -50.0, avg_score -49.32\n",
      "Environment 10: episode 46, score -50.0, avg_score -49.32\n",
      "Environment 11: episode 46, score -50.0, avg_score -49.33\n",
      "Environment 12: episode 46, score -50.0, avg_score -49.33\n",
      "Environment 13: episode 46, score -50.0, avg_score -49.33\n",
      "Environment 14: episode 46, score -50.0, avg_score -49.33\n",
      "Environment 15: episode 46, score -50.0, avg_score -49.33\n",
      "Environment 0: episode 47, score -50.0, avg_score -49.33\n",
      "Environment 1: episode 47, score -50.0, avg_score -49.33\n",
      "Environment 2: episode 47, score -50.0, avg_score -49.33\n",
      "Environment 3: episode 47, score -50.0, avg_score -49.33\n",
      "Environment 4: episode 47, score -50.0, avg_score -49.33\n",
      "Environment 5: episode 47, score -39.0, avg_score -49.22\n",
      "Environment 6: episode 47, score -50.0, avg_score -49.22\n",
      "Environment 7: episode 47, score -50.0, avg_score -49.22\n",
      "Environment 8: episode 47, score -50.0, avg_score -49.22\n",
      "Environment 9: episode 47, score -50.0, avg_score -49.22\n",
      "Environment 10: episode 47, score -50.0, avg_score -49.22\n",
      "Environment 11: episode 47, score -46.0, avg_score -49.18\n",
      "Environment 12: episode 47, score -50.0, avg_score -49.18\n",
      "Environment 13: episode 47, score -50.0, avg_score -49.18\n",
      "Environment 14: episode 47, score -50.0, avg_score -49.18\n",
      "Environment 15: episode 47, score -39.0, avg_score -49.07\n",
      "Environment 0: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 1: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 2: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 3: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 4: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 5: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 6: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 7: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 8: episode 48, score -50.0, avg_score -49.07\n",
      "Environment 9: episode 48, score -50.0, avg_score -49.09\n",
      "Environment 10: episode 48, score -50.0, avg_score -49.09\n",
      "Environment 11: episode 48, score -50.0, avg_score -49.09\n",
      "Environment 12: episode 48, score -50.0, avg_score -49.09\n",
      "Environment 13: episode 48, score -48.0, avg_score -49.07\n",
      "Environment 14: episode 48, score -44.0, avg_score -49.01\n",
      "Environment 15: episode 48, score -50.0, avg_score -49.01\n",
      "Environment 0: episode 49, score -50.0, avg_score -49.01\n",
      "Environment 1: episode 49, score -50.0, avg_score -49.01\n",
      "Environment 2: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 3: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 4: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 5: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 6: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 7: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 8: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 9: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 10: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 11: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 12: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 13: episode 49, score -50.0, avg_score -49.06\n",
      "Environment 14: episode 49, score -48.0, avg_score -49.04\n",
      "Environment 15: episode 49, score -50.0, avg_score -49.04\n",
      "Environment 0: episode 50, score -50.0, avg_score -49.04\n",
      "Environment 1: episode 50, score -50.0, avg_score -49.27\n",
      "Environment 2: episode 50, score -47.0, avg_score -49.24\n",
      "Environment 3: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 4: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 5: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 6: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 7: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 8: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 9: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 10: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 11: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 12: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 13: episode 50, score -50.0, avg_score -49.24\n",
      "Environment 14: episode 50, score -50.0, avg_score -49.24\n",
      "Rendering episode 800.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_800.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_800.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 50, score -50.0, avg_score -49.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 1: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 2: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 3: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 4: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 5: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 6: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 7: episode 51, score -50.0, avg_score -49.24\n",
      "Environment 8: episode 51, score -42.0, avg_score -49.16\n",
      "Environment 9: episode 51, score -50.0, avg_score -49.16\n",
      "Environment 10: episode 51, score -50.0, avg_score -49.16\n",
      "Environment 11: episode 51, score -50.0, avg_score -49.16\n",
      "Environment 12: episode 51, score -33.0, avg_score -48.99\n",
      "Environment 13: episode 51, score -50.0, avg_score -48.99\n",
      "Environment 14: episode 51, score -50.0, avg_score -48.99\n",
      "Environment 15: episode 51, score -50.0, avg_score -48.99\n",
      "Environment 0: episode 52, score -50.0, avg_score -48.99\n",
      "Environment 1: episode 52, score -50.0, avg_score -48.99\n",
      "Environment 2: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 3: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 4: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 5: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 6: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 7: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 8: episode 52, score -50.0, avg_score -49.11\n",
      "Environment 9: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 10: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 11: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 12: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 13: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 14: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 15: episode 52, score -50.0, avg_score -49.36\n",
      "Environment 0: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 1: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 2: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 3: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 4: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 5: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 6: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 7: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 8: episode 53, score -50.0, avg_score -49.36\n",
      "Environment 9: episode 53, score -50.0, avg_score -49.47\n",
      "Environment 10: episode 53, score -50.0, avg_score -49.47\n",
      "Environment 11: episode 53, score -50.0, avg_score -49.47\n",
      "Environment 12: episode 53, score -50.0, avg_score -49.47\n",
      "Environment 13: episode 53, score -50.0, avg_score -49.47\n",
      "Environment 14: episode 53, score -40.0, avg_score -49.37\n",
      "Environment 15: episode 53, score -50.0, avg_score -49.41\n",
      "Environment 0: episode 54, score -50.0, avg_score -49.41\n",
      "Environment 1: episode 54, score -50.0, avg_score -49.41\n",
      "Environment 2: episode 54, score -50.0, avg_score -49.41\n",
      "Environment 3: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 4: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 5: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 6: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 7: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 8: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 9: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 10: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 11: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 12: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 13: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 14: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 15: episode 54, score -50.0, avg_score -49.52\n",
      "Environment 0: episode 55, score -50.0, avg_score -49.52\n",
      "Environment 1: episode 55, score -50.0, avg_score -49.54\n",
      "Environment 2: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 3: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 4: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 5: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 6: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 7: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 8: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 9: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 10: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 11: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 12: episode 55, score -50.0, avg_score -49.6\n",
      "Environment 13: episode 55, score -45.0, avg_score -49.55\n",
      "Environment 14: episode 55, score -50.0, avg_score -49.55\n",
      "Environment 15: episode 55, score -50.0, avg_score -49.55\n",
      "Environment 0: episode 56, score -50.0, avg_score -49.55\n",
      "Environment 1: episode 56, score -50.0, avg_score -49.55\n",
      "Environment 2: episode 56, score -50.0, avg_score -49.57\n",
      "Environment 3: episode 56, score -44.0, avg_score -49.51\n",
      "Environment 4: episode 56, score -50.0, avg_score -49.51\n",
      "Environment 5: episode 56, score -50.0, avg_score -49.51\n",
      "Environment 6: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 7: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 8: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 9: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 10: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 11: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 12: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 13: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 14: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 15: episode 56, score -50.0, avg_score -49.54\n",
      "Environment 0: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 1: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 2: episode 57, score -50.0, avg_score -49.54\n",
      "Rendering episode 900.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_900.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_900.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 4: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 5: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 6: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 7: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 8: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 9: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 10: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 11: episode 57, score -50.0, avg_score -49.54\n",
      "Environment 12: episode 57, score -50.0, avg_score -49.62\n",
      "Environment 13: episode 57, score -34.0, avg_score -49.46\n",
      "Environment 14: episode 57, score -50.0, avg_score -49.46\n",
      "Environment 15: episode 57, score -50.0, avg_score -49.46\n",
      "Environment 0: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 1: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 2: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 3: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 4: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 5: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 6: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 7: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 8: episode 58, score -50.0, avg_score -49.63\n",
      "Environment 9: episode 58, score -22.0, avg_score -49.35\n",
      "Environment 10: episode 58, score -50.0, avg_score -49.35\n",
      "Environment 11: episode 58, score -47.0, avg_score -49.32\n",
      "Environment 12: episode 58, score -50.0, avg_score -49.32\n",
      "Environment 13: episode 58, score -50.0, avg_score -49.32\n",
      "Environment 14: episode 58, score -50.0, avg_score -49.32\n",
      "Environment 15: episode 58, score -50.0, avg_score -49.32\n",
      "Environment 0: episode 59, score -50.0, avg_score -49.32\n",
      "Environment 1: episode 59, score -50.0, avg_score -49.32\n",
      "Environment 2: episode 59, score -50.0, avg_score -49.32\n",
      "Environment 3: episode 59, score -39.0, avg_score -49.21\n",
      "Environment 4: episode 59, score -50.0, avg_score -49.21\n",
      "Environment 5: episode 59, score -50.0, avg_score -49.21\n",
      "Environment 6: episode 59, score -50.0, avg_score -49.21\n",
      "Environment 7: episode 59, score -50.0, avg_score -49.21\n",
      "Environment 8: episode 59, score -50.0, avg_score -49.21\n",
      "Environment 9: episode 59, score -45.0, avg_score -49.16\n",
      "Environment 10: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 11: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 12: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 13: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 14: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 15: episode 59, score -50.0, avg_score -49.16\n",
      "Environment 0: episode 60, score -50.0, avg_score -49.16\n",
      "Environment 1: episode 60, score -50.0, avg_score -49.16\n",
      "Environment 2: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 3: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 4: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 5: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 6: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 7: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 8: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 9: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 10: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 11: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 12: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 13: episode 60, score -50.0, avg_score -49.26\n",
      "Environment 14: episode 60, score -47.0, avg_score -49.23\n",
      "Environment 15: episode 60, score -48.0, avg_score -49.21\n",
      "Environment 0: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 1: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 2: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 3: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 4: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 5: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 6: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 7: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 8: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 9: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 10: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 11: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 12: episode 61, score -50.0, avg_score -49.21\n",
      "Environment 13: episode 61, score -39.0, avg_score -49.1\n",
      "Environment 14: episode 61, score -50.0, avg_score -49.1\n",
      "Environment 15: episode 61, score -50.0, avg_score -49.1\n",
      "Environment 0: episode 62, score -50.0, avg_score -49.1\n",
      "Environment 1: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 2: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 3: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 4: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 5: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 6: episode 62, score -50.0, avg_score -49.15\n",
      "Environment 7: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 8: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 9: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 10: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 11: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 12: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 13: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 14: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 15: episode 62, score -50.0, avg_score -49.21\n",
      "Environment 0: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 1: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 2: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 3: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 4: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 5: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 6: episode 63, score -50.0, avg_score -49.21\n",
      "Rendering episode 1000.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1000.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1000.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 8: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 9: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 10: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 11: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 12: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 13: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 14: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 15: episode 63, score -50.0, avg_score -49.21\n",
      "Environment 0: episode 64, score -50.0, avg_score -49.21\n",
      "Environment 1: episode 64, score -50.0, avg_score -49.37\n",
      "Environment 2: episode 64, score -38.0, avg_score -49.25\n",
      "Environment 3: episode 64, score -50.0, avg_score -49.25\n",
      "Environment 4: episode 64, score -49.0, avg_score -49.24\n",
      "Environment 5: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 6: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 7: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 8: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 9: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 10: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 11: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 12: episode 64, score -50.0, avg_score -49.24\n",
      "Environment 13: episode 64, score -50.0, avg_score -49.52\n",
      "Environment 14: episode 64, score -50.0, avg_score -49.52\n",
      "Environment 15: episode 64, score -50.0, avg_score -49.55\n",
      "Environment 0: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 1: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 2: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 3: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 4: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 5: episode 65, score -50.0, avg_score -49.55\n",
      "Environment 6: episode 65, score -49.0, avg_score -49.54\n",
      "Environment 7: episode 65, score -50.0, avg_score -49.65\n",
      "Environment 8: episode 65, score -42.0, avg_score -49.57\n",
      "Environment 9: episode 65, score -46.0, avg_score -49.53\n",
      "Environment 10: episode 65, score -50.0, avg_score -49.53\n",
      "Environment 11: episode 65, score -32.0, avg_score -49.35\n",
      "Environment 12: episode 65, score -50.0, avg_score -49.35\n",
      "Environment 13: episode 65, score -50.0, avg_score -49.4\n",
      "Environment 14: episode 65, score -50.0, avg_score -49.4\n",
      "Environment 15: episode 65, score -50.0, avg_score -49.4\n",
      "Environment 0: episode 66, score -50.0, avg_score -49.4\n",
      "Environment 1: episode 66, score -50.0, avg_score -49.4\n",
      "Environment 2: episode 66, score -50.0, avg_score -49.4\n",
      "Environment 3: episode 66, score -50.0, avg_score -49.4\n",
      "Environment 4: episode 66, score -50.0, avg_score -49.4\n",
      "Environment 5: episode 66, score -39.0, avg_score -49.29\n",
      "Environment 6: episode 66, score -50.0, avg_score -49.29\n",
      "Environment 7: episode 66, score -50.0, avg_score -49.29\n",
      "Environment 8: episode 66, score -50.0, avg_score -49.29\n",
      "Environment 9: episode 66, score -50.0, avg_score -49.29\n",
      "Environment 10: episode 66, score -49.0, avg_score -49.28\n",
      "Environment 11: episode 66, score -50.0, avg_score -49.28\n",
      "Environment 12: episode 66, score -50.0, avg_score -49.28\n",
      "Environment 13: episode 66, score -50.0, avg_score -49.28\n",
      "Environment 14: episode 66, score -50.0, avg_score -49.28\n",
      "Environment 15: episode 66, score -43.0, avg_score -49.21\n",
      "Environment 0: episode 67, score -50.0, avg_score -49.21\n",
      "Environment 1: episode 67, score -50.0, avg_score -49.21\n",
      "Environment 2: episode 67, score -50.0, avg_score -49.24\n",
      "Environment 3: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 4: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 5: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 6: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 7: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 8: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 9: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 10: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 11: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 12: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 13: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 14: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 15: episode 67, score -50.0, avg_score -49.26\n",
      "Environment 0: episode 68, score -50.0, avg_score -49.26\n",
      "Environment 1: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 2: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 3: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 4: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 5: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 6: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 7: episode 68, score -50.0, avg_score -49.37\n",
      "Environment 8: episode 68, score -36.0, avg_score -49.23\n",
      "Environment 9: episode 68, score -43.0, avg_score -49.16\n",
      "Environment 10: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 11: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 12: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 13: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 14: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 15: episode 68, score -50.0, avg_score -49.16\n",
      "Environment 0: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 1: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 2: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 3: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 4: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 5: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 6: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 7: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 8: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 9: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 10: episode 69, score -50.0, avg_score -49.16\n",
      "Rendering episode 1100.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1100.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1100.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 12: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 13: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 14: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 15: episode 69, score -50.0, avg_score -49.16\n",
      "Environment 0: episode 70, score -50.0, avg_score -49.16\n",
      "Environment 1: episode 70, score -28.0, avg_score -48.94\n",
      "Environment 2: episode 70, score -50.0, avg_score -48.94\n",
      "Environment 3: episode 70, score -50.0, avg_score -48.94\n",
      "Environment 4: episode 70, score -12.0, avg_score -48.56\n",
      "Environment 5: episode 70, score -50.0, avg_score -48.56\n",
      "Environment 6: episode 70, score -50.0, avg_score -48.68\n",
      "Environment 7: episode 70, score -50.0, avg_score -48.68\n",
      "Environment 8: episode 70, score -50.0, avg_score -48.69\n",
      "Environment 9: episode 70, score -50.0, avg_score -48.69\n",
      "Environment 10: episode 70, score -46.0, avg_score -48.65\n",
      "Environment 11: episode 70, score -50.0, avg_score -48.65\n",
      "Environment 12: episode 70, score -50.0, avg_score -48.65\n",
      "Environment 13: episode 70, score -49.0, avg_score -48.64\n",
      "Environment 14: episode 70, score -50.0, avg_score -48.64\n",
      "Environment 15: episode 70, score -50.0, avg_score -48.64\n",
      "Environment 0: episode 71, score -50.0, avg_score -48.64\n",
      "Environment 1: episode 71, score -50.0, avg_score -48.64\n",
      "Environment 2: episode 71, score -50.0, avg_score -48.64\n",
      "Environment 3: episode 71, score -50.0, avg_score -48.64\n",
      "Environment 4: episode 71, score -50.0, avg_score -48.64\n",
      "Environment 5: episode 71, score -43.0, avg_score -48.57\n",
      "Environment 6: episode 71, score -30.0, avg_score -48.37\n",
      "Environment 7: episode 71, score -50.0, avg_score -48.37\n",
      "Environment 8: episode 71, score -47.0, avg_score -48.34\n",
      "Environment 9: episode 71, score -50.0, avg_score -48.34\n",
      "Environment 10: episode 71, score -50.0, avg_score -48.35\n",
      "Environment 11: episode 71, score -50.0, avg_score -48.35\n",
      "Environment 12: episode 71, score -50.0, avg_score -48.43\n",
      "Environment 13: episode 71, score -50.0, avg_score -48.47\n",
      "Environment 14: episode 71, score -50.0, avg_score -48.47\n",
      "Environment 15: episode 71, score -50.0, avg_score -48.65\n",
      "Environment 0: episode 72, score -50.0, avg_score -48.65\n",
      "Environment 1: episode 72, score -50.0, avg_score -48.65\n",
      "Environment 2: episode 72, score -50.0, avg_score -48.65\n",
      "Environment 3: episode 72, score -50.0, avg_score -48.65\n",
      "Environment 4: episode 72, score -50.0, avg_score -48.65\n",
      "Environment 5: episode 72, score -7.0, avg_score -48.22\n",
      "Environment 6: episode 72, score -50.0, avg_score -48.22\n",
      "Environment 7: episode 72, score -50.0, avg_score -48.22\n",
      "Environment 8: episode 72, score -50.0, avg_score -48.22\n",
      "Environment 9: episode 72, score -45.0, avg_score -48.28\n",
      "Environment 10: episode 72, score -50.0, avg_score -48.28\n",
      "Environment 11: episode 72, score -50.0, avg_score -48.28\n",
      "Environment 12: episode 72, score -50.0, avg_score -48.28\n",
      "Environment 13: episode 72, score -34.0, avg_score -48.12\n",
      "Environment 14: episode 72, score -50.0, avg_score -48.13\n",
      "Environment 15: episode 72, score -50.0, avg_score -48.13\n",
      "Environment 0: episode 73, score -50.0, avg_score -48.13\n",
      "Environment 1: episode 73, score -50.0, avg_score -48.13\n",
      "Environment 2: episode 73, score -50.0, avg_score -48.13\n",
      "Environment 3: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 4: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 5: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 6: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 7: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 8: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 9: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 10: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 11: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 12: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 13: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 14: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 15: episode 73, score -50.0, avg_score -48.2\n",
      "Environment 0: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 1: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 2: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 3: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 4: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 5: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 6: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 7: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 8: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 9: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 10: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 11: episode 74, score -50.0, avg_score -48.2\n",
      "Environment 12: episode 74, score -50.0, avg_score -48.34\n",
      "Environment 13: episode 74, score -50.0, avg_score -48.41\n",
      "Environment 14: episode 74, score -50.0, avg_score -48.41\n",
      "Environment 15: episode 74, score -41.0, avg_score -48.32\n",
      "Environment 0: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 1: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 2: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 3: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 4: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 5: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 6: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 7: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 8: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 9: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 10: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 11: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 12: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 13: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 14: episode 75, score -50.0, avg_score -48.32\n",
      "Rendering episode 1200.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1200.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1200.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 75, score -50.0, avg_score -48.32\n",
      "Environment 0: episode 76, score -50.0, avg_score -48.32\n",
      "Environment 1: episode 76, score -50.0, avg_score -48.32\n",
      "Environment 2: episode 76, score -50.0, avg_score -48.32\n",
      "Environment 3: episode 76, score -50.0, avg_score -48.32\n",
      "Environment 4: episode 76, score -50.0, avg_score -48.32\n",
      "Environment 5: episode 76, score -50.0, avg_score -48.54\n",
      "Environment 6: episode 76, score -50.0, avg_score -48.54\n",
      "Environment 7: episode 76, score -50.0, avg_score -48.54\n",
      "Environment 8: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 9: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 10: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 11: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 12: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 13: episode 76, score -50.0, avg_score -48.92\n",
      "Environment 14: episode 76, score -50.0, avg_score -48.96\n",
      "Environment 15: episode 76, score -50.0, avg_score -48.96\n",
      "Environment 0: episode 77, score -50.0, avg_score -48.96\n",
      "Environment 1: episode 77, score -50.0, avg_score -48.97\n",
      "Environment 2: episode 77, score -50.0, avg_score -48.97\n",
      "Environment 3: episode 77, score -50.0, avg_score -48.97\n",
      "Environment 4: episode 77, score -50.0, avg_score -48.97\n",
      "Environment 5: episode 77, score -41.0, avg_score -48.88\n",
      "Environment 6: episode 77, score -50.0, avg_score -48.88\n",
      "Environment 7: episode 77, score -50.0, avg_score -48.88\n",
      "Environment 8: episode 77, score -48.0, avg_score -48.86\n",
      "Environment 9: episode 77, score -49.0, avg_score -48.92\n",
      "Environment 10: episode 77, score -50.0, avg_score -49.12\n",
      "Environment 11: episode 77, score -50.0, avg_score -49.12\n",
      "Environment 12: episode 77, score -50.0, avg_score -49.15\n",
      "Environment 13: episode 77, score -50.0, avg_score -49.15\n",
      "Environment 14: episode 77, score -50.0, avg_score -49.15\n",
      "Environment 15: episode 77, score -50.0, avg_score -49.15\n",
      "Environment 0: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 1: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 2: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 3: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 4: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 5: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 6: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 7: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 8: episode 78, score -50.0, avg_score -49.15\n",
      "Environment 9: episode 78, score -50.0, avg_score -49.58\n",
      "Environment 10: episode 78, score -45.0, avg_score -49.53\n",
      "Environment 11: episode 78, score -49.0, avg_score -49.52\n",
      "Environment 12: episode 78, score -50.0, avg_score -49.52\n",
      "Environment 13: episode 78, score -50.0, avg_score -49.57\n",
      "Environment 14: episode 78, score -50.0, avg_score -49.57\n",
      "Environment 15: episode 78, score -50.0, avg_score -49.57\n",
      "Environment 0: episode 79, score -50.0, avg_score -49.57\n",
      "Environment 1: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 2: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 3: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 4: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 5: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 6: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 7: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 8: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 9: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 10: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 11: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 12: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 13: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 14: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 15: episode 79, score -50.0, avg_score -49.73\n",
      "Environment 0: episode 80, score -50.0, avg_score -49.73\n",
      "Environment 1: episode 80, score -50.0, avg_score -49.73\n",
      "Environment 2: episode 80, score -50.0, avg_score -49.73\n",
      "Environment 3: episode 80, score -49.0, avg_score -49.72\n",
      "Environment 4: episode 80, score -50.0, avg_score -49.72\n",
      "Environment 5: episode 80, score -50.0, avg_score -49.72\n",
      "Environment 6: episode 80, score -39.0, avg_score -49.61\n",
      "Environment 7: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 8: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 9: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 10: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 11: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 12: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 13: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 14: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 15: episode 80, score -50.0, avg_score -49.61\n",
      "Environment 0: episode 81, score -50.0, avg_score -49.61\n",
      "Environment 1: episode 81, score -50.0, avg_score -49.61\n",
      "Environment 2: episode 81, score -50.0, avg_score -49.61\n",
      "Environment 3: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 4: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 5: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 6: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 7: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 8: episode 81, score -50.0, avg_score -49.7\n",
      "Environment 9: episode 81, score -44.0, avg_score -49.64\n",
      "Environment 10: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 11: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 12: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 13: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 14: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 15: episode 81, score -50.0, avg_score -49.64\n",
      "Environment 0: episode 82, score -50.0, avg_score -49.64\n",
      "Environment 1: episode 82, score -50.0, avg_score -49.64\n",
      "Environment 2: episode 82, score -50.0, avg_score -49.64\n",
      "Rendering episode 1300.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1300.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1300.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 82, score -40.0, avg_score -49.54\n",
      "Environment 4: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 5: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 6: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 7: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 8: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 9: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 10: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 11: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 12: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 13: episode 82, score -50.0, avg_score -49.54\n",
      "Environment 14: episode 82, score -40.0, avg_score -49.44\n",
      "Environment 15: episode 82, score -50.0, avg_score -49.44\n",
      "Environment 0: episode 83, score -19.0, avg_score -49.13\n",
      "Environment 1: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 2: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 3: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 4: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 5: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 6: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 7: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 8: episode 83, score -50.0, avg_score -49.13\n",
      "Environment 9: episode 83, score -50.0, avg_score -49.22\n",
      "Environment 10: episode 83, score -47.0, avg_score -49.19\n",
      "Environment 11: episode 83, score -50.0, avg_score -49.19\n",
      "Environment 12: episode 83, score -50.0, avg_score -49.21\n",
      "Environment 13: episode 83, score -43.0, avg_score -49.15\n",
      "Environment 14: episode 83, score -50.0, avg_score -49.15\n",
      "Environment 15: episode 83, score -50.0, avg_score -49.15\n",
      "Environment 0: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 1: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 2: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 3: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 4: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 5: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 6: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 7: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 8: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 9: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 10: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 11: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 12: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 13: episode 84, score -50.0, avg_score -49.15\n",
      "Environment 14: episode 84, score -50.0, avg_score -49.2\n",
      "Environment 15: episode 84, score -50.0, avg_score -49.21\n",
      "Environment 0: episode 85, score -39.0, avg_score -49.1\n",
      "Environment 1: episode 85, score -48.0, avg_score -49.08\n",
      "Environment 2: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 3: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 4: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 5: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 6: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 7: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 8: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 9: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 10: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 11: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 12: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 13: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 14: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 15: episode 85, score -50.0, avg_score -49.08\n",
      "Environment 0: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 1: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 2: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 3: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 4: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 5: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 6: episode 86, score -50.0, avg_score -49.08\n",
      "Environment 7: episode 86, score -50.0, avg_score -49.09\n",
      "Environment 8: episode 86, score -50.0, avg_score -49.09\n",
      "Environment 9: episode 86, score -50.0, avg_score -49.09\n",
      "Environment 10: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 11: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 12: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 13: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 14: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 15: episode 86, score -50.0, avg_score -49.2\n",
      "Environment 0: episode 87, score -50.0, avg_score -49.2\n",
      "Environment 1: episode 87, score -49.0, avg_score -49.19\n",
      "Environment 2: episode 87, score -50.0, avg_score -49.19\n",
      "Environment 3: episode 87, score -47.0, avg_score -49.16\n",
      "Environment 4: episode 87, score -14.0, avg_score -48.8\n",
      "Environment 5: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 6: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 7: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 8: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 9: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 10: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 11: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 12: episode 87, score -50.0, avg_score -48.8\n",
      "Environment 13: episode 87, score -50.0, avg_score -48.86\n",
      "Environment 14: episode 87, score -50.0, avg_score -48.86\n",
      "Environment 15: episode 87, score -50.0, avg_score -48.86\n",
      "Environment 0: episode 88, score -50.0, avg_score -48.86\n",
      "Environment 1: episode 88, score -50.0, avg_score -48.86\n",
      "Environment 2: episode 88, score -50.0, avg_score -48.86\n",
      "Environment 3: episode 88, score -50.0, avg_score -48.86\n",
      "Environment 4: episode 88, score -22.0, avg_score -48.58\n",
      "Environment 5: episode 88, score -50.0, avg_score -48.58\n",
      "Environment 6: episode 88, score -50.0, avg_score -48.58\n",
      "Rendering episode 1400.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1400.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1400.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 88, score -50.0, avg_score -48.68\n",
      "Environment 8: episode 88, score -48.0, avg_score -48.66\n",
      "Environment 9: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 10: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 11: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 12: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 13: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 14: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 15: episode 88, score -50.0, avg_score -48.66\n",
      "Environment 0: episode 89, score -50.0, avg_score -48.66\n",
      "Environment 1: episode 89, score -50.0, avg_score -48.66\n",
      "Environment 2: episode 89, score -50.0, avg_score -48.76\n",
      "Environment 3: episode 89, score -50.0, avg_score -48.76\n",
      "Environment 4: episode 89, score -50.0, avg_score -49.07\n",
      "Environment 5: episode 89, score -50.0, avg_score -49.07\n",
      "Environment 6: episode 89, score -50.0, avg_score -49.07\n",
      "Environment 7: episode 89, score -50.0, avg_score -49.07\n",
      "Environment 8: episode 89, score -50.0, avg_score -49.07\n",
      "Environment 9: episode 89, score -49.0, avg_score -49.06\n",
      "Environment 10: episode 89, score -50.0, avg_score -49.06\n",
      "Environment 11: episode 89, score -50.0, avg_score -49.06\n",
      "Environment 12: episode 89, score -50.0, avg_score -49.06\n",
      "Environment 13: episode 89, score -50.0, avg_score -49.06\n",
      "Environment 14: episode 89, score -50.0, avg_score -49.09\n",
      "Environment 15: episode 89, score -50.0, avg_score -49.09\n",
      "Environment 0: episode 90, score -50.0, avg_score -49.09\n",
      "Environment 1: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 2: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 3: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 4: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 5: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 6: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 7: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 8: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 9: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 10: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 11: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 12: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 13: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 14: episode 90, score -50.0, avg_score -49.16\n",
      "Environment 15: episode 90, score -44.0, avg_score -49.1\n",
      "Environment 0: episode 91, score -50.0, avg_score -49.1\n",
      "Environment 1: episode 91, score -50.0, avg_score -49.1\n",
      "Environment 2: episode 91, score -50.0, avg_score -49.1\n",
      "Environment 3: episode 91, score -50.0, avg_score -49.1\n",
      "Environment 4: episode 91, score -50.0, avg_score -49.21\n",
      "Environment 5: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 6: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 7: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 8: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 9: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 10: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 11: episode 91, score -50.0, avg_score -49.23\n",
      "Environment 12: episode 91, score -44.0, avg_score -49.17\n",
      "Environment 13: episode 91, score -50.0, avg_score -49.17\n",
      "Environment 14: episode 91, score -50.0, avg_score -49.17\n",
      "Environment 15: episode 91, score -50.0, avg_score -49.17\n",
      "Environment 0: episode 92, score -49.0, avg_score -49.16\n",
      "Environment 1: episode 92, score -50.0, avg_score -49.16\n",
      "Environment 2: episode 92, score -46.0, avg_score -49.12\n",
      "Environment 3: episode 92, score -50.0, avg_score -49.12\n",
      "Environment 4: episode 92, score -47.0, avg_score -49.09\n",
      "Environment 5: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 6: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 7: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 8: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 9: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 10: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 11: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 12: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 13: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 14: episode 92, score -50.0, avg_score -49.09\n",
      "Environment 15: episode 92, score -31.0, avg_score -48.9\n",
      "Environment 0: episode 93, score -50.0, avg_score -48.9\n",
      "Environment 1: episode 93, score -50.0, avg_score -48.9\n",
      "Environment 2: episode 93, score -50.0, avg_score -48.9\n",
      "Environment 3: episode 93, score -50.0, avg_score -48.9\n",
      "Environment 4: episode 93, score -50.0, avg_score -48.9\n",
      "Environment 5: episode 93, score -50.0, avg_score -48.91\n",
      "Environment 6: episode 93, score -50.0, avg_score -48.91\n",
      "Environment 7: episode 93, score -50.0, avg_score -48.94\n",
      "Environment 8: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 9: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 10: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 11: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 12: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 13: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 14: episode 93, score -50.0, avg_score -49.3\n",
      "Environment 15: episode 93, score -30.0, avg_score -49.1\n",
      "Environment 0: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 1: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 2: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 3: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 4: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 5: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 6: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 7: episode 94, score -50.0, avg_score -49.1\n",
      "Environment 8: episode 94, score -50.0, avg_score -49.38\n",
      "Environment 9: episode 94, score -50.0, avg_score -49.38\n",
      "Environment 10: episode 94, score -50.0, avg_score -49.38\n",
      "Rendering episode 1500.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1500.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1500.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 94, score -45.0, avg_score -49.33\n",
      "Environment 12: episode 94, score -50.0, avg_score -49.35\n",
      "Environment 13: episode 94, score -50.0, avg_score -49.35\n",
      "Environment 14: episode 94, score -50.0, avg_score -49.35\n",
      "Environment 15: episode 94, score -50.0, avg_score -49.35\n",
      "Environment 0: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 1: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 2: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 3: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 4: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 5: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 6: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 7: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 8: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 9: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 10: episode 95, score -50.0, avg_score -49.35\n",
      "Environment 11: episode 95, score -43.0, avg_score -49.28\n",
      "Environment 12: episode 95, score -50.0, avg_score -49.28\n",
      "Environment 13: episode 95, score -50.0, avg_score -49.29\n",
      "Environment 14: episode 95, score -35.0, avg_score -49.14\n",
      "Environment 15: episode 95, score -50.0, avg_score -49.14\n",
      "Environment 0: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 1: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 2: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 3: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 4: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 5: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 6: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 7: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 8: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 9: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 10: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 11: episode 96, score -50.0, avg_score -49.14\n",
      "Environment 12: episode 96, score -45.0, avg_score -49.09\n",
      "Environment 13: episode 96, score -50.0, avg_score -49.09\n",
      "Environment 14: episode 96, score -50.0, avg_score -49.09\n",
      "Environment 15: episode 96, score -50.0, avg_score -49.09\n",
      "Environment 0: episode 97, score -50.0, avg_score -49.09\n",
      "Environment 1: episode 97, score -47.0, avg_score -49.06\n",
      "Environment 2: episode 97, score -50.0, avg_score -49.06\n",
      "Environment 3: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 4: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 5: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 6: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 7: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 8: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 9: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 10: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 11: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 12: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 13: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 14: episode 97, score -50.0, avg_score -49.12\n",
      "Environment 15: episode 97, score -35.0, avg_score -48.97\n",
      "Environment 0: episode 98, score -50.0, avg_score -49.03\n",
      "Environment 1: episode 98, score -50.0, avg_score -49.03\n",
      "Environment 2: episode 98, score -50.0, avg_score -49.03\n",
      "Environment 3: episode 98, score -50.0, avg_score -49.03\n",
      "Environment 4: episode 98, score -50.0, avg_score -49.04\n",
      "Environment 5: episode 98, score -50.0, avg_score -49.04\n",
      "Environment 6: episode 98, score -50.0, avg_score -49.08\n",
      "Environment 7: episode 98, score -50.0, avg_score -49.08\n",
      "Environment 8: episode 98, score -50.0, avg_score -49.11\n",
      "Environment 9: episode 98, score -50.0, avg_score -49.11\n",
      "Environment 10: episode 98, score -50.0, avg_score -49.11\n",
      "Environment 11: episode 98, score -50.0, avg_score -49.11\n",
      "Environment 12: episode 98, score -50.0, avg_score -49.11\n",
      "Environment 13: episode 98, score -43.0, avg_score -49.04\n",
      "Environment 14: episode 98, score -50.0, avg_score -49.04\n",
      "Environment 15: episode 98, score -50.0, avg_score -49.04\n",
      "Environment 0: episode 99, score -50.0, avg_score -49.04\n",
      "Environment 1: episode 99, score -50.0, avg_score -49.04\n",
      "Environment 2: episode 99, score -50.0, avg_score -49.04\n",
      "Environment 3: episode 99, score -50.0, avg_score -49.23\n",
      "Environment 4: episode 99, score -50.0, avg_score -49.23\n",
      "Environment 5: episode 99, score -50.0, avg_score -49.23\n",
      "Environment 6: episode 99, score -50.0, avg_score -49.23\n",
      "Environment 7: episode 99, score -39.0, avg_score -49.12\n",
      "Environment 8: episode 99, score -50.0, avg_score -49.12\n",
      "Environment 9: episode 99, score -50.0, avg_score -49.12\n",
      "Environment 10: episode 99, score -32.0, avg_score -48.94\n",
      "Environment 11: episode 99, score -50.0, avg_score -48.94\n",
      "Environment 12: episode 99, score -50.0, avg_score -48.94\n",
      "Environment 13: episode 99, score -50.0, avg_score -48.94\n",
      "Environment 14: episode 99, score -50.0, avg_score -48.94\n",
      "Environment 15: episode 99, score -50.0, avg_score -48.94\n",
      "Environment 0: episode 100, score -50.0, avg_score -48.94\n",
      "Environment 1: episode 100, score -50.0, avg_score -48.94\n",
      "Environment 2: episode 100, score -50.0, avg_score -48.94\n",
      "Environment 3: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 4: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 5: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 6: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 7: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 8: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 9: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 10: episode 100, score -50.0, avg_score -49.14\n",
      "Environment 11: episode 100, score -41.0, avg_score -49.05\n",
      "Environment 12: episode 100, score -50.0, avg_score -49.05\n",
      "Environment 13: episode 100, score -50.0, avg_score -49.05\n",
      "Environment 14: episode 100, score -50.0, avg_score -49.05\n",
      "Rendering episode 1600.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1600.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1600.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 100, score -50.0, avg_score -49.1\n",
      "Environment 0: episode 101, score -44.0, avg_score -49.04\n",
      "Environment 1: episode 101, score -50.0, avg_score -49.04\n",
      "Environment 2: episode 101, score -50.0, avg_score -49.04\n",
      "Environment 3: episode 101, score -50.0, avg_score -49.04\n",
      "Environment 4: episode 101, score -50.0, avg_score -49.04\n",
      "Environment 5: episode 101, score -32.0, avg_score -48.86\n",
      "Environment 6: episode 101, score -50.0, avg_score -48.86\n",
      "Environment 7: episode 101, score -50.0, avg_score -48.86\n",
      "Environment 8: episode 101, score -50.0, avg_score -48.86\n",
      "Environment 9: episode 101, score -45.0, avg_score -48.81\n",
      "Environment 10: episode 101, score -50.0, avg_score -48.81\n",
      "Environment 11: episode 101, score -50.0, avg_score -48.81\n",
      "Environment 12: episode 101, score -50.0, avg_score -48.81\n",
      "Environment 13: episode 101, score -50.0, avg_score -48.81\n",
      "Environment 14: episode 101, score -50.0, avg_score -48.81\n",
      "Environment 15: episode 101, score -50.0, avg_score -48.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 5099. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0: episode 102, score -50.0, avg_score -48.88\n",
      "Environment 1: episode 102, score -50.0, avg_score -48.88\n",
      "Environment 2: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 3: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 4: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 5: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 6: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 7: episode 102, score -50.0, avg_score -49.03\n",
      "Environment 8: episode 102, score -47.0, avg_score -49.0\n",
      "Environment 9: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 10: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 11: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 12: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 13: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 14: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 15: episode 102, score -50.0, avg_score -49.0\n",
      "Environment 0: episode 103, score -50.0, avg_score -49.05\n",
      "Environment 1: episode 103, score -50.0, avg_score -49.05\n",
      "Environment 2: episode 103, score -50.0, avg_score -49.05\n",
      "Environment 3: episode 103, score -50.0, avg_score -49.05\n",
      "Environment 4: episode 103, score -50.0, avg_score -49.05\n",
      "Environment 5: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 6: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 7: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 8: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 9: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 10: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 11: episode 103, score -50.0, avg_score -49.08\n",
      "Environment 12: episode 103, score -38.0, avg_score -48.96\n",
      "Environment 13: episode 103, score -50.0, avg_score -48.96\n",
      "Environment 14: episode 103, score -50.0, avg_score -48.96\n",
      "Environment 15: episode 103, score -50.0, avg_score -48.96\n",
      "Environment 0: episode 104, score -50.0, avg_score -48.96\n",
      "Environment 1: episode 104, score -50.0, avg_score -48.96\n",
      "Environment 2: episode 104, score -50.0, avg_score -48.96\n",
      "Environment 3: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 4: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 5: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 6: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 7: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 8: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 9: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 10: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 11: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 12: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 13: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 14: episode 104, score -50.0, avg_score -49.11\n",
      "Environment 15: episode 104, score -49.0, avg_score -49.1\n",
      "Environment 0: episode 105, score -42.0, avg_score -49.02\n",
      "Environment 1: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 2: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 3: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 4: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 5: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 6: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 7: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 8: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 9: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 10: episode 105, score -50.0, avg_score -49.09\n",
      "Environment 11: episode 105, score -50.0, avg_score -49.2\n",
      "Environment 12: episode 105, score -50.0, avg_score -49.2\n",
      "Environment 13: episode 105, score -50.0, avg_score -49.2\n",
      "Environment 14: episode 105, score -50.0, avg_score -49.38\n",
      "Environment 15: episode 105, score -25.0, avg_score -49.13\n",
      "Environment 0: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 1: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 2: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 3: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 4: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 5: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 6: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 7: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 8: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 9: episode 106, score -50.0, avg_score -49.13\n",
      "Environment 10: episode 106, score -31.0, avg_score -48.94\n",
      "Environment 11: episode 106, score -50.0, avg_score -48.94\n",
      "Environment 12: episode 106, score -50.0, avg_score -48.94\n",
      "Environment 13: episode 106, score -50.0, avg_score -48.94\n",
      "Environment 14: episode 106, score -50.0, avg_score -48.94\n",
      "Environment 15: episode 106, score -50.0, avg_score -49.03\n",
      "Environment 0: episode 107, score -50.0, avg_score -49.03\n",
      "Environment 1: episode 107, score -50.0, avg_score -49.03\n",
      "Environment 2: episode 107, score -50.0, avg_score -49.03\n",
      "Rendering episode 1700.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1700.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1700.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 107, score -50.0, avg_score -49.03\n",
      "Environment 4: episode 107, score -50.0, avg_score -49.09\n",
      "Environment 5: episode 107, score -50.0, avg_score -49.09\n",
      "Environment 6: episode 107, score -50.0, avg_score -49.09\n",
      "Environment 7: episode 107, score -50.0, avg_score -49.09\n",
      "Environment 8: episode 107, score -50.0, avg_score -49.09\n",
      "Environment 9: episode 107, score -50.0, avg_score -49.27\n",
      "Environment 10: episode 107, score -50.0, avg_score -49.27\n",
      "Environment 11: episode 107, score -50.0, avg_score -49.27\n",
      "Environment 12: episode 107, score -50.0, avg_score -49.27\n",
      "Environment 13: episode 107, score -50.0, avg_score -49.32\n",
      "Environment 14: episode 107, score -50.0, avg_score -49.32\n",
      "Environment 15: episode 107, score -50.0, avg_score -49.32\n",
      "Environment 0: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 1: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 2: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 3: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 4: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 5: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 6: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 7: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 8: episode 108, score -50.0, avg_score -49.32\n",
      "Environment 9: episode 108, score -42.0, avg_score -49.24\n",
      "Environment 10: episode 108, score -50.0, avg_score -49.24\n",
      "Environment 11: episode 108, score -50.0, avg_score -49.24\n",
      "Environment 12: episode 108, score -50.0, avg_score -49.27\n",
      "Environment 13: episode 108, score -50.0, avg_score -49.27\n",
      "Environment 14: episode 108, score -50.0, avg_score -49.27\n",
      "Environment 15: episode 108, score -50.0, avg_score -49.27\n",
      "Environment 0: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 1: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 2: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 3: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 4: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 5: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 6: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 7: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 8: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 9: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 10: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 11: episode 109, score -50.0, avg_score -49.27\n",
      "Environment 12: episode 109, score -28.0, avg_score -49.05\n",
      "Environment 13: episode 109, score -50.0, avg_score -49.05\n",
      "Environment 14: episode 109, score -50.0, avg_score -49.05\n",
      "Environment 15: episode 109, score -50.0, avg_score -49.05\n",
      "Environment 0: episode 110, score -34.0, avg_score -49.01\n",
      "Environment 1: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 2: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 3: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 4: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 5: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 6: episode 110, score -50.0, avg_score -49.01\n",
      "Environment 7: episode 110, score -45.0, avg_score -48.96\n",
      "Environment 8: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 9: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 10: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 11: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 12: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 13: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 14: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 15: episode 110, score -50.0, avg_score -48.96\n",
      "Environment 0: episode 111, score -50.0, avg_score -48.96\n",
      "Environment 1: episode 111, score -49.0, avg_score -48.95\n",
      "Environment 2: episode 111, score -50.0, avg_score -48.95\n",
      "Environment 3: episode 111, score -50.0, avg_score -48.96\n",
      "Environment 4: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 5: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 6: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 7: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 8: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 9: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 10: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 11: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 12: episode 111, score -50.0, avg_score -49.04\n",
      "Environment 13: episode 111, score -49.0, avg_score -49.03\n",
      "Environment 14: episode 111, score -50.0, avg_score -49.03\n",
      "Environment 15: episode 111, score -50.0, avg_score -49.03\n",
      "Environment 0: episode 112, score -50.0, avg_score -49.03\n",
      "Environment 1: episode 112, score -50.0, avg_score -49.03\n",
      "Environment 2: episode 112, score -50.0, avg_score -49.03\n",
      "Environment 3: episode 112, score -50.0, avg_score -49.28\n",
      "Environment 4: episode 112, score -50.0, avg_score -49.28\n",
      "Environment 5: episode 112, score -50.0, avg_score -49.28\n",
      "Environment 6: episode 112, score -46.0, avg_score -49.24\n",
      "Environment 7: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 8: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 9: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 10: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 11: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 12: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 13: episode 112, score -50.0, avg_score -49.24\n",
      "Environment 14: episode 112, score -50.0, avg_score -49.43\n",
      "Environment 15: episode 112, score -50.0, avg_score -49.43\n",
      "Environment 0: episode 113, score -50.0, avg_score -49.43\n",
      "Environment 1: episode 113, score -50.0, avg_score -49.43\n",
      "Environment 2: episode 113, score -50.0, avg_score -49.43\n",
      "Environment 3: episode 113, score -36.0, avg_score -49.29\n",
      "Environment 4: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 5: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 6: episode 113, score -50.0, avg_score -49.29\n",
      "Rendering episode 1800.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1800.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1800.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 8: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 9: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 10: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 11: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 12: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 13: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 14: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 15: episode 113, score -50.0, avg_score -49.29\n",
      "Environment 0: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 1: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 2: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 3: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 4: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 5: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 6: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 7: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 8: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 9: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 10: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 11: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 12: episode 114, score -50.0, avg_score -49.29\n",
      "Environment 13: episode 114, score -50.0, avg_score -49.37\n",
      "Environment 14: episode 114, score -50.0, avg_score -49.37\n",
      "Environment 15: episode 114, score -50.0, avg_score -49.37\n",
      "Environment 0: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 1: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 2: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 3: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 4: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 5: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 6: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 7: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 8: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 9: episode 115, score -50.0, avg_score -49.37\n",
      "Environment 10: episode 115, score -45.0, avg_score -49.32\n",
      "Environment 11: episode 115, score -50.0, avg_score -49.32\n",
      "Environment 12: episode 115, score -50.0, avg_score -49.32\n",
      "Environment 13: episode 115, score -50.0, avg_score -49.32\n",
      "Environment 14: episode 115, score -50.0, avg_score -49.32\n",
      "Environment 15: episode 115, score -50.0, avg_score -49.32\n",
      "Environment 0: episode 116, score -50.0, avg_score -49.54\n",
      "Environment 1: episode 116, score -50.0, avg_score -49.54\n",
      "Environment 2: episode 116, score -50.0, avg_score -49.54\n",
      "Environment 3: episode 116, score -50.0, avg_score -49.54\n",
      "Environment 4: episode 116, score -50.0, avg_score -49.7\n",
      "Environment 5: episode 116, score -50.0, avg_score -49.7\n",
      "Environment 6: episode 116, score -50.0, avg_score -49.7\n",
      "Environment 7: episode 116, score -50.0, avg_score -49.7\n",
      "Environment 8: episode 116, score -32.0, avg_score -49.52\n",
      "Environment 9: episode 116, score -50.0, avg_score -49.52\n",
      "Environment 10: episode 116, score -50.0, avg_score -49.52\n",
      "Environment 11: episode 116, score -50.0, avg_score -49.57\n",
      "Environment 12: episode 116, score -50.0, avg_score -49.57\n",
      "Environment 13: episode 116, score -50.0, avg_score -49.57\n",
      "Environment 14: episode 116, score -50.0, avg_score -49.57\n",
      "Environment 15: episode 116, score -50.0, avg_score -49.57\n",
      "Environment 0: episode 117, score -50.0, avg_score -49.57\n",
      "Environment 1: episode 117, score -50.0, avg_score -49.57\n",
      "Environment 2: episode 117, score -50.0, avg_score -49.57\n",
      "Environment 3: episode 117, score -50.0, avg_score -49.57\n",
      "Environment 4: episode 117, score -50.0, avg_score -49.57\n",
      "Environment 5: episode 117, score -48.0, avg_score -49.56\n",
      "Environment 6: episode 117, score -50.0, avg_score -49.56\n",
      "Environment 7: episode 117, score -50.0, avg_score -49.56\n",
      "Environment 8: episode 117, score -34.0, avg_score -49.4\n",
      "Environment 9: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 10: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 11: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 12: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 13: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 14: episode 117, score -50.0, avg_score -49.4\n",
      "Environment 15: episode 117, score -34.0, avg_score -49.24\n",
      "Environment 0: episode 118, score -35.0, avg_score -49.09\n",
      "Environment 1: episode 118, score -50.0, avg_score -49.1\n",
      "Environment 2: episode 118, score -50.0, avg_score -49.1\n",
      "Environment 3: episode 118, score -42.0, avg_score -49.02\n",
      "Environment 4: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 5: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 6: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 7: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 8: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 9: episode 118, score -50.0, avg_score -49.02\n",
      "Environment 10: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 11: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 12: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 13: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 14: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 15: episode 118, score -50.0, avg_score -49.06\n",
      "Environment 0: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 1: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 2: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 3: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 4: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 5: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 6: episode 119, score -50.0, avg_score -49.06\n",
      "Environment 7: episode 119, score -50.0, avg_score -49.2\n",
      "Environment 8: episode 119, score -50.0, avg_score -49.2\n",
      "Environment 9: episode 119, score -50.0, avg_score -49.2\n",
      "Environment 10: episode 119, score -50.0, avg_score -49.2\n",
      "Rendering episode 1900.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_1900.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_1900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_1900.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 119, score -50.0, avg_score -49.2\n",
      "Environment 12: episode 119, score -50.0, avg_score -49.2\n",
      "Environment 13: episode 119, score -41.0, avg_score -49.11\n",
      "Environment 14: episode 119, score -48.0, avg_score -49.09\n",
      "Environment 15: episode 119, score -50.0, avg_score -49.09\n",
      "Environment 0: episode 120, score -50.0, avg_score -49.09\n",
      "Environment 1: episode 120, score -50.0, avg_score -49.09\n",
      "Environment 2: episode 120, score -50.0, avg_score -49.09\n",
      "Environment 3: episode 120, score -50.0, avg_score -49.09\n",
      "Environment 4: episode 120, score -50.0, avg_score -49.09\n",
      "Environment 5: episode 120, score -49.0, avg_score -49.08\n",
      "Environment 6: episode 120, score -50.0, avg_score -49.08\n",
      "Environment 7: episode 120, score -50.0, avg_score -49.08\n",
      "Environment 8: episode 120, score -50.0, avg_score -49.08\n",
      "Environment 9: episode 120, score -50.0, avg_score -49.08\n",
      "Environment 10: episode 120, score -50.0, avg_score -49.08\n",
      "Environment 11: episode 120, score -37.0, avg_score -48.95\n",
      "Environment 12: episode 120, score -50.0, avg_score -48.95\n",
      "Environment 13: episode 120, score -50.0, avg_score -48.95\n",
      "Environment 14: episode 120, score -50.0, avg_score -48.95\n",
      "Environment 15: episode 120, score -50.0, avg_score -48.95\n",
      "Environment 0: episode 121, score -50.0, avg_score -48.95\n",
      "Environment 1: episode 121, score -50.0, avg_score -48.95\n",
      "Environment 2: episode 121, score -50.0, avg_score -48.95\n",
      "Environment 3: episode 121, score -50.0, avg_score -48.95\n",
      "Environment 4: episode 121, score -50.0, avg_score -48.95\n",
      "Environment 5: episode 121, score -23.0, avg_score -48.68\n",
      "Environment 6: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 7: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 8: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 9: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 10: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 11: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 12: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 13: episode 121, score -50.0, avg_score -48.68\n",
      "Environment 14: episode 121, score -50.0, avg_score -48.73\n",
      "Environment 15: episode 121, score -50.0, avg_score -48.73\n",
      "Environment 0: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 1: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 2: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 3: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 4: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 5: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 6: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 7: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 8: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 9: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 10: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 11: episode 122, score -50.0, avg_score -48.73\n",
      "Environment 12: episode 122, score -50.0, avg_score -48.91\n",
      "Environment 13: episode 122, score -50.0, avg_score -48.91\n",
      "Environment 14: episode 122, score -50.0, avg_score -48.91\n",
      "Environment 15: episode 122, score -50.0, avg_score -48.91\n",
      "Environment 0: episode 123, score -50.0, avg_score -48.91\n",
      "Environment 1: episode 123, score -50.0, avg_score -48.91\n",
      "Environment 2: episode 123, score -50.0, avg_score -48.91\n",
      "Environment 3: episode 123, score -44.0, avg_score -48.85\n",
      "Environment 4: episode 123, score -50.0, avg_score -48.85\n",
      "Environment 5: episode 123, score -50.0, avg_score -48.85\n",
      "Environment 6: episode 123, score -50.0, avg_score -48.85\n",
      "Environment 7: episode 123, score -50.0, avg_score -48.85\n",
      "Environment 8: episode 123, score -50.0, avg_score -48.85\n",
      "Environment 9: episode 123, score -50.0, avg_score -48.87\n",
      "Environment 10: episode 123, score -50.0, avg_score -48.87\n",
      "Environment 11: episode 123, score -50.0, avg_score -48.87\n",
      "Environment 12: episode 123, score -50.0, avg_score -49.03\n",
      "Environment 13: episode 123, score -33.0, avg_score -48.86\n",
      "Environment 14: episode 123, score -50.0, avg_score -48.86\n",
      "Environment 15: episode 123, score -50.0, avg_score -48.86\n",
      "Environment 0: episode 124, score -50.0, avg_score -48.86\n",
      "Environment 1: episode 124, score -50.0, avg_score -48.86\n",
      "Environment 2: episode 124, score -50.0, avg_score -48.86\n",
      "Environment 3: episode 124, score -50.0, avg_score -49.02\n",
      "Environment 4: episode 124, score -50.0, avg_score -49.17\n",
      "Environment 5: episode 124, score -50.0, avg_score -49.17\n",
      "Environment 6: episode 124, score -50.0, avg_score -49.17\n",
      "Environment 7: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 8: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 9: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 10: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 11: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 12: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 13: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 14: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 15: episode 124, score -50.0, avg_score -49.25\n",
      "Environment 0: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 1: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 2: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 3: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 4: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 5: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 6: episode 125, score -50.0, avg_score -49.25\n",
      "Environment 7: episode 125, score -30.0, avg_score -49.05\n",
      "Environment 8: episode 125, score -50.0, avg_score -49.05\n",
      "Environment 9: episode 125, score -50.0, avg_score -49.05\n",
      "Environment 10: episode 125, score -50.0, avg_score -49.05\n",
      "Environment 11: episode 125, score -50.0, avg_score -49.05\n",
      "Environment 12: episode 125, score -14.0, avg_score -48.69\n",
      "Environment 13: episode 125, score -50.0, avg_score -48.69\n",
      "Environment 14: episode 125, score -50.0, avg_score -48.69\n",
      "Rendering episode 2000.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2000.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2000.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 125, score -50.0, avg_score -48.69\n",
      "Environment 0: episode 126, score -50.0, avg_score -48.69\n",
      "Environment 1: episode 126, score -50.0, avg_score -48.78\n",
      "Environment 2: episode 126, score -50.0, avg_score -48.8\n",
      "Environment 3: episode 126, score -50.0, avg_score -48.8\n",
      "Environment 4: episode 126, score -50.0, avg_score -48.8\n",
      "Environment 5: episode 126, score -50.0, avg_score -48.8\n",
      "Environment 6: episode 126, score -50.0, avg_score -48.8\n",
      "Environment 7: episode 126, score -40.0, avg_score -48.7\n",
      "Environment 8: episode 126, score -50.0, avg_score -48.7\n",
      "Environment 9: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 10: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 11: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 12: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 13: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 14: episode 126, score -50.0, avg_score -48.71\n",
      "Environment 15: episode 126, score -50.0, avg_score -48.84\n",
      "Environment 0: episode 127, score -49.0, avg_score -48.83\n",
      "Environment 1: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 2: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 3: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 4: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 5: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 6: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 7: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 8: episode 127, score -50.0, avg_score -48.83\n",
      "Environment 9: episode 127, score -35.0, avg_score -48.95\n",
      "Environment 10: episode 127, score -50.0, avg_score -48.95\n",
      "Environment 11: episode 127, score -50.0, avg_score -48.95\n",
      "Environment 12: episode 127, score -50.0, avg_score -48.95\n",
      "Environment 13: episode 127, score -48.0, avg_score -48.93\n",
      "Environment 14: episode 127, score -50.0, avg_score -48.93\n",
      "Environment 15: episode 127, score -50.0, avg_score -48.93\n",
      "Environment 0: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 1: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 2: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 3: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 4: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 5: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 6: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 7: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 8: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 9: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 10: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 11: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 12: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 13: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 14: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 15: episode 128, score -50.0, avg_score -48.93\n",
      "Environment 0: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 1: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 2: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 3: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 4: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 5: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 6: episode 129, score -50.0, avg_score -48.93\n",
      "Environment 7: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 8: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 9: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 10: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 11: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 12: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 13: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 14: episode 129, score -50.0, avg_score -48.99\n",
      "Environment 15: episode 129, score -44.0, avg_score -48.93\n",
      "Environment 0: episode 130, score -50.0, avg_score -48.93\n",
      "Environment 1: episode 130, score -50.0, avg_score -49.1\n",
      "Environment 2: episode 130, score -50.0, avg_score -49.1\n",
      "Environment 3: episode 130, score -50.0, avg_score -49.1\n",
      "Environment 4: episode 130, score -50.0, avg_score -49.1\n",
      "Environment 5: episode 130, score -35.0, avg_score -48.95\n",
      "Environment 6: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 7: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 8: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 9: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 10: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 11: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 12: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 13: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 14: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 15: episode 130, score -50.0, avg_score -48.95\n",
      "Environment 0: episode 131, score -50.0, avg_score -48.95\n",
      "Environment 1: episode 131, score -50.0, avg_score -48.95\n",
      "Environment 2: episode 131, score -50.0, avg_score -48.95\n",
      "Environment 3: episode 131, score -50.0, avg_score -48.95\n",
      "Environment 4: episode 131, score -50.0, avg_score -48.95\n",
      "Environment 5: episode 131, score -42.0, avg_score -48.87\n",
      "Environment 6: episode 131, score -50.0, avg_score -48.87\n",
      "Environment 7: episode 131, score -50.0, avg_score -48.87\n",
      "Environment 8: episode 131, score -50.0, avg_score -48.87\n",
      "Environment 9: episode 131, score -50.0, avg_score -48.87\n",
      "Environment 10: episode 131, score -50.0, avg_score -48.87\n",
      "Environment 11: episode 131, score -50.0, avg_score -49.07\n",
      "Environment 12: episode 131, score -50.0, avg_score -49.07\n",
      "Environment 13: episode 131, score -50.0, avg_score -49.07\n",
      "Environment 14: episode 131, score -50.0, avg_score -49.07\n",
      "Environment 15: episode 131, score -50.0, avg_score -49.07\n",
      "Environment 0: episode 132, score -50.0, avg_score -49.43\n",
      "Environment 1: episode 132, score -50.0, avg_score -49.43\n",
      "Environment 2: episode 132, score -44.0, avg_score -49.37\n",
      "Rendering episode 2100.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2100.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2100.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 132, score -50.0, avg_score -49.37\n",
      "Environment 4: episode 132, score -50.0, avg_score -49.37\n",
      "Environment 5: episode 132, score -42.0, avg_score -49.29\n",
      "Environment 6: episode 132, score -50.0, avg_score -49.29\n",
      "Environment 7: episode 132, score -50.0, avg_score -49.29\n",
      "Environment 8: episode 132, score -50.0, avg_score -49.29\n",
      "Environment 9: episode 132, score -50.0, avg_score -49.29\n",
      "Environment 10: episode 132, score -50.0, avg_score -49.29\n",
      "Environment 11: episode 132, score -50.0, avg_score -49.39\n",
      "Environment 12: episode 132, score -50.0, avg_score -49.39\n",
      "Environment 13: episode 132, score -50.0, avg_score -49.39\n",
      "Environment 14: episode 132, score -42.0, avg_score -49.31\n",
      "Environment 15: episode 132, score -50.0, avg_score -49.31\n",
      "Environment 0: episode 133, score -50.0, avg_score -49.31\n",
      "Environment 1: episode 133, score -50.0, avg_score -49.31\n",
      "Environment 2: episode 133, score -50.0, avg_score -49.31\n",
      "Environment 3: episode 133, score -50.0, avg_score -49.31\n",
      "Environment 4: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 5: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 6: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 7: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 8: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 9: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 10: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 11: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 12: episode 133, score -50.0, avg_score -49.32\n",
      "Environment 13: episode 133, score -50.0, avg_score -49.47\n",
      "Environment 14: episode 133, score -50.0, avg_score -49.47\n",
      "Environment 15: episode 133, score -50.0, avg_score -49.47\n",
      "Environment 0: episode 134, score -50.0, avg_score -49.47\n",
      "Environment 1: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 2: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 3: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 4: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 5: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 6: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 7: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 8: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 9: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 10: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 11: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 12: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 13: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 14: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 15: episode 134, score -50.0, avg_score -49.49\n",
      "Environment 0: episode 135, score -50.0, avg_score -49.49\n",
      "Environment 1: episode 135, score -50.0, avg_score -49.49\n",
      "Environment 2: episode 135, score -50.0, avg_score -49.49\n",
      "Environment 3: episode 135, score -45.0, avg_score -49.44\n",
      "Environment 4: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 5: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 6: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 7: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 8: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 9: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 10: episode 135, score -50.0, avg_score -49.44\n",
      "Environment 11: episode 135, score -34.0, avg_score -49.28\n",
      "Environment 12: episode 135, score -50.0, avg_score -49.28\n",
      "Environment 13: episode 135, score -50.0, avg_score -49.28\n",
      "Environment 14: episode 135, score -50.0, avg_score -49.28\n",
      "Environment 15: episode 135, score -50.0, avg_score -49.28\n",
      "Environment 0: episode 136, score -50.0, avg_score -49.28\n",
      "Environment 1: episode 136, score -50.0, avg_score -49.28\n",
      "Environment 2: episode 136, score -50.0, avg_score -49.28\n",
      "Environment 3: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 4: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 5: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 6: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 7: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 8: episode 136, score -50.0, avg_score -49.34\n",
      "Environment 9: episode 136, score -50.0, avg_score -49.49\n",
      "Environment 10: episode 136, score -50.0, avg_score -49.49\n",
      "Environment 11: episode 136, score -47.0, avg_score -49.46\n",
      "Environment 12: episode 136, score -45.0, avg_score -49.41\n",
      "Environment 13: episode 136, score -50.0, avg_score -49.41\n",
      "Environment 14: episode 136, score -50.0, avg_score -49.41\n",
      "Environment 15: episode 136, score -50.0, avg_score -49.41\n",
      "Environment 0: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 1: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 2: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 3: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 4: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 5: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 6: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 7: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 8: episode 137, score -50.0, avg_score -49.41\n",
      "Environment 9: episode 137, score -29.0, avg_score -49.28\n",
      "Environment 10: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 11: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 12: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 13: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 14: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 15: episode 137, score -50.0, avg_score -49.28\n",
      "Environment 0: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 1: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 2: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 3: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 4: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 5: episode 138, score -50.0, avg_score -49.28\n",
      "Environment 6: episode 138, score -50.0, avg_score -49.34\n",
      "Rendering episode 2200.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2200.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2200.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2200.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 138, score -50.0, avg_score -49.34\n",
      "Environment 8: episode 138, score -50.0, avg_score -49.34\n",
      "Environment 9: episode 138, score -50.0, avg_score -49.42\n",
      "Environment 10: episode 138, score -50.0, avg_score -49.42\n",
      "Environment 11: episode 138, score -50.0, avg_score -49.42\n",
      "Environment 12: episode 138, score -30.0, avg_score -49.22\n",
      "Environment 13: episode 138, score -50.0, avg_score -49.22\n",
      "Environment 14: episode 138, score -50.0, avg_score -49.22\n",
      "Environment 15: episode 138, score -50.0, avg_score -49.22\n",
      "Environment 0: episode 139, score -50.0, avg_score -49.22\n",
      "Environment 1: episode 139, score -50.0, avg_score -49.22\n",
      "Environment 2: episode 139, score -50.0, avg_score -49.3\n",
      "Environment 3: episode 139, score -50.0, avg_score -49.3\n",
      "Environment 4: episode 139, score -50.0, avg_score -49.3\n",
      "Environment 5: episode 139, score -50.0, avg_score -49.3\n",
      "Environment 6: episode 139, score -32.0, avg_score -49.12\n",
      "Environment 7: episode 139, score -50.0, avg_score -49.12\n",
      "Environment 8: episode 139, score -50.0, avg_score -49.12\n",
      "Environment 9: episode 139, score -50.0, avg_score -49.12\n",
      "Environment 10: episode 139, score -50.0, avg_score -49.12\n",
      "Environment 11: episode 139, score -50.0, avg_score -49.12\n",
      "Environment 12: episode 139, score -37.0, avg_score -48.99\n",
      "Environment 13: episode 139, score -47.0, avg_score -48.96\n",
      "Environment 14: episode 139, score -50.0, avg_score -48.96\n",
      "Environment 15: episode 139, score -50.0, avg_score -48.96\n",
      "Environment 0: episode 140, score -50.0, avg_score -48.96\n",
      "Environment 1: episode 140, score -46.0, avg_score -48.92\n",
      "Environment 2: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 3: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 4: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 5: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 6: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 7: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 8: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 9: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 10: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 11: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 12: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 13: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 14: episode 140, score -50.0, avg_score -48.92\n",
      "Environment 15: episode 140, score -44.0, avg_score -48.86\n",
      "Environment 0: episode 141, score -50.0, avg_score -48.86\n",
      "Environment 1: episode 141, score -44.0, avg_score -48.8\n",
      "Environment 2: episode 141, score -50.0, avg_score -48.8\n",
      "Environment 3: episode 141, score -50.0, avg_score -48.8\n",
      "Environment 4: episode 141, score -50.0, avg_score -48.8\n",
      "Environment 5: episode 141, score -50.0, avg_score -48.8\n",
      "Environment 6: episode 141, score -50.0, avg_score -48.8\n",
      "Environment 7: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 8: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 9: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 10: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 11: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 12: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 13: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 14: episode 141, score -50.0, avg_score -48.85\n",
      "Environment 15: episode 141, score -50.0, avg_score -49.01\n",
      "Environment 0: episode 142, score -50.0, avg_score -49.01\n",
      "Environment 1: episode 142, score -46.0, avg_score -48.97\n",
      "Environment 2: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 3: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 4: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 5: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 6: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 7: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 8: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 9: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 10: episode 142, score -50.0, avg_score -48.97\n",
      "Environment 11: episode 142, score -47.0, avg_score -48.94\n",
      "Environment 12: episode 142, score -50.0, avg_score -48.94\n",
      "Environment 13: episode 142, score -50.0, avg_score -48.94\n",
      "Environment 14: episode 142, score -50.0, avg_score -48.94\n",
      "Environment 15: episode 142, score -43.0, avg_score -48.9\n",
      "Environment 0: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 1: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 2: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 3: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 4: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 5: episode 143, score -50.0, avg_score -48.95\n",
      "Environment 6: episode 143, score -45.0, avg_score -48.9\n",
      "Environment 7: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 8: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 9: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 10: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 11: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 12: episode 143, score -50.0, avg_score -48.9\n",
      "Environment 13: episode 143, score -50.0, avg_score -49.11\n",
      "Environment 14: episode 143, score -50.0, avg_score -49.11\n",
      "Environment 15: episode 143, score -50.0, avg_score -49.11\n",
      "Environment 0: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 1: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 2: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 3: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 4: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 5: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 6: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 7: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 8: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 9: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 10: episode 144, score -50.0, avg_score -49.11\n",
      "Rendering episode 2300.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2300.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2300.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2300.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 12: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 13: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 14: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 15: episode 144, score -50.0, avg_score -49.11\n",
      "Environment 0: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 1: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 2: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 3: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 4: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 5: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 6: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 7: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 8: episode 145, score -50.0, avg_score -49.31\n",
      "Environment 9: episode 145, score -48.0, avg_score -49.29\n",
      "Environment 10: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 11: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 12: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 13: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 14: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 15: episode 145, score -50.0, avg_score -49.47\n",
      "Environment 0: episode 146, score -36.0, avg_score -49.46\n",
      "Environment 1: episode 146, score -50.0, avg_score -49.49\n",
      "Environment 2: episode 146, score -50.0, avg_score -49.49\n",
      "Environment 3: episode 146, score -50.0, avg_score -49.49\n",
      "Environment 4: episode 146, score -50.0, avg_score -49.49\n",
      "Environment 5: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 6: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 7: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 8: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 9: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 10: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 11: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 12: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 13: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 14: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 15: episode 146, score -50.0, avg_score -49.53\n",
      "Environment 0: episode 147, score -50.0, avg_score -49.53\n",
      "Environment 1: episode 147, score -50.0, avg_score -49.53\n",
      "Environment 2: episode 147, score -47.0, avg_score -49.5\n",
      "Environment 3: episode 147, score -50.0, avg_score -49.56\n",
      "Environment 4: episode 147, score -50.0, avg_score -49.56\n",
      "Environment 5: episode 147, score -50.0, avg_score -49.62\n",
      "Environment 6: episode 147, score -50.0, avg_score -49.62\n",
      "Environment 7: episode 147, score -50.0, avg_score -49.62\n",
      "Environment 8: episode 147, score -50.0, avg_score -49.62\n",
      "Environment 9: episode 147, score -50.0, avg_score -49.62\n",
      "Environment 10: episode 147, score -41.0, avg_score -49.53\n",
      "Environment 11: episode 147, score -50.0, avg_score -49.53\n",
      "Environment 12: episode 147, score -50.0, avg_score -49.53\n",
      "Environment 13: episode 147, score -50.0, avg_score -49.53\n",
      "Environment 14: episode 147, score -39.0, avg_score -49.42\n",
      "Environment 15: episode 147, score -50.0, avg_score -49.42\n",
      "Environment 0: episode 148, score -50.0, avg_score -49.42\n",
      "Environment 1: episode 148, score -28.0, avg_score -49.2\n",
      "Environment 2: episode 148, score -50.0, avg_score -49.2\n",
      "Environment 3: episode 148, score -50.0, avg_score -49.2\n",
      "Environment 4: episode 148, score -50.0, avg_score -49.2\n",
      "Environment 5: episode 148, score -50.0, avg_score -49.24\n",
      "Environment 6: episode 148, score -50.0, avg_score -49.24\n",
      "Environment 7: episode 148, score -45.0, avg_score -49.19\n",
      "Environment 8: episode 148, score -50.0, avg_score -49.19\n",
      "Environment 9: episode 148, score -50.0, avg_score -49.19\n",
      "Environment 10: episode 148, score -50.0, avg_score -49.19\n",
      "Environment 11: episode 148, score -50.0, avg_score -49.19\n",
      "Environment 12: episode 148, score -50.0, avg_score -49.19\n",
      "Environment 13: episode 148, score -48.0, avg_score -49.17\n",
      "Environment 14: episode 148, score -45.0, avg_score -49.12\n",
      "Environment 15: episode 148, score -50.0, avg_score -49.15\n",
      "Environment 0: episode 149, score -50.0, avg_score -49.15\n",
      "Environment 1: episode 149, score -50.0, avg_score -49.15\n",
      "Environment 2: episode 149, score -45.0, avg_score -49.1\n",
      "Environment 3: episode 149, score -50.0, avg_score -49.17\n",
      "Environment 4: episode 149, score -47.0, avg_score -49.14\n",
      "Environment 5: episode 149, score -50.0, avg_score -49.14\n",
      "Environment 6: episode 149, score -50.0, avg_score -49.14\n",
      "Environment 7: episode 149, score -50.0, avg_score -49.14\n",
      "Environment 8: episode 149, score -50.0, avg_score -49.14\n",
      "Environment 9: episode 149, score -34.0, avg_score -48.98\n",
      "Environment 10: episode 149, score -45.0, avg_score -48.98\n",
      "Environment 11: episode 149, score -46.0, avg_score -48.94\n",
      "Environment 12: episode 149, score -50.0, avg_score -48.94\n",
      "Environment 13: episode 149, score -44.0, avg_score -48.88\n",
      "Environment 14: episode 149, score -50.0, avg_score -48.88\n",
      "Environment 15: episode 149, score -50.0, avg_score -48.88\n",
      "Environment 0: episode 150, score -49.0, avg_score -48.87\n",
      "Environment 1: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 2: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 3: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 4: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 5: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 6: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 7: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 8: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 9: episode 150, score -50.0, avg_score -48.87\n",
      "Environment 10: episode 150, score -46.0, avg_score -48.83\n",
      "Environment 11: episode 150, score -50.0, avg_score -48.83\n",
      "Environment 12: episode 150, score -50.0, avg_score -48.83\n",
      "Environment 13: episode 150, score -50.0, avg_score -48.83\n",
      "Environment 14: episode 150, score -50.0, avg_score -48.83\n",
      "Rendering episode 2400.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2400.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2400.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2400.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 150, score -50.0, avg_score -48.83\n",
      "Environment 0: episode 151, score -50.0, avg_score -48.83\n",
      "Environment 1: episode 151, score -50.0, avg_score -48.83\n",
      "Environment 2: episode 151, score -50.0, avg_score -48.83\n",
      "Environment 3: episode 151, score -50.0, avg_score -48.83\n",
      "Environment 4: episode 151, score -44.0, avg_score -48.77\n",
      "Environment 5: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 6: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 7: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 8: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 9: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 10: episode 151, score -50.0, avg_score -48.77\n",
      "Environment 11: episode 151, score -46.0, avg_score -48.73\n",
      "Environment 12: episode 151, score -50.0, avg_score -48.73\n",
      "Environment 13: episode 151, score -50.0, avg_score -48.75\n",
      "Environment 14: episode 151, score -50.0, avg_score -48.75\n",
      "Environment 15: episode 151, score -40.0, avg_score -48.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 7649. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 0: episode 152, score -50.0, avg_score -48.65\n",
      "Environment 1: episode 152, score -50.0, avg_score -48.65\n",
      "Environment 2: episode 152, score -38.0, avg_score -48.53\n",
      "Environment 3: episode 152, score -44.0, avg_score -48.47\n",
      "Environment 4: episode 152, score -50.0, avg_score -48.61\n",
      "Environment 5: episode 152, score -50.0, avg_score -48.61\n",
      "Environment 6: episode 152, score -50.0, avg_score -48.61\n",
      "Environment 7: episode 152, score -49.0, avg_score -48.6\n",
      "Environment 8: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 9: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 10: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 11: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 12: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 13: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 14: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 15: episode 152, score -50.0, avg_score -48.6\n",
      "Environment 0: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 1: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 2: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 3: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 4: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 5: episode 153, score -50.0, avg_score -48.6\n",
      "Environment 6: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 7: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 8: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 9: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 10: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 11: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 12: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 13: episode 153, score -50.0, avg_score -48.63\n",
      "Environment 14: episode 153, score -50.0, avg_score -48.72\n",
      "Environment 15: episode 153, score -50.0, avg_score -48.72\n",
      "Environment 0: episode 154, score -46.0, avg_score -48.68\n",
      "Environment 1: episode 154, score -50.0, avg_score -48.68\n",
      "Environment 2: episode 154, score -50.0, avg_score -48.79\n",
      "Environment 3: episode 154, score -50.0, avg_score -48.79\n",
      "Environment 4: episode 154, score -50.0, avg_score -48.79\n",
      "Environment 5: episode 154, score -50.0, avg_score -49.01\n",
      "Environment 6: episode 154, score -50.0, avg_score -49.01\n",
      "Environment 7: episode 154, score -35.0, avg_score -48.86\n",
      "Environment 8: episode 154, score -50.0, avg_score -48.86\n",
      "Environment 9: episode 154, score -50.0, avg_score -48.86\n",
      "Environment 10: episode 154, score -50.0, avg_score -48.86\n",
      "Environment 11: episode 154, score -50.0, avg_score -48.91\n",
      "Environment 12: episode 154, score -23.0, avg_score -48.64\n",
      "Environment 13: episode 154, score -50.0, avg_score -48.64\n",
      "Environment 14: episode 154, score -50.0, avg_score -48.64\n",
      "Environment 15: episode 154, score -45.0, avg_score -48.59\n",
      "Environment 0: episode 155, score -50.0, avg_score -48.59\n",
      "Environment 1: episode 155, score -50.0, avg_score -48.61\n",
      "Environment 2: episode 155, score -47.0, avg_score -48.63\n",
      "Environment 3: episode 155, score -26.0, avg_score -48.39\n",
      "Environment 4: episode 155, score -50.0, avg_score -48.39\n",
      "Environment 5: episode 155, score -50.0, avg_score -48.39\n",
      "Environment 6: episode 155, score -50.0, avg_score -48.44\n",
      "Environment 7: episode 155, score -50.0, avg_score -48.44\n",
      "Environment 8: episode 155, score -48.0, avg_score -48.45\n",
      "Environment 9: episode 155, score -50.0, avg_score -48.45\n",
      "Environment 10: episode 155, score -50.0, avg_score -48.45\n",
      "Environment 11: episode 155, score -12.0, avg_score -48.07\n",
      "Environment 12: episode 155, score -47.0, avg_score -48.04\n",
      "Environment 13: episode 155, score -50.0, avg_score -48.2\n",
      "Environment 14: episode 155, score -50.0, avg_score -48.25\n",
      "Environment 15: episode 155, score -50.0, avg_score -48.29\n",
      "Environment 0: episode 156, score -50.0, avg_score -48.29\n",
      "Environment 1: episode 156, score -50.0, avg_score -48.35\n",
      "Environment 2: episode 156, score -39.0, avg_score -48.24\n",
      "Environment 3: episode 156, score -50.0, avg_score -48.24\n",
      "Environment 4: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 5: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 6: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 7: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 8: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 9: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 10: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 11: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 12: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 13: episode 156, score -50.0, avg_score -48.25\n",
      "Environment 14: episode 156, score -31.0, avg_score -48.1\n",
      "Environment 15: episode 156, score -50.0, avg_score -48.1\n",
      "Environment 0: episode 157, score -50.0, avg_score -48.1\n",
      "Environment 1: episode 157, score -50.0, avg_score -48.1\n",
      "Environment 2: episode 157, score -50.0, avg_score -48.1\n",
      "Rendering episode 2500.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2500.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2500.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2500.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 157, score -48.0, avg_score -48.08\n",
      "Environment 4: episode 157, score -50.0, avg_score -48.08\n",
      "Environment 5: episode 157, score -50.0, avg_score -48.08\n",
      "Environment 6: episode 157, score -50.0, avg_score -48.08\n",
      "Environment 7: episode 157, score -50.0, avg_score -48.08\n",
      "Environment 8: episode 157, score -50.0, avg_score -48.14\n",
      "Environment 9: episode 157, score -50.0, avg_score -48.14\n",
      "Environment 10: episode 157, score -50.0, avg_score -48.14\n",
      "Environment 11: episode 157, score -47.0, avg_score -48.11\n",
      "Environment 12: episode 157, score -50.0, avg_score -48.11\n",
      "Environment 13: episode 157, score -50.0, avg_score -48.11\n",
      "Environment 14: episode 157, score -50.0, avg_score -48.11\n",
      "Environment 15: episode 157, score -50.0, avg_score -48.15\n",
      "Environment 0: episode 158, score -50.0, avg_score -48.15\n",
      "Environment 1: episode 158, score -50.0, avg_score -48.15\n",
      "Environment 2: episode 158, score -50.0, avg_score -48.15\n",
      "Environment 3: episode 158, score -47.0, avg_score -48.22\n",
      "Environment 4: episode 158, score -50.0, avg_score -48.22\n",
      "Environment 5: episode 158, score -50.0, avg_score -48.22\n",
      "Environment 6: episode 158, score -50.0, avg_score -48.34\n",
      "Environment 7: episode 158, score -50.0, avg_score -48.4\n",
      "Environment 8: episode 158, score -50.0, avg_score -48.4\n",
      "Environment 9: episode 158, score -50.0, avg_score -48.4\n",
      "Environment 10: episode 158, score -50.0, avg_score -48.4\n",
      "Environment 11: episode 158, score -50.0, avg_score -48.41\n",
      "Environment 12: episode 158, score -50.0, avg_score -48.41\n",
      "Environment 13: episode 158, score -50.0, avg_score -48.41\n",
      "Environment 14: episode 158, score -50.0, avg_score -48.41\n",
      "Environment 15: episode 158, score -50.0, avg_score -48.41\n",
      "Environment 0: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 1: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 2: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 3: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 4: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 5: episode 159, score -50.0, avg_score -48.41\n",
      "Environment 6: episode 159, score -46.0, avg_score -48.37\n",
      "Environment 7: episode 159, score -50.0, avg_score -48.37\n",
      "Environment 8: episode 159, score -50.0, avg_score -48.37\n",
      "Environment 9: episode 159, score -50.0, avg_score -48.37\n",
      "Environment 10: episode 159, score -32.0, avg_score -48.19\n",
      "Environment 11: episode 159, score -50.0, avg_score -48.19\n",
      "Environment 12: episode 159, score -50.0, avg_score -48.19\n",
      "Environment 13: episode 159, score -50.0, avg_score -48.19\n",
      "Environment 14: episode 159, score -50.0, avg_score -48.19\n",
      "Environment 15: episode 159, score -46.0, avg_score -48.15\n",
      "Environment 0: episode 160, score -38.0, avg_score -48.03\n",
      "Environment 1: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 2: episode 160, score -42.0, avg_score -47.95\n",
      "Environment 3: episode 160, score -50.0, avg_score -47.95\n",
      "Environment 4: episode 160, score -50.0, avg_score -47.99\n",
      "Environment 5: episode 160, score -50.0, avg_score -47.99\n",
      "Environment 6: episode 160, score -50.0, avg_score -47.99\n",
      "Environment 7: episode 160, score -50.0, avg_score -47.99\n",
      "Environment 8: episode 160, score -49.0, avg_score -47.98\n",
      "Environment 9: episode 160, score -40.0, avg_score -47.88\n",
      "Environment 10: episode 160, score -50.0, avg_score -47.88\n",
      "Environment 11: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 12: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 13: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 14: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 15: episode 160, score -50.0, avg_score -48.03\n",
      "Environment 0: episode 161, score -50.0, avg_score -48.3\n",
      "Environment 1: episode 161, score -50.0, avg_score -48.3\n",
      "Environment 2: episode 161, score -50.0, avg_score -48.3\n",
      "Environment 3: episode 161, score -50.0, avg_score -48.35\n",
      "Environment 4: episode 161, score -50.0, avg_score -48.35\n",
      "Environment 5: episode 161, score -45.0, avg_score -48.3\n",
      "Environment 6: episode 161, score -50.0, avg_score -48.33\n",
      "Environment 7: episode 161, score -49.0, avg_score -48.56\n",
      "Environment 8: episode 161, score -50.0, avg_score -48.56\n",
      "Environment 9: episode 161, score -50.0, avg_score -48.56\n",
      "Environment 10: episode 161, score -50.0, avg_score -48.56\n",
      "Environment 11: episode 161, score -47.0, avg_score -48.53\n",
      "Environment 12: episode 161, score -25.0, avg_score -48.3\n",
      "Environment 13: episode 161, score -50.0, avg_score -48.3\n",
      "Environment 14: episode 161, score -50.0, avg_score -48.3\n",
      "Environment 15: episode 161, score -50.0, avg_score -48.68\n",
      "Environment 0: episode 162, score -50.0, avg_score -48.71\n",
      "Environment 1: episode 162, score -44.0, avg_score -48.65\n",
      "Environment 2: episode 162, score -50.0, avg_score -48.65\n",
      "Environment 3: episode 162, score -50.0, avg_score -48.65\n",
      "Environment 4: episode 162, score -50.0, avg_score -48.65\n",
      "Environment 5: episode 162, score -50.0, avg_score -48.65\n",
      "Environment 6: episode 162, score -50.0, avg_score -48.76\n",
      "Environment 7: episode 162, score -46.0, avg_score -48.72\n",
      "Environment 8: episode 162, score -16.0, avg_score -48.38\n",
      "Environment 9: episode 162, score -50.0, avg_score -48.38\n",
      "Environment 10: episode 162, score -50.0, avg_score -48.38\n",
      "Environment 11: episode 162, score -32.0, avg_score -48.2\n",
      "Environment 12: episode 162, score -50.0, avg_score -48.2\n",
      "Environment 13: episode 162, score -50.0, avg_score -48.2\n",
      "Environment 14: episode 162, score -50.0, avg_score -48.2\n",
      "Environment 15: episode 162, score -50.0, avg_score -48.2\n",
      "Environment 0: episode 163, score -50.0, avg_score -48.2\n",
      "Environment 1: episode 163, score -50.0, avg_score -48.2\n",
      "Environment 2: episode 163, score -50.0, avg_score -48.39\n",
      "Environment 3: episode 163, score -50.0, avg_score -48.39\n",
      "Environment 4: episode 163, score -50.0, avg_score -48.39\n",
      "Environment 5: episode 163, score -50.0, avg_score -48.39\n",
      "Environment 6: episode 163, score -50.0, avg_score -48.39\n",
      "Rendering episode 2600.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2600.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2600.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2600.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 163, score -50.0, avg_score -48.41\n",
      "Environment 8: episode 163, score -50.0, avg_score -48.41\n",
      "Environment 9: episode 163, score -50.0, avg_score -48.41\n",
      "Environment 10: episode 163, score -35.0, avg_score -48.26\n",
      "Environment 11: episode 163, score -50.0, avg_score -48.26\n",
      "Environment 12: episode 163, score -50.0, avg_score -48.26\n",
      "Environment 13: episode 163, score -50.0, avg_score -48.26\n",
      "Environment 14: episode 163, score -50.0, avg_score -48.26\n",
      "Environment 15: episode 163, score -35.0, avg_score -48.14\n",
      "Environment 0: episode 164, score -48.0, avg_score -48.12\n",
      "Environment 1: episode 164, score -50.0, avg_score -48.12\n",
      "Environment 2: episode 164, score -50.0, avg_score -48.12\n",
      "Environment 3: episode 164, score -50.0, avg_score -48.12\n",
      "Environment 4: episode 164, score -50.0, avg_score -48.12\n",
      "Environment 5: episode 164, score -36.0, avg_score -47.98\n",
      "Environment 6: episode 164, score -50.0, avg_score -47.98\n",
      "Environment 7: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 8: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 9: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 10: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 11: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 12: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 13: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 14: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 15: episode 164, score -50.0, avg_score -48.01\n",
      "Environment 0: episode 165, score -50.0, avg_score -48.01\n",
      "Environment 1: episode 165, score -50.0, avg_score -48.01\n",
      "Environment 2: episode 165, score -50.0, avg_score -48.01\n",
      "Environment 3: episode 165, score -50.0, avg_score -48.01\n",
      "Environment 4: episode 165, score -42.0, avg_score -47.93\n",
      "Environment 5: episode 165, score -50.0, avg_score -47.93\n",
      "Environment 6: episode 165, score -50.0, avg_score -47.93\n",
      "Environment 7: episode 165, score -50.0, avg_score -47.93\n",
      "Environment 8: episode 165, score -50.0, avg_score -47.93\n",
      "Environment 9: episode 165, score -50.0, avg_score -47.93\n",
      "Environment 10: episode 165, score -50.0, avg_score -47.97\n",
      "Environment 11: episode 165, score -50.0, avg_score -47.97\n",
      "Environment 12: episode 165, score -50.0, avg_score -47.97\n",
      "Environment 13: episode 165, score -50.0, avg_score -47.97\n",
      "Environment 14: episode 165, score -9.0, avg_score -47.74\n",
      "Environment 15: episode 165, score -50.0, avg_score -47.74\n",
      "Environment 0: episode 166, score -48.0, avg_score -47.72\n",
      "Environment 1: episode 166, score -50.0, avg_score -47.72\n",
      "Environment 2: episode 166, score -50.0, avg_score -47.72\n",
      "Environment 3: episode 166, score -50.0, avg_score -47.76\n",
      "Environment 4: episode 166, score -50.0, avg_score -47.88\n",
      "Environment 5: episode 166, score -50.0, avg_score -47.88\n",
      "Environment 6: episode 166, score -49.0, avg_score -47.95\n",
      "Environment 7: episode 166, score -50.0, avg_score -47.95\n",
      "Environment 8: episode 166, score -44.0, avg_score -47.89\n",
      "Environment 9: episode 166, score -50.0, avg_score -47.89\n",
      "Environment 10: episode 166, score -50.0, avg_score -47.89\n",
      "Environment 11: episode 166, score -50.0, avg_score -47.89\n",
      "Environment 12: episode 166, score -50.0, avg_score -47.9\n",
      "Environment 13: episode 166, score -50.0, avg_score -48.0\n",
      "Environment 14: episode 166, score -50.0, avg_score -48.0\n",
      "Environment 15: episode 166, score -50.0, avg_score -48.0\n",
      "Environment 0: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 1: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 2: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 3: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 4: episode 167, score -48.0, avg_score -47.98\n",
      "Environment 5: episode 167, score -50.0, avg_score -47.98\n",
      "Environment 6: episode 167, score -47.0, avg_score -47.95\n",
      "Environment 7: episode 167, score -50.0, avg_score -47.95\n",
      "Environment 8: episode 167, score -50.0, avg_score -47.95\n",
      "Environment 9: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 10: episode 167, score -49.0, avg_score -47.99\n",
      "Environment 11: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 12: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 13: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 14: episode 167, score -50.0, avg_score -48.0\n",
      "Environment 15: episode 167, score -29.0, avg_score -47.82\n",
      "Environment 0: episode 168, score -50.0, avg_score -48.07\n",
      "Environment 1: episode 168, score -50.0, avg_score -48.07\n",
      "Environment 2: episode 168, score -50.0, avg_score -48.07\n",
      "Environment 3: episode 168, score -50.0, avg_score -48.07\n",
      "Environment 4: episode 168, score -50.0, avg_score -48.07\n",
      "Environment 5: episode 168, score -50.0, avg_score -48.13\n",
      "Environment 6: episode 168, score -47.0, avg_score -48.1\n",
      "Environment 7: episode 168, score -50.0, avg_score -48.1\n",
      "Environment 8: episode 168, score -50.0, avg_score -48.1\n",
      "Environment 9: episode 168, score -19.0, avg_score -47.79\n",
      "Environment 10: episode 168, score -50.0, avg_score -47.79\n",
      "Environment 11: episode 168, score -50.0, avg_score -47.83\n",
      "Environment 12: episode 168, score -50.0, avg_score -48.17\n",
      "Environment 13: episode 168, score -50.0, avg_score -48.17\n",
      "Environment 14: episode 168, score -50.0, avg_score -48.17\n",
      "Environment 15: episode 168, score -50.0, avg_score -48.35\n",
      "Environment 0: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 1: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 2: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 3: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 4: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 5: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 6: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 7: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 8: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 9: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 10: episode 169, score -50.0, avg_score -48.35\n",
      "Rendering episode 2700.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2700.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2700.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2700.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 12: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 13: episode 169, score -50.0, avg_score -48.35\n",
      "Environment 14: episode 169, score -50.0, avg_score -48.5\n",
      "Environment 15: episode 169, score -50.0, avg_score -48.5\n",
      "Environment 0: episode 170, score -37.0, avg_score -48.37\n",
      "Environment 1: episode 170, score -50.0, avg_score -48.37\n",
      "Environment 2: episode 170, score -50.0, avg_score -48.37\n",
      "Environment 3: episode 170, score -50.0, avg_score -48.52\n",
      "Environment 4: episode 170, score -50.0, avg_score -48.54\n",
      "Environment 5: episode 170, score -50.0, avg_score -48.54\n",
      "Environment 6: episode 170, score -50.0, avg_score -48.54\n",
      "Environment 7: episode 170, score -50.0, avg_score -48.54\n",
      "Environment 8: episode 170, score -50.0, avg_score -48.54\n",
      "Environment 9: episode 170, score -24.0, avg_score -48.42\n",
      "Environment 10: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 11: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 12: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 13: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 14: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 15: episode 170, score -50.0, avg_score -48.42\n",
      "Environment 0: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 1: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 2: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 3: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 4: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 5: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 6: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 7: episode 171, score -50.0, avg_score -48.42\n",
      "Environment 8: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 9: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 10: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 11: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 12: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 13: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 14: episode 171, score -50.0, avg_score -48.5\n",
      "Environment 15: episode 171, score -34.0, avg_score -48.34\n",
      "Environment 0: episode 172, score -50.0, avg_score -48.34\n",
      "Environment 1: episode 172, score -50.0, avg_score -48.34\n",
      "Environment 2: episode 172, score -50.0, avg_score -48.75\n",
      "Environment 3: episode 172, score -50.0, avg_score -48.75\n",
      "Environment 4: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 5: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 6: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 7: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 8: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 9: episode 172, score -50.0, avg_score -48.77\n",
      "Environment 10: episode 172, score -50.0, avg_score -48.78\n",
      "Environment 11: episode 172, score -50.0, avg_score -48.78\n",
      "Environment 12: episode 172, score -50.0, avg_score -48.84\n",
      "Environment 13: episode 172, score -50.0, avg_score -48.84\n",
      "Environment 14: episode 172, score -48.0, avg_score -48.82\n",
      "Environment 15: episode 172, score -50.0, avg_score -48.82\n",
      "Environment 0: episode 173, score -50.0, avg_score -48.82\n",
      "Environment 1: episode 173, score -50.0, avg_score -48.82\n",
      "Environment 2: episode 173, score -50.0, avg_score -48.82\n",
      "Environment 3: episode 173, score -50.0, avg_score -48.82\n",
      "Environment 4: episode 173, score -37.0, avg_score -48.69\n",
      "Environment 5: episode 173, score -50.0, avg_score -48.69\n",
      "Environment 6: episode 173, score -50.0, avg_score -48.69\n",
      "Environment 7: episode 173, score -50.0, avg_score -48.69\n",
      "Environment 8: episode 173, score -50.0, avg_score -48.71\n",
      "Environment 9: episode 173, score -50.0, avg_score -48.71\n",
      "Environment 10: episode 173, score -50.0, avg_score -48.74\n",
      "Environment 11: episode 173, score -50.0, avg_score -48.74\n",
      "Environment 12: episode 173, score -50.0, avg_score -48.74\n",
      "Environment 13: episode 173, score -50.0, avg_score -48.74\n",
      "Environment 14: episode 173, score -50.0, avg_score -48.75\n",
      "Environment 15: episode 173, score -50.0, avg_score -48.75\n",
      "Environment 0: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 1: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 2: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 3: episode 174, score -50.0, avg_score -48.96\n",
      "Environment 4: episode 174, score -38.0, avg_score -48.84\n",
      "Environment 5: episode 174, score -50.0, avg_score -48.84\n",
      "Environment 6: episode 174, score -50.0, avg_score -48.84\n",
      "Environment 7: episode 174, score -38.0, avg_score -48.72\n",
      "Environment 8: episode 174, score -50.0, avg_score -48.72\n",
      "Environment 9: episode 174, score -50.0, avg_score -48.72\n",
      "Environment 10: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 11: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 12: episode 174, score -50.0, avg_score -48.75\n",
      "Environment 13: episode 174, score -50.0, avg_score -49.06\n",
      "Environment 14: episode 174, score -50.0, avg_score -49.06\n",
      "Environment 15: episode 174, score -50.0, avg_score -49.06\n",
      "Environment 0: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 1: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 2: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 3: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 4: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 5: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 6: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 7: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 8: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 9: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 10: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 11: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 12: episode 175, score -50.0, avg_score -49.06\n",
      "Environment 13: episode 175, score -48.0, avg_score -49.04\n",
      "Environment 14: episode 175, score -50.0, avg_score -49.04\n",
      "Rendering episode 2800.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2800.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2800.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2800.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 15: episode 175, score -50.0, avg_score -49.04\n",
      "Environment 0: episode 176, score -50.0, avg_score -49.04\n",
      "Environment 1: episode 176, score -41.0, avg_score -48.95\n",
      "Environment 2: episode 176, score -50.0, avg_score -48.95\n",
      "Environment 3: episode 176, score -47.0, avg_score -48.92\n",
      "Environment 4: episode 176, score -50.0, avg_score -49.05\n",
      "Environment 5: episode 176, score -50.0, avg_score -49.05\n",
      "Environment 6: episode 176, score -49.0, avg_score -49.04\n",
      "Environment 7: episode 176, score -22.0, avg_score -48.76\n",
      "Environment 8: episode 176, score -50.0, avg_score -48.76\n",
      "Environment 9: episode 176, score -50.0, avg_score -48.76\n",
      "Environment 10: episode 176, score -50.0, avg_score -48.76\n",
      "Environment 11: episode 176, score -50.0, avg_score -48.76\n",
      "Environment 12: episode 176, score -50.0, avg_score -48.76\n",
      "Environment 13: episode 176, score -39.0, avg_score -48.91\n",
      "Environment 14: episode 176, score -40.0, avg_score -48.81\n",
      "Environment 15: episode 176, score -50.0, avg_score -48.81\n",
      "Environment 0: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 1: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 2: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 3: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 4: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 5: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 6: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 7: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 8: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 9: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 10: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 11: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 12: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 13: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 14: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 15: episode 177, score -50.0, avg_score -48.81\n",
      "Environment 0: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 1: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 2: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 3: episode 178, score -50.0, avg_score -48.97\n",
      "Environment 4: episode 178, score -50.0, avg_score -48.97\n",
      "Environment 5: episode 178, score -50.0, avg_score -48.97\n",
      "Environment 6: episode 178, score -50.0, avg_score -48.97\n",
      "Environment 7: episode 178, score -34.0, avg_score -48.81\n",
      "Environment 8: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 9: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 10: episode 178, score -50.0, avg_score -48.81\n",
      "Environment 11: episode 178, score -28.0, avg_score -48.59\n",
      "Environment 12: episode 178, score -50.0, avg_score -48.59\n",
      "Environment 13: episode 178, score -36.0, avg_score -48.45\n",
      "Environment 14: episode 178, score -50.0, avg_score -48.45\n",
      "Environment 15: episode 178, score -50.0, avg_score -48.45\n",
      "Environment 0: episode 179, score -50.0, avg_score -48.45\n",
      "Environment 1: episode 179, score -34.0, avg_score -48.29\n",
      "Environment 2: episode 179, score -46.0, avg_score -48.27\n",
      "Environment 3: episode 179, score -50.0, avg_score -48.27\n",
      "Environment 4: episode 179, score -50.0, avg_score -48.27\n",
      "Environment 5: episode 179, score -50.0, avg_score -48.27\n",
      "Environment 6: episode 179, score -50.0, avg_score -48.27\n",
      "Environment 7: episode 179, score -50.0, avg_score -48.27\n",
      "Environment 8: episode 179, score -50.0, avg_score -48.4\n",
      "Environment 9: episode 179, score -50.0, avg_score -48.4\n",
      "Environment 10: episode 179, score -46.0, avg_score -48.36\n",
      "Environment 11: episode 179, score -50.0, avg_score -48.36\n",
      "Environment 12: episode 179, score -50.0, avg_score -48.36\n",
      "Environment 13: episode 179, score -50.0, avg_score -48.36\n",
      "Environment 14: episode 179, score -50.0, avg_score -48.36\n",
      "Environment 15: episode 179, score -50.0, avg_score -48.36\n",
      "Environment 0: episode 180, score -50.0, avg_score -48.36\n",
      "Environment 1: episode 180, score -50.0, avg_score -48.36\n",
      "Environment 2: episode 180, score -49.0, avg_score -48.35\n",
      "Environment 3: episode 180, score -50.0, avg_score -48.35\n",
      "Environment 4: episode 180, score -50.0, avg_score -48.35\n",
      "Environment 5: episode 180, score -50.0, avg_score -48.35\n",
      "Environment 6: episode 180, score -50.0, avg_score -48.35\n",
      "Environment 7: episode 180, score -50.0, avg_score -48.35\n",
      "Environment 8: episode 180, score -50.0, avg_score -48.47\n",
      "Environment 9: episode 180, score -50.0, avg_score -48.47\n",
      "Environment 10: episode 180, score -32.0, avg_score -48.29\n",
      "Environment 11: episode 180, score -45.0, avg_score -48.36\n",
      "Environment 12: episode 180, score -50.0, avg_score -48.36\n",
      "Environment 13: episode 180, score -26.0, avg_score -48.12\n",
      "Environment 14: episode 180, score -50.0, avg_score -48.12\n",
      "Environment 15: episode 180, score -50.0, avg_score -48.12\n",
      "Environment 0: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 1: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 2: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 3: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 4: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 5: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 6: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 7: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 8: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 9: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 10: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 11: episode 181, score -50.0, avg_score -48.12\n",
      "Environment 12: episode 181, score -30.0, avg_score -47.92\n",
      "Environment 13: episode 181, score -50.0, avg_score -47.92\n",
      "Environment 14: episode 181, score -50.0, avg_score -47.92\n",
      "Environment 15: episode 181, score -50.0, avg_score -47.92\n",
      "Environment 0: episode 182, score -50.0, avg_score -47.92\n",
      "Environment 1: episode 182, score -50.0, avg_score -47.94\n",
      "Environment 2: episode 182, score -50.0, avg_score -47.94\n",
      "Rendering episode 2900.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_2900.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_2900.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_2900.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 3: episode 182, score -50.0, avg_score -47.94\n",
      "Environment 4: episode 182, score -50.0, avg_score -47.94\n",
      "Environment 5: episode 182, score -50.0, avg_score -48.03\n",
      "Environment 6: episode 182, score -24.0, avg_score -47.77\n",
      "Environment 7: episode 182, score -50.0, avg_score -47.8\n",
      "Environment 8: episode 182, score -50.0, avg_score -47.8\n",
      "Environment 9: episode 182, score -50.0, avg_score -47.8\n",
      "Environment 10: episode 182, score -50.0, avg_score -47.81\n",
      "Environment 11: episode 182, score -50.0, avg_score -48.09\n",
      "Environment 12: episode 182, score -50.0, avg_score -48.09\n",
      "Environment 13: episode 182, score -50.0, avg_score -48.09\n",
      "Environment 14: episode 182, score -24.0, avg_score -47.83\n",
      "Environment 15: episode 182, score -50.0, avg_score -47.83\n",
      "Environment 0: episode 183, score -50.0, avg_score -47.83\n",
      "Environment 1: episode 183, score -50.0, avg_score -47.94\n",
      "Environment 2: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 3: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 4: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 5: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 6: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 7: episode 183, score -50.0, avg_score -48.04\n",
      "Environment 8: episode 183, score -44.0, avg_score -47.98\n",
      "Environment 9: episode 183, score -50.0, avg_score -47.98\n",
      "Environment 10: episode 183, score -34.0, avg_score -47.82\n",
      "Environment 11: episode 183, score -50.0, avg_score -47.82\n",
      "Environment 12: episode 183, score -49.0, avg_score -47.81\n",
      "Environment 13: episode 183, score -50.0, avg_score -47.81\n",
      "Environment 14: episode 183, score -50.0, avg_score -47.81\n",
      "Environment 15: episode 183, score -50.0, avg_score -47.81\n",
      "Environment 0: episode 184, score -40.0, avg_score -47.71\n",
      "Environment 1: episode 184, score -50.0, avg_score -47.71\n",
      "Environment 2: episode 184, score -50.0, avg_score -47.71\n",
      "Environment 3: episode 184, score -49.0, avg_score -47.7\n",
      "Environment 4: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 5: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 6: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 7: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 8: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 9: episode 184, score -50.0, avg_score -47.7\n",
      "Environment 10: episode 184, score -48.0, avg_score -47.68\n",
      "Environment 11: episode 184, score -50.0, avg_score -47.84\n",
      "Environment 12: episode 184, score -50.0, avg_score -47.84\n",
      "Environment 13: episode 184, score -50.0, avg_score -47.84\n",
      "Environment 14: episode 184, score -49.0, avg_score -47.83\n",
      "Environment 15: episode 184, score -48.0, avg_score -48.03\n",
      "Environment 0: episode 185, score -50.0, avg_score -48.03\n",
      "Environment 1: episode 185, score -50.0, avg_score -48.17\n",
      "Environment 2: episode 185, score -33.0, avg_score -48.0\n",
      "Environment 3: episode 185, score -50.0, avg_score -48.0\n",
      "Environment 4: episode 185, score -50.0, avg_score -48.0\n",
      "Environment 5: episode 185, score -50.0, avg_score -48.16\n",
      "Environment 6: episode 185, score -50.0, avg_score -48.2\n",
      "Environment 7: episode 185, score -50.0, avg_score -48.2\n",
      "Environment 8: episode 185, score -50.0, avg_score -48.2\n",
      "Environment 9: episode 185, score -50.0, avg_score -48.2\n",
      "Environment 10: episode 185, score -50.0, avg_score -48.2\n",
      "Environment 11: episode 185, score -34.0, avg_score -48.04\n",
      "Environment 12: episode 185, score -50.0, avg_score -48.04\n",
      "Environment 13: episode 185, score -50.0, avg_score -48.04\n",
      "Environment 14: episode 185, score -50.0, avg_score -48.08\n",
      "Environment 15: episode 185, score -50.0, avg_score -48.08\n",
      "Environment 0: episode 186, score -41.0, avg_score -47.99\n",
      "Environment 1: episode 186, score -50.0, avg_score -47.99\n",
      "Environment 2: episode 186, score -50.0, avg_score -47.99\n",
      "Environment 3: episode 186, score -50.0, avg_score -47.99\n",
      "Environment 4: episode 186, score -50.0, avg_score -47.99\n",
      "Environment 5: episode 186, score -50.0, avg_score -47.99\n",
      "Environment 6: episode 186, score -50.0, avg_score -48.0\n",
      "Environment 7: episode 186, score -50.0, avg_score -48.0\n",
      "Environment 8: episode 186, score -50.0, avg_score -48.0\n",
      "Environment 9: episode 186, score -50.0, avg_score -48.0\n",
      "Environment 10: episode 186, score -41.0, avg_score -47.91\n",
      "Environment 11: episode 186, score -50.0, avg_score -47.91\n",
      "Environment 12: episode 186, score -50.0, avg_score -47.91\n",
      "Environment 13: episode 186, score -50.0, avg_score -47.91\n",
      "Environment 14: episode 186, score -50.0, avg_score -48.09\n",
      "Environment 15: episode 186, score -50.0, avg_score -48.14\n",
      "Environment 0: episode 187, score -50.0, avg_score -48.14\n",
      "Environment 1: episode 187, score -50.0, avg_score -48.38\n",
      "Environment 2: episode 187, score -27.0, avg_score -48.15\n",
      "Environment 3: episode 187, score -50.0, avg_score -48.15\n",
      "Environment 4: episode 187, score -50.0, avg_score -48.15\n",
      "Environment 5: episode 187, score -50.0, avg_score -48.15\n",
      "Environment 6: episode 187, score -50.0, avg_score -48.15\n",
      "Environment 7: episode 187, score -50.0, avg_score -48.15\n",
      "Environment 8: episode 187, score -49.0, avg_score -48.14\n",
      "Environment 9: episode 187, score -50.0, avg_score -48.14\n",
      "Environment 10: episode 187, score -50.0, avg_score -48.14\n",
      "Environment 11: episode 187, score -41.0, avg_score -48.05\n",
      "Environment 12: episode 187, score -50.0, avg_score -48.05\n",
      "Environment 13: episode 187, score -47.0, avg_score -48.02\n",
      "Environment 14: episode 187, score -24.0, avg_score -47.76\n",
      "Environment 15: episode 187, score -50.0, avg_score -47.76\n",
      "Environment 0: episode 188, score -50.0, avg_score -47.96\n",
      "Environment 1: episode 188, score -15.0, avg_score -47.61\n",
      "Environment 2: episode 188, score -50.0, avg_score -47.61\n",
      "Environment 3: episode 188, score -50.0, avg_score -47.61\n",
      "Environment 4: episode 188, score -50.0, avg_score -47.61\n",
      "Environment 5: episode 188, score -50.0, avg_score -47.61\n",
      "Environment 6: episode 188, score -50.0, avg_score -47.61\n",
      "Rendering episode 3000.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_3000.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_3000.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_3000.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 7: episode 188, score -37.0, avg_score -47.48\n",
      "Environment 8: episode 188, score -50.0, avg_score -47.48\n",
      "Environment 9: episode 188, score -50.0, avg_score -47.48\n",
      "Environment 10: episode 188, score -50.0, avg_score -47.74\n",
      "Environment 11: episode 188, score -45.0, avg_score -47.69\n",
      "Environment 12: episode 188, score -39.0, avg_score -47.58\n",
      "Environment 13: episode 188, score -46.0, avg_score -47.54\n",
      "Environment 14: episode 188, score -50.0, avg_score -47.54\n",
      "Environment 15: episode 188, score -50.0, avg_score -47.54\n",
      "Environment 0: episode 189, score -50.0, avg_score -47.54\n",
      "Environment 1: episode 189, score -50.0, avg_score -47.54\n",
      "Environment 2: episode 189, score -50.0, avg_score -47.8\n",
      "Environment 3: episode 189, score -50.0, avg_score -47.8\n",
      "Environment 4: episode 189, score -38.0, avg_score -47.68\n",
      "Environment 5: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 6: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 7: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 8: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 9: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 10: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 11: episode 189, score -50.0, avg_score -47.68\n",
      "Environment 12: episode 189, score -22.0, avg_score -47.46\n",
      "Environment 13: episode 189, score -50.0, avg_score -47.46\n",
      "Environment 14: episode 189, score -50.0, avg_score -47.62\n",
      "Environment 15: episode 189, score -50.0, avg_score -47.62\n",
      "Environment 0: episode 190, score -50.0, avg_score -47.63\n",
      "Environment 1: episode 190, score -50.0, avg_score -47.63\n",
      "Environment 2: episode 190, score -50.0, avg_score -47.63\n",
      "Environment 3: episode 190, score -50.0, avg_score -47.63\n",
      "Environment 4: episode 190, score -50.0, avg_score -47.73\n",
      "Environment 5: episode 190, score -50.0, avg_score -47.73\n",
      "Environment 6: episode 190, score -50.0, avg_score -47.73\n",
      "Environment 7: episode 190, score -50.0, avg_score -47.74\n",
      "Environment 8: episode 190, score -50.0, avg_score -47.74\n",
      "Environment 9: episode 190, score -50.0, avg_score -47.74\n",
      "Environment 10: episode 190, score -50.0, avg_score -47.74\n",
      "Environment 11: episode 190, score -50.0, avg_score -47.74\n",
      "Environment 12: episode 190, score -41.0, avg_score -47.65\n",
      "Environment 13: episode 190, score -50.0, avg_score -47.65\n",
      "Environment 14: episode 190, score -50.0, avg_score -47.67\n",
      "Environment 15: episode 190, score -50.0, avg_score -47.67\n",
      "Environment 0: episode 191, score -29.0, avg_score -47.46\n",
      "Environment 1: episode 191, score -50.0, avg_score -47.46\n",
      "Environment 2: episode 191, score -50.0, avg_score -47.47\n",
      "Environment 3: episode 191, score -50.0, avg_score -47.49\n",
      "Environment 4: episode 191, score -50.0, avg_score -47.49\n",
      "Environment 5: episode 191, score -50.0, avg_score -47.49\n",
      "Environment 6: episode 191, score -50.0, avg_score -47.66\n",
      "Environment 7: episode 191, score -48.0, avg_score -47.64\n",
      "Environment 8: episode 191, score -41.0, avg_score -47.55\n",
      "Environment 9: episode 191, score -50.0, avg_score -47.55\n",
      "Environment 10: episode 191, score -50.0, avg_score -47.55\n",
      "Environment 11: episode 191, score -47.0, avg_score -47.52\n",
      "Environment 12: episode 191, score -50.0, avg_score -47.52\n",
      "Environment 13: episode 191, score -38.0, avg_score -47.4\n",
      "Environment 14: episode 191, score -48.0, avg_score -47.38\n",
      "Environment 15: episode 191, score -50.0, avg_score -47.54\n",
      "Environment 0: episode 192, score -34.0, avg_score -47.38\n",
      "Environment 1: episode 192, score -50.0, avg_score -47.38\n",
      "Environment 2: episode 192, score -50.0, avg_score -47.38\n",
      "Environment 3: episode 192, score -40.0, avg_score -47.28\n",
      "Environment 4: episode 192, score -38.0, avg_score -47.25\n",
      "Environment 5: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 6: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 7: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 8: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 9: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 10: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 11: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 12: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 13: episode 192, score -50.0, avg_score -47.25\n",
      "Environment 14: episode 192, score -50.0, avg_score -47.34\n",
      "Environment 15: episode 192, score -50.0, avg_score -47.34\n",
      "Environment 0: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 1: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 2: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 3: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 4: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 5: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 6: episode 193, score -50.0, avg_score -47.57\n",
      "Environment 7: episode 193, score -50.0, avg_score -47.57\n",
      "Environment 8: episode 193, score -50.0, avg_score -47.57\n",
      "Environment 9: episode 193, score -35.0, avg_score -47.42\n",
      "Environment 10: episode 193, score -37.0, avg_score -47.29\n",
      "Environment 11: episode 193, score -50.0, avg_score -47.29\n",
      "Environment 12: episode 193, score -50.0, avg_score -47.3\n",
      "Environment 13: episode 193, score -50.0, avg_score -47.3\n",
      "Environment 14: episode 193, score -45.0, avg_score -47.25\n",
      "Environment 15: episode 193, score -50.0, avg_score -47.34\n",
      "Environment 0: episode 194, score -38.0, avg_score -47.22\n",
      "Environment 1: episode 194, score -50.0, avg_score -47.25\n",
      "Environment 2: episode 194, score -50.0, avg_score -47.51\n",
      "Environment 3: episode 194, score -50.0, avg_score -47.51\n",
      "Environment 4: episode 194, score -50.0, avg_score -47.51\n",
      "Environment 5: episode 194, score -50.0, avg_score -47.86\n",
      "Environment 6: episode 194, score -50.0, avg_score -47.86\n",
      "Environment 7: episode 194, score -50.0, avg_score -47.86\n",
      "Environment 8: episode 194, score -50.0, avg_score -47.86\n",
      "Environment 9: episode 194, score -50.0, avg_score -47.86\n",
      "Environment 10: episode 194, score -50.0, avg_score -47.86\n",
      "Rendering episode 3100.0 during training...\n",
      "rendering episode...\n",
      "Moviepy - Building video models/her/renders/train/episode_3100.0.mp4.\n",
      "Moviepy - Writing video models/her/renders/train/episode_3100.0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready models/her/renders/train/episode_3100.0.mp4\n",
      "episode rendered\n",
      "Environment 0: Episode 1/1 Score: -50.0 Avg Score: -50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 11: episode 194, score -32.0, avg_score -47.81\n",
      "Environment 12: episode 194, score -50.0, avg_score -47.81\n",
      "Environment 13: episode 194, score -50.0, avg_score -47.81\n",
      "Environment 14: episode 194, score -46.0, avg_score -47.77\n",
      "Environment 15: episode 194, score -50.0, avg_score -47.82\n",
      "Environment 0: episode 195, score -50.0, avg_score -47.93\n",
      "Environment 1: episode 195, score -41.0, avg_score -47.88\n",
      "Environment 2: episode 195, score -50.0, avg_score -47.88\n",
      "Environment 3: episode 195, score -50.0, avg_score -47.88\n",
      "Environment 4: episode 195, score -50.0, avg_score -47.88\n",
      "Environment 5: episode 195, score -50.0, avg_score -47.88\n",
      "Environment 6: episode 195, score -45.0, avg_score -47.83\n",
      "Environment 7: episode 195, score -50.0, avg_score -47.83\n",
      "Environment 8: episode 195, score -50.0, avg_score -47.95\n",
      "Environment 9: episode 195, score -50.0, avg_score -47.95\n",
      "Environment 10: episode 195, score -35.0, avg_score -47.8\n",
      "Environment 11: episode 195, score -41.0, avg_score -47.71\n",
      "Environment 12: episode 195, score -50.0, avg_score -47.71\n",
      "Environment 13: episode 195, score -50.0, avg_score -47.71\n",
      "Environment 14: episode 195, score -43.0, avg_score -47.64\n",
      "Environment 15: episode 195, score -42.0, avg_score -47.56\n",
      "Environment 0: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 1: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 2: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 3: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 4: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 5: episode 196, score -50.0, avg_score -47.84\n",
      "Environment 6: episode 196, score -48.0, avg_score -47.82\n",
      "Environment 7: episode 196, score -36.0, avg_score -47.68\n",
      "Environment 8: episode 196, score -50.0, avg_score -47.68\n",
      "Environment 9: episode 196, score -50.0, avg_score -47.68\n",
      "Environment 10: episode 196, score -42.0, avg_score -47.6\n",
      "Environment 11: episode 196, score -50.0, avg_score -47.6\n",
      "Environment 12: episode 196, score -50.0, avg_score -47.6\n",
      "Environment 13: episode 196, score -50.0, avg_score -47.6\n",
      "Environment 14: episode 196, score -50.0, avg_score -47.6\n",
      "Environment 15: episode 196, score -50.0, avg_score -47.6\n",
      "Environment 0: episode 197, score -50.0, avg_score -47.69\n",
      "Environment 1: episode 197, score -39.0, avg_score -47.58\n",
      "Environment 2: episode 197, score -50.0, avg_score -47.58\n",
      "Environment 3: episode 197, score -50.0, avg_score -47.58\n",
      "Environment 4: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 5: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 6: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 7: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 8: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 9: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 10: episode 197, score -50.0, avg_score -47.79\n",
      "Environment 11: episode 197, score -47.0, avg_score -47.78\n",
      "Environment 12: episode 197, score -44.0, avg_score -47.81\n",
      "Environment 13: episode 197, score -50.0, avg_score -47.81\n",
      "Environment 14: episode 197, score -50.0, avg_score -47.81\n",
      "Environment 15: episode 197, score -50.0, avg_score -47.84\n",
      "Environment 0: episode 198, score -45.0, avg_score -47.79\n",
      "Environment 1: episode 198, score -44.0, avg_score -47.85\n",
      "Environment 2: episode 198, score -50.0, avg_score -47.87\n",
      "Environment 3: episode 198, score -50.0, avg_score -47.87\n",
      "Environment 4: episode 198, score -50.0, avg_score -48.03\n",
      "Environment 5: episode 198, score -43.0, avg_score -47.96\n",
      "Environment 6: episode 198, score -50.0, avg_score -47.96\n",
      "Environment 7: episode 198, score -43.0, avg_score -47.99\n",
      "Environment 8: episode 198, score -50.0, avg_score -48.11\n",
      "Environment 9: episode 198, score -50.0, avg_score -48.11\n",
      "Environment 10: episode 198, score -50.0, avg_score -48.11\n",
      "Environment 11: episode 198, score -37.0, avg_score -47.98\n",
      "Environment 12: episode 198, score -38.0, avg_score -47.86\n",
      "Environment 13: episode 198, score -49.0, avg_score -47.85\n",
      "Environment 14: episode 198, score -50.0, avg_score -47.85\n",
      "Environment 15: episode 198, score -50.0, avg_score -47.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m num_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      7\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cycles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/RL_Agents/src/app/rl_agents.py:4359\u001b[0m, in \u001b[0;36mHER.train\u001b[0;34m(self, num_epochs, num_cycles, num_episodes_per_cycle, num_updates, render_freq, num_envs, seed)\u001b[0m\n\u001b[1;32m   4351\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mget_action(\n\u001b[1;32m   4352\u001b[0m     states[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   4353\u001b[0m     states[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesired_goal\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4356\u001b[0m     goal_normalizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_normalizer\n\u001b[1;32m   4357\u001b[0m )\n\u001b[1;32m   4358\u001b[0m formatted_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mformat_actions(actions)\n\u001b[0;32m-> 4359\u001b[0m next_states, rewards, terms, truncs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4360\u001b[0m dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terms, truncs)\n\u001b[1;32m   4361\u001b[0m episode_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m/workspaces/RL_Agents/src/app/env_wrapper.py:304\u001b[0m, in \u001b[0;36mGymnasiumWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    Take an action in the environment.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m        Tuple: Observation, reward, done flag, and additional info.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py:222\u001b[0m, in \u001b[0;36mSyncVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     (\n\u001b[1;32m    217\u001b[0m         env_obs,\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminations[i],\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i],\n\u001b[1;32m    221\u001b[0m         env_info,\n\u001b[0;32m--> 222\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m observations\u001b[38;5;241m.\u001b[39mappend(env_obs)\n\u001b[1;32m    225\u001b[0m infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_info(infos, env_info, i)\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium_robotics/envs/robot_env.py:135\u001b[0m, in \u001b[0;36mBaseRobotEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_action(action)\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mujoco_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_callback()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/rl_env/lib/python3.10/site-packages/gymnasium_robotics/envs/robot_env.py:341\u001b[0m, in \u001b[0;36mMujocoRobotEnv._mujoco_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_mujoco_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_substeps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_cycles = 50\n",
    "num_episodes = 1\n",
    "num_updates = 40\n",
    "render_freq = 100\n",
    "num_envs = 16\n",
    "seed = 42\n",
    "\n",
    "her.train(num_epochs, num_cycles, num_episodes, num_updates, render_freq, num_envs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.unique(her.agent.replay_buffer.states, dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.replay_buffer.states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.count_nonzero(her.agent.replay_buffer.states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/FetchPickAndPlace_HER_DDPG_PER_b/her/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = HER.load(config, load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.replay_buffer.sum_tree.tree[her.agent.replay_buffer.sum_tree.capacity-1:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "num_cycles = 50\n",
    "num_episodes = 1\n",
    "num_updates = 40\n",
    "render_freq = 100\n",
    "num_envs = 16\n",
    "seed = 42\n",
    "\n",
    "her.train(num_epochs, num_cycles, num_episodes, num_updates, render_freq, num_envs, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v4')\n",
    "env_spec = env.spec\n",
    "env_wrap = GymnasiumWrapper(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrap.env_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v3\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "env_id = 'Humanoid-v5'\n",
    "# env_id = \"Reacher-v5\"\n",
    "# env_id = \"Walker2d-v5\"\n",
    "# env_id = 'ALE/SpaceInvaders-ram-v5'\n",
    "# env_id = \"CarRacing-v2\"\n",
    "# env_id = \"BipedalWalkerHardcore-v3\"\n",
    "\n",
    "timesteps = 1_000_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 16\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "policy_clip = 0.2\n",
    "entropy_coeff = 0.001\n",
    "loss = 'hybrid'\n",
    "kl_coeff = 0.0\n",
    "normalize_advantages = True\n",
    "normalize_values = False\n",
    "norm_clip = np.inf\n",
    "grad_clip = 40.0\n",
    "reward_clip = 1.0\n",
    "lambda_ = 0.0\n",
    "distribution = 'beta'\n",
    "device = 'cuda'\n",
    "\n",
    "# Render Settings\n",
    "render_freq = 100\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'Humanoid-v5'\n",
    "run_name = None\n",
    "callbacks = [WandbCallback(project_name, run_name)]\n",
    "# callbacks = []\n",
    "\n",
    "seed = 42\n",
    "env = gym.make(env_id)\n",
    "\n",
    "save_dir = 'Humanoid'\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "\n",
    "# Build policy model\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "layer_config = [\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 32, 'kernel_size': (8, 8), 'stride': 4, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (4, 4), 'stride': 2, 'padding': 0}},\n",
    "    # {'type': 'cnn', 'params': {'out_channels': 64, 'kernel_size': (3, 3), 'stride': 1, 'padding': 0}},\n",
    "    # {'type': 'flatten'},\n",
    "    {'type': 'dense', 'params': {'units': 128, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "    {'type': 'dense', 'params': {'units': 64, 'kernel': 'default', 'kernel params':{}}},\n",
    "    {'type': 'tanh'},\n",
    "]\n",
    "output_layer_kernel = {'type': 'dense', 'params': {'kernel': 'default', 'kernel params':{}}},\n",
    "policy = StochasticContinuousPolicy(env, layer_config, output_layer_kernel, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "# dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, layer_config, output_layer_kernel, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=policy_clip, entropy_coefficient=entropy_coeff,\n",
    "          loss=loss, kl_coefficient=kl_coeff, normalize_advantages=normalize_advantages, normalize_values=normalize_values, value_normalizer_clip=norm_clip, policy_grad_clip=grad_clip,\n",
    "          reward_clip=reward_clip, lambda_=lambda_, callbacks=callbacks, save_dir=save_dir,device=device)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs, seed=seed, render_freq=render_freq)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/pong_v5_3/ppo/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['wrappers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = PPO.load(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.env = pong.env._initialize_env(num_envs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 2\n",
    "action_shape = (3,1)\n",
    "obs_shape = (3,)\n",
    "\n",
    "observation_space = gym.spaces.Box(low=0, high=1, shape=(num_envs, *obs_shape))\n",
    "action_space = gym.spaces.Box(low=0, high=1, shape=(num_envs, *action_shape)) if len(action_shape) > 1 else gym.spaces.MultiDiscrete([action_shape[0] for n in range(num_envs)])\n",
    "single_observation_space = gym.spaces.Box(low=0, high=1, shape=obs_shape)\n",
    "single_action_space = gym.spaces.Box(low=0, high=1, shape=action_shape) if len(action_shape) > 1 else gym.spaces.Discrete(action_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_obs = T.tensor(single_observation_space.sample())\n",
    "state, info = (T.stack([single_obs for _ in range(observation_space.shape[0])]), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = T.stack([single_obs for _ in range(observation_space.shape[0])])\n",
    "reward = T.zeros(observation_space.shape[0])\n",
    "terminated = T.zeros(observation_space.shape[0], dtype=T.bool)\n",
    "truncated = T.zeros(observation_space.shape[0], dtype=T.bool)\n",
    "info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = gym.make_vec(\"LunarLanderContinuous-v3\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ones(vec_env.single_action_space.shape).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "num_envs = 2\n",
    "expected_mu = T.stack([T.tensor([1.65, 1.65, 1.65]) for t in range(num_envs)])\n",
    "expected_sigma = T.stack([T.tensor([3.9, 3.9, 3.9]) for t in range(num_envs)])\n",
    "expected_dist = Normal(expected_mu, expected_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_dist.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.train(2000000, 128, 32, 3, 12, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[1] = 1\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium.wrappers as base_wrappers\n",
    "\n",
    "WRAPPER_REGISTRY = {\n",
    "    \"AtariPreprocessing\": {\n",
    "        \"cls\": base_wrappers.AtariPreprocessing,\n",
    "        \"default_params\": {\n",
    "            \"frame_skip\": 1,\n",
    "            \"grayscale_obs\": True,\n",
    "            \"scale_obs\": True\n",
    "        }\n",
    "    },\n",
    "    \"TimeLimit\": {\n",
    "        \"cls\": base_wrappers.TimeLimit,\n",
    "        \"default_params\": {\n",
    "            \"max_episode_steps\": 1000\n",
    "        }\n",
    "    },\n",
    "    \"TimeAwareObservation\": {\n",
    "        \"cls\": base_wrappers.TimeAwareObservation,\n",
    "        \"default_params\": {\n",
    "            \"flatten\": False,\n",
    "            \"normalize_time\": False\n",
    "        }\n",
    "    },\n",
    "    \"FrameStackObservation\": {\n",
    "        \"cls\": base_wrappers.FrameStackObservation,\n",
    "        \"default_params\": {\n",
    "            \"stack_size\": 4\n",
    "        }\n",
    "    },\n",
    "    \"ResizeObservation\": {\n",
    "        \"cls\": base_wrappers.ResizeObservation,\n",
    "        \"default_params\": {\n",
    "            \"shape\": 84\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrappers = [\n",
    "    {'type': \"AtariPreprocessing\", 'params': {'frame_skip':1, 'grayscale_obs':True, 'scale_obs':True}},\n",
    "    {'type': \"FrameStackObservation\", 'params': {'stack_size':4}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_env(vec_env, wrappers):\n",
    "    wrapper_list = []\n",
    "    for wrapper in wrappers:\n",
    "        if wrapper['type'] in WRAPPER_REGISTRY:\n",
    "            print(f'wrapper type:{wrapper[\"type\"]}')\n",
    "            # Use a copy of default_params to avoid modifying the registry\n",
    "            default_params = WRAPPER_REGISTRY[wrapper['type']][\"default_params\"].copy()\n",
    "            \n",
    "            if wrapper['type'] == \"ResizeObservation\":\n",
    "                # Ensure shape is a tuple for ResizeObservation\n",
    "                default_params['shape'] = (default_params['shape'], default_params['shape']) if isinstance(default_params['shape'], int) else default_params['shape']\n",
    "            \n",
    "            print(f'default params:{default_params}')\n",
    "            override_params = wrapper.get(\"params\", {})\n",
    "            \n",
    "            if wrapper['type'] == \"ResizeObservation\":\n",
    "                # Ensure override_params shape is a tuple\n",
    "                if 'shape' in override_params:\n",
    "                    override_params['shape'] = (override_params['shape'], override_params['shape']) if isinstance(override_params['shape'], int) else override_params['shape']\n",
    "            \n",
    "            print(f'override params:{override_params}')\n",
    "            final_params = {**default_params, **override_params}\n",
    "            print(f'final params:{final_params}')\n",
    "            \n",
    "            def wrapper_factory(env, cls=WRAPPER_REGISTRY[wrapper['type']][\"cls\"], params=final_params):\n",
    "                return cls(env, **params)\n",
    "            \n",
    "            wrapper_list.append(wrapper_factory)\n",
    "    \n",
    "    # Define apply_wrappers outside the loop\n",
    "    def apply_wrappers(env):\n",
    "        for wrapper in wrapper_list:\n",
    "            env = wrapper(env)\n",
    "            print(f'length of obs space:{len(env.observation_space.shape)}')\n",
    "            print(f'env obs space shape:{env.observation_space.shape}')\n",
    "        return env\n",
    "    \n",
    "    print(f'wrapper list:{wrapper_list}')\n",
    "    envs = [lambda: apply_wrappers(gym.make(vec_env.spec.id, render_mode=\"rgb_array\")) for _ in range(vec_env.num_envs)]    \n",
    "    return SyncVectorEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = gym.make_vec(\"ALE/Pong-v5\", render_mode=\"rgb_array\", num_envs=8)\n",
    "wrapped_vec = wrap_env(vec_env, wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_vec.single_observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in wrapped_vec.envs:\n",
    "    print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_wrappers(wrapper_store):\n",
    "    wrappers_dict = {}\n",
    "    for key, value in wrapper_store.items():\n",
    "        # Split the key into wrapper type and parameter name\n",
    "        parts = key.split('_param:')\n",
    "        print(f'parts:{parts}')\n",
    "        wrapper_type = parts[0].split('wrapper:')[1]\n",
    "        print(f'wrapper_type:{wrapper_type}')\n",
    "        param_name = parts[1]\n",
    "        print(f'param name:{param_name}')\n",
    "        \n",
    "        # If the wrapper type already exists in the dictionary, append to its params\n",
    "        if wrapper_type not in wrappers_dict:\n",
    "            wrappers_dict[wrapper_type] = {'type': wrapper_type, 'params': {}}\n",
    "        \n",
    "        wrappers_dict[wrapper_type]['params'][param_name] = value\n",
    "    \n",
    "    # Convert the dictionary to a list of dictionaries\n",
    "    formatted_wrappers = list(wrappers_dict.values())\n",
    "    \n",
    "    return formatted_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_params = {'wrapper:AtariPreprocessing_param:frame_skip': 1, 'wrapper:AtariPreprocessing_param:grayscale_obs': True, 'wrapper:AtariPreprocessing_param:scale_obs': True, 'wrapper:FrameStackObservation_param:stack_size': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_wrappers = format_wrappers(wrapper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_params = {'wrapper:AtariPreprocessing_param:frame_skip': 1, 'wrapper:AtariPreprocessing_param:grayscale_obs': True, 'wrapper:AtariPreprocessing_param:scale_obs': True, 'wrapper:FrameStackObservation_param:stack_size': 4}\n",
    "formatted_wrappers = dash_utils.format_wrappers(wrapper_params)\n",
    "#DEBUG\n",
    "print(f'formatted wrappers:{formatted_wrappers}')\n",
    "env = dash_utils.instantiate_envwrapper_obj(\"gymnasium\", \"ALE/Pong-v5\", formatted_wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '/workspaces/RL_Agents/src/app/humanoid_v5_2/ppo/config.json'\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "ppo = PPO.load(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.env.env = ppo.env._initialize_env(0, 8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in ppo.env.env.envs:\n",
    "    print(env.spec.pprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.train(2_000_000, 128, 64, 10, 8, 42, render_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, _ = ppo.env.reset()\n",
    "steps = 10\n",
    "all_states = []\n",
    "all_next_states = []\n",
    "for step in range(steps):\n",
    "    actions, log_probs = ppo.get_action(states)\n",
    "    next_states, rewards, terms, truncs, infos = ppo.env.step(actions)\n",
    "    all_states.append(states)\n",
    "    all_next_states.append(next_states)\n",
    "    states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, step_states in enumerate(all_states):\n",
    "    print(f'step states shape:{step_states.shape}')\n",
    "    for i in range(len(step_states)):\n",
    "        for j in range(i + 1, len(step_states)):  # Compare each environment with others\n",
    "            print(f'step state {i} shape:{step_states[i].shape}')\n",
    "            print(f'step state {j} shape:{step_states[j].shape}')\n",
    "            assert np.allclose(step_states[i], step_states[j]), f\"Environments {i} and {j} differ at step {step}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_states)):\n",
    "    for j in range(i + 1, len(all_states)):  # Note the change here\n",
    "        print(np.allclose(all_states[i], all_states[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = []\n",
    "obs = np.ones((8,1,84,84))\n",
    "for _ in range(10):\n",
    "    all_obs.append(obs)\n",
    "# all_obs = np.array(all_obs)\n",
    "all_obs = T.stack([T.tensor(s, dtype=T.float32) for s in all_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Box(low=0, high=1, shape=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_advantages = []\n",
    "all_returns = []\n",
    "all_values = []\n",
    "advantage = T.ones(128)\n",
    "return_ = T.ones(128)\n",
    "value = T.ones(128)\n",
    "num_envs = 2\n",
    "\n",
    "for _ in range(num_envs):\n",
    "    all_advantages.append(advantage)\n",
    "    all_returns.append(return_)\n",
    "    all_values.append(value)\n",
    "\n",
    "advantages = T.stack(all_advantages, dim=1)\n",
    "returns = T.stack(all_returns, dim=1)\n",
    "values = T.stack(all_values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = pong.env.reset()\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns, r, term, trunc, _ = pong.env.step(pong.env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.env.envs[0].spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = pong.env.reset()\n",
    "states = T.tensor(states)\n",
    "dist, _ = pong.policy_model(states)\n",
    "sample = dist.sample()\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_reward(reward):\n",
    "    \"\"\"\n",
    "    Clip rewards to the specified range.\n",
    "\n",
    "    Args:\n",
    "        reward (float): Reward to clip.\n",
    "\n",
    "    Returns:\n",
    "        float: Clipped reward.\n",
    "    \"\"\"\n",
    "    if reward > 1:\n",
    "        return 1\n",
    "    elif reward < -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make_vec(\"ALE/Pong-v5\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "all_dones = []\n",
    "for _ in range(10):\n",
    "    next_states, rewards, terms, truncs, infos = env.step(env.action_space.sample())\n",
    "    all_rewards.append(rewards)\n",
    "    all_dones.append(np.logical_or(terms, truncs))\n",
    "rewards = T.stack([T.tensor(r, dtype=T.float32) for r in all_rewards])\n",
    "dones = T.stack([T.tensor(d, dtype=T.float32) for d in all_dones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[clip_reward(reward) for reward in rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 6000  # Total steps\n",
    "eta_max = 1.0  # Initial noise stddev\n",
    "eta_min = 0.1  # Minimum noise stddev\n",
    "\n",
    "t = np.linspace(0, T_max, 1000)  # Sample points\n",
    "value = eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(t * np.pi / T_max))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t, value, 'b-', label='Cosine Annealing (stddev)')\n",
    "plt.axhline(y=eta_max, color='r', linestyle='--', label='Initial (1.0)')\n",
    "plt.axhline(y=eta_min, color='g', linestyle='--', label='Minimum (0.1)')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Noise StdDev')\n",
    "plt.title('Cosine Annealing Curve for Noise (stddev)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)  # Shows all directories Python checks for imports\n",
    "\n",
    "# Try to find mcp specifically\n",
    "try:\n",
    "    import mcp\n",
    "    print(f\"MCP found at: {mcp.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"MCP not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcp\n",
    "print(dir(mcp))  # This will show all available attributes/modules in mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
