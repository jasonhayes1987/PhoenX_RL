{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, StochasticDiscretePolicy\n",
    "import cnn_models\n",
    "from rl_agents import PPO, DDPG, TD3, Reinforce, ActorCritic, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "import helper\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000)\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(100, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [19. 19. 19. 19. 19. 19. 19. 19. 19. 19.]; total steps: 1000; avg scores/10 episodes: [-68.58064596 -65.13287911 -65.13530187 -65.96526694 -70.80303461\n",
      " -64.67457014 -63.59939802 -71.15287264 -62.44316704 -65.57396124]; avg total score: -66.30610975748019\n",
      "episode: [39. 39. 39. 39. 39. 39. 39. 39. 39. 39.]; total steps: 2000; avg scores/10 episodes: [-70.97301115 -65.35901021 -67.78348466 -66.8459225  -65.47245948\n",
      " -65.22289332 -69.77069283 -68.62092455 -63.83979165 -67.8417122 ]; avg total score: -67.17299025487803\n",
      "learning timestep: 2000\n",
      "num batches:312\n",
      "Policy Loss: 0.190664142370224\n",
      "Value Loss: 76.92449951171875\n",
      "Entropy: 130.26785278320312\n",
      "KL Divergence: 19.669567108154297\n",
      "episode: [58. 58. 58. 58. 58. 58. 58. 58. 58. 58.]; total steps: 3000; avg scores/10 episodes: [-60.84612575 -57.65096136 -61.13381045 -58.73533353 -63.29071261\n",
      " -62.02591107 -59.24047647 -55.87218632 -59.45199715 -63.62587441]; avg total score: -60.18733891191607\n",
      "episode: [78. 78. 78. 78. 78. 78. 78. 78. 78. 78.]; total steps: 4000; avg scores/10 episodes: [-58.27427057 -66.21766044 -57.56421481 -58.47706697 -67.29294883\n",
      " -56.33204245 -62.59308071 -67.77433402 -60.37107516 -62.46244998]; avg total score: -61.73591439381529\n",
      "learning timestep: 4000\n",
      "num batches:312\n",
      "Policy Loss: 0.027491115033626556\n",
      "Value Loss: 40.872642517089844\n",
      "Entropy: 128.36520385742188\n",
      "KL Divergence: 19.08441162109375\n",
      "episode: [98. 98. 98. 98. 98. 98. 98. 98. 98. 98.]; total steps: 5000; avg scores/10 episodes: [-58.11766611 -59.19704744 -56.3067576  -53.71949451 -57.64447923\n",
      " -57.69349121 -63.06801412 -48.66583967 -56.52236841 -61.37455541]; avg total score: -57.23097137131585\n",
      "episode: [117. 117. 117. 117. 117. 117. 117. 117. 117. 117.]; total steps: 6000; avg scores/10 episodes: [-57.73462519 -54.59150125 -54.63958057 -54.54845517 -56.18224277\n",
      " -56.95890874 -56.87308387 -59.38510007 -58.21850572 -53.31943832]; avg total score: -56.24514416664357\n",
      "learning timestep: 6000\n",
      "num batches:312\n",
      "Policy Loss: -0.03566686436533928\n",
      "Value Loss: 29.610244750976562\n",
      "Entropy: 122.02469635009766\n",
      "KL Divergence: 24.57318878173828\n",
      "episode: [137. 137. 137. 137. 137. 137. 137. 137. 137. 137.]; total steps: 7000; avg scores/10 episodes: [-57.47909774 -51.65558133 -60.14897542 -54.56847565 -57.48880408\n",
      " -55.81689731 -59.72455609 -57.44948451 -61.88614496 -55.02994514]; avg total score: -57.12479622154134\n",
      "episode: [156. 156. 156. 156. 156. 156. 156. 156. 156. 156.]; total steps: 8000; avg scores/10 episodes: [-57.20430697 -54.84183324 -55.80874568 -49.62761346 -59.09210096\n",
      " -53.92016301 -58.63853105 -56.94824038 -53.47505885 -55.30125089]; avg total score: -55.48578445015028\n",
      "learning timestep: 8000\n",
      "num batches:312\n",
      "Policy Loss: -0.12017221003770828\n",
      "Value Loss: 31.02895164489746\n",
      "Entropy: 119.832275390625\n",
      "KL Divergence: 21.717693328857422\n",
      "episode: [176. 176. 176. 176. 176. 176. 176. 176. 176. 176.]; total steps: 9000; avg scores/10 episodes: [-50.0787642  -52.47217126 -53.58294083 -52.1387177  -52.56249763\n",
      " -54.31435056 -53.89145975 -51.91296955 -52.47812878 -58.99558906]; avg total score: -53.24275893087179\n",
      "episode: [196. 196. 196. 196. 196. 196. 196. 196. 196. 196.]; total steps: 10000; avg scores/10 episodes: [-47.53304557 -53.91909572 -48.8514487  -52.26628463 -54.72646263\n",
      " -50.66050523 -53.86654445 -57.95388503 -53.96028857 -53.79396021]; avg total score: -52.753152072240525\n",
      "learning timestep: 10000\n",
      "num batches:312\n",
      "Policy Loss: -0.22505426406860352\n",
      "Value Loss: 12.248147964477539\n",
      "Entropy: 113.28885650634766\n",
      "KL Divergence: 21.424503326416016\n",
      "episode: [215. 215. 215. 215. 215. 215. 215. 215. 215. 215.]; total steps: 11000; avg scores/10 episodes: [-55.04820896 -50.66461997 -48.85843996 -50.00585153 -50.93525826\n",
      " -52.08408338 -51.17253652 -55.32929649 -44.83238222 -47.69503686]; avg total score: -50.66257141657304\n",
      "episode: [235. 235. 235. 235. 235. 235. 235. 235. 235. 235.]; total steps: 12000; avg scores/10 episodes: [-47.65898957 -44.3523922  -46.97350214 -49.18127742 -49.38441509\n",
      " -49.27056461 -52.79640828 -49.65786376 -52.44175879 -51.96086881]; avg total score: -49.36780406772973\n",
      "learning timestep: 12000\n",
      "num batches:312\n",
      "Policy Loss: 0.09902477264404297\n",
      "Value Loss: 12.936774253845215\n",
      "Entropy: 106.66642761230469\n",
      "KL Divergence: 19.650314331054688\n",
      "episode: [254. 254. 254. 254. 254. 254. 254. 254. 254. 254.]; total steps: 13000; avg scores/10 episodes: [-42.19577771 -43.59530428 -49.42686459 -47.70024332 -48.86401352\n",
      " -45.46577507 -46.2953257  -47.05076062 -45.74307906 -46.14111715]; avg total score: -46.24782610226508\n",
      "episode: [274. 274. 274. 274. 274. 274. 274. 274. 274. 274.]; total steps: 14000; avg scores/10 episodes: [-44.91864862 -47.12714572 -51.38886194 -50.11752156 -46.15452634\n",
      " -43.54675431 -41.84247986 -42.76935399 -45.82721886 -44.64105361]; avg total score: -45.833356482800994\n",
      "learning timestep: 14000\n",
      "num batches:312\n",
      "Policy Loss: -0.12982183694839478\n",
      "Value Loss: 6.974724769592285\n",
      "Entropy: 96.48606872558594\n",
      "KL Divergence: 17.162511825561523\n",
      "episode: [294. 294. 294. 294. 294. 294. 294. 294. 294. 294.]; total steps: 15000; avg scores/10 episodes: [-42.06437071 -36.39123692 -40.52763242 -40.59925265 -40.77041203\n",
      " -41.00737367 -36.3912119  -38.8002646  -41.91171009 -40.34820823]; avg total score: -39.88116732142911\n",
      "episode: [313. 313. 313. 313. 313. 313. 313. 313. 313. 313.]; total steps: 16000; avg scores/10 episodes: [-36.8639408  -38.51538447 -43.20247277 -38.54583505 -38.21866616\n",
      " -42.54179815 -40.33031165 -36.71064076 -38.59565513 -44.817252  ]; avg total score: -39.834195692307084\n",
      "learning timestep: 16000\n",
      "num batches:312\n",
      "Policy Loss: -0.05663240700960159\n",
      "Value Loss: 3.4982640743255615\n",
      "Entropy: 84.52716827392578\n",
      "KL Divergence: 17.998085021972656\n",
      "episode: [333. 333. 333. 333. 333. 333. 333. 333. 333. 333.]; total steps: 17000; avg scores/10 episodes: [-32.39427141 -36.21942504 -35.44253976 -33.79995896 -33.0297045\n",
      " -34.71480181 -32.18656298 -33.31244566 -32.77950362 -34.88321905]; avg total score: -33.87624327993609\n",
      "episode: [352. 352. 352. 352. 352. 352. 352. 352. 352. 352.]; total steps: 18000; avg scores/10 episodes: [-33.83645199 -30.00865913 -35.30733925 -34.76569974 -31.12473358\n",
      " -33.30147607 -33.72529194 -36.5135715  -31.04405935 -32.76477107]; avg total score: -33.23920536251431\n",
      "learning timestep: 18000\n",
      "num batches:312\n",
      "Policy Loss: -0.1642351746559143\n",
      "Value Loss: 1.160556674003601\n",
      "Entropy: 69.14036560058594\n",
      "KL Divergence: 18.412267684936523\n",
      "episode: [372. 372. 372. 372. 372. 372. 372. 372. 372. 372.]; total steps: 19000; avg scores/10 episodes: [-28.7543058  -28.64226934 -29.92841359 -29.4178222  -29.26473232\n",
      " -32.02738089 -30.06524705 -31.32964795 -30.07625838 -29.96629972]; avg total score: -29.94723772446391\n",
      "episode: [392. 392. 392. 392. 392. 392. 392. 392. 392. 392.]; total steps: 20000; avg scores/10 episodes: [-29.41252496 -30.49151188 -30.05832209 -26.80246733 -28.94820489\n",
      " -30.76643876 -26.67157713 -28.75553751 -29.94065631 -30.07953283]; avg total score: -29.192677371202382\n",
      "learning timestep: 20000\n",
      "num batches:312\n",
      "Policy Loss: 0.07461881637573242\n",
      "Value Loss: 0.8343260288238525\n",
      "Entropy: 60.31925964355469\n",
      "KL Divergence: 14.276776313781738\n",
      "episode: [411. 411. 411. 411. 411. 411. 411. 411. 411. 411.]; total steps: 21000; avg scores/10 episodes: [-26.59416125 -25.06625156 -28.04937115 -25.34242379 -26.20767715\n",
      " -23.3796214  -24.11623509 -26.87784214 -27.50700675 -25.61732537]; avg total score: -25.875791564857575\n",
      "episode: [431. 431. 431. 431. 431. 431. 431. 431. 431. 431.]; total steps: 22000; avg scores/10 episodes: [-22.47980608 -24.52807591 -25.04012062 -27.39433957 -23.91643192\n",
      " -23.64229425 -24.28411521 -24.52599279 -24.37134611 -24.88611055]; avg total score: -24.506863302868588\n",
      "learning timestep: 22000\n",
      "num batches:312\n",
      "Policy Loss: 0.00984904170036316\n",
      "Value Loss: 1.1130422353744507\n",
      "Entropy: 42.964073181152344\n",
      "KL Divergence: 12.161556243896484\n",
      "episode: [450. 450. 450. 450. 450. 450. 450. 450. 450. 450.]; total steps: 23000; avg scores/10 episodes: [-22.56657697 -21.61840573 -22.01740425 -23.00408456 -20.70331925\n",
      " -20.77061466 -21.79128387 -23.63804257 -20.50903781 -20.81602412]; avg total score: -21.743479378819295\n",
      "episode: [470. 470. 470. 470. 470. 470. 470. 470. 470. 470.]; total steps: 24000; avg scores/10 episodes: [-21.99704875 -21.84695895 -23.26325    -22.02458914 -23.6003448\n",
      " -22.86962891 -23.59862944 -23.22598394 -23.59149193 -23.58196056]; avg total score: -22.959988642447847\n",
      "learning timestep: 24000\n",
      "num batches:312\n",
      "Policy Loss: -0.1602635532617569\n",
      "Value Loss: 0.8316903710365295\n",
      "Entropy: 30.243694305419922\n",
      "KL Divergence: 12.769973754882812\n",
      "episode: [490. 490. 490. 490. 490. 490. 490. 490. 490. 490.]; total steps: 25000; avg scores/10 episodes: [-19.87542361 -18.85902339 -19.52693519 -19.65814478 -21.56778898\n",
      " -21.80169379 -21.58542615 -20.01524235 -19.33714959 -21.16476806]; avg total score: -20.339159589339303\n",
      "episode: [509. 509. 509. 509. 509. 509. 509. 509. 509. 509.]; total steps: 26000; avg scores/10 episodes: [-20.26467841 -21.13111908 -19.26095469 -21.74360867 -21.59593342\n",
      " -21.09896306 -19.16493553 -19.27599368 -21.26293866 -19.87739054]; avg total score: -20.467651573034697\n",
      "learning timestep: 26000\n",
      "num batches:312\n",
      "Policy Loss: 0.054934725165367126\n",
      "Value Loss: 0.7073953747749329\n",
      "Entropy: 20.87028694152832\n",
      "KL Divergence: 8.344589233398438\n",
      "episode: [529. 529. 529. 529. 529. 529. 529. 529. 529. 529.]; total steps: 27000; avg scores/10 episodes: [-17.4338327  -19.36313789 -18.10914426 -18.48061041 -19.46062002\n",
      " -18.02716028 -19.0365542  -20.59088545 -18.0897624  -17.56659011]; avg total score: -18.615829773432523\n",
      "episode: [549. 549. 549. 549. 549. 549. 549. 549. 549. 549.]; total steps: 28000; avg scores/10 episodes: [-18.24472584 -19.76291833 -16.41014776 -17.4231836  -17.84126358\n",
      " -16.98295589 -18.13751643 -19.93985394 -18.07323677 -19.24134294]; avg total score: -18.205714507058815\n",
      "learning timestep: 28000\n",
      "num batches:312\n",
      "Policy Loss: -0.08280813694000244\n",
      "Value Loss: 0.4923079013824463\n",
      "Entropy: 7.976537227630615\n",
      "KL Divergence: 9.06043815612793\n",
      "episode: [568. 568. 568. 568. 568. 568. 568. 568. 568. 568.]; total steps: 29000; avg scores/10 episodes: [-16.91049255 -18.64000761 -15.6544662  -18.17096269 -19.2293896\n",
      " -17.48773843 -19.42495922 -17.3969645  -16.59185771 -17.00138676]; avg total score: -17.650822528198546\n",
      "episode: [588. 588. 588. 588. 588. 588. 588. 588. 588. 588.]; total steps: 30000; avg scores/10 episodes: [-20.03009165 -17.58952866 -17.40159612 -17.3416258  -17.33347616\n",
      " -17.37830692 -15.38505132 -15.55304712 -17.06135848 -16.09065761]; avg total score: -17.11647398450527\n",
      "learning timestep: 30000\n",
      "num batches:312\n",
      "Policy Loss: 0.14676719903945923\n",
      "Value Loss: 0.2935625910758972\n",
      "Entropy: -2.4276785850524902\n",
      "KL Divergence: 7.80988883972168\n",
      "episode: [607. 607. 607. 607. 607. 607. 607. 607. 607. 607.]; total steps: 31000; avg scores/10 episodes: [-15.40221413 -15.73776723 -14.50117962 -15.77905575 -14.9465147\n",
      " -14.84695507 -17.07810419 -15.12462872 -15.85835278 -15.6760962 ]; avg total score: -15.495086838174254\n",
      "episode: [627. 627. 627. 627. 627. 627. 627. 627. 627. 627.]; total steps: 32000; avg scores/10 episodes: [-16.87524192 -16.76372324 -15.70022822 -14.77239566 -16.87723294\n",
      " -14.61241581 -16.47345013 -15.13296402 -15.36581013 -16.56693911]; avg total score: -15.914040117988458\n",
      "learning timestep: 32000\n",
      "num batches:312\n",
      "Policy Loss: 0.04404062405228615\n",
      "Value Loss: 0.3146623969078064\n",
      "Entropy: -15.321857452392578\n",
      "KL Divergence: 4.194047451019287\n",
      "episode: [647. 647. 647. 647. 647. 647. 647. 647. 647. 647.]; total steps: 33000; avg scores/10 episodes: [-15.69411096 -15.35315729 -16.06983065 -15.12756619 -15.90244187\n",
      " -16.48603108 -15.44527263 -12.64676438 -13.80975264 -12.34532333]; avg total score: -14.888025099359487\n",
      "episode: [666. 666. 666. 666. 666. 666. 666. 666. 666. 666.]; total steps: 34000; avg scores/10 episodes: [-16.70506107 -14.12359043 -15.7973402  -16.1875255  -13.53488544\n",
      " -14.32904017 -15.18726091 -12.99533071 -14.94543256 -16.72501955]; avg total score: -15.053048656185027\n",
      "learning timestep: 34000\n",
      "num batches:312\n",
      "Policy Loss: 0.10316547751426697\n",
      "Value Loss: 0.2928401827812195\n",
      "Entropy: -22.416454315185547\n",
      "KL Divergence: 4.716578483581543\n",
      "episode: [686. 686. 686. 686. 686. 686. 686. 686. 686. 686.]; total steps: 35000; avg scores/10 episodes: [-15.47280084 -14.86928436 -15.72142555 -13.87881558 -15.07941568\n",
      " -16.7735797  -13.93817765 -13.37815711 -13.18627434 -13.15199771]; avg total score: -14.544992851252408\n",
      "episode: [705. 705. 705. 705. 705. 705. 705. 705. 705. 705.]; total steps: 36000; avg scores/10 episodes: [-13.08549155 -14.52597265 -15.30035584 -14.01258266 -16.61898357\n",
      " -14.39849247 -14.16731316 -12.83583619 -13.25972327 -14.30845504]; avg total score: -14.25132064153126\n",
      "learning timestep: 36000\n",
      "num batches:312\n",
      "Policy Loss: -0.2247495800256729\n",
      "Value Loss: 0.3782057762145996\n",
      "Entropy: -30.807693481445312\n",
      "KL Divergence: 3.546416759490967\n",
      "episode: [725. 725. 725. 725. 725. 725. 725. 725. 725. 725.]; total steps: 37000; avg scores/10 episodes: [-16.61414131 -16.24054633 -14.97671932 -14.08480449 -14.42768813\n",
      " -15.60056731 -14.62617731 -12.27863541 -14.97123329 -13.42334745]; avg total score: -14.72438603402576\n",
      "episode: [745. 745. 745. 745. 745. 745. 745. 745. 745. 745.]; total steps: 38000; avg scores/10 episodes: [-14.62241533 -13.71789677 -13.22324154 -16.04039124 -14.29503696\n",
      " -15.00706232 -15.18337285 -12.38785213 -12.97534475 -14.08040978]; avg total score: -14.15330236793265\n",
      "learning timestep: 38000\n",
      "num batches:312\n",
      "Policy Loss: 0.12331726402044296\n",
      "Value Loss: 0.4473611116409302\n",
      "Entropy: -40.27711868286133\n",
      "KL Divergence: 3.085400104522705\n",
      "episode: [764. 764. 764. 764. 764. 764. 764. 764. 764. 764.]; total steps: 39000; avg scores/10 episodes: [-12.6620065  -14.66493732 -15.0917641  -12.47272296 -13.48095936\n",
      " -13.79071423 -13.59415105 -16.47956644 -13.74871487 -13.65941504]; avg total score: -13.96449518801426\n",
      "episode: [784. 784. 784. 784. 784. 784. 784. 784. 784. 784.]; total steps: 40000; avg scores/10 episodes: [-12.14257161 -11.72104744 -15.29612886 -12.50390474 -13.27880049\n",
      " -13.51899842 -13.04813165 -12.23104677 -12.04636498 -12.9059782 ]; avg total score: -12.869297316882205\n",
      "learning timestep: 40000\n",
      "num batches:312\n",
      "Policy Loss: -0.0841112732887268\n",
      "Value Loss: 0.4794231355190277\n",
      "Entropy: -49.2161979675293\n",
      "KL Divergence: 3.0266942977905273\n",
      "episode: [803. 803. 803. 803. 803. 803. 803. 803. 803. 803.]; total steps: 41000; avg scores/10 episodes: [-14.59958267 -14.9128119  -12.20036946 -13.08920463 -14.25052134\n",
      " -12.60141876 -14.87844832 -13.77110349 -12.56567712 -13.72127726]; avg total score: -13.65904149428113\n",
      "episode: [823. 823. 823. 823. 823. 823. 823. 823. 823. 823.]; total steps: 42000; avg scores/10 episodes: [-14.85561053 -12.93687722 -12.18711599 -14.92675541 -14.41528538\n",
      " -13.17881832 -14.58329139 -13.23758771 -13.38380894 -12.54660595]; avg total score: -13.625175685009276\n",
      "learning timestep: 42000\n",
      "num batches:312\n",
      "Policy Loss: -0.15657612681388855\n",
      "Value Loss: 0.564200758934021\n",
      "Entropy: -52.970279693603516\n",
      "KL Divergence: 3.507751941680908\n",
      "episode: [843. 843. 843. 843. 843. 843. 843. 843. 843. 843.]; total steps: 43000; avg scores/10 episodes: [-10.47180413 -15.0195413  -11.35033597 -14.56768416 -13.57211541\n",
      " -13.3118685  -11.85221333 -12.66231702 -11.03923711 -13.71091246]; avg total score: -12.755802937523738\n",
      "episode: [862. 862. 862. 862. 862. 862. 862. 862. 862. 862.]; total steps: 44000; avg scores/10 episodes: [-13.06294007 -13.21753819 -14.09904476 -14.38155004 -12.79573035\n",
      " -13.78935186 -12.17428369 -11.48851349 -13.02830642 -13.43331647]; avg total score: -13.14705753481221\n",
      "learning timestep: 44000\n",
      "num batches:312\n",
      "Policy Loss: 0.11689147353172302\n",
      "Value Loss: 0.28178876638412476\n",
      "Entropy: -61.32775115966797\n",
      "KL Divergence: 3.228264808654785\n",
      "episode: [882. 882. 882. 882. 882. 882. 882. 882. 882. 882.]; total steps: 45000; avg scores/10 episodes: [-12.82338786 -12.5362853  -12.57120605 -11.99634128 -12.86782409\n",
      " -11.90368797 -10.77795127 -12.26195056 -12.08722876 -11.63310873]; avg total score: -12.14589718803921\n",
      "episode: [901. 901. 901. 901. 901. 901. 901. 901. 901. 901.]; total steps: 46000; avg scores/10 episodes: [-12.53246339 -14.46623252 -12.45164203 -10.96787391 -11.26571794\n",
      " -14.19435911 -10.99010627 -11.77365458 -11.0515978  -11.31923775]; avg total score: -12.101288531647505\n",
      "learning timestep: 46000\n",
      "num batches:312\n",
      "Policy Loss: -0.018422216176986694\n",
      "Value Loss: 0.426780641078949\n",
      "Entropy: -68.12759399414062\n",
      "KL Divergence: 3.9015390872955322\n",
      "episode: [921. 921. 921. 921. 921. 921. 921. 921. 921. 921.]; total steps: 47000; avg scores/10 episodes: [-13.45471808 -11.63313859 -11.51726343 -11.84469289 -10.05593172\n",
      " -12.84223441 -12.18767102 -12.8554876  -12.74304865 -13.25161727]; avg total score: -12.238580366293442\n",
      "episode: [941. 941. 941. 941. 941. 941. 941. 941. 941. 941.]; total steps: 48000; avg scores/10 episodes: [-13.45761519 -11.09723968 -11.95612117 -12.57738792 -12.57553812\n",
      " -10.44551637 -11.83543741 -14.1975239  -12.50076584 -12.14846436]; avg total score: -12.279160995872425\n",
      "learning timestep: 48000\n",
      "num batches:312\n",
      "Policy Loss: -0.19993051886558533\n",
      "Value Loss: 0.35681527853012085\n",
      "Entropy: -77.22831726074219\n",
      "KL Divergence: 4.417993545532227\n",
      "episode: [960. 960. 960. 960. 960. 960. 960. 960. 960. 960.]; total steps: 49000; avg scores/10 episodes: [-12.81090138 -11.64727792 -13.22131485 -12.75843252 -12.11870227\n",
      " -12.82430109 -11.15939125 -12.03566228 -11.92642313 -11.06446687]; avg total score: -12.156687355647602\n",
      "episode: [980. 980. 980. 980. 980. 980. 980. 980. 980. 980.]; total steps: 50000; avg scores/10 episodes: [-11.3319781   -9.37058881 -13.9131753  -12.66661    -13.41149072\n",
      " -11.26412687 -11.66240484 -11.43482197 -15.37255997 -12.32759423]; avg total score: -12.275535081645074\n",
      "learning timestep: 50000\n",
      "num batches:312\n",
      "Policy Loss: 0.06700682640075684\n",
      "Value Loss: 0.3487376868724823\n",
      "Entropy: -81.65864562988281\n",
      "KL Divergence: 8.519319534301758\n",
      "episode: [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]; total steps: 51000; avg scores/10 episodes: [-10.16848432 -11.88519142 -10.31608215 -12.03901417 -10.77433956\n",
      " -10.03375704 -10.77322209 -13.40468643 -11.85608125 -11.30426903]; avg total score: -11.255512744432545\n",
      "episode: [1019. 1019. 1019. 1019. 1019. 1019. 1019. 1019. 1019. 1019.]; total steps: 52000; avg scores/10 episodes: [-10.94947772 -12.04036011 -12.84376297  -8.99504658 -13.2620733\n",
      " -10.81946836 -11.63805777 -11.40269045 -12.7392443  -11.59336006]; avg total score: -11.628354162797056\n",
      "learning timestep: 52000\n",
      "num batches:312\n",
      "Policy Loss: 0.06564339995384216\n",
      "Value Loss: 0.40096497535705566\n",
      "Entropy: -90.8472900390625\n",
      "KL Divergence: 5.34700870513916\n",
      "episode: [1039. 1039. 1039. 1039. 1039. 1039. 1039. 1039. 1039. 1039.]; total steps: 53000; avg scores/10 episodes: [-10.87500661 -11.74458382  -9.87061422 -11.20906704 -10.32284224\n",
      " -10.0885722  -11.55764981 -10.85495853 -12.02573882 -11.76947602]; avg total score: -11.031850932161172\n",
      "episode: [1058. 1058. 1058. 1058. 1058. 1058. 1058. 1058. 1058. 1058.]; total steps: 54000; avg scores/10 episodes: [-11.33614268 -10.96666525 -10.78523684 -10.4379639  -10.69414501\n",
      " -11.02432864 -10.72540045 -10.24072545 -11.32943368 -11.4768957 ]; avg total score: -10.90169375964307\n",
      "learning timestep: 54000\n",
      "num batches:312\n",
      "Policy Loss: -0.26469501852989197\n",
      "Value Loss: 0.34367597103118896\n",
      "Entropy: -89.63426971435547\n",
      "KL Divergence: 6.4030303955078125\n",
      "episode: [1078. 1078. 1078. 1078. 1078. 1078. 1078. 1078. 1078. 1078.]; total steps: 55000; avg scores/10 episodes: [-10.65473446 -12.33116742 -10.86912054 -11.21771934 -12.07799025\n",
      "  -9.80141946 -11.26116428 -10.0880328  -10.31619205 -11.39104656]; avg total score: -11.000858716920174\n",
      "episode: [1098. 1098. 1098. 1098. 1098. 1098. 1098. 1098. 1098. 1098.]; total steps: 56000; avg scores/10 episodes: [-11.32932586 -10.50201998 -12.65035589 -10.66033463  -9.9296639\n",
      " -11.1949543   -9.83202526 -10.53900708 -11.99303173  -9.14646688]; avg total score: -10.777718551058275\n",
      "learning timestep: 56000\n",
      "num batches:312\n",
      "Policy Loss: -0.11958403885364532\n",
      "Value Loss: 0.40566426515579224\n",
      "Entropy: -91.26596069335938\n",
      "KL Divergence: 11.027923583984375\n",
      "episode: [1117. 1117. 1117. 1117. 1117. 1117. 1117. 1117. 1117. 1117.]; total steps: 57000; avg scores/10 episodes: [-10.91066188 -11.02726579 -10.6631453  -11.36874721 -11.30124912\n",
      " -11.16486957 -11.59116369 -12.47104691 -10.63554269 -10.12526012]; avg total score: -11.125895227090819\n",
      "episode: [1137. 1137. 1137. 1137. 1137. 1137. 1137. 1137. 1137. 1137.]; total steps: 58000; avg scores/10 episodes: [-11.30355076  -9.5995968  -10.5313705   -9.58851773 -12.04313662\n",
      " -12.44586248 -10.21198583 -12.17777035  -9.85189671 -10.82148714]; avg total score: -10.857517492511649\n",
      "learning timestep: 58000\n",
      "num batches:312\n",
      "Policy Loss: -0.04928012937307358\n",
      "Value Loss: 0.6858807802200317\n",
      "Entropy: -93.67195892333984\n",
      "KL Divergence: 10.556001663208008\n",
      "episode: [1156. 1156. 1156. 1156. 1156. 1156. 1156. 1156. 1156. 1156.]; total steps: 59000; avg scores/10 episodes: [-10.56638368 -11.1956189   -9.76292817 -11.04819428 -10.28403413\n",
      "  -9.51547526 -10.92078863 -10.50054001  -8.83495505 -12.42183113]; avg total score: -10.505074925279398\n",
      "episode: [1176. 1176. 1176. 1176. 1176. 1176. 1176. 1176. 1176. 1176.]; total steps: 60000; avg scores/10 episodes: [-11.99134704 -10.21213405 -10.38585733 -12.02639101 -10.49693487\n",
      " -12.14105428 -12.64852842  -9.66059342 -10.06332461 -10.58940193]; avg total score: -11.021556695751478\n",
      "learning timestep: 60000\n",
      "num batches:312\n",
      "Policy Loss: 0.1234375536441803\n",
      "Value Loss: 0.5353347063064575\n",
      "Entropy: -97.6132583618164\n",
      "KL Divergence: 11.498818397521973\n",
      "episode: [1196. 1196. 1196. 1196. 1196. 1196. 1196. 1196. 1196. 1196.]; total steps: 61000; avg scores/10 episodes: [-11.5984528  -10.80645901 -10.42787731 -10.15839647 -10.59319737\n",
      " -11.98714184  -9.40712224 -10.41112654 -10.08293285 -10.61377895]; avg total score: -10.60864853934559\n",
      "episode: [1215. 1215. 1215. 1215. 1215. 1215. 1215. 1215. 1215. 1215.]; total steps: 62000; avg scores/10 episodes: [-11.35881514 -11.57656949 -10.38443562  -8.27329627 -11.15655947\n",
      "  -9.60508913 -11.07819653 -11.60266544 -10.4030424   -8.99258353]; avg total score: -10.443125303425452\n",
      "learning timestep: 62000\n",
      "num batches:312\n",
      "Policy Loss: -0.07637552171945572\n",
      "Value Loss: 1.1501805782318115\n",
      "Entropy: -97.51405334472656\n",
      "KL Divergence: 17.787715911865234\n",
      "episode: [1235. 1235. 1235. 1235. 1235. 1235. 1235. 1235. 1235. 1235.]; total steps: 63000; avg scores/10 episodes: [-10.72726177  -9.93646583 -10.57042057 -11.76332593 -11.38120226\n",
      " -11.14432506  -9.74733313 -10.88736826 -10.88467589 -11.40657331]; avg total score: -10.844895202362778\n",
      "episode: [1254. 1254. 1254. 1254. 1254. 1254. 1254. 1254. 1254. 1254.]; total steps: 64000; avg scores/10 episodes: [ -8.73989579  -9.89897723 -10.24620868 -11.12944115 -11.45144333\n",
      " -10.21505678  -9.532319    -9.50582465 -10.1300341   -8.98368245]; avg total score: -9.983288315882952\n",
      "learning timestep: 64000\n",
      "num batches:312\n",
      "Policy Loss: 0.0700618252158165\n",
      "Value Loss: 1.3790669441223145\n",
      "Entropy: -103.85042572021484\n",
      "KL Divergence: 16.30598258972168\n",
      "episode: [1274. 1274. 1274. 1274. 1274. 1274. 1274. 1274. 1274. 1274.]; total steps: 65000; avg scores/10 episodes: [-10.8082677   -9.98253296 -10.56373985 -10.23849347 -10.71585807\n",
      " -11.57253556  -9.57750587 -10.39858391 -10.69661053 -11.90909915]; avg total score: -10.646322706437548\n",
      "episode: [1294. 1294. 1294. 1294. 1294. 1294. 1294. 1294. 1294. 1294.]; total steps: 66000; avg scores/10 episodes: [-10.4057154  -10.41719693 -10.79718352  -8.99317995  -9.49030386\n",
      " -10.85822069 -11.42033798 -11.45344399  -9.58913081  -9.98086116]; avg total score: -10.340557429184788\n",
      "learning timestep: 66000\n",
      "num batches:312\n",
      "Policy Loss: 0.13678179681301117\n",
      "Value Loss: 1.6733577251434326\n",
      "Entropy: -106.74391174316406\n",
      "KL Divergence: 11.373891830444336\n",
      "episode: [1313. 1313. 1313. 1313. 1313. 1313. 1313. 1313. 1313. 1313.]; total steps: 67000; avg scores/10 episodes: [ -8.8989089  -11.78065263  -9.65325227 -11.7461     -12.78850655\n",
      " -11.83375498 -10.65178793 -10.84586503 -10.5725055  -11.54620567]; avg total score: -11.031753946739064\n",
      "episode: [1333. 1333. 1333. 1333. 1333. 1333. 1333. 1333. 1333. 1333.]; total steps: 68000; avg scores/10 episodes: [ -9.9939309  -11.16582713 -10.1455817  -10.10846205  -9.45447189\n",
      "  -9.39864121  -9.00763766 -10.16014957  -9.98303627  -9.93437713]; avg total score: -9.93521155089524\n",
      "learning timestep: 68000\n",
      "num batches:312\n",
      "Policy Loss: 0.19321192800998688\n",
      "Value Loss: 14.642743110656738\n",
      "Entropy: -103.4227294921875\n",
      "KL Divergence: 11.045119285583496\n",
      "episode: [1352. 1352. 1352. 1352. 1352. 1352. 1352. 1352. 1352. 1352.]; total steps: 69000; avg scores/10 episodes: [-10.05093305  -9.12425964 -10.69773762  -9.12740796 -10.27266427\n",
      " -10.39588746  -8.58682876 -11.71615802 -10.26910649  -9.96758919]; avg total score: -10.02085724457358\n",
      "episode: [1372. 1372. 1372. 1372. 1372. 1372. 1372. 1372. 1372. 1372.]; total steps: 70000; avg scores/10 episodes: [ -9.79003589 -10.64534058 -11.04629477  -9.25484776  -8.58535876\n",
      "  -7.90812007 -11.06133623  -9.50229395  -9.25062623 -11.04775612]; avg total score: -9.809201036164385\n",
      "learning timestep: 70000\n",
      "num batches:312\n",
      "Policy Loss: 0.27516597509384155\n",
      "Value Loss: 7.315476417541504\n",
      "Entropy: -102.67520141601562\n",
      "KL Divergence: 18.722827911376953\n",
      "episode: [1392. 1392. 1392. 1392. 1392. 1392. 1392. 1392. 1392. 1392.]; total steps: 71000; avg scores/10 episodes: [-11.47473565  -8.51064339 -10.17446699 -10.21941889  -9.64794907\n",
      " -11.62868842 -10.14198465  -9.99682778 -10.18113756  -9.65155372]; avg total score: -10.162740612881596\n",
      "episode: [1411. 1411. 1411. 1411. 1411. 1411. 1411. 1411. 1411. 1411.]; total steps: 72000; avg scores/10 episodes: [-10.29238817  -9.62348024 -10.48827518  -9.74873091 -11.13555016\n",
      " -10.37509884  -9.77236005  -9.90562299  -8.66473847  -9.47957618]; avg total score: -9.948582117506257\n",
      "learning timestep: 72000\n",
      "num batches:312\n",
      "Policy Loss: 0.12241823971271515\n",
      "Value Loss: 11.997560501098633\n",
      "Entropy: -100.71126556396484\n",
      "KL Divergence: 19.545692443847656\n",
      "episode: [1431. 1431. 1431. 1431. 1431. 1431. 1431. 1431. 1431. 1431.]; total steps: 73000; avg scores/10 episodes: [ -8.95381952  -9.94107721 -10.48057149  -9.89901174 -11.1583993\n",
      " -10.46162235  -8.7367443   -9.50539033  -9.84789009 -11.05550228]; avg total score: -10.004002861434127\n",
      "episode: [1450. 1450. 1450. 1450. 1450. 1450. 1450. 1450. 1450. 1450.]; total steps: 74000; avg scores/10 episodes: [-10.03511983  -9.85565225  -9.63566724 -10.63164845 -10.47443233\n",
      " -11.18751631 -10.27026089  -9.66282326 -10.32852988  -9.69464247]; avg total score: -10.17762929112709\n",
      "learning timestep: 74000\n",
      "num batches:312\n",
      "Policy Loss: 0.2321489453315735\n",
      "Value Loss: 5.595086097717285\n",
      "Entropy: -101.7786636352539\n",
      "KL Divergence: 16.950580596923828\n",
      "episode: [1470. 1470. 1470. 1470. 1470. 1470. 1470. 1470. 1470. 1470.]; total steps: 75000; avg scores/10 episodes: [ -9.81134885  -9.97685124 -10.11364666  -9.87520985  -9.1919018\n",
      "  -9.36675708 -10.28393244 -10.4697534  -10.39769032 -10.04853521]; avg total score: -9.953562684077662\n",
      "episode: [1490. 1490. 1490. 1490. 1490. 1490. 1490. 1490. 1490. 1490.]; total steps: 76000; avg scores/10 episodes: [-10.90845665  -9.49015131  -9.57616133  -9.99295964  -9.34150786\n",
      " -10.80613749  -9.64666029  -9.14819956  -9.64927283  -9.50053846]; avg total score: -9.806004540821034\n",
      "learning timestep: 76000\n",
      "num batches:312\n",
      "Policy Loss: 0.29215380549430847\n",
      "Value Loss: 14.011655807495117\n",
      "Entropy: -103.53817749023438\n",
      "KL Divergence: 20.979394912719727\n",
      "episode: [1509. 1509. 1509. 1509. 1509. 1509. 1509. 1509. 1509. 1509.]; total steps: 77000; avg scores/10 episodes: [ -9.36899718 -10.25542161 -11.13191667  -9.78937273 -10.36173701\n",
      " -10.7552875  -10.69528622 -10.01532985  -9.56141442 -10.41373348]; avg total score: -10.234849667422594\n",
      "episode: [1529. 1529. 1529. 1529. 1529. 1529. 1529. 1529. 1529. 1529.]; total steps: 78000; avg scores/10 episodes: [-10.53592383 -10.62460258 -10.67049848  -8.73454211 -10.79943107\n",
      " -10.51928653  -9.31433918 -11.6773488   -9.46236074  -8.63489727]; avg total score: -10.097323059901914\n",
      "learning timestep: 78000\n",
      "num batches:312\n",
      "Policy Loss: -0.05276213586330414\n",
      "Value Loss: 68.74813842773438\n",
      "Entropy: -92.259521484375\n",
      "KL Divergence: 16.360641479492188\n",
      "episode: [1549. 1549. 1549. 1549. 1549. 1549. 1549. 1549. 1549. 1549.]; total steps: 79000; avg scores/10 episodes: [-10.59387365 -12.551458    -8.62325894 -10.61859877  -9.58379507\n",
      " -10.65262544 -10.11090913 -11.08735461  -8.50902525 -10.12882465]; avg total score: -10.245972348994187\n",
      "episode: [1568. 1568. 1568. 1568. 1568. 1568. 1568. 1568. 1568. 1568.]; total steps: 80000; avg scores/10 episodes: [-11.25099477 -10.74749521 -10.23936709 -10.22226241 -10.25176393\n",
      " -11.7575659  -10.27384348 -10.76390464 -10.39319399  -9.59743506]; avg total score: -10.549782647080189\n",
      "learning timestep: 80000\n",
      "num batches:312\n",
      "Policy Loss: 0.0404219850897789\n",
      "Value Loss: 10.883727073669434\n",
      "Entropy: -94.33824157714844\n",
      "KL Divergence: 15.186986923217773\n",
      "episode: [1588. 1588. 1588. 1588. 1588. 1588. 1588. 1588. 1588. 1588.]; total steps: 81000; avg scores/10 episodes: [ -9.69992588 -11.17732401 -10.86928857 -10.72413761 -10.10200581\n",
      "  -9.16643305 -10.34379698 -11.00714658 -10.78522838  -9.88522289]; avg total score: -10.376050974671957\n",
      "episode: [1607. 1607. 1607. 1607. 1607. 1607. 1607. 1607. 1607. 1607.]; total steps: 82000; avg scores/10 episodes: [ -9.09438998 -11.22350239 -10.05227177  -9.87615056 -10.18944117\n",
      "  -9.36206614 -10.06091993 -11.42360921 -10.34511115  -9.68379229]; avg total score: -10.131125461438018\n",
      "learning timestep: 82000\n",
      "num batches:312\n",
      "Policy Loss: 0.12597638368606567\n",
      "Value Loss: 25.176761627197266\n",
      "Entropy: -92.82026672363281\n",
      "KL Divergence: 20.092222213745117\n",
      "episode: [1627. 1627. 1627. 1627. 1627. 1627. 1627. 1627. 1627. 1627.]; total steps: 83000; avg scores/10 episodes: [-10.95644413  -9.67081041 -11.16210856 -11.657162    -9.32410642\n",
      " -11.27341974  -9.74500309 -11.06005405 -11.14801256 -11.60972816]; avg total score: -10.76068491233962\n",
      "episode: [1647. 1647. 1647. 1647. 1647. 1647. 1647. 1647. 1647. 1647.]; total steps: 84000; avg scores/10 episodes: [-10.8539815   -8.63241237 -10.75597632 -11.34648187 -11.16435021\n",
      " -10.50426028 -11.83121532  -9.25933016  -9.85320878 -10.85674131]; avg total score: -10.505795812753226\n",
      "learning timestep: 84000\n",
      "num batches:312\n",
      "Policy Loss: -0.11378203332424164\n",
      "Value Loss: 495.70904541015625\n",
      "Entropy: -91.21696472167969\n",
      "KL Divergence: 29.149028778076172\n",
      "episode: [1666. 1666. 1666. 1666. 1666. 1666. 1666. 1666. 1666. 1666.]; total steps: 85000; avg scores/10 episodes: [-11.49564312 -10.32664721 -10.19789672 -10.45321562 -12.33271662\n",
      "  -9.88376008 -11.03209542 -12.37441409  -9.60991571  -9.91766957]; avg total score: -10.762397417016452\n",
      "episode: [1686. 1686. 1686. 1686. 1686. 1686. 1686. 1686. 1686. 1686.]; total steps: 86000; avg scores/10 episodes: [-10.38404599 -10.08302551 -10.95538567 -12.30911205 -10.10282726\n",
      " -11.34229397 -11.61282566 -10.39623703 -10.09664811  -9.87064326]; avg total score: -10.715304450349183\n",
      "learning timestep: 86000\n",
      "num batches:312\n",
      "Policy Loss: -0.04186709225177765\n",
      "Value Loss: 38.09637451171875\n",
      "Entropy: -90.12371826171875\n",
      "KL Divergence: 19.862821578979492\n",
      "episode: [1705. 1705. 1705. 1705. 1705. 1705. 1705. 1705. 1705. 1705.]; total steps: 87000; avg scores/10 episodes: [-12.90549004 -12.69202436 -11.08016439 -10.95510558  -9.73764682\n",
      " -11.89476257  -9.20824235  -9.83751854 -11.40214438 -11.74555783]; avg total score: -11.145865686355185\n",
      "episode: [1725. 1725. 1725. 1725. 1725. 1725. 1725. 1725. 1725. 1725.]; total steps: 88000; avg scores/10 episodes: [-10.3597722  -12.68490675 -11.53146346 -10.73106797 -11.22435376\n",
      " -11.26081778  -9.87169172 -10.0867118  -11.28979921 -11.06108477]; avg total score: -11.010166941632061\n",
      "learning timestep: 88000\n",
      "num batches:312\n",
      "Policy Loss: -0.06349645555019379\n",
      "Value Loss: 5.012845993041992\n",
      "Entropy: -81.44700622558594\n",
      "KL Divergence: 20.668689727783203\n",
      "episode: [1745. 1745. 1745. 1745. 1745. 1745. 1745. 1745. 1745. 1745.]; total steps: 89000; avg scores/10 episodes: [-11.61971986 -11.63273026 -10.947676   -11.84785346 -11.03590064\n",
      " -11.88593995 -12.40765106 -11.88828187 -10.45712805 -12.12948813]; avg total score: -11.585236928322878\n",
      "episode: [1764. 1764. 1764. 1764. 1764. 1764. 1764. 1764. 1764. 1764.]; total steps: 90000; avg scores/10 episodes: [-11.80182359 -12.35966562 -11.98259127 -12.64104245 -11.07568151\n",
      " -10.33827243 -12.75048522  -9.55428742 -10.8315254  -10.92947324]; avg total score: -11.42648481399841\n",
      "learning timestep: 90000\n",
      "num batches:312\n",
      "Policy Loss: -0.12169704586267471\n",
      "Value Loss: 10.008668899536133\n",
      "Entropy: -78.30441284179688\n",
      "KL Divergence: 26.452360153198242\n",
      "episode: [1784. 1784. 1784. 1784. 1784. 1784. 1784. 1784. 1784. 1784.]; total steps: 91000; avg scores/10 episodes: [-11.43702177 -10.83203689 -10.53760363 -11.00231245 -10.80451996\n",
      " -11.78234804 -11.17788835  -9.75472324 -11.80375271 -12.39632168]; avg total score: -11.15285287106953\n",
      "episode: [1803. 1803. 1803. 1803. 1803. 1803. 1803. 1803. 1803. 1803.]; total steps: 92000; avg scores/10 episodes: [-11.53156141 -11.71638711 -10.66227492 -10.77079265 -12.34162054\n",
      "  -9.4821817  -11.33894896 -10.84340269 -10.16392968 -11.35783012]; avg total score: -11.020892978855382\n",
      "learning timestep: 92000\n",
      "num batches:312\n",
      "Policy Loss: -0.03670887649059296\n",
      "Value Loss: 84.68294525146484\n",
      "Entropy: -80.5721664428711\n",
      "KL Divergence: 18.624893188476562\n",
      "episode: [1823. 1823. 1823. 1823. 1823. 1823. 1823. 1823. 1823. 1823.]; total steps: 93000; avg scores/10 episodes: [-11.68805429 -10.03221563 -10.99950283 -10.75991173 -10.56082468\n",
      " -10.55150467 -10.61084817 -10.00962393 -11.32847722 -11.90894454]; avg total score: -10.84499076881014\n",
      "episode: [1843. 1843. 1843. 1843. 1843. 1843. 1843. 1843. 1843. 1843.]; total steps: 94000; avg scores/10 episodes: [-11.06432637 -11.85308399 -10.2430832  -10.64780847  -9.67525277\n",
      " -12.04535848  -9.73753954 -11.30321486 -10.03629051 -12.37991861]; avg total score: -10.898587680669571\n",
      "learning timestep: 94000\n",
      "num batches:312\n",
      "Policy Loss: -0.15599830448627472\n",
      "Value Loss: 32.13597869873047\n",
      "Entropy: -79.4151611328125\n",
      "KL Divergence: 18.271686553955078\n",
      "episode: [1862. 1862. 1862. 1862. 1862. 1862. 1862. 1862. 1862. 1862.]; total steps: 95000; avg scores/10 episodes: [ -9.53220145 -11.72647948 -12.30529427 -10.96156455 -10.78270611\n",
      "  -9.90875844 -10.82676293 -10.95991664 -11.57702022 -11.17269901]; avg total score: -10.975340312893222\n",
      "episode: [1882. 1882. 1882. 1882. 1882. 1882. 1882. 1882. 1882. 1882.]; total steps: 96000; avg scores/10 episodes: [-11.52004396 -11.93737511 -12.54717577 -13.18482752 -10.36209647\n",
      " -10.86660119 -12.27717409 -10.66538738 -10.84136006 -11.61625173]; avg total score: -11.581829329259914\n",
      "learning timestep: 96000\n",
      "num batches:312\n",
      "Policy Loss: -0.24063843488693237\n",
      "Value Loss: 20.433387756347656\n",
      "Entropy: -75.42591857910156\n",
      "KL Divergence: 24.006996154785156\n",
      "episode: [1901. 1901. 1901. 1901. 1901. 1901. 1901. 1901. 1901. 1901.]; total steps: 97000; avg scores/10 episodes: [-11.14083365 -11.03775966 -12.81929474 -11.16207484 -10.7483728\n",
      " -11.42800683 -11.74035413 -12.86056175 -11.20141444 -10.27741451]; avg total score: -11.441608734480042\n",
      "episode: [1921. 1921. 1921. 1921. 1921. 1921. 1921. 1921. 1921. 1921.]; total steps: 98000; avg scores/10 episodes: [-11.48703411 -12.16538108  -9.70589908 -12.90695238 -11.55704482\n",
      " -11.15171036 -11.30053208 -11.02323721 -12.31217305 -11.2331209 ]; avg total score: -11.484308506201565\n",
      "learning timestep: 98000\n",
      "num batches:312\n",
      "Policy Loss: -0.20353280007839203\n",
      "Value Loss: 24.335376739501953\n",
      "Entropy: -71.89862060546875\n",
      "KL Divergence: 16.024276733398438\n",
      "episode: [1941. 1941. 1941. 1941. 1941. 1941. 1941. 1941. 1941. 1941.]; total steps: 99000; avg scores/10 episodes: [-11.93191947 -11.00565479 -11.17102766 -10.79222725  -9.59676594\n",
      "  -9.68827942 -10.10956125 -10.83361759 -11.88263932 -11.71875301]; avg total score: -10.873044569650244\n",
      "episode: [1960. 1960. 1960. 1960. 1960. 1960. 1960. 1960. 1960. 1960.]; total steps: 100000; avg scores/10 episodes: [-12.30502419 -10.90960245 -10.59871716 -12.09581623 -11.53820041\n",
      " -11.09115377 -10.83713917 -10.19596677  -9.26243778 -13.78030187]; avg total score: -11.261435980447333\n"
     ]
    }
   ],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "# env_id = 'Humanoid-v5'\n",
    "env_id = \"Reacher-v5\"\n",
    "\n",
    "timesteps = 100_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 10\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.01\n",
    "kl_coeff = 3.0\n",
    "loss = 'clipped'\n",
    "lambda_ = 0.0\n",
    "distribution = 'Normal'\n",
    "device = 'cuda'\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'PPO-Test'\n",
    "run_name = None\n",
    "# callbacks = [WandbCallback(project_name, run_name)]\n",
    "callbacks = []\n",
    "\n",
    "seed = 46\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff,\n",
    "          kl_coefficient=kl_coeff, loss=loss, lambda_=lambda_, callbacks=callbacks)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"Reacher-v5\"\n",
    "num_envs = 10\n",
    "env = gym.make_vec(env_id, num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = env.spec.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make_vec(gym.envs.registration.EnvSpec.from_json(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config_path = \"/workspaces/RL_Agents/pytorch/src/app/models/ppo/config.json\"\n",
    "with open(agent_config_path, 'r', encoding=\"utf-8\") as f:\n",
    "            agent_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO.load(agent_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.test(10, render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"ppo_test/renders\"\n",
    "num_episodes = 10\n",
    "render_freq = 1\n",
    "\n",
    "# Set the policy and value function models to evaluation mode\n",
    "ppo_agent_hybrid1.policy.eval()\n",
    "ppo_agent_hybrid1.value_model.eval()\n",
    "\n",
    "# Create the render directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "env = gym.make_vec(\n",
    "    ppo_agent_hybrid1.env.spec.id,\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        lambda env: gym.wrappers.RecordVideo(\n",
    "            env,\n",
    "            save_dir + \"/renders/test\",\n",
    "            episode_trigger=lambda episode_id: (episode_id + 1) % render_freq == 0\n",
    "        )\n",
    "    ],\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "scores = []\n",
    "entropy_list = []\n",
    "kl_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    state, _ = ppo_agent_hybrid1.env.reset()\n",
    "    score = 0\n",
    "    episode_entropy = 0\n",
    "    episode_kl = 0\n",
    "    steps = 0\n",
    "\n",
    "    prev_dist = None  # To track the previous distribution for KL divergence\n",
    "\n",
    "    # Video writer setup\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video_path = os.path.join(render_dir, f\"episode_{episode+1}.mp4\")\n",
    "    #     frame = self.env.render(mode='rgb_array')\n",
    "    #     height, width, layers = frame.shape\n",
    "    #     video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n",
    "\n",
    "    while not done:\n",
    "        # Render the environment and write the frame to the video file\n",
    "        # if episode % render_freq == 0:\n",
    "        #     frame = self.env.render(mode='rgb_array')\n",
    "        #     video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Get action and log probability from the current policy\n",
    "        action, log_prob = ppo_agent_hybrid1.get_action(state)\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            act = ppo_agent_hybrid1.action_adapter(action)\n",
    "        else:\n",
    "            act = action\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, term, trunc, _ = ppo_agent_hybrid1.env.step(act)\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "\n",
    "        # Calculate the distribution and entropy\n",
    "        dist = ppo_agent_hybrid1.policy(T.tensor(state, dtype=T.float32, device=ppo_agent_hybrid1.policy.device))\n",
    "        entropy = dist.entropy().sum().item()  # Sum entropy over actions\n",
    "\n",
    "        # Update KL divergence\n",
    "        if prev_dist is not None:\n",
    "            kl = kl_divergence(prev_dist, dist).sum().item()  # Sum KL divergence over actions\n",
    "        else:\n",
    "            kl = 0  # No KL divergence for the first step in the episode\n",
    "\n",
    "        # Update the previous distribution to the current one\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            param1_prev = dist.concentration1.clone().detach()\n",
    "            param2_prev = dist.concentration0.clone().detach()\n",
    "            prev_dist = Beta(param1_prev, param2_prev)\n",
    "        elif ppo_agent_hybrid1.distribution == 'Normal':\n",
    "            param1_prev = dist.loc.clone().detach()\n",
    "            param2_prev = dist.scale.clone().detach()\n",
    "            prev_dist = Normal(param1_prev, param2_prev)\n",
    "\n",
    "        # Accumulate the score, entropy, and KL divergence for the episode\n",
    "        score += reward\n",
    "        episode_entropy += entropy\n",
    "        episode_kl += kl\n",
    "        steps += 1\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Release the video writer\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video.release()\n",
    "\n",
    "    # Append the results for the episode\n",
    "    scores.append(score)\n",
    "    entropy_list.append(episode_entropy / steps)  # Average entropy over the episode\n",
    "    kl_list.append(episode_kl / steps)  # Average KL divergence over the episode\n",
    "\n",
    "    print(f'Episode {episode+1}/{num_episodes} - Score: {score}, Avg Entropy: {entropy_list[-1]}, Avg KL Divergence: {kl_list[-1]}')\n",
    "\n",
    "# close the environment\n",
    "ppo_agent_hybrid1.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
