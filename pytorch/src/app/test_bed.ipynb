{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, StochasticDiscretePolicy\n",
    "import cnn_models\n",
    "from rl_agents import PPO, DDPG, TD3, Reinforce, ActorCritic, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "import helper\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000)\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(100, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/pytorch/src/app/wandb/run-20240822_022108-6mcbidkn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/PPO-Test/runs/6mcbidkn' target=\"_blank\">train-28</a></strong> to <a href='https://wandb.ai/jasonhayes1987/PPO-Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/PPO-Test' target=\"_blank\">https://wandb.ai/jasonhayes1987/PPO-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/PPO-Test/runs/6mcbidkn' target=\"_blank\">https://wandb.ai/jasonhayes1987/PPO-Test/runs/6mcbidkn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]; total steps: 1000; avg scores/10 episodes: [-1443.33148989 -1159.94969145 -1238.18671389 -1121.57276187\n",
      " -1240.62911184 -1025.52938336  -993.56244371 -1149.04193582\n",
      " -1482.21391396 -1498.64048666]; avg total score: -1235.265793245621\n",
      "episode: [9. 9. 9. 9. 9. 9. 9. 9. 9. 9.]; total steps: 2000; avg scores/10 episodes: [-1278.81936313 -1233.8599339  -1208.17378052 -1177.67891041\n",
      " -1145.39946602 -1184.70371588 -1108.54858696 -1208.60483809\n",
      " -1261.94113535 -1380.22936354]; avg total score: -1218.7959093785248\n",
      "learning timestep: 2048\n",
      "num batches:32\n",
      "Policy Loss: -0.07108922302722931\n",
      "Value Loss: 4.323733806610107\n",
      "Entropy: 742.5741577148438\n",
      "KL Divergence: 2.2265703678131104\n",
      "Lambda: 0.5046228170394897\n",
      "episode: [14. 14. 14. 14. 14. 14. 14. 14. 14. 14.]; total steps: 3000; avg scores/10 episodes: [-1201.32930899 -1304.03190283 -1272.98320143 -1224.45404497\n",
      " -1162.90734844 -1224.20600471 -1166.23559144 -1186.5136796\n",
      " -1122.43329891 -1271.96549677]; avg total score: -1213.7059878073956\n",
      "episode: [19. 19. 19. 19. 19. 19. 19. 19. 19. 19.]; total steps: 4000; avg scores/10 episodes: [-1224.88742195 -1180.27490006 -1363.69920037 -1339.91100709\n",
      " -1168.98548638 -1101.06031727 -1220.45633241 -1311.24656009\n",
      " -1239.89604122 -1270.99786304]; avg total score: -1242.1415129884901\n",
      "learning timestep: 4096\n",
      "num batches:32\n",
      "Policy Loss: 0.035682715475559235\n",
      "Value Loss: 2.8852367401123047\n",
      "Entropy: 754.25537109375\n",
      "KL Divergence: 0.4074416756629944\n",
      "Lambda: 0.5145431160926819\n",
      "episode: [24. 24. 24. 24. 24. 24. 24. 24. 24. 24.]; total steps: 5000; avg scores/10 episodes: [-1334.54349208 -1256.93713917 -1212.45189662 -1276.00603489\n",
      " -1152.87009052 -1187.82372378 -1292.88545521 -1243.62044547\n",
      " -1371.20742984 -1236.7373442 ]; avg total score: -1256.5083051786064\n",
      "episode: [29. 29. 29. 29. 29. 29. 29. 29. 29. 29.]; total steps: 6000; avg scores/10 episodes: [-1421.51616846 -1451.45470783 -1115.53106584 -1107.8343063\n",
      " -1089.18659122 -1329.98697376 -1189.36398078 -1276.55176946\n",
      " -1305.8744308  -1257.1197276 ]; avg total score: -1254.4419722049875\n",
      "learning timestep: 6144\n",
      "num batches:32\n",
      "Policy Loss: 0.010326378978788853\n",
      "Value Loss: 1.7181533575057983\n",
      "Entropy: 765.142333984375\n",
      "KL Divergence: 0.3414054811000824\n",
      "Lambda: 0.5287555456161499\n",
      "episode: [34. 34. 34. 34. 34. 34. 34. 34. 34. 34.]; total steps: 7000; avg scores/10 episodes: [-1335.13751218 -1316.44355802 -1194.81933446 -1086.00258527\n",
      " -1033.96995592 -1239.49141088 -1106.15695153 -1466.25259668\n",
      " -1288.06293516 -1247.05503636]; avg total score: -1231.3391876451483\n",
      "episode: [39. 39. 39. 39. 39. 39. 39. 39. 39. 39.]; total steps: 8000; avg scores/10 episodes: [-1298.97026334 -1131.13661698 -1219.4552244  -1121.66246482\n",
      " -1301.24431365 -1129.20179362 -1131.56823346 -1267.51952871\n",
      " -1305.1977897  -1152.77858233]; avg total score: -1205.8734811011714\n",
      "learning timestep: 8192\n",
      "num batches:32\n",
      "Policy Loss: 0.00181225361302495\n",
      "Value Loss: 1.755999207496643\n",
      "Entropy: 776.2638549804688\n",
      "KL Divergence: 0.9168233275413513\n",
      "Lambda: 0.5456318259239197\n",
      "episode: [44. 44. 44. 44. 44. 44. 44. 44. 44. 44.]; total steps: 9000; avg scores/10 episodes: [-1197.7261734  -1197.22560331 -1273.26498737 -1216.02566675\n",
      " -1385.25698661 -1205.86283708 -1301.41907361 -1120.93717086\n",
      " -1314.45142781 -1208.14977902]; avg total score: -1242.0319705832483\n",
      "episode: [49. 49. 49. 49. 49. 49. 49. 49. 49. 49.]; total steps: 10000; avg scores/10 episodes: [-1147.89134591 -1165.05189137 -1417.65483729 -1179.90624699\n",
      " -1248.23079905 -1120.415783   -1402.51021239 -1152.46637954\n",
      " -1300.9861548  -1173.34024018]; avg total score: -1230.8453890531985\n",
      "learning timestep: 10240\n",
      "num batches:32\n",
      "Policy Loss: -0.048241205513477325\n",
      "Value Loss: 2.350571870803833\n",
      "Entropy: 787.9473266601562\n",
      "KL Divergence: 0.5417995452880859\n",
      "Lambda: 0.5637246966362\n",
      "episode: [54. 54. 54. 54. 54. 54. 54. 54. 54. 54.]; total steps: 11000; avg scores/10 episodes: [-1340.00460105 -1100.03638719 -1330.71057403 -1066.23015815\n",
      " -1318.68145546 -1051.55816191 -1361.62256485 -1161.18656547\n",
      " -1105.39271288 -1142.1209057 ]; avg total score: -1197.7544086687035\n",
      "episode: [59. 59. 59. 59. 59. 59. 59. 59. 59. 59.]; total steps: 12000; avg scores/10 episodes: [-1300.20451165 -1319.14344442 -1289.6663959  -1197.04182116\n",
      " -1365.72713871 -1243.61085176 -1264.0641659  -1174.03000213\n",
      " -1032.06479889 -1266.81996285]; avg total score: -1245.2373093364722\n",
      "learning timestep: 12288\n",
      "num batches:32\n",
      "Policy Loss: -0.0223994143307209\n",
      "Value Loss: 1.5214468240737915\n",
      "Entropy: 804.3780517578125\n",
      "KL Divergence: 0.24866758286952972\n",
      "Lambda: 0.5823937654495239\n",
      "episode: [64. 64. 64. 64. 64. 64. 64. 64. 64. 64.]; total steps: 13000; avg scores/10 episodes: [-1194.41739835 -1370.08160621 -1345.02342406 -1243.60366573\n",
      " -1345.56864741 -1406.83632552 -1203.66151195 -1220.29698373\n",
      " -1159.84330997 -1320.57221614]; avg total score: -1280.9905089062352\n",
      "episode: [69. 69. 69. 69. 69. 69. 69. 69. 69. 69.]; total steps: 14000; avg scores/10 episodes: [-1339.50633852 -1361.49329627 -1215.17535549 -1208.54854402\n",
      " -1413.70489515 -1339.04772425 -1116.57576438 -1272.39269255\n",
      " -1265.5701988  -1306.10423724]; avg total score: -1283.8119046677934\n",
      "learning timestep: 14336\n",
      "num batches:32\n",
      "Policy Loss: -0.08482611924409866\n",
      "Value Loss: 1.6268692016601562\n",
      "Entropy: 809.5429077148438\n",
      "KL Divergence: 1.7266191244125366\n",
      "Lambda: 0.6022858023643494\n",
      "episode: [74. 74. 74. 74. 74. 74. 74. 74. 74. 74.]; total steps: 15000; avg scores/10 episodes: [-1408.60647831 -1273.67178238 -1160.44177676 -1261.79414563\n",
      " -1327.3674079  -1296.10691922  -941.31274318 -1481.01633877\n",
      " -1165.72226879 -1227.41284313]; avg total score: -1254.3452704088327\n",
      "episode: [79. 79. 79. 79. 79. 79. 79. 79. 79. 79.]; total steps: 16000; avg scores/10 episodes: [-1320.34775456 -1182.95959376 -1219.18448163 -1234.39193343\n",
      " -1211.66928097 -1305.14205715 -1076.1899298  -1405.43661382\n",
      " -1138.93295888 -1087.56832981]; avg total score: -1218.1822933819033\n",
      "learning timestep: 16384\n",
      "num batches:32\n",
      "Policy Loss: -0.009537335485219955\n",
      "Value Loss: 1.1447806358337402\n",
      "Entropy: 817.6968994140625\n",
      "KL Divergence: 0.9928903579711914\n",
      "Lambda: 0.6230905055999756\n",
      "episode: [84. 84. 84. 84. 84. 84. 84. 84. 84. 84.]; total steps: 17000; avg scores/10 episodes: [-1368.8940776  -1304.75747629 -1270.28636071 -1211.03133832\n",
      " -1229.82170688 -1271.88083784 -1337.53631596 -1308.06058418\n",
      " -1358.75363148 -1004.67382619]; avg total score: -1266.5696155445723\n",
      "episode: [89. 89. 89. 89. 89. 89. 89. 89. 89. 89.]; total steps: 18000; avg scores/10 episodes: [-1332.06973371 -1240.99922975 -1330.10194588 -1187.21979608\n",
      " -1265.94088082 -1350.73561186 -1352.66455128 -1262.49179959\n",
      " -1435.03224629 -1160.8185585 ]; avg total score: -1291.8074353768225\n",
      "learning timestep: 18432\n",
      "num batches:32\n",
      "Policy Loss: -0.05454205721616745\n",
      "Value Loss: 1.7973804473876953\n",
      "Entropy: 836.3986206054688\n",
      "KL Divergence: 0.7099252939224243\n",
      "Lambda: 0.644087553024292\n",
      "episode: [94. 94. 94. 94. 94. 94. 94. 94. 94. 94.]; total steps: 19000; avg scores/10 episodes: [-1310.20388095 -1136.97442811 -1379.20425138 -1239.75241721\n",
      " -1303.97491801 -1225.44903688 -1425.91830842 -1119.13186964\n",
      " -1284.12931057 -1134.78696558]; avg total score: -1255.9525386743353\n",
      "episode: [99. 99. 99. 99. 99. 99. 99. 99. 99. 99.]; total steps: 20000; avg scores/10 episodes: [-1342.88918585 -1142.8649217  -1291.86692741 -1169.55714757\n",
      " -1288.69633571 -1156.90508995 -1424.843535   -1100.62626604\n",
      " -1241.10722191 -1071.82681672]; avg total score: -1223.1183447852313\n",
      "learning timestep: 20480\n",
      "num batches:32\n",
      "Policy Loss: -0.04891696572303772\n",
      "Value Loss: 2.600440740585327\n",
      "Entropy: 851.8198852539062\n",
      "KL Divergence: 1.696067214012146\n",
      "Lambda: 0.6684500575065613\n",
      "episode: [104. 104. 104. 104. 104. 104. 104. 104. 104. 104.]; total steps: 21000; avg scores/10 episodes: [-1166.3129495  -1087.46962569 -1296.8545041  -1231.06964988\n",
      " -1379.84781493 -1191.69040957 -1337.36542284 -1108.96894442\n",
      " -1234.97312739 -1209.74431062]; avg total score: -1224.4296758949085\n",
      "episode: [109. 109. 109. 109. 109. 109. 109. 109. 109. 109.]; total steps: 22000; avg scores/10 episodes: [-1090.90851907 -1065.88331447 -1474.59323539 -1275.4436449\n",
      " -1316.59787832 -1273.64302003 -1287.63924042 -1037.84267823\n",
      " -1253.90230593 -1195.59314678]; avg total score: -1227.2046983532393\n",
      "learning timestep: 22528\n",
      "num batches:32\n",
      "Policy Loss: -0.01079760305583477\n",
      "Value Loss: 2.4900612831115723\n",
      "Entropy: 860.6795654296875\n",
      "KL Divergence: 1.7089804410934448\n",
      "Lambda: 0.6924732327461243\n",
      "episode: [114. 114. 114. 114. 114. 114. 114. 114. 114. 114.]; total steps: 23000; avg scores/10 episodes: [-1174.98995547 -1110.37471192 -1384.21456359 -1219.44442367\n",
      " -1129.22239631 -1334.50968013 -1348.99024974 -1023.61028589\n",
      " -1293.06063925 -1206.75281817]; avg total score: -1222.516972414614\n",
      "episode: [119. 119. 119. 119. 119. 119. 119. 119. 119. 119.]; total steps: 24000; avg scores/10 episodes: [-1322.92480854 -1115.70320441 -1216.19364823 -1176.65564944\n",
      " -1228.80579136 -1154.9518298  -1361.72410106 -1081.25237926\n",
      " -1172.77133241 -1287.66280222]; avg total score: -1211.8645546733846\n",
      "learning timestep: 24576\n",
      "num batches:32\n",
      "Policy Loss: -0.07978887856006622\n",
      "Value Loss: 2.7148044109344482\n",
      "Entropy: 852.2825927734375\n",
      "KL Divergence: 2.260899066925049\n",
      "Lambda: 0.7153369188308716\n",
      "episode: [124. 124. 124. 124. 124. 124. 124. 124. 124. 124.]; total steps: 25000; avg scores/10 episodes: [-1341.9621676  -1027.54737273 -1153.63946229 -1028.74937337\n",
      " -1264.49330722 -1103.05317427 -1197.04153554 -1063.05913931\n",
      " -1234.98414553 -1298.71380114]; avg total score: -1171.3243479010348\n",
      "episode: [129. 129. 129. 129. 129. 129. 129. 129. 129. 129.]; total steps: 26000; avg scores/10 episodes: [-1250.31688265 -1072.77503433 -1107.14712351 -1257.84145814\n",
      " -1193.22988914 -1216.8975281  -1069.05370998 -1209.25166865\n",
      " -1317.70246482 -1320.4482137 ]; avg total score: -1201.4663973021156\n",
      "learning timestep: 26624\n",
      "num batches:32\n",
      "Policy Loss: -0.01654965803027153\n",
      "Value Loss: 3.4293289184570312\n",
      "Entropy: 858.2987670898438\n",
      "KL Divergence: 2.862481117248535\n",
      "Lambda: 0.7373802065849304\n",
      "episode: [134. 134. 134. 134. 134. 134. 134. 134. 134. 134.]; total steps: 27000; avg scores/10 episodes: [-1228.20732768 -1158.15343047 -1070.30147884 -1365.35966028\n",
      " -1181.33073622 -1263.4675777  -1086.68265607 -1201.18814418\n",
      " -1231.59184638 -1325.71098026]; avg total score: -1211.19938380771\n",
      "episode: [139. 139. 139. 139. 139. 139. 139. 139. 139. 139.]; total steps: 28000; avg scores/10 episodes: [-1218.94344916 -1255.4151923  -1080.97963678 -1189.49193791\n",
      " -1230.86475505 -1205.59241384 -1227.41590823 -1210.19746086\n",
      " -1215.70997899 -1192.78660757]; avg total score: -1202.7397340688353\n",
      "learning timestep: 28672\n",
      "num batches:32\n",
      "Policy Loss: 0.0017725457437336445\n",
      "Value Loss: 3.1859004497528076\n",
      "Entropy: 876.4508666992188\n",
      "KL Divergence: 2.503099203109741\n",
      "Lambda: 0.7582085132598877\n",
      "episode: [144. 144. 144. 144. 144. 144. 144. 144. 144. 144.]; total steps: 29000; avg scores/10 episodes: [-1208.23028348 -1388.64553123 -1189.46911964 -1165.23955556\n",
      " -1263.42435214 -1085.49630024 -1315.04853374 -1272.5463824\n",
      " -1070.72104105 -1147.19788236]; avg total score: -1210.6018981837678\n",
      "episode: [149. 149. 149. 149. 149. 149. 149. 149. 149. 149.]; total steps: 30000; avg scores/10 episodes: [-1147.9662743  -1390.90812609 -1171.5368988  -1120.37535525\n",
      " -1345.08520518 -1113.39728578 -1243.04485047 -1199.37693036\n",
      " -1079.24851593 -1173.6154176 ]; avg total score: -1198.4554859765235\n",
      "learning timestep: 30720\n",
      "num batches:32\n",
      "Policy Loss: 0.03812643885612488\n",
      "Value Loss: 2.8441925048828125\n",
      "Entropy: 871.542236328125\n",
      "KL Divergence: 3.7967424392700195\n",
      "Lambda: 0.7778163552284241\n",
      "episode: [154. 154. 154. 154. 154. 154. 154. 154. 154. 154.]; total steps: 31000; avg scores/10 episodes: [-1108.1514643  -1227.41430287 -1217.21019119 -1072.97896259\n",
      " -1449.23065678 -1242.61976773 -1025.03353994 -1205.15219096\n",
      " -1229.697183   -1180.01233237]; avg total score: -1195.7500591742994\n",
      "episode: [159. 159. 159. 159. 159. 159. 159. 159. 159. 159.]; total steps: 32000; avg scores/10 episodes: [-1134.65856612 -1046.74644519 -1266.94587834 -1146.49781816\n",
      " -1300.19901968 -1073.7551582  -1142.86987198 -1244.58770766\n",
      " -1332.27937041 -1201.47713876]; avg total score: -1189.0016974504479\n",
      "learning timestep: 32768\n",
      "num batches:32\n",
      "Policy Loss: 0.0070670051500201225\n",
      "Value Loss: 3.8231217861175537\n",
      "Entropy: 891.064697265625\n",
      "KL Divergence: 9.215319633483887\n",
      "Lambda: 0.7995033264160156\n",
      "episode: [164. 164. 164. 164. 164. 164. 164. 164. 164. 164.]; total steps: 33000; avg scores/10 episodes: [-1226.16082361 -1134.21839794 -1252.35610317 -1104.49469956\n",
      " -1222.01767304  -997.93255529 -1132.7846097  -1203.18223782\n",
      " -1178.21123144 -1275.97563044]; avg total score: -1172.73339620046\n",
      "episode: [169. 169. 169. 169. 169. 169. 169. 169. 169. 169.]; total steps: 34000; avg scores/10 episodes: [-1254.54076084 -1306.0954135  -1283.77517121  -982.55370719\n",
      " -1243.70897258 -1140.98138664 -1151.49043223 -1075.8377765\n",
      " -1051.33990427 -1449.81390852]; avg total score: -1194.0137433494988\n",
      "learning timestep: 34816\n",
      "num batches:32\n",
      "Policy Loss: 0.03705583140254021\n",
      "Value Loss: 3.4468657970428467\n",
      "Entropy: 888.0090942382812\n",
      "KL Divergence: 6.679487228393555\n",
      "Lambda: 0.8171204924583435\n",
      "episode: [174. 174. 174. 174. 174. 174. 174. 174. 174. 174.]; total steps: 35000; avg scores/10 episodes: [-1207.66202435 -1250.9382895  -1151.60022068 -1115.20463316\n",
      " -1174.51694438 -1206.51441899 -1241.0033901  -1204.88292685\n",
      " -1340.0948852  -1361.70691049]; avg total score: -1225.4124643691375\n",
      "episode: [179. 179. 179. 179. 179. 179. 179. 179. 179. 179.]; total steps: 36000; avg scores/10 episodes: [-1329.34014863 -1169.33118707 -1176.21948439 -1366.58473311\n",
      " -1238.81485097 -1131.71632235 -1089.4713253  -1313.95023041\n",
      " -1340.33341392 -1141.66648077]; avg total score: -1229.7428176909953\n",
      "learning timestep: 36864\n",
      "num batches:32\n",
      "Policy Loss: 0.020072460174560547\n",
      "Value Loss: 3.7070748805999756\n",
      "Entropy: 892.98681640625\n",
      "KL Divergence: 10.924610137939453\n",
      "Lambda: 0.8333693146705627\n",
      "episode: [184. 184. 184. 184. 184. 184. 184. 184. 184. 184.]; total steps: 37000; avg scores/10 episodes: [-1389.76586481 -1184.04072649 -1211.7580297  -1337.56524076\n",
      " -1271.18662616 -1028.94172182 -1081.80316416 -1200.32415529\n",
      " -1238.37668911 -1071.5002417 ]; avg total score: -1201.526246000766\n",
      "episode: [189. 189. 189. 189. 189. 189. 189. 189. 189. 189.]; total steps: 38000; avg scores/10 episodes: [-1305.21887516 -1237.16764378 -1104.99077467 -1092.4678077\n",
      " -1136.12373711 -1089.20702529 -1179.71900379 -1139.95228777\n",
      " -1271.08836676 -1127.5539116 ]; avg total score: -1168.3489433612854\n",
      "learning timestep: 38912\n",
      "num batches:32\n",
      "Policy Loss: 0.019254185259342194\n",
      "Value Loss: 4.219930171966553\n",
      "Entropy: 909.993408203125\n",
      "KL Divergence: 9.263640403747559\n",
      "Lambda: 0.8490137457847595\n",
      "episode: [194. 194. 194. 194. 194. 194. 194. 194. 194. 194.]; total steps: 39000; avg scores/10 episodes: [-1148.32974205 -1211.34795459 -1030.53477116 -1003.21232267\n",
      " -1113.85101159 -1040.52345878 -1227.09237654 -1174.89028025\n",
      " -1201.21013871 -1244.88507844]; avg total score: -1139.5877134788705\n",
      "episode: [199. 199. 199. 199. 199. 199. 199. 199. 199. 199.]; total steps: 40000; avg scores/10 episodes: [-1072.87243831 -1152.47976415  -945.79079051 -1134.18769427\n",
      " -1325.97561603 -1026.56273202 -1211.33594076 -1266.87041564\n",
      " -1117.66379534 -1155.64309459]; avg total score: -1140.9382281622034\n",
      "learning timestep: 40960\n",
      "num batches:32\n",
      "Policy Loss: 0.012191799469292164\n",
      "Value Loss: 1.858677864074707\n",
      "Entropy: 924.5414428710938\n",
      "KL Divergence: 19.621597290039062\n",
      "Lambda: 0.862998366355896\n",
      "episode: [203. 203. 203. 203. 203. 203. 203. 203. 203. 203.]; total steps: 41000; avg scores/10 episodes: [-1144.06755706 -1057.77062739 -1045.07210149 -1302.1849844\n",
      " -1371.75598071 -1203.51096433 -1164.99070137 -1248.98921789\n",
      " -1099.2810965  -1037.35667125]; avg total score: -1167.4979902395094\n",
      "episode: [208. 208. 208. 208. 208. 208. 208. 208. 208. 208.]; total steps: 42000; avg scores/10 episodes: [-1065.77832712 -1103.08960245 -1093.77555526 -1246.13144499\n",
      " -1297.21833232 -1254.90172921  -938.16611018 -1230.17912914\n",
      " -1066.84028098 -1011.84444152]; avg total score: -1130.7924953167756\n",
      "episode: [213. 213. 213. 213. 213. 213. 213. 213. 213. 213.]; total steps: 43000; avg scores/10 episodes: [ -961.03134826 -1214.85719859 -1191.32163862 -1120.00308248\n",
      " -1448.04321119 -1089.73032171 -1031.34162013 -1228.05081997\n",
      " -1058.28680906 -1175.44173406]; avg total score: -1151.8107784073636\n",
      "learning timestep: 43008\n",
      "num batches:32\n",
      "Policy Loss: 0.001752778422087431\n",
      "Value Loss: 2.2203657627105713\n",
      "Entropy: 934.3917236328125\n",
      "KL Divergence: 14.877473831176758\n",
      "Lambda: 0.8744686245918274\n",
      "episode: [218. 218. 218. 218. 218. 218. 218. 218. 218. 218.]; total steps: 44000; avg scores/10 episodes: [-1060.05705291 -1282.92976744 -1246.05080526 -1241.64617635\n",
      " -1292.05045332 -1072.5371055  -1131.10760882 -1265.01853494\n",
      " -1019.63523961 -1131.59301265]; avg total score: -1174.2625756811865\n",
      "episode: [223. 223. 223. 223. 223. 223. 223. 223. 223. 223.]; total steps: 45000; avg scores/10 episodes: [-1192.75880857 -1156.99823306 -1134.33569278 -1139.21796581\n",
      " -1079.19512839 -1328.94114771 -1241.00721086 -1294.86425922\n",
      " -1198.32037312 -1156.22685872]; avg total score: -1192.18656782361\n",
      "learning timestep: 45056\n",
      "num batches:32\n",
      "Policy Loss: -0.0696415901184082\n",
      "Value Loss: 2.756803274154663\n",
      "Entropy: 944.845947265625\n",
      "KL Divergence: 20.042598724365234\n",
      "Lambda: 0.8860338926315308\n",
      "episode: [228. 228. 228. 228. 228. 228. 228. 228. 228. 228.]; total steps: 46000; avg scores/10 episodes: [-1185.44598365 -1078.65652764 -1142.24972948 -1030.29303063\n",
      " -1147.05054279 -1179.83488987 -1302.37170517 -1159.40661151\n",
      " -1252.04331035 -1144.80874207]; avg total score: -1162.2161073164439\n",
      "episode: [233. 233. 233. 233. 233. 233. 233. 233. 233. 233.]; total steps: 47000; avg scores/10 episodes: [-1144.63273846 -1143.12878017 -1111.85694809 -1176.71180834\n",
      " -1220.6340684   -998.76115266 -1120.68919511 -1106.29177409\n",
      " -1290.47983005 -1001.77832585]; avg total score: -1131.4964621228842\n",
      "learning timestep: 47104\n",
      "num batches:32\n",
      "Policy Loss: 0.03696576505899429\n",
      "Value Loss: 3.0969295501708984\n",
      "Entropy: 947.5623168945312\n",
      "KL Divergence: 25.99547576904297\n",
      "Lambda: 0.8976297974586487\n",
      "episode: [238. 238. 238. 238. 238. 238. 238. 238. 238. 238.]; total steps: 48000; avg scores/10 episodes: [-1214.54141356 -1260.54006409 -1165.60840332 -1130.30715628\n",
      " -1176.25510985 -1034.15302995 -1087.28804236 -1051.26093881\n",
      " -1300.79475063 -1201.01753071]; avg total score: -1162.1766439567346\n",
      "episode: [243. 243. 243. 243. 243. 243. 243. 243. 243. 243.]; total steps: 49000; avg scores/10 episodes: [-1299.70809495 -1331.36495295 -1231.59993062 -1131.29885108\n",
      " -1116.66313217 -1099.88373058 -1119.00415576  -952.96554185\n",
      " -1260.96257879 -1209.35349913]; avg total score: -1175.2804467897213\n",
      "learning timestep: 49152\n",
      "num batches:32\n",
      "Policy Loss: 0.0995214506983757\n",
      "Value Loss: 2.7968506813049316\n",
      "Entropy: 971.18505859375\n",
      "KL Divergence: 21.427696228027344\n",
      "Lambda: 0.9071296453475952\n",
      "episode: [248. 248. 248. 248. 248. 248. 248. 248. 248. 248.]; total steps: 50000; avg scores/10 episodes: [-1091.50449993 -1148.1147483  -1122.94239401 -1186.20897586\n",
      " -1088.58289333 -1329.97749004 -1098.22702709 -1022.96846098\n",
      " -1314.21939837 -1149.04212505]; avg total score: -1155.1788012961133\n",
      "episode: [253. 253. 253. 253. 253. 253. 253. 253. 253. 253.]; total steps: 51000; avg scores/10 episodes: [ -928.46516336  -973.67221148  -995.4001506  -1056.11603215\n",
      " -1043.19182028 -1266.66240116 -1135.81443585 -1211.82602965\n",
      " -1175.96992062 -1116.26786577]; avg total score: -1090.3386030929128\n",
      "learning timestep: 51200\n",
      "num batches:32\n",
      "Policy Loss: 0.00601415429264307\n",
      "Value Loss: 2.163397789001465\n",
      "Entropy: 981.04931640625\n",
      "KL Divergence: 35.446495056152344\n",
      "Lambda: 0.9161307215690613\n",
      "episode: [258. 258. 258. 258. 258. 258. 258. 258. 258. 258.]; total steps: 52000; avg scores/10 episodes: [-1047.14241418 -1222.25730931  -979.02306736 -1123.03612778\n",
      " -1015.22112466 -1152.06458183 -1219.56339295 -1117.45041915\n",
      " -1075.20798047 -1143.41578686]; avg total score: -1109.4382204552553\n",
      "episode: [263. 263. 263. 263. 263. 263. 263. 263. 263. 263.]; total steps: 53000; avg scores/10 episodes: [-1134.07197372 -1301.9037454  -1063.78290068 -1220.71186392\n",
      " -1054.52374045 -1149.88967696 -1096.27511973 -1009.0130353\n",
      " -1146.44568182 -1190.13115635]; avg total score: -1136.6748894326374\n",
      "learning timestep: 53248\n",
      "num batches:32\n",
      "Policy Loss: -0.06223921477794647\n",
      "Value Loss: 2.3544952869415283\n",
      "Entropy: 999.946533203125\n",
      "KL Divergence: 44.05937957763672\n",
      "Lambda: 0.9250290393829346\n",
      "episode: [268. 268. 268. 268. 268. 268. 268. 268. 268. 268.]; total steps: 54000; avg scores/10 episodes: [-1125.46706593 -1099.38822836 -1148.47279983 -1185.61696906\n",
      " -1161.04290762 -1048.13168735 -1075.67412308 -1006.09244522\n",
      " -1146.89914487 -1077.51508755]; avg total score: -1107.430045887229\n",
      "episode: [273. 273. 273. 273. 273. 273. 273. 273. 273. 273.]; total steps: 55000; avg scores/10 episodes: [-1106.11912095  -992.29512834 -1091.22679501 -1100.16945144\n",
      " -1246.14899513 -1092.81132092 -1202.8161814  -1092.84036543\n",
      " -1029.97265058 -1126.87429028]; avg total score: -1108.1274299494207\n",
      "learning timestep: 55296\n",
      "num batches:32\n",
      "Policy Loss: 0.01979711838066578\n",
      "Value Loss: 4.3434648513793945\n",
      "Entropy: 1018.2598876953125\n",
      "KL Divergence: 57.617435455322266\n",
      "Lambda: 0.9331902861595154\n",
      "episode: [278. 278. 278. 278. 278. 278. 278. 278. 278. 278.]; total steps: 56000; avg scores/10 episodes: [-1052.3292562   -909.98156157 -1049.97679944  -976.19070943\n",
      " -1198.27333776 -1040.30978326 -1048.47486359 -1091.83449788\n",
      " -1154.43979895 -1058.74043088]; avg total score: -1058.0551038943863\n",
      "episode: [283. 283. 283. 283. 283. 283. 283. 283. 283. 283.]; total steps: 57000; avg scores/10 episodes: [-1037.75425049  -919.09591765 -1100.7740633  -1039.78325113\n",
      " -1060.82640733  -970.18921748  -915.65986547 -1003.73178663\n",
      " -1204.79559531  -991.66509912]; avg total score: -1024.4275453896903\n",
      "learning timestep: 57344\n",
      "num batches:32\n",
      "Policy Loss: -0.0399724505841732\n",
      "Value Loss: 3.377485990524292\n",
      "Entropy: 1037.585693359375\n",
      "KL Divergence: 61.4117317199707\n",
      "Lambda: 0.9402284026145935\n",
      "episode: [288. 288. 288. 288. 288. 288. 288. 288. 288. 288.]; total steps: 58000; avg scores/10 episodes: [-1154.58873193 -1114.95152871 -1017.70579675 -1180.54291829\n",
      " -1008.97506424  -966.85472971  -954.13133068 -1016.46411241\n",
      " -1041.15177529 -1127.7674859 ]; avg total score: -1058.3133473915877\n",
      "episode: [293. 293. 293. 293. 293. 293. 293. 293. 293. 293.]; total steps: 59000; avg scores/10 episodes: [-1256.53617269 -1119.48155461 -1057.09171866 -1112.00370676\n",
      " -1050.9075307   -922.28021149  -961.46264214 -1004.81759871\n",
      "  -947.78974232 -1106.26205555]; avg total score: -1053.86329336071\n",
      "learning timestep: 59392\n",
      "num batches:32\n",
      "Policy Loss: -0.022472286596894264\n",
      "Value Loss: 3.6309564113616943\n",
      "Entropy: 1044.9599609375\n",
      "KL Divergence: 87.60350036621094\n",
      "Lambda: 0.9464847445487976\n",
      "episode: [298. 298. 298. 298. 298. 298. 298. 298. 298. 298.]; total steps: 60000; avg scores/10 episodes: [-1227.97284575  -923.91932956 -1127.41311873 -1019.44492338\n",
      " -1224.94306152  -978.27416344  -880.57348    -1080.0319895\n",
      "  -959.51549132 -1001.08226701]; avg total score: -1042.3170670208856\n",
      "episode: [303. 303. 303. 303. 303. 303. 303. 303. 303. 303.]; total steps: 61000; avg scores/10 episodes: [-1003.46586862  -908.030401    -989.95157545  -962.86401708\n",
      " -1129.2100184  -1053.01101318  -909.27073159  -951.34896099\n",
      " -1100.69243759 -1008.20091414]; avg total score: -1001.6045938048798\n",
      "learning timestep: 61440\n",
      "num batches:32\n",
      "Policy Loss: -0.012541169300675392\n",
      "Value Loss: 3.8083107471466064\n",
      "Entropy: 1058.744384765625\n",
      "KL Divergence: 101.70062255859375\n",
      "Lambda: 0.9526652097702026\n",
      "episode: [308. 308. 308. 308. 308. 308. 308. 308. 308. 308.]; total steps: 62000; avg scores/10 episodes: [ -866.66627442  -936.36705526  -994.45830005 -1052.51191846\n",
      "  -914.54290655 -1033.88973666  -941.19629277  -946.16568494\n",
      "  -977.20847771 -1026.50122417]; avg total score: -968.9507870970361\n",
      "episode: [313. 313. 313. 313. 313. 313. 313. 313. 313. 313.]; total steps: 63000; avg scores/10 episodes: [ -930.14466712  -861.14558402 -1053.32399338 -1062.46812011\n",
      "  -941.9443899   -988.31812243  -945.29867623  -976.59269817\n",
      "  -851.52370628  -953.25569875]; avg total score: -956.4015656387428\n",
      "learning timestep: 63488\n",
      "num batches:32\n",
      "Policy Loss: -0.06109226495027542\n",
      "Value Loss: 4.323866367340088\n",
      "Entropy: 1068.371826171875\n",
      "KL Divergence: 129.9513397216797\n",
      "Lambda: 0.9579974412918091\n",
      "episode: [318. 318. 318. 318. 318. 318. 318. 318. 318. 318.]; total steps: 64000; avg scores/10 episodes: [ -953.77143846  -777.61556749 -1120.63748061  -888.04410962\n",
      "  -972.98837274  -943.50530495 -1096.98929764  -887.84467612\n",
      "  -901.54060594  -903.95471431]; avg total score: -944.6891567889445\n",
      "episode: [323. 323. 323. 323. 323. 323. 323. 323. 323. 323.]; total steps: 65000; avg scores/10 episodes: [ -892.04026129  -957.71958257 -1064.35878809  -871.06205704\n",
      "  -993.26347371  -927.28354826 -1011.68194575  -890.64307364\n",
      "  -956.17247832  -864.89380895]; avg total score: -942.9119017606281\n",
      "learning timestep: 65536\n",
      "num batches:32\n",
      "Policy Loss: -0.035441093146800995\n",
      "Value Loss: 11.410543441772461\n",
      "Entropy: 1081.4310302734375\n",
      "KL Divergence: 160.8055419921875\n",
      "Lambda: 0.962497889995575\n",
      "episode: [328. 328. 328. 328. 328. 328. 328. 328. 328. 328.]; total steps: 66000; avg scores/10 episodes: [ -796.68379121  -955.47788103  -923.64998518  -887.70787532\n",
      "  -992.77679174  -928.415993    -909.01895198  -910.38956259\n",
      " -1003.77846746  -818.15135891]; avg total score: -912.605065840967\n",
      "episode: [333. 333. 333. 333. 333. 333. 333. 333. 333. 333.]; total steps: 67000; avg scores/10 episodes: [ -757.93515294  -849.24644583 -1004.85837956  -889.80025831\n",
      "  -898.29108528 -1012.3204919   -868.51979488  -865.47974724\n",
      "  -861.48544656  -850.72311705]; avg total score: -885.8659919553471\n",
      "learning timestep: 67584\n",
      "num batches:32\n",
      "Policy Loss: 0.04840622842311859\n",
      "Value Loss: 16.03968048095703\n",
      "Entropy: 1091.490966796875\n",
      "KL Divergence: 192.60012817382812\n",
      "Lambda: 0.966616690158844\n",
      "episode: [338. 338. 338. 338. 338. 338. 338. 338. 338. 338.]; total steps: 68000; avg scores/10 episodes: [-788.4388453  -812.18679041 -927.08420438 -793.03280264 -854.11246153\n",
      " -957.15811859 -842.37555823 -849.03320405 -848.58220019 -870.47311312]; avg total score: -854.2477298434593\n",
      "episode: [343. 343. 343. 343. 343. 343. 343. 343. 343. 343.]; total steps: 69000; avg scores/10 episodes: [-816.24303377 -814.39692187 -797.72215913 -721.510991   -810.48983949\n",
      " -796.84836145 -806.0436678  -864.735729   -794.48610051 -779.76104432]; avg total score: -800.223784832978\n",
      "learning timestep: 69632\n",
      "num batches:32\n",
      "Policy Loss: 0.07671123743057251\n",
      "Value Loss: 21.87392234802246\n",
      "Entropy: 1106.34619140625\n",
      "KL Divergence: 225.20693969726562\n",
      "Lambda: 0.9699808955192566\n",
      "episode: [348. 348. 348. 348. 348. 348. 348. 348. 348. 348.]; total steps: 70000; avg scores/10 episodes: [-737.08071491 -873.56474674 -880.99670607 -791.37730952 -800.64602628\n",
      " -833.89201747 -616.65380146 -838.23543154 -764.61305336 -773.62646247]; avg total score: -791.0686269832491\n",
      "episode: [353. 353. 353. 353. 353. 353. 353. 353. 353. 353.]; total steps: 71000; avg scores/10 episodes: [-736.61932631 -793.14631974 -875.0578237  -752.69065924 -947.65975079\n",
      " -846.76624788 -663.94011191 -826.8218799  -812.962406   -752.71221115]; avg total score: -800.8376736626793\n",
      "learning timestep: 71680\n",
      "num batches:32\n",
      "Policy Loss: 0.027629252523183823\n",
      "Value Loss: 9.847640037536621\n",
      "Entropy: 1131.5\n",
      "KL Divergence: 255.05197143554688\n",
      "Lambda: 0.9731333255767822\n",
      "episode: [358. 358. 358. 358. 358. 358. 358. 358. 358. 358.]; total steps: 72000; avg scores/10 episodes: [ -841.69410533  -734.33098331  -721.43481998  -727.69139682\n",
      " -1055.92037057  -754.18638756  -683.73325977  -765.16668345\n",
      "  -696.47485051  -779.95950546]; avg total score: -776.0592362754398\n",
      "episode: [363. 363. 363. 363. 363. 363. 363. 363. 363. 363.]; total steps: 73000; avg scores/10 episodes: [-814.2585495  -629.19671912 -655.63006309 -748.48978728 -965.71900945\n",
      " -659.52973231 -999.94558774 -635.82855553 -622.01146297 -794.97894552]; avg total score: -752.5588412510858\n",
      "learning timestep: 73728\n",
      "num batches:32\n",
      "Policy Loss: 0.10430518537759781\n",
      "Value Loss: 35.41337203979492\n",
      "Entropy: 1134.7401123046875\n",
      "KL Divergence: 351.5008239746094\n",
      "Lambda: 0.9759656190872192\n",
      "episode: [368. 368. 368. 368. 368. 368. 368. 368. 368. 368.]; total steps: 74000; avg scores/10 episodes: [ -768.39347415  -595.53203347  -605.2529669   -670.93084378\n",
      "  -786.07424222  -610.98234078 -1043.09636554  -561.36297795\n",
      "  -645.94702226  -739.06431519]; avg total score: -702.6636582222746\n",
      "episode: [373. 373. 373. 373. 373. 373. 373. 373. 373. 373.]; total steps: 75000; avg scores/10 episodes: [-618.48088238 -605.56072941 -574.69928558 -575.12654597 -597.34750491\n",
      " -646.40729537 -624.18941812 -488.52603138 -650.54176294 -655.94800064]; avg total score: -603.6827456701351\n",
      "learning timestep: 75776\n",
      "num batches:32\n",
      "Policy Loss: -0.012800734490156174\n",
      "Value Loss: 13.87449836730957\n",
      "Entropy: 1130.014404296875\n",
      "KL Divergence: 353.68206787109375\n",
      "Lambda: 0.9784770607948303\n",
      "episode: [378. 378. 378. 378. 378. 378. 378. 378. 378. 378.]; total steps: 76000; avg scores/10 episodes: [-548.37882938 -592.78182327 -492.00344003 -572.58918528 -652.6843774\n",
      " -662.88059123 -573.45731713 -553.23842231 -661.06430318 -745.51860445]; avg total score: -605.4596893650556\n",
      "episode: [383. 383. 383. 383. 383. 383. 383. 383. 383. 383.]; total steps: 77000; avg scores/10 episodes: [-647.51468853 -563.94441312 -523.06853778 -511.23338629 -816.59387782\n",
      " -525.70025833 -549.52251319 -808.89729026 -590.11949889 -772.46604035]; avg total score: -630.906050456676\n",
      "learning timestep: 77824\n",
      "num batches:32\n",
      "Policy Loss: 0.02511747181415558\n",
      "Value Loss: 45.89955139160156\n",
      "Entropy: 1155.84912109375\n",
      "KL Divergence: 378.99395751953125\n",
      "Lambda: 0.9803962707519531\n",
      "episode: [388. 388. 388. 388. 388. 388. 388. 388. 388. 388.]; total steps: 78000; avg scores/10 episodes: [-558.49632099 -606.0155906  -640.65463751 -413.95667482 -637.79124608\n",
      " -431.76147984 -475.6324653  -831.48743437 -445.43747148 -608.58106111]; avg total score: -564.9814382104443\n",
      "episode: [393. 393. 393. 393. 393. 393. 393. 393. 393. 393.]; total steps: 79000; avg scores/10 episodes: [-474.31825382 -722.13167575 -552.26543691 -387.06614141 -474.7548583\n",
      " -507.54488196 -362.63709314 -573.68770146 -369.44584721 -437.64761125]; avg total score: -486.1499501209074\n",
      "learning timestep: 79872\n",
      "num batches:32\n",
      "Policy Loss: -0.002240873873233795\n",
      "Value Loss: 26.950910568237305\n",
      "Entropy: 1150.9251708984375\n",
      "KL Divergence: 436.6695556640625\n",
      "Lambda: 0.9821991324424744\n",
      "episode: [398. 398. 398. 398. 398. 398. 398. 398. 398. 398.]; total steps: 80000; avg scores/10 episodes: [-474.08457919 -559.39281148 -603.81810642 -418.91022374 -356.92463657\n",
      " -607.53063151 -449.62401346 -551.4167957  -224.32752776 -351.15267306]; avg total score: -459.7181998884538\n",
      "episode: [402. 402. 402. 402. 402. 402. 402. 402. 402. 402.]; total steps: 81000; avg scores/10 episodes: [-348.799671   -464.86697881 -666.38506269 -453.39746086 -265.40093432\n",
      " -479.42973655 -462.65443062 -520.06476949 -228.60883499 -495.47013001]; avg total score: -438.5078009328161\n",
      "learning timestep: 81920\n",
      "num batches:32\n",
      "Policy Loss: 0.00513702817261219\n",
      "Value Loss: 45.035980224609375\n",
      "Entropy: 1176.31298828125\n",
      "KL Divergence: 324.316650390625\n",
      "Lambda: 0.9835790991783142\n",
      "episode: [407. 407. 407. 407. 407. 407. 407. 407. 407. 407.]; total steps: 82000; avg scores/10 episodes: [-394.32877964 -476.29550024 -478.14320666 -548.10908358 -213.06683915\n",
      " -307.98422019 -274.30928443 -342.28612367 -251.54441276 -606.64880943]; avg total score: -389.271625975116\n",
      "episode: [412. 412. 412. 412. 412. 412. 412. 412. 412. 412.]; total steps: 83000; avg scores/10 episodes: [-405.5739858  -389.11060835 -336.31492782 -556.12031828 -393.11046187\n",
      " -235.47667794 -350.13904031 -503.62482194 -336.97263887 -485.37067774]; avg total score: -399.18141589117573\n",
      "learning timestep: 83968\n",
      "num batches:32\n",
      "Policy Loss: -0.027677495032548904\n",
      "Value Loss: 22.073659896850586\n",
      "Entropy: 1206.9326171875\n",
      "KL Divergence: 416.1746826171875\n",
      "Lambda: 0.9849663972854614\n",
      "episode: [417. 417. 417. 417. 417. 417. 417. 417. 417. 417.]; total steps: 84000; avg scores/10 episodes: [-360.61076609 -322.68120664 -292.28726628 -415.09489227 -557.83718934\n",
      " -353.31329203 -407.87531914 -641.7728725  -324.35406863 -361.85672666]; avg total score: -403.7683599560725\n",
      "episode: [422. 422. 422. 422. 422. 422. 422. 422. 422. 422.]; total steps: 85000; avg scores/10 episodes: [-227.59281857 -388.5219064  -487.5307601  -469.21604313 -391.44346394\n",
      " -547.91393757 -374.35639782 -408.75583856 -198.93948658 -310.96665326]; avg total score: -380.523730591758\n",
      "episode: [427. 427. 427. 427. 427. 427. 427. 427. 427. 427.]; total steps: 86000; avg scores/10 episodes: [-211.22525912 -491.29153488 -605.02407771 -527.79353218 -299.87345493\n",
      " -464.94676983 -299.70580004 -224.11313823 -174.73997902 -162.41632344]; avg total score: -346.1129869367751\n",
      "learning timestep: 86016\n",
      "num batches:32\n",
      "Policy Loss: 0.022486887872219086\n",
      "Value Loss: 91.44386291503906\n",
      "Entropy: 1194.928466796875\n",
      "KL Divergence: 456.71466064453125\n",
      "Lambda: 0.986192524433136\n",
      "episode: [432. 432. 432. 432. 432. 432. 432. 432. 432. 432.]; total steps: 87000; avg scores/10 episodes: [-326.669268   -539.1601029  -657.35099558 -456.18936489 -308.92309347\n",
      " -308.24876727 -209.72256944 -356.6018414  -236.30527742 -399.53126636]; avg total score: -379.87025467341255\n",
      "episode: [437. 437. 437. 437. 437. 437. 437. 437. 437. 437.]; total steps: 88000; avg scores/10 episodes: [-318.70938357 -301.41458778 -487.72001728 -442.0065884  -245.58843646\n",
      " -319.13409999 -253.6577041  -409.44147819 -212.15862628 -476.4354215 ]; avg total score: -346.62663435435815\n",
      "learning timestep: 88064\n",
      "num batches:32\n",
      "Policy Loss: 0.10588130354881287\n",
      "Value Loss: 259.5769958496094\n",
      "Entropy: 1218.841796875\n",
      "KL Divergence: 373.0880126953125\n",
      "Lambda: 0.9871823787689209\n",
      "episode: [442. 442. 442. 442. 442. 442. 442. 442. 442. 442.]; total steps: 89000; avg scores/10 episodes: [-238.77365298 -279.46508965 -390.28475416 -407.31299408 -172.66049536\n",
      " -304.91537522 -427.76909752 -345.89370135 -199.06045396 -356.05752069]; avg total score: -312.2193134974909\n",
      "episode: [447. 447. 447. 447. 447. 447. 447. 447. 447. 447.]; total steps: 90000; avg scores/10 episodes: [-265.99528415 -435.85591664 -452.25919606 -416.80168777 -305.80362211\n",
      " -209.48996218 -426.55339663 -257.38398633 -248.65509087 -462.99494203]; avg total score: -348.17930847848777\n",
      "learning timestep: 90112\n",
      "num batches:32\n",
      "Policy Loss: -0.05548267066478729\n",
      "Value Loss: 80.50463104248047\n",
      "Entropy: 1247.456298828125\n",
      "KL Divergence: 377.1496887207031\n",
      "Lambda: 0.9881088137626648\n",
      "episode: [452. 452. 452. 452. 452. 452. 452. 452. 452. 452.]; total steps: 91000; avg scores/10 episodes: [-415.16903384 -295.67186361 -344.34412191 -455.45608969 -405.77793815\n",
      " -211.92870478 -253.53845078 -242.65997258 -388.75283798 -534.61563934]; avg total score: -354.79146526443964\n",
      "episode: [457. 457. 457. 457. 457. 457. 457. 457. 457. 457.]; total steps: 92000; avg scores/10 episodes: [-484.27436488 -214.91949008 -289.39404331 -466.57435583 -474.94168875\n",
      " -347.46635545 -257.52115594 -303.10640588 -325.32294227 -407.32008961]; avg total score: -357.08408919977944\n",
      "learning timestep: 92160\n",
      "num batches:32\n",
      "Policy Loss: -0.0612795427441597\n",
      "Value Loss: 9.365744590759277\n",
      "Entropy: 1269.125\n",
      "KL Divergence: 387.85638427734375\n",
      "Lambda: 0.9889454245567322\n",
      "episode: [462. 462. 462. 462. 462. 462. 462. 462. 462. 462.]; total steps: 93000; avg scores/10 episodes: [-310.26814382 -291.74545882 -326.93283276 -519.36204193 -539.24006875\n",
      " -512.78695171 -317.76820684 -349.94473622 -197.6536091  -354.58543789]; avg total score: -372.02874878327833\n",
      "episode: [467. 467. 467. 467. 467. 467. 467. 467. 467. 467.]; total steps: 94000; avg scores/10 episodes: [-244.55891995 -392.96347059 -290.5697022  -345.93140327 -424.0970278\n",
      " -403.51509028 -224.88311166 -287.5640033  -340.81532873 -346.93212374]; avg total score: -330.1830181517481\n",
      "learning timestep: 94208\n",
      "num batches:32\n",
      "Policy Loss: 0.03444923460483551\n",
      "Value Loss: 98.40860748291016\n",
      "Entropy: 1300.62353515625\n",
      "KL Divergence: 410.37872314453125\n",
      "Lambda: 0.9897186756134033\n",
      "episode: [472. 472. 472. 472. 472. 472. 472. 472. 472. 472.]; total steps: 95000; avg scores/10 episodes: [-269.12641012 -340.4239639  -366.33511101 -262.98439824 -295.43720046\n",
      " -292.24397096 -224.73048973 -202.69331437 -291.30428535 -272.05996905]; avg total score: -281.73391131875275\n",
      "episode: [477. 477. 477. 477. 477. 477. 477. 477. 477. 477.]; total steps: 96000; avg scores/10 episodes: [-357.79890538 -200.13093625 -430.0525666  -283.29840871 -304.40077436\n",
      " -293.04244225 -286.86130528 -364.5802874  -287.06805233 -305.45736462]; avg total score: -311.2691043160661\n",
      "learning timestep: 96256\n",
      "num batches:32\n",
      "Policy Loss: 0.1269829273223877\n",
      "Value Loss: 302.3349304199219\n",
      "Entropy: 1331.4952392578125\n",
      "KL Divergence: 400.8354797363281\n",
      "Lambda: 0.990484356880188\n",
      "episode: [482. 482. 482. 482. 482. 482. 482. 482. 482. 482.]; total steps: 97000; avg scores/10 episodes: [-372.17126386 -227.39709604 -391.31489422 -404.69997869 -295.30608469\n",
      " -213.03745041 -338.01898901 -358.01501117 -315.37907679 -375.51370708]; avg total score: -329.08535519563804\n",
      "episode: [487. 487. 487. 487. 487. 487. 487. 487. 487. 487.]; total steps: 98000; avg scores/10 episodes: [-376.71046176 -314.02425195 -471.97117881 -369.13123142 -248.40080253\n",
      " -188.12158491 -342.87555431 -269.08340245 -164.60638546 -357.83112569]; avg total score: -310.2755979293296\n",
      "learning timestep: 98304\n",
      "num batches:32\n",
      "Policy Loss: 0.020036425441503525\n",
      "Value Loss: 200.40476989746094\n",
      "Entropy: 1359.079833984375\n",
      "KL Divergence: 425.4203796386719\n",
      "Lambda: 0.9912531971931458\n",
      "episode: [492. 492. 492. 492. 492. 492. 492. 492. 492. 492.]; total steps: 99000; avg scores/10 episodes: [-364.60893056 -393.89135325 -413.44366259 -175.33851705 -248.07215048\n",
      " -228.31894702 -330.26184792 -382.64426134 -195.164386   -329.59307424]; avg total score: -306.1337130458963\n",
      "episode: [497. 497. 497. 497. 497. 497. 497. 497. 497. 497.]; total steps: 100000; avg scores/10 episodes: [-290.95528794 -281.33083721 -318.4126081  -294.46616848 -228.44001516\n",
      " -216.47217684 -252.65133487 -333.04813383 -318.62446347 -363.86949057]; avg total score: -289.8270516473545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td></td></tr><tr><td>best</td><td></td></tr><tr><td>critic_loss</td><td></td></tr><tr><td>entropy</td><td></td></tr><tr><td>episode</td><td></td></tr><tr><td>episode_reward</td><td></td></tr><tr><td>kl_divergence</td><td></td></tr><tr><td>lambda</td><td></td></tr><tr><td>step_reward</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>0.02004</td></tr><tr><td>best</td><td>True</td></tr><tr><td>critic_loss</td><td>200.40477</td></tr><tr><td>entropy</td><td>1359.07983</td></tr><tr><td>episode</td><td>489.0</td></tr><tr><td>episode_reward</td><td>-372.7284</td></tr><tr><td>kl_divergence</td><td>425.42038</td></tr><tr><td>lambda</td><td>0.99125</td></tr><tr><td>step_reward</td><td>-0.38506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train-28</strong> at: <a href='https://wandb.ai/jasonhayes1987/PPO-Test/runs/6mcbidkn' target=\"_blank\">https://wandb.ai/jasonhayes1987/PPO-Test/runs/6mcbidkn</a><br/> View project at: <a href='https://wandb.ai/jasonhayes1987/PPO-Test' target=\"_blank\">https://wandb.ai/jasonhayes1987/PPO-Test</a><br/>Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240822_022108-6mcbidkn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PARAMS ##\n",
    "env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-3\n",
    "entropy_coeff = 0.01\n",
    "kl_coeff = 3.0\n",
    "loss = 'hybrid'\n",
    "lambda_ = 0.0\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "distribution = 'Normal'\n",
    "device = 'cuda'\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'PPO-Test'\n",
    "run_name = None\n",
    "\n",
    "seed = 46\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff,\n",
    "          kl_coefficient=kl_coeff, loss=loss, lambda_=lambda_, callbacks=[WandbCallback(project_name, run_name)])\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"ppo_test/renders\"\n",
    "num_episodes = 10\n",
    "render_freq = 1\n",
    "\n",
    "# Set the policy and value function models to evaluation mode\n",
    "ppo_agent_hybrid1.policy.eval()\n",
    "ppo_agent_hybrid1.value_model.eval()\n",
    "\n",
    "# Create the render directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "env = gym.make_vec(\n",
    "    ppo_agent_hybrid1.env.spec.id,\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        lambda env: gym.wrappers.RecordVideo(\n",
    "            env,\n",
    "            save_dir + \"/renders/test\",\n",
    "            episode_trigger=lambda episode_id: (episode_id + 1) % render_freq == 0\n",
    "        )\n",
    "    ],\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "scores = []\n",
    "entropy_list = []\n",
    "kl_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    state, _ = ppo_agent_hybrid1.env.reset()\n",
    "    score = 0\n",
    "    episode_entropy = 0\n",
    "    episode_kl = 0\n",
    "    steps = 0\n",
    "\n",
    "    prev_dist = None  # To track the previous distribution for KL divergence\n",
    "\n",
    "    # Video writer setup\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video_path = os.path.join(render_dir, f\"episode_{episode+1}.mp4\")\n",
    "    #     frame = self.env.render(mode='rgb_array')\n",
    "    #     height, width, layers = frame.shape\n",
    "    #     video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n",
    "\n",
    "    while not done:\n",
    "        # Render the environment and write the frame to the video file\n",
    "        # if episode % render_freq == 0:\n",
    "        #     frame = self.env.render(mode='rgb_array')\n",
    "        #     video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Get action and log probability from the current policy\n",
    "        action, log_prob = ppo_agent_hybrid1.get_action(state)\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            act = ppo_agent_hybrid1.action_adapter(action)\n",
    "        else:\n",
    "            act = action\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, term, trunc, _ = ppo_agent_hybrid1.env.step(act)\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "\n",
    "        # Calculate the distribution and entropy\n",
    "        dist = ppo_agent_hybrid1.policy(T.tensor(state, dtype=T.float32, device=ppo_agent_hybrid1.policy.device))\n",
    "        entropy = dist.entropy().sum().item()  # Sum entropy over actions\n",
    "\n",
    "        # Update KL divergence\n",
    "        if prev_dist is not None:\n",
    "            kl = kl_divergence(prev_dist, dist).sum().item()  # Sum KL divergence over actions\n",
    "        else:\n",
    "            kl = 0  # No KL divergence for the first step in the episode\n",
    "\n",
    "        # Update the previous distribution to the current one\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            param1_prev = dist.concentration1.clone().detach()\n",
    "            param2_prev = dist.concentration0.clone().detach()\n",
    "            prev_dist = Beta(param1_prev, param2_prev)\n",
    "        elif ppo_agent_hybrid1.distribution == 'Normal':\n",
    "            param1_prev = dist.loc.clone().detach()\n",
    "            param2_prev = dist.scale.clone().detach()\n",
    "            prev_dist = Normal(param1_prev, param2_prev)\n",
    "\n",
    "        # Accumulate the score, entropy, and KL divergence for the episode\n",
    "        score += reward\n",
    "        episode_entropy += entropy\n",
    "        episode_kl += kl\n",
    "        steps += 1\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Release the video writer\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video.release()\n",
    "\n",
    "    # Append the results for the episode\n",
    "    scores.append(score)\n",
    "    entropy_list.append(episode_entropy / steps)  # Average entropy over the episode\n",
    "    kl_list.append(episode_kl / steps)  # Average KL divergence over the episode\n",
    "\n",
    "    print(f'Episode {episode+1}/{num_episodes} - Score: {score}, Avg Entropy: {entropy_list[-1]}, Avg KL Divergence: {kl_list[-1]}')\n",
    "\n",
    "# close the environment\n",
    "ppo_agent_hybrid1.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
