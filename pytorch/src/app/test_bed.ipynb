{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "import models\n",
    "import cnn_models\n",
    "import rl_agents\n",
    "import rl_callbacks\n",
    "import helper\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "import ale_py\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build actor\n",
    "\n",
    "# dense_layers = [\n",
    "#     (\n",
    "#         400,\n",
    "#         \"relu\",\n",
    "#         {\n",
    "#             \"variance scaling\": {\n",
    "#                 \"scale\": 1.0,\n",
    "#                 \"mode\": \"fan_in\",\n",
    "#                 \"distribution\": \"uniform\",\n",
    "#             }\n",
    "#         },\n",
    "#     ),\n",
    "#     (\n",
    "#         300,\n",
    "#         \"relu\",\n",
    "#         {\n",
    "#             \"variance scaling\": {\n",
    "#                 \"scale\": 1.0,\n",
    "#                 \"mode\": \"fan_in\",\n",
    "#                 \"distribution\": \"uniform\",\n",
    "#             }\n",
    "#         },\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "#                           optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "# output_kernel = {\"uniform\":{\"a\":-0.003, \"b\":0.003}}\n",
    "output_kernel = {\"default\":{}}\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, output_layer_kernel=output_kernel,\n",
    "                          optimizer='Adam', optimizer_params={'weight_decay':0.0}, learning_rate=0.001,\n",
    "                          normalize_layers=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build critic\n",
    "\n",
    "# state_layers = [\n",
    "#     (\n",
    "#         400,\n",
    "#         \"relu\",\n",
    "#         {\n",
    "#             \"variance scaling\": {\n",
    "#                 \"scale\": 1.0,\n",
    "#                 \"mode\": \"fan_in\",\n",
    "#                 \"distribution\": \"uniform\",\n",
    "#             }\n",
    "#         },\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# merged_layers = [\n",
    "#     (\n",
    "#         300,\n",
    "#         \"relu\",\n",
    "#         {\n",
    "#             \"variance scaling\": {\n",
    "#                 \"scale\": 1.0,\n",
    "#                 \"mode\": \"fan_in\",\n",
    "#                 \"distribution\": \"uniform\",\n",
    "#             }\n",
    "#         },\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "\n",
    "# critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "#                             optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "# build actor\n",
    "\n",
    "state_layers = []\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "# output_kernel = {\"uniform\":{\"a\":-0.003, \"b\":0.003}}\n",
    "output_kernel = {\"default\":{}}\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            output_layer_kernel=output_kernel, optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},learning_rate=0.001, normalize_layers=False,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, device=device)\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('LunarLander-v2-continuous')],\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash_callbacks import run_agent\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 2\n",
    "for agent in range(num_agents):\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_spec = env.spec.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(gym.envs.registration.EnvSpec.from_json(env_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_spec(env_spec_dict):\n",
    "    # Create a new EnvSpec instance using the dictionary\n",
    "    env_spec = gym.envs.registration.EnvSpec(**env_spec_dict)\n",
    "    return env_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_env_spec(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'sweep/agent_config_58.json'\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    agent_config = json.load(file)\n",
    "\n",
    "her = rl_agents.HER.load(agent_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('LunarLanderContinuous-v3')\n",
    "# env = gym.make(\"BipedalWalker-v3\")\n",
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:282: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Overwriting existing videos at /workspaces/RL_Agents/pytorch/src/app/td3_test/td3/renders/training folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env.spec, render_mode=\"rgb_array\")\n",
    "save_dir = \"/workspaces/RL_Agents/pytorch/src/app/td3_test/td3\"\n",
    "os.makedirs(save_dir + \"/renders/training\", exist_ok=True)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    save_dir + \"/renders/training\",\n",
    "    episode_trigger=lambda episode_id: (episode_id+1) % render_freq == 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, disable_env_checker=False, kwargs={'render_mode': 'rgb_array'}, namespace=None, name='Pendulum', version=1, additional_wrappers=(WrapperSpec(name='RecordVideo', entry_point='gymnasium.wrappers.rendering:RecordVideo', kwargs={'video_folder': '/workspaces/RL_Agents/pytorch/src/app/td3_test/td3/renders/training', 'episode_trigger': <function <lambda> at 0x7fa1a0c68dc0>, 'step_trigger': None, 'video_length': 0, 'name_prefix': 'rl-video', 'disable_logger': True}),), vector_entry_point=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"Pendulum-v1\", \"entry_point\": \"gymnasium.envs.classic_control.pendulum:PendulumEnv\", \"reward_threshold\": null, \"nondeterministic\": false, \"max_episode_steps\": 200, \"order_enforce\": true, \"disable_env_checker\": false, \"kwargs\": {}, \"additional_wrappers\": [], \"vector_entry_point\": null}'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"Pendulum-v1\", \"entry_point\": \"gymnasium.envs.classic_control.pendulum:PendulumEnv\", \"reward_threshold\": null, \"nondeterministic\": false, \"max_episode_steps\": 200, \"order_enforce\": true, \"disable_env_checker\": false, \"kwargs\": {}, \"additional_wrappers\": [], \"vector_entry_point\": null}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import EnvSpec\n",
    "\n",
    "def serialize_env_spec(env_spec):\n",
    "    \"\"\"Extracts and serializes the relevant parts of the environment specification.\"\"\"\n",
    "    env_spec_dict = {\n",
    "        \"id\": env_spec.id,\n",
    "        \"entry_point\": env_spec.entry_point,\n",
    "        \"reward_threshold\": env_spec.reward_threshold,\n",
    "        \"nondeterministic\": env_spec.nondeterministic,\n",
    "        \"max_episode_steps\": env_spec.max_episode_steps,\n",
    "        \"order_enforce\": env_spec.order_enforce,\n",
    "        \"disable_env_checker\": env_spec.disable_env_checker,\n",
    "        \"kwargs\": env_spec.kwargs,\n",
    "        \"additional_wrappers\": env_spec.additional_wrappers,\n",
    "        \"vector_entry_point\": env_spec.vector_entry_point,\n",
    "    }\n",
    "    return env_spec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_json_spec = gym.envs.registration.EnvSpec.from_json(json_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_json_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:Pendulum-v1;<class 'str'>\n",
      "entry_point:gymnasium.envs.classic_control.pendulum:PendulumEnv;<class 'str'>\n",
      "reward_threshold:None;<class 'NoneType'>\n",
      "nondeterministic:False;<class 'bool'>\n",
      "max_episode_steps:200;<class 'int'>\n",
      "order_enforce:True;<class 'bool'>\n",
      "disable_env_checker:False;<class 'bool'>\n",
      "kwargs:{};<class 'dict'>\n",
      "additional_wrappers:();<class 'tuple'>\n",
      "vector_entry_point:None;<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "for key,val in env_spec.items():\n",
    "    print(f'{key}:{val};{type(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnvSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/gymnasium/envs/registration.py:178\u001b[0m, in \u001b[0;36mEnvSpec.from_json\u001b[0;34m(json_env_spec)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_json\u001b[39m(json_env_spec: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EnvSpec:\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a JSON string into a specification stack.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m        An environment spec\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     parsed_env_spec \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_env_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     applied_wrapper_specs: \u001b[38;5;28mlist\u001b[39m[WrapperSpec] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m wrapper_spec_json \u001b[38;5;129;01min\u001b[39;00m parsed_env_spec\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_wrappers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "gym.envs.registration.EnvSpec.from_json(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/gymnasium/envs/registration.py:685\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         env_spec\u001b[38;5;241m.\u001b[39madditional_wrappers \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;66;03m# For string id's, load the environment spec from the registry then make the environment spec\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m _find_spec(\u001b[38;5;28mid\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "# output_kernel = {\"uniform\":{\"a\":-0.003, \"b\":0.003}}\n",
    "output_kernel = {\"default\":{}}\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, output_layer_kernel=output_kernel,\n",
    "                          optimizer='Adam', optimizer_params={'weight_decay':0.0}, learning_rate=3e-4,\n",
    "                          normalize_layers=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in actor.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = []\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"default\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "# output_kernel = {\"uniform\":{\"a\":-0.003, \"b\":0.003}}\n",
    "output_kernel = {\"default\":{}}\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            output_layer_kernel=output_kernel, optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},learning_rate=3e-4, normalize_layers=False,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in critic.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, device=device)\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = rl_agents.TD3(\n",
    "    env=env,\n",
    "    actor_model=actor,\n",
    "    critic_model=critic,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=256,\n",
    "    noise=noise,\n",
    "    actor_update_delay=2,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"BipedalWalker-v3\")],\n",
    "    use_mpi=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in td3.critic_model_a.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in td3.critic_model_b.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/pytorch/src/app/wandb/run-20240723_200906-mjn7t4i5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/mjn7t4i5' target=\"_blank\">train-119</a></strong> to <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/mjn7t4i5' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/mjn7t4i5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models/td3)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1, score -71.89686065415344, avg_score -71.89686065415344, episode_time 10.32s, avg_episode_time 10.32s, avg_step_time 0.015053s, avg_learn_time 0.013915s, avg_steps_per_episode 1600.00\n",
      "episode 2, score -116.3688738818268, avg_score -94.13286726799012, episode_time 0.83s, avg_episode_time 5.57s, avg_step_time 0.014766s, avg_learn_time 0.013542s, avg_steps_per_episode 827.50\n",
      "episode 3, score -113.92613937068731, avg_score -100.73062463555584, episode_time 0.71s, avg_episode_time 3.95s, avg_step_time 0.014797s, avg_learn_time 0.013563s, avg_steps_per_episode 567.00\n",
      "episode 4, score -108.52828200667413, avg_score -102.68003897833542, episode_time 1.55s, avg_episode_time 3.35s, avg_step_time 0.014134s, avg_learn_time 0.013056s, avg_steps_per_episode 452.00\n",
      "episode 5, score -113.49380857307712, avg_score -104.84279289728377, episode_time 2.16s, avg_episode_time 3.11s, avg_step_time 0.015202s, avg_learn_time 0.014061s, avg_steps_per_episode 389.40\n",
      "episode 6, score -121.84719833813297, avg_score -107.67686047075864, episode_time 0.66s, avg_episode_time 2.70s, avg_step_time 0.015125s, avg_learn_time 0.013988s, avg_steps_per_episode 331.83\n",
      "episode 7, score -117.62260013413274, avg_score -109.09768042266923, episode_time 2.56s, avg_episode_time 2.68s, avg_step_time 0.014294s, avg_learn_time 0.013206s, avg_steps_per_episode 309.14\n",
      "episode 8, score -123.89218956697607, avg_score -110.94699406570757, episode_time 1.67s, avg_episode_time 2.56s, avg_step_time 0.014852s, avg_learn_time 0.013745s, avg_steps_per_episode 284.25\n",
      "episode 9, score -157.96831468209814, avg_score -116.17158524530653, episode_time 23.85s, avg_episode_time 4.92s, avg_step_time 0.014217s, avg_learn_time 0.013147s, avg_steps_per_episode 427.22\n",
      "episode 10, score -103.20705930434106, avg_score -114.87513265121, episode_time 1.69s, avg_episode_time 4.60s, avg_step_time 0.014147s, avg_learn_time 0.013037s, avg_steps_per_episode 396.10\n",
      "episode 11, score -145.30540954470726, avg_score -117.64152145970975, episode_time 14.56s, avg_episode_time 5.50s, avg_step_time 0.014747s, avg_learn_time 0.013647s, avg_steps_per_episode 452.27\n",
      "episode 12, score -159.00073308338483, avg_score -121.08812242834934, episode_time 9.97s, avg_episode_time 5.88s, avg_step_time 0.014066s, avg_learn_time 0.013001s, avg_steps_per_episode 470.17\n",
      "episode 13, score -114.12454404764746, avg_score -120.55246255291074, episode_time 2.80s, avg_episode_time 5.64s, avg_step_time 0.014513s, avg_learn_time 0.013429s, avg_steps_per_episode 448.54\n",
      "episode 14, score -110.49102694487571, avg_score -119.83378858090825, episode_time 1.91s, avg_episode_time 5.37s, avg_step_time 0.014313s, avg_learn_time 0.013246s, avg_steps_per_episode 425.71\n",
      "episode 15, score -110.14048816984892, avg_score -119.18756855350428, episode_time 2.36s, avg_episode_time 5.17s, avg_step_time 0.013926s, avg_learn_time 0.012888s, avg_steps_per_episode 408.20\n",
      "episode 16, score -115.46684542279938, avg_score -118.9550233578352, episode_time 0.62s, avg_episode_time 4.89s, avg_step_time 0.014225s, avg_learn_time 0.013183s, avg_steps_per_episode 385.25\n",
      "episode 17, score -109.95414987346034, avg_score -118.42556021169551, episode_time 1.64s, avg_episode_time 4.70s, avg_step_time 0.014797s, avg_learn_time 0.013664s, avg_steps_per_episode 368.94\n",
      "episode 18, score -108.95318590651887, avg_score -117.89931719474124, episode_time 1.46s, avg_episode_time 4.52s, avg_step_time 0.014497s, avg_learn_time 0.013402s, avg_steps_per_episode 353.89\n",
      "episode 19, score -109.68041644838824, avg_score -117.46674347124899, episode_time 2.98s, avg_episode_time 4.44s, avg_step_time 0.014630s, avg_learn_time 0.013525s, avg_steps_per_episode 345.47\n",
      "episode 20, score -111.54823611705811, avg_score -117.17081810353943, episode_time 1.71s, avg_episode_time 4.30s, avg_step_time 0.014206s, avg_learn_time 0.013143s, avg_steps_per_episode 334.05\n",
      "episode 21, score -114.42184824915107, avg_score -117.03991477713998, episode_time 0.63s, avg_episode_time 4.13s, avg_step_time 0.014171s, avg_learn_time 0.013084s, avg_steps_per_episode 320.19\n",
      "episode 22, score -110.43242015269772, avg_score -116.73957411239262, episode_time 4.10s, avg_episode_time 4.12s, avg_step_time 0.014464s, avg_learn_time 0.013351s, avg_steps_per_episode 318.23\n",
      "episode 23, score -114.18048915194844, avg_score -116.62830954889503, episode_time 0.78s, avg_episode_time 3.98s, avg_step_time 0.014956s, avg_learn_time 0.013816s, avg_steps_per_episode 306.52\n",
      "episode 24, score -109.3514172051226, avg_score -116.32510570123786, episode_time 1.71s, avg_episode_time 3.88s, avg_step_time 0.014367s, avg_learn_time 0.013277s, avg_steps_per_episode 298.58\n",
      "episode 25, score -116.06362868682791, avg_score -116.31464662066146, episode_time 0.75s, avg_episode_time 3.76s, avg_step_time 0.014341s, avg_learn_time 0.013240s, avg_steps_per_episode 288.64\n",
      "episode 26, score -118.39832462649606, avg_score -116.39478808242433, episode_time 0.69s, avg_episode_time 3.64s, avg_step_time 0.014294s, avg_learn_time 0.013193s, avg_steps_per_episode 279.38\n",
      "episode 27, score -114.16407021163094, avg_score -116.31216890202458, episode_time 1.17s, avg_episode_time 3.55s, avg_step_time 0.014685s, avg_learn_time 0.013556s, avg_steps_per_episode 271.89\n",
      "episode 28, score -114.97214680832748, avg_score -116.26431097010683, episode_time 0.81s, avg_episode_time 3.45s, avg_step_time 0.014699s, avg_learn_time 0.013575s, avg_steps_per_episode 264.14\n",
      "episode 29, score -120.48988658015585, avg_score -116.41002047390162, episode_time 1.48s, avg_episode_time 3.38s, avg_step_time 0.014704s, avg_learn_time 0.013554s, avg_steps_per_episode 258.41\n",
      "episode 30, score -117.55794703550077, avg_score -116.4482846926216, episode_time 1.07s, avg_episode_time 3.31s, avg_step_time 0.014584s, avg_learn_time 0.013419s, avg_steps_per_episode 252.13\n",
      "episode 31, score -116.8303855239215, avg_score -116.46061052588934, episode_time 0.74s, avg_episode_time 3.22s, avg_step_time 0.015191s, avg_learn_time 0.014010s, avg_steps_per_episode 245.48\n",
      "episode 32, score -111.39964133307275, avg_score -116.30245523861382, episode_time 2.04s, avg_episode_time 3.19s, avg_step_time 0.014373s, avg_learn_time 0.013281s, avg_steps_per_episode 242.06\n",
      "episode 33, score -120.10722953271741, avg_score -116.41775142934424, episode_time 1.40s, avg_episode_time 3.13s, avg_step_time 0.014171s, avg_learn_time 0.013071s, avg_steps_per_episode 237.64\n",
      "episode 34, score -114.91362096349584, avg_score -116.37351229799575, episode_time 1.87s, avg_episode_time 3.10s, avg_step_time 0.014024s, avg_learn_time 0.012950s, avg_steps_per_episode 234.44\n",
      "episode 35, score -118.0603398325735, avg_score -116.42170737041225, episode_time 1.18s, avg_episode_time 3.04s, avg_step_time 0.014549s, avg_learn_time 0.013446s, avg_steps_per_episode 229.97\n",
      "episode 36, score -119.69303834755098, avg_score -116.51257767533278, episode_time 1.08s, avg_episode_time 2.99s, avg_step_time 0.014341s, avg_learn_time 0.013258s, avg_steps_per_episode 225.58\n",
      "episode 37, score -129.09980611837966, avg_score -116.85277303865837, episode_time 1.91s, avg_episode_time 2.96s, avg_step_time 0.014146s, avg_learn_time 0.013032s, avg_steps_per_episode 222.97\n",
      "episode 38, score -114.39832817205352, avg_score -116.78818238427405, episode_time 1.20s, avg_episode_time 2.91s, avg_step_time 0.014349s, avg_learn_time 0.013243s, avg_steps_per_episode 219.24\n",
      "episode 39, score -117.27444166166832, avg_score -116.80065057087388, episode_time 1.36s, avg_episode_time 2.87s, avg_step_time 0.014081s, avg_learn_time 0.012980s, avg_steps_per_episode 216.03\n",
      "episode 40, score -109.0264034178049, avg_score -116.60629439204715, episode_time 1.67s, avg_episode_time 2.84s, avg_step_time 0.014213s, avg_learn_time 0.013057s, avg_steps_per_episode 213.50\n",
      "episode 41, score -113.62705789366488, avg_score -116.53363008720855, episode_time 0.92s, avg_episode_time 2.79s, avg_step_time 0.014021s, avg_learn_time 0.012954s, avg_steps_per_episode 209.83\n",
      "episode 42, score -121.1788743127659, avg_score -116.64423114019803, episode_time 1.76s, avg_episode_time 2.77s, avg_step_time 0.014845s, avg_learn_time 0.013658s, avg_steps_per_episode 207.60\n",
      "episode 43, score -110.76552778814484, avg_score -116.50751710875493, episode_time 1.12s, avg_episode_time 2.73s, avg_step_time 0.014395s, avg_learn_time 0.013281s, avg_steps_per_episode 204.56\n",
      "episode 44, score -110.42823268819166, avg_score -116.36935155374212, episode_time 1.61s, avg_episode_time 2.71s, avg_step_time 0.014810s, avg_learn_time 0.013676s, avg_steps_per_episode 202.30\n",
      "episode 45, score -110.38133687043128, avg_score -116.23628456077965, episode_time 2.06s, avg_episode_time 2.69s, avg_step_time 0.014236s, avg_learn_time 0.013160s, avg_steps_per_episode 200.93\n",
      "episode 46, score -125.53710161462797, avg_score -116.43847623586332, episode_time 3.50s, avg_episode_time 2.71s, avg_step_time 0.014136s, avg_learn_time 0.013037s, avg_steps_per_episode 201.72\n",
      "episode 47, score -112.0168082933904, avg_score -116.34439819453411, episode_time 2.19s, avg_episode_time 2.70s, avg_step_time 0.014220s, avg_learn_time 0.013166s, avg_steps_per_episode 200.62\n",
      "episode 48, score -109.2978494877629, avg_score -116.19759509647638, episode_time 1.31s, avg_episode_time 2.67s, avg_step_time 0.014848s, avg_learn_time 0.013706s, avg_steps_per_episode 198.21\n",
      "episode 49, score -110.99340746627377, avg_score -116.09138718565592, episode_time 1.08s, avg_episode_time 2.64s, avg_step_time 0.014964s, avg_learn_time 0.013801s, avg_steps_per_episode 195.63\n",
      "episode 50, score -113.8822099976875, avg_score -116.04720364189656, episode_time 1.24s, avg_episode_time 2.61s, avg_step_time 0.015008s, avg_learn_time 0.013906s, avg_steps_per_episode 193.32\n",
      "episode 51, score -114.0623676691583, avg_score -116.00828528948993, episode_time 1.20s, avg_episode_time 2.58s, avg_step_time 0.014502s, avg_learn_time 0.013400s, avg_steps_per_episode 191.12\n",
      "episode 52, score -114.92819985812096, avg_score -115.98751441580976, episode_time 0.89s, avg_episode_time 2.55s, avg_step_time 0.014143s, avg_learn_time 0.013074s, avg_steps_per_episode 188.60\n",
      "episode 53, score -114.99727032892716, avg_score -115.96883056511385, episode_time 0.97s, avg_episode_time 2.52s, avg_step_time 0.014149s, avg_learn_time 0.013090s, avg_steps_per_episode 186.30\n",
      "episode 54, score -112.41500852771786, avg_score -115.90301904590281, episode_time 1.02s, avg_episode_time 2.49s, avg_step_time 0.014133s, avg_learn_time 0.013080s, avg_steps_per_episode 184.15\n",
      "episode 55, score -119.69179358575369, avg_score -115.97190585571829, episode_time 1.18s, avg_episode_time 2.47s, avg_step_time 0.014473s, avg_learn_time 0.013346s, avg_steps_per_episode 182.22\n",
      "episode 56, score -133.08332456666602, avg_score -116.27746690412808, episode_time 2.26s, avg_episode_time 2.46s, avg_step_time 0.014644s, avg_learn_time 0.013511s, avg_steps_per_episode 181.62\n",
      "episode 57, score -115.01698669589125, avg_score -116.25535321626427, episode_time 0.98s, avg_episode_time 2.44s, avg_step_time 0.014478s, avg_learn_time 0.013350s, avg_steps_per_episode 179.60\n",
      "episode 58, score -123.12981942149065, avg_score -116.37387849566473, episode_time 2.58s, avg_episode_time 2.44s, avg_step_time 0.014391s, avg_learn_time 0.013289s, avg_steps_per_episode 179.47\n",
      "episode 59, score -141.3874825334759, avg_score -116.7978378861361, episode_time 23.83s, avg_episode_time 2.80s, avg_step_time 0.014173s, avg_learn_time 0.013081s, avg_steps_per_episode 203.54\n",
      "episode 60, score -112.23884771328854, avg_score -116.72185471658864, episode_time 2.08s, avg_episode_time 2.79s, avg_step_time 0.014852s, avg_learn_time 0.013670s, avg_steps_per_episode 202.43\n",
      "episode 61, score -109.53639477946484, avg_score -116.60406029138989, episode_time 0.71s, avg_episode_time 2.76s, avg_step_time 0.014455s, avg_learn_time 0.013223s, avg_steps_per_episode 199.90\n",
      "episode 62, score -131.57436048064753, avg_score -116.84551674605532, episode_time 7.98s, avg_episode_time 2.84s, avg_step_time 0.014813s, avg_learn_time 0.013640s, avg_steps_per_episode 205.34\n",
      "episode 63, score -131.10395475588243, avg_score -117.07184115890972, episode_time 3.21s, avg_episode_time 2.85s, avg_step_time 0.014270s, avg_learn_time 0.013155s, avg_steps_per_episode 205.48\n",
      "episode 64, score -106.31207142183355, avg_score -116.90371975676791, episode_time 1.11s, avg_episode_time 2.82s, avg_step_time 0.014090s, avg_learn_time 0.013004s, avg_steps_per_episode 203.45\n",
      "episode 65, score -108.64094725384564, avg_score -116.77660017979987, episode_time 0.90s, avg_episode_time 2.79s, avg_step_time 0.014448s, avg_learn_time 0.013297s, avg_steps_per_episode 201.25\n",
      "episode 66, score -116.49177708379924, avg_score -116.77228467834532, episode_time 1.08s, avg_episode_time 2.76s, avg_step_time 0.014059s, avg_learn_time 0.012869s, avg_steps_per_episode 199.35\n",
      "episode 67, score -106.17797231571873, avg_score -116.61416061323149, episode_time 0.75s, avg_episode_time 2.73s, avg_step_time 0.014237s, avg_learn_time 0.013063s, avg_steps_per_episode 197.12\n",
      "episode 68, score -106.45045267577531, avg_score -116.4646943200336, episode_time 0.93s, avg_episode_time 2.71s, avg_step_time 0.013293s, avg_learn_time 0.012250s, avg_steps_per_episode 195.22\n",
      "episode 69, score -113.29950517296233, avg_score -116.4188220135543, episode_time 2.00s, avg_episode_time 2.70s, avg_step_time 0.014308s, avg_learn_time 0.013089s, avg_steps_per_episode 194.38\n",
      "episode 70, score -109.68630925125939, avg_score -116.32264325980724, episode_time 1.41s, avg_episode_time 2.68s, avg_step_time 0.013705s, avg_learn_time 0.012570s, avg_steps_per_episode 193.03\n",
      "episode 71, score -108.4184155754447, avg_score -116.21131610932328, episode_time 0.59s, avg_episode_time 2.65s, avg_step_time 0.014160s, avg_learn_time 0.013062s, avg_steps_per_episode 190.86\n",
      "episode 72, score -114.46273385577649, avg_score -116.18703024469067, episode_time 1.42s, avg_episode_time 2.63s, avg_step_time 0.014198s, avg_learn_time 0.012983s, avg_steps_per_episode 189.57\n",
      "episode 73, score -102.5265193688528, avg_score -115.99989995872029, episode_time 0.89s, avg_episode_time 2.61s, avg_step_time 0.014807s, avg_learn_time 0.013545s, avg_steps_per_episode 187.73\n",
      "episode 74, score -100.42850997633052, avg_score -115.78947576976908, episode_time 1.46s, avg_episode_time 2.59s, avg_step_time 0.015264s, avg_learn_time 0.014122s, avg_steps_per_episode 186.47\n",
      "episode 75, score -108.36662881797366, avg_score -115.69050447707848, episode_time 0.85s, avg_episode_time 2.57s, avg_step_time 0.014247s, avg_learn_time 0.013154s, avg_steps_per_episode 184.76\n",
      "episode 76, score -115.67131190031115, avg_score -115.69025194317364, episode_time 3.42s, avg_episode_time 2.58s, avg_step_time 0.014028s, avg_learn_time 0.012934s, avg_steps_per_episode 185.49\n",
      "episode 77, score -143.2025837717359, avg_score -116.04755495393418, episode_time 22.78s, avg_episode_time 2.84s, avg_step_time 0.014350s, avg_learn_time 0.013239s, avg_steps_per_episode 203.86\n",
      "episode 78, score -103.79207199010501, avg_score -115.89043337747484, episode_time 0.83s, avg_episode_time 2.82s, avg_step_time 0.014365s, avg_learn_time 0.013272s, avg_steps_per_episode 201.96\n",
      "episode 79, score -103.21606361345947, avg_score -115.72999831717084, episode_time 0.76s, avg_episode_time 2.79s, avg_step_time 0.014410s, avg_learn_time 0.013324s, avg_steps_per_episode 200.05\n",
      "episode 80, score -103.84042429497404, avg_score -115.5813786418934, episode_time 0.78s, avg_episode_time 2.77s, avg_step_time 0.014331s, avg_learn_time 0.013213s, avg_steps_per_episode 198.22\n",
      "episode 81, score -104.35503278353997, avg_score -115.44278177944459, episode_time 0.80s, avg_episode_time 2.74s, avg_step_time 0.014073s, avg_learn_time 0.013000s, avg_steps_per_episode 196.46\n",
      "episode 82, score -103.04571876043391, avg_score -115.2915980840908, episode_time 1.75s, avg_episode_time 2.73s, avg_step_time 0.013972s, avg_learn_time 0.012852s, avg_steps_per_episode 195.55\n",
      "episode 83, score -237.31555842372575, avg_score -116.76176628095386, episode_time 21.81s, avg_episode_time 2.96s, avg_step_time 0.014243s, avg_learn_time 0.013133s, avg_steps_per_episode 211.49\n",
      "episode 84, score -108.27958898770747, avg_score -116.66078797984379, episode_time 0.68s, avg_episode_time 2.93s, avg_step_time 0.014661s, avg_learn_time 0.013558s, avg_steps_per_episode 209.51\n",
      "episode 85, score -152.51151584937054, avg_score -117.08256124889704, episode_time 6.13s, avg_episode_time 2.97s, avg_step_time 0.013944s, avg_learn_time 0.012863s, avg_steps_per_episode 211.95\n",
      "episode 86, score -215.7352393009764, avg_score -118.2296854122933, episode_time 20.09s, avg_episode_time 3.17s, avg_step_time 0.014118s, avg_learn_time 0.013014s, avg_steps_per_episode 225.27\n",
      "episode 87, score -123.25365460364888, avg_score -118.28743218460774, episode_time 23.50s, avg_episode_time 3.40s, avg_step_time 0.012542s, avg_learn_time 0.011539s, avg_steps_per_episode 241.07\n",
      "episode 88, score -137.80717602574953, avg_score -118.5092474555298, episode_time 23.72s, avg_episode_time 3.63s, avg_step_time 0.015114s, avg_learn_time 0.013905s, avg_steps_per_episode 256.51\n",
      "episode 89, score -109.55444440072837, avg_score -118.40863169086911, episode_time 0.63s, avg_episode_time 3.60s, avg_step_time 0.014825s, avg_learn_time 0.013649s, avg_steps_per_episode 254.11\n",
      "episode 90, score -109.35105648788499, avg_score -118.30799196639151, episode_time 0.68s, avg_episode_time 3.57s, avg_step_time 0.014463s, avg_learn_time 0.013279s, avg_steps_per_episode 251.79\n",
      "episode 91, score -109.66292721914687, avg_score -118.21299125488333, episode_time 0.58s, avg_episode_time 3.53s, avg_step_time 0.013887s, avg_learn_time 0.012742s, avg_steps_per_episode 249.49\n",
      "episode 92, score -124.6073157731661, avg_score -118.28249478225598, episode_time 1.27s, avg_episode_time 3.51s, avg_step_time 0.012552s, avg_learn_time 0.011560s, avg_steps_per_episode 247.85\n",
      "episode 93, score -127.7808891919038, avg_score -118.38462805547799, episode_time 0.89s, avg_episode_time 3.48s, avg_step_time 0.012375s, avg_learn_time 0.011396s, avg_steps_per_episode 245.94\n",
      "episode 94, score -100.66876034434505, avg_score -118.19616137769998, episode_time 23.00s, avg_episode_time 3.69s, avg_step_time 0.013196s, avg_learn_time 0.012154s, avg_steps_per_episode 260.34\n",
      "episode 95, score -120.91082402244707, avg_score -118.22473677396047, episode_time 1.28s, avg_episode_time 3.66s, avg_step_time 0.013799s, avg_learn_time 0.012669s, avg_steps_per_episode 258.55\n",
      "episode 96, score -128.13336761046762, avg_score -118.32795167850743, episode_time 1.81s, avg_episode_time 3.64s, avg_step_time 0.014466s, avg_learn_time 0.013283s, avg_steps_per_episode 257.16\n",
      "episode 97, score -112.75228401966766, avg_score -118.27047056862249, episode_time 0.89s, avg_episode_time 3.62s, avg_step_time 0.013565s, avg_learn_time 0.012502s, avg_steps_per_episode 255.16\n",
      "episode 98, score -112.89640104215965, avg_score -118.21563312447492, episode_time 0.89s, avg_episode_time 3.59s, avg_step_time 0.013367s, avg_learn_time 0.012318s, avg_steps_per_episode 253.21\n",
      "episode 99, score -110.8851283815739, avg_score -118.14158762202138, episode_time 1.67s, avg_episode_time 3.57s, avg_step_time 0.013971s, avg_learn_time 0.012887s, avg_steps_per_episode 251.82\n",
      "episode 100, score -117.24429710220049, avg_score -118.13261471682317, episode_time 0.99s, avg_episode_time 3.54s, avg_step_time 0.014104s, avg_learn_time 0.013013s, avg_steps_per_episode 249.99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>action_0</td><td></td></tr><tr><td>action_1</td><td></td></tr><tr><td>action_2</td><td></td></tr><tr><td>action_3</td><td></td></tr><tr><td>actor_loss</td><td></td></tr><tr><td>actor_predictions</td><td></td></tr><tr><td>avg_reward</td><td></td></tr><tr><td>best</td><td></td></tr><tr><td>critic_loss</td><td></td></tr><tr><td>critic_predictions</td><td></td></tr><tr><td>episode</td><td></td></tr><tr><td>episode_reward</td><td></td></tr><tr><td>episode_time</td><td></td></tr><tr><td>noise_0</td><td></td></tr><tr><td>noise_1</td><td></td></tr><tr><td>noise_2</td><td></td></tr><tr><td>noise_3</td><td></td></tr><tr><td>step_reward</td><td></td></tr><tr><td>step_time</td><td></td></tr><tr><td>target_actor_predictions</td><td></td></tr><tr><td>target_critic_predictions</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>action_0</td><td>-0.45116</td></tr><tr><td>action_1</td><td>-0.49909</td></tr><tr><td>action_2</td><td>0.06091</td></tr><tr><td>action_3</td><td>-0.65854</td></tr><tr><td>actor_loss</td><td>37.8564</td></tr><tr><td>actor_predictions</td><td>-0.07779</td></tr><tr><td>avg_reward</td><td>-118.13261</td></tr><tr><td>best</td><td>False</td></tr><tr><td>critic_loss</td><td>106.45586</td></tr><tr><td>critic_predictions</td><td>-37.8564</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-117.2443</td></tr><tr><td>episode_time</td><td>0.99127</td></tr><tr><td>noise_0</td><td>0.00393</td></tr><tr><td>noise_1</td><td>-0.14859</td></tr><tr><td>noise_2</td><td>-0.06459</td></tr><tr><td>noise_3</td><td>0.19672</td></tr><tr><td>step_reward</td><td>-100</td></tr><tr><td>step_time</td><td>0.0136</td></tr><tr><td>target_actor_predictions</td><td>-0.02628</td></tr><tr><td>target_critic_predictions</td><td>-38.04234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train-119</strong> at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/mjn7t4i5' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/mjn7t4i5</a><br/> View project at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a><br/>Synced 6 W&B file(s), 3 media file(s), 17 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240723_200906-mjn7t4i5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "td3.train(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(td3.replay_buffer.dones) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.save('src/app/models/td3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/RL_Agents/pytorch/src/app/models.py:808: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "/workspaces/RL_Agents/pytorch/src/app/models.py:1029: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "/workspaces/RL_Agents/pytorch/src/app/helper.py:765: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "with open('/workspaces/RL_Agents/pytorch/src/app/td3_test/td3/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "td3 = rl_agents.TD3.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_type:<class 'str'>;TD3\n",
      "env:<class 'str'>;{\"id\": \"Pendulum-v1\", \"entry_point\": \"gymnasium.envs.classic_control.pendulum:PendulumEnv\", \"reward_threshold\": null, \"nondeterministic\": false, \"max_episode_steps\": 200, \"order_enforce\": true, \"disable_env_checker\": false, \"kwargs\": {}, \"additional_wrappers\": [], \"vector_entry_point\": null}\n",
      "actor_model:<class 'dict'>;{'env': 'Pendulum-v1', 'cnn_model': None, 'num_layers': 4, 'dense_layers': [[256, 'relu', {'default': {}}], [256, 'relu', {'default': {}}]], 'output_layer_kernel': {'default': {}}, 'goal_shape': None, 'optimizer': 'Adam', 'optimizer_params': {'weight_decay': 0}, 'learning_rate': 0.0001, 'normalize_layers': False}\n",
      "critic_model:<class 'dict'>;{'env': 'Pendulum-v1', 'cnn_model': None, 'num_layers': 4, 'state_layers': [], 'merged_layers': [[256, 'relu', {'default': {}}], [256, 'relu', {'default': {}}]], 'output_layer_kernel': {'default': {}}, 'goal_shape': None, 'optimizer': 'Adam', 'optimizer_params': {'weight_decay': 0}, 'learning_rate': 0.0001, 'normalize_layers': False}\n",
      "discount:<class 'float'>;0.99\n",
      "tau:<class 'float'>;0.005\n",
      "action_epsilon:<class 'float'>;0.2\n",
      "replay_buffer:<class 'dict'>;{'class_name': 'ReplayBuffer', 'config': {'env': <TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>, 'buffer_size': 100000, 'goal_shape': None}}\n",
      "batch_size:<class 'int'>;256\n",
      "noise:<class 'dict'>;{'class_name': 'NormalNoise', 'config': {'shape': [1], 'mean': 0.0, 'stddev': 0.10000000149011612}}\n",
      "target_noise_stddev:<class 'float'>;0.2\n",
      "target_noise_clip:<class 'float'>;0.5\n",
      "actor_update_delay:<class 'int'>;2\n",
      "normalize_inputs:<class 'str'>;False\n",
      "normalizer_clip:<class 'NoneType'>;None\n",
      "normalizer_eps:<class 'float'>;0.01\n",
      "warmup:<class 'int'>;1000\n",
      "callbacks:<class 'list'>;[{'class_name': 'DashCallback', 'config': {'dash_app_url': 'http://127.0.0.1:8050'}}, {'class_name': 'WandbCallback', 'config': {'project_name': 'Pendulum-v1', 'run_name': None, 'chkpt_freq': 100, '_sweep': False}}]\n",
      "save_dir:<class 'str'>;/workspaces/RL_Agents/pytorch/src/app/td3_test/td3/\n",
      "device:<class 'str'>;cuda\n"
     ]
    }
   ],
   "source": [
    "for key,val in config.items():\n",
    "    print(f'{key}:{type(val)};{val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_type': 'TD3',\n",
       " 'env': '{\"id\": \"Pendulum-v1\", \"entry_point\": \"gymnasium.envs.classic_control.pendulum:PendulumEnv\", \"reward_threshold\": null, \"nondeterministic\": false, \"max_episode_steps\": 200, \"order_enforce\": true, \"disable_env_checker\": false, \"kwargs\": {}, \"additional_wrappers\": [], \"vector_entry_point\": null}',\n",
       " 'actor_model': {'env': 'Pendulum-v1',\n",
       "  'cnn_model': None,\n",
       "  'num_layers': 4,\n",
       "  'dense_layers': [[256, 'relu', {'default': {}}],\n",
       "   [256, 'relu', {'default': {}}]],\n",
       "  'output_layer_kernel': {'default': {}},\n",
       "  'goal_shape': None,\n",
       "  'optimizer': 'Adam',\n",
       "  'optimizer_params': {'weight_decay': 0},\n",
       "  'learning_rate': 0.0001,\n",
       "  'normalize_layers': False},\n",
       " 'critic_model': {'env': 'Pendulum-v1',\n",
       "  'cnn_model': None,\n",
       "  'num_layers': 4,\n",
       "  'state_layers': [],\n",
       "  'merged_layers': [[256, 'relu', {'default': {}}],\n",
       "   [256, 'relu', {'default': {}}]],\n",
       "  'output_layer_kernel': {'default': {}},\n",
       "  'goal_shape': None,\n",
       "  'optimizer': 'Adam',\n",
       "  'optimizer_params': {'weight_decay': 0},\n",
       "  'learning_rate': 0.0001,\n",
       "  'normalize_layers': False},\n",
       " 'discount': 0.99,\n",
       " 'tau': 0.005,\n",
       " 'action_epsilon': 0.2,\n",
       " 'replay_buffer': {'class_name': 'ReplayBuffer',\n",
       "  'config': {'env': 'Pendulum-v1', 'buffer_size': 100000, 'goal_shape': None}},\n",
       " 'batch_size': 256,\n",
       " 'noise': {'class_name': 'NormalNoise',\n",
       "  'config': {'shape': [1], 'mean': 0.0, 'stddev': 0.10000000149011612}},\n",
       " 'target_noise_stddev': 0.2,\n",
       " 'target_noise_clip': 0.5,\n",
       " 'actor_update_delay': 2,\n",
       " 'normalize_inputs': 'False',\n",
       " 'normalizer_clip': None,\n",
       " 'normalizer_eps': 0.01,\n",
       " 'warmup': 1000,\n",
       " 'callbacks': [{'class_name': 'DashCallback',\n",
       "   'config': {'dash_app_url': 'http://127.0.0.1:8050'}},\n",
       "  {'class_name': 'WandbCallback',\n",
       "   'config': {'project_name': 'Pendulum-v1',\n",
       "    'run_name': None,\n",
       "    'chkpt_freq': 100,\n",
       "    '_sweep': False}}],\n",
       " 'save_dir': '/workspaces/RL_Agents/pytorch/src/app/td3_test/td3/'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td3.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new save dir: models/td3/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:282: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Overwriting existing videos at /workspaces/RL_Agents/pytorch/src/app/models/td3/renders/testing folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/RL_Agents/pytorch/src/app/wandb/run-20240723_211154-hvx9z60p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/hvx9z60p' target=\"_blank\">test-119</a></strong> to <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/hvx9z60p' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/hvx9z60p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1, score -132.20585432354616, avg_score -132.20585432354616\n",
      "episode 2, score -128.11128140719205, avg_score -130.1585678653691\n",
      "episode 3, score -112.35276252416146, avg_score -124.22329941829987\n",
      "episode 4, score -114.97414180118714, avg_score -121.9110100140217\n",
      "episode 5, score -125.37473318914644, avg_score -122.60375464904664\n",
      "episode 6, score -111.36876013943242, avg_score -120.73125556411094\n",
      "episode 7, score -130.2563140302084, avg_score -122.09197820212486\n",
      "episode 8, score -125.88898792958807, avg_score -122.56660441805778\n",
      "episode 9, score -127.68787832038116, avg_score -123.13563485164926\n",
      "episode 10, score -121.85973918368853, avg_score -123.00804528485318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td></td></tr><tr><td>episode_reward</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-123.00805</td></tr><tr><td>episode_reward</td><td>-121.85974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-119</strong> at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/hvx9z60p' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3/runs/hvx9z60p</a><br/> View project at: <a href='https://wandb.ai/jasonhayes1987/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/jasonhayes1987/BipedalWalker-v3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240723_211154-hvx9z60p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "td3.test(10, True, 1, td3.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
