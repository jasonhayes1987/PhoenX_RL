{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, StochasticDiscretePolicy\n",
    "import cnn_models\n",
    "from rl_agents import PPO, DDPG, TD3, Reinforce, ActorCritic, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "import helper\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000)\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(100, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [19. 19. 19. 19. 19. 19. 19. 19. 19. 19.]; total steps: 1000; avg scores/10 episodes: [-68.10239765 -65.69785197 -62.53430581 -66.17193885 -69.8615859\n",
      " -64.9397408  -62.39274841 -69.45490755 -61.2827754  -64.75273364]; avg total score: -65.51909859689749\n",
      "episode: [39. 39. 39. 39. 39. 39. 39. 39. 39. 39.]; total steps: 2000; avg scores/10 episodes: [-69.17764913 -63.61088188 -67.83780027 -67.74897223 -66.07980366\n",
      " -64.31013986 -68.495759   -69.59622271 -67.30612362 -66.34631098]; avg total score: -67.05096633519588\n",
      "learning timestep: 2000\n",
      "num batches:312\n",
      "Policy Loss: -0.1184292659163475\n",
      "Value Loss: 76.12649536132812\n",
      "Entropy: -19.36493492126465\n",
      "KL Divergence: tensor([[0.6511],\n",
      "        [0.7132],\n",
      "        [0.5212],\n",
      "        [0.9265],\n",
      "        [3.2466],\n",
      "        [0.6552],\n",
      "        [1.2682],\n",
      "        [0.7399],\n",
      "        [1.1991],\n",
      "        [0.1408],\n",
      "        [0.8170],\n",
      "        [1.4339],\n",
      "        [0.9206],\n",
      "        [0.2762],\n",
      "        [0.5442],\n",
      "        [2.8555],\n",
      "        [0.3493],\n",
      "        [1.1908],\n",
      "        [1.1552],\n",
      "        [0.9043],\n",
      "        [0.3481],\n",
      "        [0.3607],\n",
      "        [1.8940],\n",
      "        [0.8727],\n",
      "        [3.6163],\n",
      "        [4.5634],\n",
      "        [5.8402],\n",
      "        [0.1978],\n",
      "        [2.8956],\n",
      "        [0.9025],\n",
      "        [0.9763],\n",
      "        [0.0740],\n",
      "        [1.7233],\n",
      "        [0.3109],\n",
      "        [1.0166],\n",
      "        [4.8230],\n",
      "        [0.0943],\n",
      "        [0.6771],\n",
      "        [1.2216],\n",
      "        [0.7827],\n",
      "        [0.8811],\n",
      "        [1.1602],\n",
      "        [1.1129],\n",
      "        [0.4032],\n",
      "        [0.5017],\n",
      "        [0.4833],\n",
      "        [0.8009],\n",
      "        [4.0364],\n",
      "        [1.4654],\n",
      "        [2.4361],\n",
      "        [0.3615],\n",
      "        [0.6086],\n",
      "        [1.1309],\n",
      "        [0.7792],\n",
      "        [0.7022],\n",
      "        [4.8395],\n",
      "        [0.4900],\n",
      "        [4.6610],\n",
      "        [2.1757],\n",
      "        [0.8682],\n",
      "        [0.0599],\n",
      "        [8.8415],\n",
      "        [1.3689],\n",
      "        [1.7198]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [58. 58. 58. 58. 58. 58. 58. 58. 58. 58.]; total steps: 3000; avg scores/10 episodes: [-46.19056342 -45.49612454 -49.06959334 -41.69377196 -53.31775461\n",
      " -44.79975813 -36.85758915 -47.63300626 -45.89981428 -53.48761999]; avg total score: -46.44455956819081\n",
      "episode: [78. 78. 78. 78. 78. 78. 78. 78. 78. 78.]; total steps: 4000; avg scores/10 episodes: [-36.17881681 -41.56447709 -41.70665385 -33.66833468 -48.09074919\n",
      " -40.46261853 -46.05577111 -50.12762534 -35.59093265 -37.97804418]; avg total score: -41.14240234371121\n",
      "learning timestep: 4000\n",
      "num batches:312\n",
      "Policy Loss: -0.21803276240825653\n",
      "Value Loss: 24.100475311279297\n",
      "Entropy: -14.325084686279297\n",
      "KL Divergence: tensor([[ 0.3574],\n",
      "        [ 0.2437],\n",
      "        [ 1.0790],\n",
      "        [ 0.8298],\n",
      "        [ 0.1938],\n",
      "        [ 4.1175],\n",
      "        [ 1.3852],\n",
      "        [ 3.2990],\n",
      "        [ 1.1777],\n",
      "        [ 5.8470],\n",
      "        [ 0.7565],\n",
      "        [22.9943],\n",
      "        [ 2.5035],\n",
      "        [ 4.8450],\n",
      "        [ 0.9282],\n",
      "        [ 0.6224],\n",
      "        [ 0.9305],\n",
      "        [ 0.0408],\n",
      "        [ 1.7932],\n",
      "        [ 0.1106],\n",
      "        [ 1.0868],\n",
      "        [ 1.2266],\n",
      "        [ 4.0659],\n",
      "        [ 5.1198],\n",
      "        [ 1.1677],\n",
      "        [ 2.6642],\n",
      "        [ 4.4734],\n",
      "        [ 7.5542],\n",
      "        [ 2.7953],\n",
      "        [ 6.3256],\n",
      "        [ 0.2253],\n",
      "        [ 2.1349],\n",
      "        [ 2.8456],\n",
      "        [ 1.4373],\n",
      "        [ 4.2424],\n",
      "        [ 0.7933],\n",
      "        [ 7.7530],\n",
      "        [ 2.7977],\n",
      "        [ 1.1075],\n",
      "        [12.6643],\n",
      "        [ 4.5511],\n",
      "        [ 5.1103],\n",
      "        [ 1.4535],\n",
      "        [ 0.0562],\n",
      "        [ 2.9346],\n",
      "        [ 3.2528],\n",
      "        [ 3.5320],\n",
      "        [ 0.5776],\n",
      "        [ 0.5234],\n",
      "        [22.7894],\n",
      "        [ 6.4299],\n",
      "        [ 0.6983],\n",
      "        [ 3.2301],\n",
      "        [ 8.9729],\n",
      "        [11.3658],\n",
      "        [ 0.6055],\n",
      "        [ 5.2611],\n",
      "        [ 0.0261],\n",
      "        [ 0.0616],\n",
      "        [ 1.5299],\n",
      "        [ 2.1251],\n",
      "        [13.1349],\n",
      "        [ 6.0593],\n",
      "        [ 6.4748]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [98. 98. 98. 98. 98. 98. 98. 98. 98. 98.]; total steps: 5000; avg scores/10 episodes: [-20.39736201 -23.88104837 -22.46780467 -21.42379284 -22.15094536\n",
      " -21.77891529 -21.18901782 -21.97734309 -21.12005834 -22.70023784]; avg total score: -21.908652563357812\n",
      "episode: [117. 117. 117. 117. 117. 117. 117. 117. 117. 117.]; total steps: 6000; avg scores/10 episodes: [-22.38943886 -21.82198021 -22.87082055 -20.39378501 -22.73939772\n",
      " -20.48547751 -24.65750837 -20.25452107 -20.71596674 -21.89291088]; avg total score: -21.8221806917016\n",
      "learning timestep: 6000\n",
      "num batches:312\n",
      "Policy Loss: -0.11610198765993118\n",
      "Value Loss: 18.96494483947754\n",
      "Entropy: 39.999000549316406\n",
      "KL Divergence: tensor([[5.7192e-03],\n",
      "        [2.1477e+01],\n",
      "        [9.5077e-01],\n",
      "        [1.6848e+00],\n",
      "        [8.1098e-03],\n",
      "        [2.8576e-02],\n",
      "        [2.6474e-01],\n",
      "        [3.0644e+00],\n",
      "        [6.4867e+00],\n",
      "        [1.4435e-01],\n",
      "        [2.2539e+01],\n",
      "        [1.8495e+01],\n",
      "        [2.3039e-01],\n",
      "        [1.2897e-01],\n",
      "        [4.9582e-02],\n",
      "        [6.4179e-02],\n",
      "        [8.7460e+00],\n",
      "        [1.7234e+01],\n",
      "        [2.0338e-01],\n",
      "        [1.6278e+01],\n",
      "        [7.5551e+00],\n",
      "        [3.2459e+00],\n",
      "        [1.7954e+01],\n",
      "        [5.2852e-02],\n",
      "        [6.3154e+00],\n",
      "        [2.4779e+00],\n",
      "        [1.1841e-01],\n",
      "        [8.0373e+00],\n",
      "        [9.1339e+00],\n",
      "        [9.1873e-02],\n",
      "        [1.3695e-01],\n",
      "        [5.3461e-02],\n",
      "        [3.2014e-01],\n",
      "        [2.7366e-01],\n",
      "        [1.0599e-02],\n",
      "        [8.5524e+00],\n",
      "        [6.1566e+00],\n",
      "        [8.8789e+00],\n",
      "        [7.0730e+00],\n",
      "        [1.4226e+01],\n",
      "        [5.8193e-02],\n",
      "        [9.0891e+00],\n",
      "        [9.8727e-01],\n",
      "        [8.6662e-01],\n",
      "        [2.5389e-02],\n",
      "        [1.3035e-01],\n",
      "        [1.1395e-01],\n",
      "        [7.9948e+00],\n",
      "        [2.1361e+00],\n",
      "        [7.4972e+00],\n",
      "        [3.5678e+00],\n",
      "        [4.8180e-02],\n",
      "        [2.7320e-03],\n",
      "        [3.3770e-02],\n",
      "        [7.8214e+00],\n",
      "        [7.8601e+00],\n",
      "        [8.2781e+00],\n",
      "        [1.5476e+01],\n",
      "        [1.4356e-01],\n",
      "        [2.9982e-01],\n",
      "        [2.6961e+01],\n",
      "        [7.8585e+00],\n",
      "        [8.2951e+00],\n",
      "        [3.4710e-01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [137. 137. 137. 137. 137. 137. 137. 137. 137. 137.]; total steps: 7000; avg scores/10 episodes: [ -87.85468859  -86.27672847 -100.58296037  -94.25415306  -88.51859775\n",
      "  -92.15631611  -87.62668997  -94.63094787  -98.24463732  -88.49301137]; avg total score: -91.86387308876445\n",
      "episode: [156. 156. 156. 156. 156. 156. 156. 156. 156. 156.]; total steps: 8000; avg scores/10 episodes: [ -90.20887167  -93.4841992   -89.4603898   -77.5043809   -88.92402108\n",
      "  -91.45109251  -88.09691194 -100.76967716  -85.2472105   -91.70262982]; avg total score: -89.68493845632887\n",
      "learning timestep: 8000\n",
      "num batches:312\n",
      "Policy Loss: 0.11024996638298035\n",
      "Value Loss: 686.7332763671875\n",
      "Entropy: 119.34196472167969\n",
      "KL Divergence: tensor([[4.1016e+00],\n",
      "        [1.0404e+00],\n",
      "        [5.6067e+00],\n",
      "        [5.3160e+00],\n",
      "        [4.0859e+00],\n",
      "        [2.3806e+01],\n",
      "        [7.4515e-01],\n",
      "        [6.0586e-01],\n",
      "        [4.0674e+00],\n",
      "        [1.4207e+01],\n",
      "        [9.4608e-01],\n",
      "        [2.2273e+01],\n",
      "        [1.7013e+01],\n",
      "        [7.0757e+00],\n",
      "        [2.7811e+01],\n",
      "        [1.1188e+01],\n",
      "        [4.5947e+00],\n",
      "        [1.0863e-02],\n",
      "        [7.2691e-01],\n",
      "        [6.7805e+00],\n",
      "        [7.7660e+00],\n",
      "        [3.2699e-02],\n",
      "        [6.5925e+00],\n",
      "        [4.2348e+01],\n",
      "        [4.2881e+00],\n",
      "        [1.1883e+01],\n",
      "        [7.0223e+00],\n",
      "        [3.8757e-01],\n",
      "        [8.3931e-01],\n",
      "        [2.2833e+01],\n",
      "        [9.1506e+00],\n",
      "        [2.7819e+00],\n",
      "        [1.1160e+01],\n",
      "        [6.2714e+00],\n",
      "        [4.1865e+00],\n",
      "        [6.7879e+00],\n",
      "        [6.4598e+00],\n",
      "        [3.5719e+00],\n",
      "        [4.4773e+00],\n",
      "        [3.3046e+00],\n",
      "        [7.3722e+00],\n",
      "        [8.4883e-02],\n",
      "        [4.6326e+00],\n",
      "        [1.0949e+01],\n",
      "        [3.1454e+00],\n",
      "        [4.1443e+00],\n",
      "        [3.3848e+01],\n",
      "        [5.3340e-02],\n",
      "        [3.3040e+00],\n",
      "        [4.7752e-01],\n",
      "        [3.9552e+00],\n",
      "        [2.0721e+00],\n",
      "        [1.0105e+01],\n",
      "        [3.9034e-03],\n",
      "        [1.3124e+01],\n",
      "        [2.6274e+00],\n",
      "        [2.0406e+00],\n",
      "        [3.5992e+00],\n",
      "        [2.3458e+00],\n",
      "        [2.1490e+01],\n",
      "        [2.6013e+00],\n",
      "        [1.6272e+01],\n",
      "        [7.6269e-01],\n",
      "        [2.3269e+01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [176. 176. 176. 176. 176. 176. 176. 176. 176. 176.]; total steps: 9000; avg scores/10 episodes: [-145.91917209 -152.4514736  -140.27040225 -142.66688159 -147.27900185\n",
      " -143.5693518  -146.30779163 -158.53998078 -138.21837412 -147.66023789]; avg total score: -146.28826676105567\n",
      "episode: [196. 196. 196. 196. 196. 196. 196. 196. 196. 196.]; total steps: 10000; avg scores/10 episodes: [-140.87608605 -150.01374456 -154.06937396 -147.84344872 -158.44478473\n",
      " -143.65786576 -150.71020937 -157.89680927 -142.296483   -151.74984264]; avg total score: -149.7558648045318\n",
      "learning timestep: 10000\n",
      "num batches:312\n",
      "Policy Loss: -0.09311118721961975\n",
      "Value Loss: 136.86053466796875\n",
      "Entropy: 3.4625189304351807\n",
      "KL Divergence: tensor([[6.2253e-01],\n",
      "        [7.7747e-01],\n",
      "        [2.6443e+00],\n",
      "        [2.3284e+00],\n",
      "        [6.5496e-01],\n",
      "        [1.0816e+00],\n",
      "        [1.7128e+02],\n",
      "        [1.2591e+01],\n",
      "        [6.7700e+00],\n",
      "        [2.4229e+00],\n",
      "        [3.0257e+00],\n",
      "        [5.9679e-02],\n",
      "        [1.6241e+00],\n",
      "        [1.9839e+00],\n",
      "        [5.5079e+00],\n",
      "        [3.7293e+00],\n",
      "        [8.0088e+00],\n",
      "        [1.5423e+00],\n",
      "        [3.7246e+00],\n",
      "        [1.3562e+00],\n",
      "        [5.7707e-04],\n",
      "        [1.5086e+02],\n",
      "        [4.4401e+00],\n",
      "        [5.8936e+01],\n",
      "        [5.0138e+00],\n",
      "        [2.5661e-01],\n",
      "        [5.7869e-01],\n",
      "        [2.0231e+00],\n",
      "        [1.2411e+01],\n",
      "        [4.5843e-01],\n",
      "        [4.8503e+00],\n",
      "        [1.2350e+01],\n",
      "        [1.8052e+01],\n",
      "        [9.3385e-03],\n",
      "        [1.9473e-01],\n",
      "        [1.8167e+00],\n",
      "        [5.2814e-04],\n",
      "        [1.2047e-01],\n",
      "        [4.8982e+00],\n",
      "        [3.5262e+00],\n",
      "        [6.6582e-03],\n",
      "        [5.5102e+00],\n",
      "        [3.7085e+00],\n",
      "        [2.2392e+00],\n",
      "        [5.0694e+00],\n",
      "        [6.2516e+00],\n",
      "        [9.5782e-04],\n",
      "        [1.1395e+00],\n",
      "        [1.1050e+00],\n",
      "        [1.2402e+00],\n",
      "        [5.5333e+00],\n",
      "        [2.0779e-01],\n",
      "        [1.8727e+01],\n",
      "        [6.5350e+00],\n",
      "        [1.0860e-03],\n",
      "        [4.2118e-01],\n",
      "        [1.0750e+01],\n",
      "        [9.4864e-01],\n",
      "        [3.8336e-01],\n",
      "        [1.1478e+00],\n",
      "        [3.4639e+00],\n",
      "        [4.7040e+00],\n",
      "        [1.3299e+00],\n",
      "        [1.5315e-03]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [215. 215. 215. 215. 215. 215. 215. 215. 215. 215.]; total steps: 11000; avg scores/10 episodes: [-312.27410823 -331.19052825 -321.16675527 -313.19987272 -312.64082922\n",
      " -297.28334763 -298.18137912 -302.0896578  -282.09637356 -321.20334247]; avg total score: -309.13261942527305\n",
      "episode: [235. 235. 235. 235. 235. 235. 235. 235. 235. 235.]; total steps: 12000; avg scores/10 episodes: [-311.47581137 -305.34893391 -286.44338353 -324.79952286 -303.86598466\n",
      " -322.35853846 -308.80488802 -308.47815906 -299.55042291 -300.23880524]; avg total score: -307.1364450022307\n",
      "learning timestep: 12000\n",
      "num batches:312\n",
      "Policy Loss: -0.14715272188186646\n",
      "Value Loss: 78.83245849609375\n",
      "Entropy: 117.01763153076172\n",
      "KL Divergence: tensor([[5.3185e-02],\n",
      "        [3.6344e-01],\n",
      "        [1.8001e+00],\n",
      "        [1.0430e+01],\n",
      "        [1.3376e+00],\n",
      "        [8.0564e-01],\n",
      "        [9.8724e+00],\n",
      "        [7.2142e-01],\n",
      "        [1.9617e+00],\n",
      "        [4.5005e+00],\n",
      "        [1.8882e+00],\n",
      "        [3.4048e+00],\n",
      "        [1.5302e+01],\n",
      "        [2.5189e+00],\n",
      "        [7.9735e+00],\n",
      "        [8.2315e-02],\n",
      "        [3.4396e+00],\n",
      "        [6.4448e+00],\n",
      "        [5.3966e+00],\n",
      "        [6.4920e-02],\n",
      "        [2.1301e+00],\n",
      "        [5.7220e+00],\n",
      "        [2.4892e+00],\n",
      "        [1.1036e+01],\n",
      "        [6.1225e+00],\n",
      "        [7.4100e+01],\n",
      "        [3.6377e+00],\n",
      "        [2.1475e+00],\n",
      "        [1.3302e+00],\n",
      "        [4.7304e-01],\n",
      "        [3.4182e+00],\n",
      "        [1.7500e+01],\n",
      "        [3.0917e+00],\n",
      "        [2.0502e+01],\n",
      "        [6.2730e-02],\n",
      "        [6.4059e+00],\n",
      "        [2.5262e-01],\n",
      "        [9.1848e-02],\n",
      "        [3.4615e+00],\n",
      "        [3.4869e+00],\n",
      "        [2.3845e+01],\n",
      "        [2.7978e+00],\n",
      "        [2.0855e+01],\n",
      "        [2.4004e-01],\n",
      "        [1.6023e+00],\n",
      "        [3.1747e+00],\n",
      "        [4.3975e-01],\n",
      "        [8.6302e+00],\n",
      "        [5.9536e+00],\n",
      "        [1.5980e+01],\n",
      "        [1.5826e+01],\n",
      "        [9.9637e+00],\n",
      "        [1.9631e+00],\n",
      "        [2.4580e+00],\n",
      "        [1.5754e+01],\n",
      "        [1.4044e+01],\n",
      "        [7.9896e+00],\n",
      "        [1.6637e+00],\n",
      "        [8.7200e+00],\n",
      "        [1.4923e+00],\n",
      "        [2.0325e+01],\n",
      "        [6.1101e+00],\n",
      "        [1.0521e+01],\n",
      "        [3.2579e+01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [254. 254. 254. 254. 254. 254. 254. 254. 254. 254.]; total steps: 13000; avg scores/10 episodes: [-150.29472923 -163.38870979 -136.22907085 -156.08523353 -174.57129583\n",
      " -137.12953283 -138.68163567 -159.78518316 -159.34788525 -148.99132798]; avg total score: -152.45046041189738\n",
      "episode: [274. 274. 274. 274. 274. 274. 274. 274. 274. 274.]; total steps: 14000; avg scores/10 episodes: [-131.83747798 -150.41056564 -121.86666156 -145.39731553 -150.48945346\n",
      " -160.16817896 -146.25626002 -126.0842314  -153.31002344 -132.31335461]; avg total score: -141.81335225888762\n",
      "learning timestep: 14000\n",
      "num batches:312\n",
      "Policy Loss: 0.13082894682884216\n",
      "Value Loss: 33.407196044921875\n",
      "Entropy: 124.54548645019531\n",
      "KL Divergence: tensor([[7.6993e-01],\n",
      "        [2.6311e+00],\n",
      "        [1.5893e+00],\n",
      "        [5.4985e-02],\n",
      "        [3.2761e+00],\n",
      "        [1.8051e+00],\n",
      "        [7.2896e+00],\n",
      "        [2.9409e-01],\n",
      "        [2.2504e+00],\n",
      "        [7.3222e-02],\n",
      "        [2.3713e+00],\n",
      "        [1.6088e+00],\n",
      "        [2.5285e-01],\n",
      "        [1.0075e-01],\n",
      "        [3.4788e-01],\n",
      "        [3.2153e-01],\n",
      "        [7.1911e-01],\n",
      "        [2.0777e+00],\n",
      "        [3.2620e-02],\n",
      "        [5.0482e+00],\n",
      "        [9.3306e+00],\n",
      "        [1.6113e-01],\n",
      "        [1.4969e-01],\n",
      "        [4.2080e+00],\n",
      "        [2.4849e-01],\n",
      "        [4.9263e+00],\n",
      "        [9.9112e-01],\n",
      "        [6.7749e+00],\n",
      "        [1.5122e+00],\n",
      "        [3.9723e+00],\n",
      "        [8.7136e+00],\n",
      "        [1.9503e+00],\n",
      "        [6.9341e+00],\n",
      "        [4.1092e+00],\n",
      "        [4.9047e-01],\n",
      "        [1.5022e+00],\n",
      "        [1.2635e-01],\n",
      "        [1.4482e+00],\n",
      "        [2.7936e+00],\n",
      "        [5.1856e+00],\n",
      "        [3.7367e+00],\n",
      "        [6.0485e+00],\n",
      "        [5.0268e-01],\n",
      "        [7.0790e-02],\n",
      "        [4.8424e+00],\n",
      "        [5.0959e+00],\n",
      "        [4.4947e+00],\n",
      "        [9.3214e-02],\n",
      "        [4.2418e+00],\n",
      "        [2.4179e-01],\n",
      "        [2.1114e+00],\n",
      "        [4.6274e+00],\n",
      "        [7.6777e+00],\n",
      "        [4.3784e-01],\n",
      "        [2.9937e+00],\n",
      "        [4.8410e+00],\n",
      "        [4.9464e+01],\n",
      "        [1.1218e+01],\n",
      "        [4.3793e-01],\n",
      "        [7.0382e+00],\n",
      "        [1.6528e+00],\n",
      "        [5.4958e+00],\n",
      "        [5.5622e+01],\n",
      "        [2.8525e-02]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [294. 294. 294. 294. 294. 294. 294. 294. 294. 294.]; total steps: 15000; avg scores/10 episodes: [-205.71149956 -217.88987957 -156.10805084 -181.62409838 -189.36593382\n",
      " -166.97939962 -187.9592005  -209.42444012 -198.74747922 -208.62222229]; avg total score: -192.24322039032387\n",
      "episode: [313. 313. 313. 313. 313. 313. 313. 313. 313. 313.]; total steps: 16000; avg scores/10 episodes: [-182.59067432 -153.14219427 -147.41409775 -183.76401173 -191.21529574\n",
      " -208.56669797 -234.66358928 -211.18543997 -178.71700076 -184.01876168]; avg total score: -187.5277763475628\n",
      "learning timestep: 16000\n",
      "num batches:312\n",
      "Policy Loss: -0.18152004480361938\n",
      "Value Loss: 30.49258041381836\n",
      "Entropy: 72.18985748291016\n",
      "KL Divergence: tensor([[6.9255e+00],\n",
      "        [5.0165e+00],\n",
      "        [3.3714e+00],\n",
      "        [4.7731e-02],\n",
      "        [2.4894e+00],\n",
      "        [4.4780e+00],\n",
      "        [6.0656e+00],\n",
      "        [1.6599e+00],\n",
      "        [1.9304e-02],\n",
      "        [7.1266e-02],\n",
      "        [2.3719e+00],\n",
      "        [3.2157e+01],\n",
      "        [1.0920e+01],\n",
      "        [1.1489e+00],\n",
      "        [1.3614e+01],\n",
      "        [1.2286e-01],\n",
      "        [1.0222e+01],\n",
      "        [5.7543e+00],\n",
      "        [1.0075e+00],\n",
      "        [3.5475e+00],\n",
      "        [3.4887e+00],\n",
      "        [1.3245e-02],\n",
      "        [2.6023e+01],\n",
      "        [2.9899e+01],\n",
      "        [4.5976e+01],\n",
      "        [2.4054e+00],\n",
      "        [2.4422e-01],\n",
      "        [3.4012e-02],\n",
      "        [1.6786e-01],\n",
      "        [3.4327e+00],\n",
      "        [1.7950e-01],\n",
      "        [1.3998e+00],\n",
      "        [2.8826e+00],\n",
      "        [2.3705e+00],\n",
      "        [3.9893e+00],\n",
      "        [3.3829e-01],\n",
      "        [3.9665e+01],\n",
      "        [3.7702e+01],\n",
      "        [7.1155e+00],\n",
      "        [4.0065e+00],\n",
      "        [2.2269e-01],\n",
      "        [7.5259e+00],\n",
      "        [2.3284e+00],\n",
      "        [4.4586e-01],\n",
      "        [6.1693e+00],\n",
      "        [1.5054e-01],\n",
      "        [8.1282e+00],\n",
      "        [4.4303e+00],\n",
      "        [1.2836e+01],\n",
      "        [3.7348e+00],\n",
      "        [1.6469e-01],\n",
      "        [1.4914e-01],\n",
      "        [1.9707e-02],\n",
      "        [8.4903e-01],\n",
      "        [1.3018e+00],\n",
      "        [1.4242e+01],\n",
      "        [3.1362e+01],\n",
      "        [8.4522e+01],\n",
      "        [6.1074e-01],\n",
      "        [1.3201e-01],\n",
      "        [3.7221e+00],\n",
      "        [5.8531e-02],\n",
      "        [2.0373e+01],\n",
      "        [3.8211e+00]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [333. 333. 333. 333. 333. 333. 333. 333. 333. 333.]; total steps: 17000; avg scores/10 episodes: [-101.62295002 -106.88921595  -99.0500919  -100.95130336  -96.99791192\n",
      " -104.21009774 -105.64149637  -95.50403646  -96.0986962  -105.71806103]; avg total score: -101.26838609396256\n",
      "episode: [352. 352. 352. 352. 352. 352. 352. 352. 352. 352.]; total steps: 18000; avg scores/10 episodes: [-104.49741591  -98.88947737 -101.79843877 -100.97241855  -96.80319845\n",
      " -104.05758143 -107.83828721 -100.24000551 -103.10597916 -108.80812361]; avg total score: -102.70109259751284\n",
      "learning timestep: 18000\n",
      "num batches:312\n",
      "Policy Loss: -0.010679125785827637\n",
      "Value Loss: 24.120685577392578\n",
      "Entropy: 63.38216781616211\n",
      "KL Divergence: tensor([[4.4976e-01],\n",
      "        [7.8054e-01],\n",
      "        [2.6907e-01],\n",
      "        [5.1266e+00],\n",
      "        [6.4314e-01],\n",
      "        [1.1998e+00],\n",
      "        [1.0685e+00],\n",
      "        [8.9261e+00],\n",
      "        [2.3965e-01],\n",
      "        [1.9989e+00],\n",
      "        [5.2954e+01],\n",
      "        [7.6915e-02],\n",
      "        [2.0722e+00],\n",
      "        [4.1223e+00],\n",
      "        [6.8818e-01],\n",
      "        [2.2051e-02],\n",
      "        [4.5926e-02],\n",
      "        [2.5015e+01],\n",
      "        [1.0117e+01],\n",
      "        [1.4428e+01],\n",
      "        [1.0209e+01],\n",
      "        [2.7004e-01],\n",
      "        [1.4815e-01],\n",
      "        [8.5203e-01],\n",
      "        [6.2756e+00],\n",
      "        [7.6795e-01],\n",
      "        [7.8436e+00],\n",
      "        [7.6774e+00],\n",
      "        [8.1852e-01],\n",
      "        [7.9475e-01],\n",
      "        [2.1623e+00],\n",
      "        [8.6238e-01],\n",
      "        [2.2798e+00],\n",
      "        [4.7522e-01],\n",
      "        [6.9293e+00],\n",
      "        [1.5658e+01],\n",
      "        [3.0016e-01],\n",
      "        [6.6039e-01],\n",
      "        [4.8969e-02],\n",
      "        [4.8607e+00],\n",
      "        [1.0082e+00],\n",
      "        [3.8049e-01],\n",
      "        [6.5152e+00],\n",
      "        [4.2782e-01],\n",
      "        [8.8112e+00],\n",
      "        [4.4817e-01],\n",
      "        [1.2923e+00],\n",
      "        [7.2440e-01],\n",
      "        [1.9214e+00],\n",
      "        [8.3536e-02],\n",
      "        [5.5346e+00],\n",
      "        [6.2762e-04],\n",
      "        [2.2172e+00],\n",
      "        [1.7084e+00],\n",
      "        [2.0701e+00],\n",
      "        [5.3631e+00],\n",
      "        [3.0947e+00],\n",
      "        [4.7131e+01],\n",
      "        [3.8728e+00],\n",
      "        [2.4626e+00],\n",
      "        [2.2164e+00],\n",
      "        [1.1725e-01],\n",
      "        [4.9386e+00],\n",
      "        [1.8842e-01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [372. 372. 372. 372. 372. 372. 372. 372. 372. 372.]; total steps: 19000; avg scores/10 episodes: [-64.84926175 -71.38014749 -80.1635879  -66.49928238 -84.90278486\n",
      " -71.60774306 -98.13554271 -67.22008697 -68.37315348 -72.71013043]; avg total score: -74.58417210400049\n",
      "episode: [392. 392. 392. 392. 392. 392. 392. 392. 392. 392.]; total steps: 20000; avg scores/10 episodes: [ -99.68065919  -70.10972256 -103.1630228   -60.65601514  -85.38219937\n",
      "  -82.8837217   -73.88881763  -92.66216913  -69.93816145  -58.15648136]; avg total score: -79.65209703197135\n",
      "learning timestep: 20000\n",
      "num batches:312\n",
      "Policy Loss: 0.14536583423614502\n",
      "Value Loss: 73.34494018554688\n",
      "Entropy: 91.79229736328125\n",
      "KL Divergence: tensor([[2.9010e-02],\n",
      "        [5.0311e-01],\n",
      "        [4.1346e+00],\n",
      "        [6.9427e+00],\n",
      "        [5.2496e+00],\n",
      "        [2.0442e+00],\n",
      "        [6.3777e-01],\n",
      "        [6.1292e+00],\n",
      "        [4.3814e-01],\n",
      "        [7.3118e-02],\n",
      "        [3.8208e-01],\n",
      "        [1.0844e+01],\n",
      "        [3.8069e-01],\n",
      "        [1.1542e+00],\n",
      "        [1.4919e+01],\n",
      "        [2.7241e-01],\n",
      "        [2.6647e-02],\n",
      "        [8.9052e+00],\n",
      "        [4.9764e+00],\n",
      "        [8.5687e-02],\n",
      "        [9.3375e-02],\n",
      "        [8.1934e+00],\n",
      "        [2.7375e-02],\n",
      "        [4.3548e+00],\n",
      "        [7.8637e+00],\n",
      "        [1.6178e-02],\n",
      "        [8.2703e+00],\n",
      "        [7.3057e-02],\n",
      "        [1.2643e-02],\n",
      "        [9.7119e+00],\n",
      "        [3.9708e+00],\n",
      "        [9.0959e-02],\n",
      "        [2.9453e-01],\n",
      "        [2.8469e-01],\n",
      "        [7.7533e+00],\n",
      "        [2.9179e-01],\n",
      "        [1.1072e+01],\n",
      "        [6.6289e+00],\n",
      "        [1.4503e+01],\n",
      "        [1.4258e+00],\n",
      "        [3.4687e-02],\n",
      "        [6.8009e+00],\n",
      "        [9.6659e-02],\n",
      "        [2.4937e-02],\n",
      "        [5.7183e+00],\n",
      "        [1.1926e+01],\n",
      "        [1.2634e-01],\n",
      "        [7.4442e-01],\n",
      "        [8.4450e+00],\n",
      "        [4.7945e-02],\n",
      "        [6.3781e-01],\n",
      "        [1.8740e-02],\n",
      "        [6.2015e-01],\n",
      "        [4.2718e-01],\n",
      "        [4.5736e+00],\n",
      "        [7.1004e-01],\n",
      "        [3.7235e-01],\n",
      "        [7.0703e+00],\n",
      "        [1.3058e-01],\n",
      "        [2.3830e-01],\n",
      "        [1.0121e+01],\n",
      "        [9.5479e+00],\n",
      "        [8.2337e+00],\n",
      "        [2.8721e+00]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [411. 411. 411. 411. 411. 411. 411. 411. 411. 411.]; total steps: 21000; avg scores/10 episodes: [ -77.82967927  -97.05168526 -112.27008121  -77.20217261 -126.32851507\n",
      "  -84.11554615  -90.04079854  -87.54648605  -75.69737402  -74.92702728]; avg total score: -90.3009365463884\n",
      "episode: [431. 431. 431. 431. 431. 431. 431. 431. 431. 431.]; total steps: 22000; avg scores/10 episodes: [-97.6698567  -89.42960594 -96.29240827 -67.34856566 -66.62724245\n",
      " -69.72938049 -82.69015748 -92.3567508  -57.279202   -77.84150323]; avg total score: -79.72646730253555\n",
      "learning timestep: 22000\n",
      "num batches:312\n",
      "Policy Loss: -0.0357944518327713\n",
      "Value Loss: 63.40750503540039\n",
      "Entropy: 162.17556762695312\n",
      "KL Divergence: tensor([[2.1969e-02],\n",
      "        [1.5002e+00],\n",
      "        [4.4597e-01],\n",
      "        [3.1992e-01],\n",
      "        [3.5335e-01],\n",
      "        [9.2695e+00],\n",
      "        [1.1371e+00],\n",
      "        [5.3734e-01],\n",
      "        [3.7737e-01],\n",
      "        [1.8192e+00],\n",
      "        [3.1161e-01],\n",
      "        [8.5416e-01],\n",
      "        [1.8502e+00],\n",
      "        [1.7883e+00],\n",
      "        [3.9874e-02],\n",
      "        [1.8744e+00],\n",
      "        [1.0967e+00],\n",
      "        [6.5678e-01],\n",
      "        [3.9994e-01],\n",
      "        [2.3477e-01],\n",
      "        [2.7410e-01],\n",
      "        [1.5180e+00],\n",
      "        [1.4713e+00],\n",
      "        [8.6634e-01],\n",
      "        [1.5272e+00],\n",
      "        [1.8106e+00],\n",
      "        [1.0999e-01],\n",
      "        [5.9090e-01],\n",
      "        [2.8342e-01],\n",
      "        [1.0210e+00],\n",
      "        [1.2977e+00],\n",
      "        [1.7645e+00],\n",
      "        [4.3767e-01],\n",
      "        [7.5513e+00],\n",
      "        [7.5564e+00],\n",
      "        [3.2811e+00],\n",
      "        [1.9058e-02],\n",
      "        [9.6333e-01],\n",
      "        [1.2167e+01],\n",
      "        [3.5365e-01],\n",
      "        [6.5585e-01],\n",
      "        [1.5254e+00],\n",
      "        [8.3190e+00],\n",
      "        [1.2444e+00],\n",
      "        [6.6697e+00],\n",
      "        [4.1257e-02],\n",
      "        [1.2482e+00],\n",
      "        [1.0066e+01],\n",
      "        [7.5405e-01],\n",
      "        [1.8996e-01],\n",
      "        [1.8758e+00],\n",
      "        [7.9550e-01],\n",
      "        [5.5856e-01],\n",
      "        [4.8577e+00],\n",
      "        [1.7184e+00],\n",
      "        [6.2020e+00],\n",
      "        [4.9367e-03],\n",
      "        [1.2453e+00],\n",
      "        [2.2465e-01],\n",
      "        [7.4830e-03],\n",
      "        [4.0823e+00],\n",
      "        [7.6792e+00],\n",
      "        [1.3411e+00],\n",
      "        [1.0992e-03]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [450. 450. 450. 450. 450. 450. 450. 450. 450. 450.]; total steps: 23000; avg scores/10 episodes: [-209.03318304 -212.36687371 -210.88780322 -212.77761099 -231.93796498\n",
      " -201.71649989 -204.48589064 -224.89264117 -222.57014177 -187.99209369]; avg total score: -211.86607031008535\n",
      "episode: [470. 470. 470. 470. 470. 470. 470. 470. 470. 470.]; total steps: 24000; avg scores/10 episodes: [-206.21958552 -205.52401297 -218.97522427 -213.03116972 -205.62694878\n",
      " -200.66432279 -196.60556654 -229.9193838  -204.4928403  -216.08663657]; avg total score: -209.71456912630438\n",
      "learning timestep: 24000\n",
      "num batches:312\n",
      "Policy Loss: 0.04260510951280594\n",
      "Value Loss: 140.5312042236328\n",
      "Entropy: 127.87234497070312\n",
      "KL Divergence: tensor([[1.1707e+01],\n",
      "        [8.6066e-01],\n",
      "        [4.7422e+00],\n",
      "        [9.8751e-03],\n",
      "        [4.2253e-01],\n",
      "        [4.1976e+01],\n",
      "        [2.2398e-01],\n",
      "        [4.2236e-01],\n",
      "        [5.3462e-01],\n",
      "        [1.4483e+01],\n",
      "        [1.0496e+01],\n",
      "        [5.5990e+00],\n",
      "        [3.5369e+00],\n",
      "        [3.0010e+01],\n",
      "        [6.6107e-02],\n",
      "        [2.0810e+00],\n",
      "        [5.0829e+00],\n",
      "        [8.0189e-01],\n",
      "        [1.2696e-01],\n",
      "        [1.4551e-01],\n",
      "        [6.6292e-01],\n",
      "        [7.9842e+00],\n",
      "        [4.0214e+00],\n",
      "        [7.5778e-01],\n",
      "        [1.9669e+00],\n",
      "        [5.1588e+00],\n",
      "        [3.3765e-01],\n",
      "        [7.9372e-02],\n",
      "        [4.2809e+00],\n",
      "        [1.0255e+01],\n",
      "        [1.1699e-01],\n",
      "        [6.2822e+00],\n",
      "        [3.8406e-01],\n",
      "        [2.9794e+00],\n",
      "        [4.0051e+00],\n",
      "        [1.0797e+00],\n",
      "        [1.7798e-01],\n",
      "        [3.8689e-01],\n",
      "        [6.6582e-01],\n",
      "        [2.2392e+00],\n",
      "        [4.4473e+00],\n",
      "        [9.9444e+00],\n",
      "        [6.8351e-01],\n",
      "        [1.7713e+00],\n",
      "        [3.7728e-02],\n",
      "        [1.5605e+00],\n",
      "        [6.1674e-02],\n",
      "        [2.5173e+00],\n",
      "        [2.9796e-02],\n",
      "        [7.5798e+00],\n",
      "        [2.1069e+01],\n",
      "        [2.0014e-02],\n",
      "        [8.1517e-01],\n",
      "        [2.4735e-02],\n",
      "        [3.0498e+00],\n",
      "        [3.3153e-01],\n",
      "        [2.1153e+00],\n",
      "        [2.1142e+00],\n",
      "        [5.3355e+00],\n",
      "        [9.3535e-01],\n",
      "        [1.8426e+00],\n",
      "        [1.3881e+00],\n",
      "        [1.7286e+01],\n",
      "        [8.3838e-01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [490. 490. 490. 490. 490. 490. 490. 490. 490. 490.]; total steps: 25000; avg scores/10 episodes: [-1691.95521179 -1674.53296558 -1644.83656305 -1693.39428475\n",
      " -1679.77908378 -1672.01491928 -1654.64921696 -1676.60468057\n",
      " -1687.35938916 -1673.32225739]; avg total score: -1674.8448572326947\n",
      "episode: [509. 509. 509. 509. 509. 509. 509. 509. 509. 509.]; total steps: 26000; avg scores/10 episodes: [-1659.71747843 -1678.2573673  -1672.48817685 -1679.24318664\n",
      " -1668.03609128 -1682.54245794 -1680.36959029 -1653.67141279\n",
      " -1679.84540815 -1673.19397699]; avg total score: -1672.736514665336\n",
      "learning timestep: 26000\n",
      "num batches:312\n",
      "Policy Loss: -0.0041870977729558945\n",
      "Value Loss: 107.7142333984375\n",
      "Entropy: 67.14390563964844\n",
      "KL Divergence: tensor([[1.0835e-03],\n",
      "        [1.6604e-02],\n",
      "        [5.4780e-03],\n",
      "        [3.4365e-02],\n",
      "        [1.5992e-02],\n",
      "        [6.8339e-05],\n",
      "        [3.4120e-05],\n",
      "        [2.0501e+01],\n",
      "        [2.2469e+00],\n",
      "        [1.0232e-03],\n",
      "        [1.7572e-03],\n",
      "        [1.0615e-02],\n",
      "        [6.4552e-03],\n",
      "        [2.5297e+00],\n",
      "        [3.8171e-02],\n",
      "        [2.4963e-03],\n",
      "        [1.7025e+01],\n",
      "        [7.9187e-03],\n",
      "        [2.2945e+01],\n",
      "        [2.0771e+01],\n",
      "        [1.5996e-02],\n",
      "        [1.3807e-03],\n",
      "        [1.0004e-01],\n",
      "        [4.3310e-02],\n",
      "        [1.8258e-03],\n",
      "        [1.0517e-02],\n",
      "        [3.0749e+00],\n",
      "        [4.6930e-02],\n",
      "        [8.9047e-04],\n",
      "        [9.8349e-05],\n",
      "        [3.1774e+01],\n",
      "        [1.3848e+00],\n",
      "        [1.4260e+00],\n",
      "        [1.8275e+01],\n",
      "        [6.7865e-04],\n",
      "        [4.3374e-03],\n",
      "        [2.0852e+01],\n",
      "        [2.1814e-03],\n",
      "        [3.1006e-03],\n",
      "        [2.2056e+01],\n",
      "        [1.6381e-03],\n",
      "        [2.1401e+01],\n",
      "        [1.5186e-02],\n",
      "        [1.5269e+00],\n",
      "        [2.3121e-02],\n",
      "        [2.1376e+01],\n",
      "        [3.1285e-02],\n",
      "        [5.1943e-04],\n",
      "        [9.7888e-04],\n",
      "        [8.0721e-04],\n",
      "        [7.4414e-04],\n",
      "        [7.0250e-03],\n",
      "        [4.7995e-03],\n",
      "        [3.3814e-02],\n",
      "        [3.8605e-02],\n",
      "        [3.2620e+00],\n",
      "        [1.4081e-02],\n",
      "        [1.6152e+00],\n",
      "        [2.1249e+01],\n",
      "        [3.5427e-03],\n",
      "        [1.8628e+01],\n",
      "        [1.5338e+00],\n",
      "        [2.0331e-01],\n",
      "        [2.3247e+01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [529. 529. 529. 529. 529. 529. 529. 529. 529. 529.]; total steps: 27000; avg scores/10 episodes: [-1223.27041074 -1218.35305566 -1221.37150201 -1215.7026813\n",
      " -1230.3951656  -1215.71079252 -1219.63223803 -1220.95071492\n",
      " -1216.08094744 -1206.95058023]; avg total score: -1218.841808843733\n",
      "episode: [549. 549. 549. 549. 549. 549. 549. 549. 549. 549.]; total steps: 28000; avg scores/10 episodes: [-1197.28473126 -1203.7022085  -1210.07608759 -1207.20329659\n",
      " -1210.30160431 -1213.64184155 -1237.82683495 -1212.56573686\n",
      " -1204.92383043 -1209.20278695]; avg total score: -1210.6728958998146\n",
      "learning timestep: 28000\n",
      "num batches:312\n",
      "Policy Loss: -0.5122063755989075\n",
      "Value Loss: 86.6979751586914\n",
      "Entropy: 71.01483917236328\n",
      "KL Divergence: tensor([[6.5655e-03],\n",
      "        [4.4969e+00],\n",
      "        [6.0128e-02],\n",
      "        [7.9343e-02],\n",
      "        [1.2620e+00],\n",
      "        [1.4028e-01],\n",
      "        [1.1160e-01],\n",
      "        [6.9808e-03],\n",
      "        [2.5266e-02],\n",
      "        [8.9968e-01],\n",
      "        [6.0144e+00],\n",
      "        [1.3024e+00],\n",
      "        [1.0799e-02],\n",
      "        [9.4142e-02],\n",
      "        [6.2763e-01],\n",
      "        [5.1530e-02],\n",
      "        [5.5126e-02],\n",
      "        [1.7816e+00],\n",
      "        [1.1309e+00],\n",
      "        [3.9139e+00],\n",
      "        [6.6048e-02],\n",
      "        [1.4269e+00],\n",
      "        [3.5007e-03],\n",
      "        [1.2626e+00],\n",
      "        [9.9071e-03],\n",
      "        [8.0615e-03],\n",
      "        [8.8557e-02],\n",
      "        [1.2622e+00],\n",
      "        [2.1339e+00],\n",
      "        [2.2129e+00],\n",
      "        [2.0234e-01],\n",
      "        [9.3511e-02],\n",
      "        [9.5449e-04],\n",
      "        [1.5766e-02],\n",
      "        [5.9979e+00],\n",
      "        [2.9802e-01],\n",
      "        [8.0839e+00],\n",
      "        [5.2829e+00],\n",
      "        [1.9959e-02],\n",
      "        [3.7203e+00],\n",
      "        [4.8022e-02],\n",
      "        [7.6271e-01],\n",
      "        [1.4511e+00],\n",
      "        [4.7576e-03],\n",
      "        [6.4568e-03],\n",
      "        [2.3073e-02],\n",
      "        [7.8303e-03],\n",
      "        [5.0962e-02],\n",
      "        [4.1669e-02],\n",
      "        [3.4526e-03],\n",
      "        [7.4052e-03],\n",
      "        [3.5904e+00],\n",
      "        [2.3330e-02],\n",
      "        [1.1937e+00],\n",
      "        [2.1833e-02],\n",
      "        [2.4713e-04],\n",
      "        [4.6967e-04],\n",
      "        [8.0349e-02],\n",
      "        [1.5507e+00],\n",
      "        [5.4873e+00],\n",
      "        [3.8625e-02],\n",
      "        [1.2393e+00],\n",
      "        [7.9571e-01],\n",
      "        [1.1684e-02]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [568. 568. 568. 568. 568. 568. 568. 568. 568. 568.]; total steps: 29000; avg scores/10 episodes: [-1334.94822158 -1325.70474326 -1318.70799451 -1324.71626745\n",
      " -1328.90162627 -1326.84876553 -1328.33023182 -1333.71933939\n",
      " -1309.73072334 -1316.38929404]; avg total score: -1324.7997207184394\n",
      "episode: [588. 588. 588. 588. 588. 588. 588. 588. 588. 588.]; total steps: 30000; avg scores/10 episodes: [-1331.40983732 -1345.85044805 -1340.2269377  -1324.51806231\n",
      " -1327.98829057 -1330.79581461 -1331.84892122 -1329.26404065\n",
      " -1316.11115916 -1325.85382171]; avg total score: -1330.386733330182\n",
      "learning timestep: 30000\n",
      "num batches:312\n",
      "Policy Loss: 0.013329981826245785\n",
      "Value Loss: 57.57063293457031\n",
      "Entropy: 92.54905700683594\n",
      "KL Divergence: tensor([[7.2473e-04],\n",
      "        [1.0144e-01],\n",
      "        [2.5561e-01],\n",
      "        [1.1720e+00],\n",
      "        [5.1212e-01],\n",
      "        [6.4597e-01],\n",
      "        [6.8884e-02],\n",
      "        [6.1302e-04],\n",
      "        [7.8476e-01],\n",
      "        [2.8997e-02],\n",
      "        [1.2920e+00],\n",
      "        [4.8175e-02],\n",
      "        [1.1721e+00],\n",
      "        [1.4038e+00],\n",
      "        [2.3350e-02],\n",
      "        [2.3156e-03],\n",
      "        [1.1800e+00],\n",
      "        [1.2522e+00],\n",
      "        [2.4048e-03],\n",
      "        [1.3744e-01],\n",
      "        [6.6461e-04],\n",
      "        [1.9214e-02],\n",
      "        [5.4545e-03],\n",
      "        [8.2061e-02],\n",
      "        [3.5901e-04],\n",
      "        [1.3869e+00],\n",
      "        [5.3307e-01],\n",
      "        [2.3959e-02],\n",
      "        [5.1367e-03],\n",
      "        [6.7745e-01],\n",
      "        [1.2064e-02],\n",
      "        [9.4735e-02],\n",
      "        [9.2488e-02],\n",
      "        [7.1837e-01],\n",
      "        [1.8168e-02],\n",
      "        [1.2378e-02],\n",
      "        [5.4121e-03],\n",
      "        [1.4148e+00],\n",
      "        [1.1117e-01],\n",
      "        [1.3556e-04],\n",
      "        [3.8750e-02],\n",
      "        [2.0030e-03],\n",
      "        [5.1586e+00],\n",
      "        [1.1178e-02],\n",
      "        [1.9703e-02],\n",
      "        [9.8416e-01],\n",
      "        [5.8836e-01],\n",
      "        [1.1483e+00],\n",
      "        [1.0317e-01],\n",
      "        [4.4269e+00],\n",
      "        [5.6466e-02],\n",
      "        [2.6847e+00],\n",
      "        [3.0358e-02],\n",
      "        [1.1569e-02],\n",
      "        [9.2644e-01],\n",
      "        [7.4807e-01],\n",
      "        [3.0097e-02],\n",
      "        [4.9388e-02],\n",
      "        [1.1903e+00],\n",
      "        [8.5088e-01],\n",
      "        [1.2883e+00],\n",
      "        [3.8220e-03],\n",
      "        [1.2678e-02],\n",
      "        [8.8246e+00]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [607. 607. 607. 607. 607. 607. 607. 607. 607. 607.]; total steps: 31000; avg scores/10 episodes: [-2287.86235764 -2313.70390207 -2292.52572739 -2301.56214099\n",
      " -2307.69303214 -2311.26424354 -2304.41144494 -2299.17019668\n",
      " -2304.43877758 -2300.84879336]; avg total score: -2302.3480616320767\n",
      "episode: [627. 627. 627. 627. 627. 627. 627. 627. 627. 627.]; total steps: 32000; avg scores/10 episodes: [-2330.47601293 -2288.9296997  -2295.5278119  -2282.55192194\n",
      " -2308.30151168 -2283.99820135 -2284.33674923 -2291.80162882\n",
      " -2307.09459314 -2312.74872854]; avg total score: -2298.5766859228343\n",
      "learning timestep: 32000\n",
      "num batches:312\n",
      "Policy Loss: -0.017133481800556183\n",
      "Value Loss: 14.161338806152344\n",
      "Entropy: 55.76576232910156\n",
      "KL Divergence: tensor([[1.0019e+00],\n",
      "        [3.1337e+00],\n",
      "        [1.7578e-05],\n",
      "        [1.9162e+00],\n",
      "        [7.7242e-01],\n",
      "        [1.7879e+01],\n",
      "        [3.8457e+01],\n",
      "        [3.4809e+00],\n",
      "        [1.3007e+01],\n",
      "        [1.1146e-01],\n",
      "        [1.2795e+01],\n",
      "        [6.9385e+00],\n",
      "        [3.5392e+00],\n",
      "        [1.1584e+01],\n",
      "        [1.5627e-03],\n",
      "        [1.3926e+00],\n",
      "        [2.5984e-01],\n",
      "        [8.4743e-02],\n",
      "        [1.2708e-01],\n",
      "        [3.9208e-01],\n",
      "        [1.0974e+01],\n",
      "        [1.3454e+00],\n",
      "        [1.1087e-01],\n",
      "        [1.5527e-02],\n",
      "        [4.1918e-01],\n",
      "        [3.6663e+00],\n",
      "        [5.3822e+00],\n",
      "        [5.7824e-02],\n",
      "        [4.1457e+00],\n",
      "        [8.7046e+00],\n",
      "        [1.4791e-05],\n",
      "        [3.2240e+00],\n",
      "        [2.3731e+00],\n",
      "        [3.2965e-03],\n",
      "        [7.5250e+00],\n",
      "        [2.1270e-02],\n",
      "        [2.7189e+01],\n",
      "        [3.0988e-01],\n",
      "        [1.5628e-02],\n",
      "        [1.3101e+01],\n",
      "        [5.5643e+01],\n",
      "        [1.4858e+01],\n",
      "        [9.6212e-02],\n",
      "        [1.7201e+00],\n",
      "        [3.5345e-02],\n",
      "        [9.4216e-02],\n",
      "        [4.9896e-02],\n",
      "        [1.1634e+01],\n",
      "        [1.9550e-03],\n",
      "        [1.7107e-01],\n",
      "        [1.9086e+00],\n",
      "        [7.7272e-02],\n",
      "        [1.0312e-01],\n",
      "        [2.8049e+00],\n",
      "        [2.2041e+00],\n",
      "        [2.3508e-01],\n",
      "        [3.5678e-03],\n",
      "        [8.8305e-01],\n",
      "        [4.9450e-04],\n",
      "        [1.5348e-01],\n",
      "        [2.4488e+00],\n",
      "        [3.1914e+00],\n",
      "        [1.3883e-02],\n",
      "        [2.0001e-01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [647. 647. 647. 647. 647. 647. 647. 647. 647. 647.]; total steps: 33000; avg scores/10 episodes: [-3126.62811381 -3127.11851548 -3114.55173846 -3082.5308222\n",
      " -3106.25702967 -3108.92679335 -3116.14463622 -3125.16966989\n",
      " -3100.9261076  -3077.68555751]; avg total score: -3108.5938984171416\n",
      "episode: [666. 666. 666. 666. 666. 666. 666. 666. 666. 666.]; total steps: 34000; avg scores/10 episodes: [-3119.17721724 -3069.26814823 -3133.89833765 -3135.34347685\n",
      " -3150.18115147 -3107.5327096  -3095.08577709 -3085.86969007\n",
      " -3110.43356088 -3126.0768646 ]; avg total score: -3113.286693368126\n",
      "learning timestep: 34000\n",
      "num batches:312\n",
      "Policy Loss: 0.06897450983524323\n",
      "Value Loss: 12.103827476501465\n",
      "Entropy: 88.65518951416016\n",
      "KL Divergence: tensor([[2.7654e+00],\n",
      "        [2.3627e-01],\n",
      "        [9.3157e-03],\n",
      "        [2.4238e+00],\n",
      "        [3.3511e-01],\n",
      "        [5.6154e-02],\n",
      "        [3.9930e+00],\n",
      "        [1.1308e+00],\n",
      "        [1.0317e-01],\n",
      "        [8.0377e-03],\n",
      "        [3.3393e+00],\n",
      "        [1.8468e-02],\n",
      "        [2.2265e+00],\n",
      "        [9.9996e-02],\n",
      "        [2.8905e-03],\n",
      "        [2.5192e+00],\n",
      "        [4.8087e-01],\n",
      "        [5.1196e-03],\n",
      "        [1.2604e-02],\n",
      "        [8.1605e-03],\n",
      "        [2.8266e+00],\n",
      "        [8.1803e-01],\n",
      "        [2.9407e-03],\n",
      "        [5.3795e-04],\n",
      "        [8.6035e-03],\n",
      "        [1.4417e-02],\n",
      "        [9.8679e+00],\n",
      "        [1.4908e+01],\n",
      "        [5.7392e-01],\n",
      "        [4.8004e-02],\n",
      "        [3.2130e-02],\n",
      "        [1.7379e+01],\n",
      "        [6.1467e-03],\n",
      "        [6.3222e-03],\n",
      "        [1.9629e-05],\n",
      "        [9.9781e+00],\n",
      "        [2.9872e-04],\n",
      "        [2.0995e-03],\n",
      "        [2.1620e+00],\n",
      "        [1.2538e-02],\n",
      "        [1.0147e-01],\n",
      "        [9.6593e+00],\n",
      "        [2.9399e-03],\n",
      "        [1.2378e-03],\n",
      "        [2.8193e+00],\n",
      "        [7.1930e-04],\n",
      "        [3.7338e+00],\n",
      "        [4.5864e-03],\n",
      "        [2.4925e+00],\n",
      "        [4.2039e-03],\n",
      "        [1.0084e-01],\n",
      "        [1.8973e-04],\n",
      "        [9.1911e-03],\n",
      "        [5.4598e+00],\n",
      "        [3.9061e-01],\n",
      "        [3.2241e-02],\n",
      "        [3.3185e-06],\n",
      "        [2.5093e-03],\n",
      "        [1.6122e+01],\n",
      "        [3.9535e-02],\n",
      "        [6.0112e-04],\n",
      "        [4.3906e-03],\n",
      "        [1.7219e-04],\n",
      "        [1.6662e-03]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [686. 686. 686. 686. 686. 686. 686. 686. 686. 686.]; total steps: 35000; avg scores/10 episodes: [-2772.58077446 -2763.00935322 -2789.9817246  -2733.79970135\n",
      " -2757.26595746 -2750.52997467 -2740.87926945 -2784.79872834\n",
      " -2775.55383567 -2759.31027149]; avg total score: -2762.7709590712193\n",
      "episode: [705. 705. 705. 705. 705. 705. 705. 705. 705. 705.]; total steps: 36000; avg scores/10 episodes: [-2747.64404204 -2760.98162281 -2755.0903299  -2748.03806366\n",
      " -2723.02021121 -2745.00366688 -2790.7245314  -2746.88977092\n",
      " -2756.36727283 -2734.34537216]; avg total score: -2750.810488381723\n",
      "learning timestep: 36000\n",
      "num batches:312\n",
      "Policy Loss: -0.004592474550008774\n",
      "Value Loss: 8.989895820617676\n",
      "Entropy: 50.71346664428711\n",
      "KL Divergence: tensor([[6.7939e+00],\n",
      "        [2.0068e-02],\n",
      "        [4.8855e-03],\n",
      "        [9.7248e-03],\n",
      "        [1.4534e-01],\n",
      "        [7.6852e-01],\n",
      "        [2.2181e-01],\n",
      "        [3.8709e-02],\n",
      "        [1.0568e-02],\n",
      "        [2.8571e-01],\n",
      "        [4.3094e-01],\n",
      "        [2.0668e-02],\n",
      "        [3.6396e-02],\n",
      "        [1.1534e-03],\n",
      "        [4.9342e-01],\n",
      "        [2.0011e-02],\n",
      "        [2.2214e-02],\n",
      "        [5.3253e-01],\n",
      "        [3.7489e-01],\n",
      "        [4.8838e-03],\n",
      "        [4.6172e-01],\n",
      "        [1.0602e+00],\n",
      "        [2.1612e-03],\n",
      "        [5.0183e-04],\n",
      "        [1.5879e-02],\n",
      "        [1.2704e+00],\n",
      "        [2.7743e-01],\n",
      "        [3.6833e-01],\n",
      "        [1.0333e-03],\n",
      "        [4.7275e-01],\n",
      "        [3.0129e-03],\n",
      "        [7.1239e-01],\n",
      "        [5.9320e-01],\n",
      "        [2.6983e-02],\n",
      "        [1.1440e-05],\n",
      "        [3.1452e-02],\n",
      "        [1.4665e-02],\n",
      "        [1.1659e-02],\n",
      "        [4.2809e-02],\n",
      "        [1.6994e-03],\n",
      "        [7.7763e-03],\n",
      "        [3.2048e-01],\n",
      "        [2.5756e-03],\n",
      "        [3.6857e-03],\n",
      "        [3.5272e-02],\n",
      "        [9.2392e-02],\n",
      "        [7.1115e-01],\n",
      "        [3.3777e-01],\n",
      "        [9.5712e-02],\n",
      "        [3.9197e-01],\n",
      "        [7.1280e-04],\n",
      "        [1.7156e-02],\n",
      "        [6.4960e-02],\n",
      "        [3.1131e-02],\n",
      "        [6.8875e-02],\n",
      "        [5.3933e-03],\n",
      "        [3.1803e-02],\n",
      "        [6.3336e-02],\n",
      "        [3.4073e+01],\n",
      "        [6.3240e-04],\n",
      "        [6.7163e+00],\n",
      "        [8.7543e-03],\n",
      "        [2.5369e-02],\n",
      "        [4.6020e-01]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "episode: [725. 725. 725. 725. 725. 725. 725. 725. 725. 725.]; total steps: 37000; avg scores/10 episodes: [-2996.70566042 -3016.80353155 -3019.66080226 -3011.14150907\n",
      " -3002.60721708 -3016.70910015 -3011.05556883 -3052.34521316\n",
      " -2993.80807661 -3001.56704696]; avg total score: -3012.2403726091916\n",
      "episode: [745. 745. 745. 745. 745. 745. 745. 745. 745. 745.]; total steps: 38000; avg scores/10 episodes: [-2995.92299189 -3010.83085534 -3010.51992165 -3012.70374881\n",
      " -3017.44876644 -3033.7470222  -3029.16315338 -2977.52460253\n",
      " -3014.47871191 -3030.30094234]; avg total score: -3013.2640716489464\n",
      "learning timestep: 38000\n",
      "num batches:312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb Cell 209\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f726368222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f7263685c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c2532304167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c204167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb#Y415sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m value_function \u001b[39m=\u001b[39m ValueModel(env, dense_layers, learning_rate\u001b[39m=\u001b[39mvalue_lr, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f726368222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f7263685c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c2532304167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c204167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb#Y415sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m ppo \u001b[39m=\u001b[39m PPO(env, policy, value_function, distribution\u001b[39m=\u001b[39mdistribution, discount\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m, gae_coefficient\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, policy_clip\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, entropy_coefficient\u001b[39m=\u001b[39mentropy_coeff,\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f726368222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f7263685c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c2532304167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c204167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb#Y415sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m           kl_coefficient\u001b[39m=\u001b[39mkl_coeff, loss\u001b[39m=\u001b[39mloss, lambda_\u001b[39m=\u001b[39mlambda_, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f726368222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f7263685c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c2532304167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c204167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb#Y415sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m hybrid_train_info_2 \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mtrain(timesteps\u001b[39m=\u001b[39;49mtimesteps, trajectory_length\u001b[39m=\u001b[39;49mtrajectory_length, batch_size\u001b[39m=\u001b[39;49mbatch_size, learning_epochs\u001b[39m=\u001b[39;49mlearning_epochs, num_envs\u001b[39m=\u001b[39;49mnum_envs)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f726368222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c6a61736f6e5c5c4f6e6544726976655c5c446f63756d656e74735c5c41495c5c50726f6a656374735c5c524c204167656e74735c5c524c5f4167656e74735c5c7079746f7263685c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c2532304167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f6a61736f6e2f4f6e6544726976652f446f63756d656e74732f41492f50726f6a656374732f524c204167656e74732f524c5f4167656e74732f7079746f7263682f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/RL_Agents/pytorch/src/app/test_bed.ipynb#Y415sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# ppo.test(10,\"ppo_test\", 1)\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/RL_Agents/pytorch/src/app/rl_agents.py:5331\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, timesteps, trajectory_length, batch_size, learning_epochs, num_envs, avg_num, render_freq, save_dir, run_number)\u001b[0m\n\u001b[1;32m   5329\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlearning timestep: \u001b[39m\u001b[39m{\u001b[39;00mtimestep\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   5330\u001b[0m trajectory \u001b[39m=\u001b[39m (all_states, all_actions, all_log_probs, all_rewards, all_next_states, all_dones)\n\u001b[0;32m-> 5331\u001b[0m policy_loss, value_loss, entropy, kl, time, lambda_value, param1, param2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(trajectory, batch_size, learning_epochs)\n\u001b[1;32m   5332\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_episode_config[\u001b[39m\"\u001b[39m\u001b[39mactor_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m policy_loss\n\u001b[1;32m   5333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_episode_config[\u001b[39m\"\u001b[39m\u001b[39mcritic_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m value_loss\n",
      "File \u001b[0;32m/workspaces/RL_Agents/pytorch/src/app/rl_agents.py:5448\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, trajectory, batch_size, learning_epochs)\u001b[0m\n\u001b[1;32m   5445\u001b[0m advantages_batch \u001b[39m=\u001b[39m advantages[batch]\n\u001b[1;32m   5446\u001b[0m returns_batch \u001b[39m=\u001b[39m returns[batch]\n\u001b[0;32m-> 5448\u001b[0m dist, param1, param2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(states_batch)\n\u001b[1;32m   5449\u001b[0m dist_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   5450\u001b[0m \u001b[39m# Create prev_dist by recreating the distribution from the previous step's parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1732\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1729\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1730\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapped_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1734\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "# env_id = 'Humanoid-v5'\n",
    "env_id = \"Reacher-v5\"\n",
    "\n",
    "timesteps = 100_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 10\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.01\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "lambda_ = 0.0\n",
    "distribution = 'Normal'\n",
    "device = 'cuda'\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'PPO-Test'\n",
    "run_name = None\n",
    "# callbacks = [WandbCallback(project_name, run_name)]\n",
    "callbacks = []\n",
    "\n",
    "seed = 46\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff,\n",
    "          kl_coefficient=kl_coeff, loss=loss, lambda_=lambda_, callbacks=callbacks)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"ppo_test/renders\"\n",
    "num_episodes = 10\n",
    "render_freq = 1\n",
    "\n",
    "# Set the policy and value function models to evaluation mode\n",
    "ppo_agent_hybrid1.policy.eval()\n",
    "ppo_agent_hybrid1.value_model.eval()\n",
    "\n",
    "# Create the render directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "env = gym.make_vec(\n",
    "    ppo_agent_hybrid1.env.spec.id,\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        lambda env: gym.wrappers.RecordVideo(\n",
    "            env,\n",
    "            save_dir + \"/renders/test\",\n",
    "            episode_trigger=lambda episode_id: (episode_id + 1) % render_freq == 0\n",
    "        )\n",
    "    ],\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "scores = []\n",
    "entropy_list = []\n",
    "kl_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    state, _ = ppo_agent_hybrid1.env.reset()\n",
    "    score = 0\n",
    "    episode_entropy = 0\n",
    "    episode_kl = 0\n",
    "    steps = 0\n",
    "\n",
    "    prev_dist = None  # To track the previous distribution for KL divergence\n",
    "\n",
    "    # Video writer setup\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video_path = os.path.join(render_dir, f\"episode_{episode+1}.mp4\")\n",
    "    #     frame = self.env.render(mode='rgb_array')\n",
    "    #     height, width, layers = frame.shape\n",
    "    #     video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n",
    "\n",
    "    while not done:\n",
    "        # Render the environment and write the frame to the video file\n",
    "        # if episode % render_freq == 0:\n",
    "        #     frame = self.env.render(mode='rgb_array')\n",
    "        #     video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Get action and log probability from the current policy\n",
    "        action, log_prob = ppo_agent_hybrid1.get_action(state)\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            act = ppo_agent_hybrid1.action_adapter(action)\n",
    "        else:\n",
    "            act = action\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, term, trunc, _ = ppo_agent_hybrid1.env.step(act)\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "\n",
    "        # Calculate the distribution and entropy\n",
    "        dist = ppo_agent_hybrid1.policy(T.tensor(state, dtype=T.float32, device=ppo_agent_hybrid1.policy.device))\n",
    "        entropy = dist.entropy().sum().item()  # Sum entropy over actions\n",
    "\n",
    "        # Update KL divergence\n",
    "        if prev_dist is not None:\n",
    "            kl = kl_divergence(prev_dist, dist).sum().item()  # Sum KL divergence over actions\n",
    "        else:\n",
    "            kl = 0  # No KL divergence for the first step in the episode\n",
    "\n",
    "        # Update the previous distribution to the current one\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            param1_prev = dist.concentration1.clone().detach()\n",
    "            param2_prev = dist.concentration0.clone().detach()\n",
    "            prev_dist = Beta(param1_prev, param2_prev)\n",
    "        elif ppo_agent_hybrid1.distribution == 'Normal':\n",
    "            param1_prev = dist.loc.clone().detach()\n",
    "            param2_prev = dist.scale.clone().detach()\n",
    "            prev_dist = Normal(param1_prev, param2_prev)\n",
    "\n",
    "        # Accumulate the score, entropy, and KL divergence for the episode\n",
    "        score += reward\n",
    "        episode_entropy += entropy\n",
    "        episode_kl += kl\n",
    "        steps += 1\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Release the video writer\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video.release()\n",
    "\n",
    "    # Append the results for the episode\n",
    "    scores.append(score)\n",
    "    entropy_list.append(episode_entropy / steps)  # Average entropy over the episode\n",
    "    kl_list.append(episode_kl / steps)  # Average KL divergence over the episode\n",
    "\n",
    "    print(f'Episode {episode+1}/{num_episodes} - Score: {score}, Avg Entropy: {entropy_list[-1]}, Avg KL Divergence: {kl_list[-1]}')\n",
    "\n",
    "# close the environment\n",
    "ppo_agent_hybrid1.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
