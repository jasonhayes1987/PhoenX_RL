{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch_utils\n",
    "from torch import distributions\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics as gym_robo\n",
    "# import models\n",
    "from models import ValueModel, StochasticContinuousPolicy, ActorModel, StochasticDiscretePolicy\n",
    "import cnn_models\n",
    "from rl_agents import PPO, DDPG, TD3, Reinforce, ActorCritic, HER\n",
    "import rl_callbacks\n",
    "from rl_callbacks import WandbCallback\n",
    "import helper\n",
    "import gym_helper\n",
    "import wandb_support\n",
    "import wandb\n",
    "import gym_helper\n",
    "\n",
    "# from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.MjModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    cuda_available = T.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        print(\"CUDA is available.\")\n",
    "        num_gpus = T.cuda.device_count()\n",
    "        print(f\"Number of GPUs detected: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = T.cuda.get_device_name(i)\n",
    "            gpu_memory = T.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            print(f\"Total memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Returns the default device for computations, GPU if available, otherwise CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_robo.register_robotics_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='758ac5ba01e12a3df504d2db2fec8ba4f391f7e6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', max_episode_steps=100, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, 'test/', episode_trigger=lambda i: i%1==0)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        obs, r, term, trunc, dict = env.step(env.action_space.sample())\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\")\n",
    "env.reset()\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "# The following always has to hold:\n",
    "assert reward == env.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert truncated == env.compute_truncated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)\n",
    "assert terminated == env.compute_terminated(obs[\"achieved_goal\"], obs[\"desired_goal\"], info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(env, \"distance_threshold\"):\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_wrapper_attr(\"distance_threshold\"):\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers,\n",
    "                            optimizer='Adam', optimizer_params={'weight_decay':0.01}, learning_rate=0.002, normalize_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000)\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(100, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', \"kaiming normal\"),\n",
    "    (256, 'relu', \"kaiming normal\"),\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env=env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers=dense_layers, optimizer='Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in value_model.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = rl_agents.ActorCritic(env,\n",
    "                                     policy_model,\n",
    "                                     value_model,\n",
    "                                     discount=0.99,\n",
    "                                     policy_trace_decay=0.5,\n",
    "                                     value_trace_decay=0.5,\n",
    "                                     callbacks=[rl_callbacks.WandbCallback('CartPole-v1-Actor-Critic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    (128, 'relu', {\n",
    "                    \"kaiming normal\": {\n",
    "                        \"a\":1.0,\n",
    "                        \"mode\":'fan_in'\n",
    "                    }\n",
    "                },\n",
    "    ),\n",
    "    # (256, 'relu', {\n",
    "    #                 \"kaiming_normal\": {\n",
    "    #                     \"a\":0.0,\n",
    "    #                     \"mode\":'fan_in'\n",
    "    #                 }\n",
    "    #             },\n",
    "    # )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [(128, 'relu', \"kaiming normal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = models.ValueModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in value_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = models.PolicyModel(env, dense_layers, 'Adam', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in policy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce = rl_agents.Reinforce(env, policy_model, value_model, 0.99, [rl_callbacks.WandbCallback('CartPole-v0_REINFORCE', chkpt_freq=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.train(200, True, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=cnn, dense_layers=dense_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=cnn, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape=(1,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, mean=0.0, theta=0.15, sigma=0.01, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discount=0.98,\n",
    "    tau=0.05,\n",
    "    action_epsilon=0.2,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=128,\n",
    "    noise=noise,\n",
    "    callbacks=[rl_callbacks.WandbCallback(\"CarRacing-v2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.train(1000, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal = gym_helper.reacher_achieved_goal(env)\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "print(f'observation: {env.get_wrapper_attr(\"_get_obs\")()}')\n",
    "print(f'distance to goal: {env.get_wrapper_attr(\"_get_obs\")()[8::]}')\n",
    "print(f'fingertip: {env.get_wrapper_attr(\"get_body_com\")(\"fingertip\")}')\n",
    "print(f'target: {env.get_wrapper_attr(\"get_body_com\")(\"target\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_achieved_goal = env.get_wrapper_attr(\"_get_obs\")()[8::]\n",
    "desired_goal = [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func(env, action, achieved_goal, next_achieved_goal, desired_goal, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=None,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(3,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.0001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=None,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(3,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.0001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Reacher-v4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=0.001,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(10, 50, 16, 40, True, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.agent.state_normalizer.running_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_her.test(10, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER w/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = [\n",
    "    # {\n",
    "    #     \"batchnorm\":\n",
    "    #     {\n",
    "    #         \"num_features\":3\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 7,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 5,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"relu\":\n",
    "        {\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"batchnorm\":\n",
    "        {\n",
    "            \"num_features\":32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"conv\":\n",
    "        {\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 3,\n",
    "            \"padding\": 'valid',\n",
    "            \"bias\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "cnn = cnn_models.CNN(cnn_layers, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env,\n",
    "                          cnn_model=cnn,\n",
    "                          dense_layers=dense_layers,\n",
    "                          goal_shape=(1,),\n",
    "                          optimizer=\"Adam\",\n",
    "                          optimizer_params={'weight_decay':0.0},\n",
    "                          learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        256,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env,\n",
    "                            cnn_model=cnn,\n",
    "                            state_layers=state_layers,\n",
    "                            merged_layers=merged_layers,\n",
    "                            goal_shape=(1,),\n",
    "                            optimizer=\"Adam\",\n",
    "                            optimizer_params={'weight_decay':0.0},\n",
    "                            learning_rate=0.001,\n",
    "                            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape\n",
    "replay_buffer = helper.ReplayBuffer(env, 100000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape,\n",
    "#                        mean=0.0,\n",
    "#                        theta=0.05,\n",
    "#                        sigma=0.15,\n",
    "#                        dt=1.0, device='cuda')\n",
    "\n",
    "noise=helper.NormalNoise(shape=env.action_space.shape,\n",
    "                         mean = 0.0,\n",
    "                         stddev=0.05,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('CarRacing-v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(ddpg_agent,\n",
    "                    strategy='future',\n",
    "                    num_goals=4,\n",
    "                    tolerance=1,\n",
    "                    desired_goal=desired_goal_func,\n",
    "                    achieved_goal=achieved_goal_func,\n",
    "                    reward_fn=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=20,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "state, _ = her.agent.env.reset()\n",
    "# instantiate empty lists to store current episode trajectory\n",
    "states, actions, next_states, dones, state_achieved_goals, \\\n",
    "next_state_achieved_goals, desired_goals = [], [], [], [], [], [], []\n",
    "# set desired goal\n",
    "desired_goal = her.desired_goal_func(her.agent.env)\n",
    "# set achieved goal\n",
    "state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "# add initial state and goals to local normalizer stats\n",
    "her.state_normalizer.update_local_stats(state)\n",
    "her.goal_normalizer.update_local_stats(desired_goal)\n",
    "her.goal_normalizer.update_local_stats(state_achieved_goal)\n",
    "# set done flag\n",
    "done = False\n",
    "# reset episode reward to 0\n",
    "episode_reward = 0\n",
    "# reset steps counter for the episode\n",
    "episode_steps = 0\n",
    "\n",
    "while not done:\n",
    "    # get normalized values for state and desired goal\n",
    "    state_norm = her.state_normalizer.normalize(state)\n",
    "    desired_goal_norm = her.goal_normalizer.normalize(desired_goal)\n",
    "    # get action\n",
    "    action = her.agent.get_action(state_norm, desired_goal_norm, grad=False)\n",
    "    # take action\n",
    "    next_state, reward, term, trunc, _ = her.agent.env.step(action)\n",
    "    # get next state achieved goal\n",
    "    next_state_achieved_goal = her.achieved_goal_func(her.agent.env)\n",
    "    # add next state and next state achieved goal to normalizers\n",
    "    her.state_normalizer.update_local_stats(next_state)\n",
    "    her.goal_normalizer.update_local_stats(next_state_achieved_goal)\n",
    "    # store trajectory in replay buffer (non normalized!)\n",
    "    her.agent.replay_buffer.add(state, action, reward, next_state, done,\\\n",
    "                                    state_achieved_goal, next_state_achieved_goal, desired_goal)\n",
    "    \n",
    "    # append step state, action, next state, and goals to respective lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "    dones.append(done)\n",
    "    state_achieved_goals.append(state_achieved_goal)\n",
    "    next_state_achieved_goals.append(next_state_achieved_goal)\n",
    "    desired_goals.append(desired_goal)\n",
    "\n",
    "    # add to episode reward and increment steps counter\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    # update state and state achieved goal\n",
    "    state = next_state\n",
    "    state_achieved_goal = next_state_achieved_goal\n",
    "    # update done flag\n",
    "    if term or trunc:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package episode states, actions, next states, and goals into trajectory tuple\n",
    "trajectory = (states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals = trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (s, a, ns, d, sag, nsag, dg) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "    print(f'a={a}, d={d}, sag={sag}, nsag={nsag}, dg={dg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"future\"\n",
    "num_goals = 4\n",
    "\n",
    "# loop over each step in the trajectory to set new achieved goals, calculate new reward, and save to replay buffer\n",
    "for idx, (state, action, next_state, done, state_achieved_goal, next_state_achieved_goal, desired_goal) in enumerate(zip(states, actions, next_states, dones, state_achieved_goals, next_state_achieved_goals, desired_goals)):\n",
    "\n",
    "    if strategy == \"final\":\n",
    "        new_desired_goal = next_state_achieved_goals[-1]\n",
    "        new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "        print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "        her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "\n",
    "    if strategy == 'future':\n",
    "        for i in range(num_goals):\n",
    "            if idx + i + 1 >= len(states):\n",
    "                break\n",
    "            goal_idx = np.random.randint(idx + 1, len(states))\n",
    "            new_desired_goal = next_state_achieved_goals[goal_idx]\n",
    "            new_reward = her.reward_fn(state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "            print(f'transition: action={action}, reward={new_reward}, done={done}, state_achieved_goal={state_achieved_goal}, next_state_achieved_goal={next_state_achieved_goal}, desired_goal={new_desired_goal}')\n",
    "            her.agent.replay_buffer.add(state, action, new_reward, next_state, done, state_achieved_goal, next_state_achieved_goal, new_desired_goal)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, d, sag, nsag, dg = her.agent.replay_buffer.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}: a={a[i]}, r={r[i]}, d={d[i]}, sag={sag[i]}, nsag={nsag[i]}, dg={dg[i]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        400,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        300,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"variance scaling\": {\n",
    "                \"scale\": 1.0,\n",
    "                \"mode\": \"fan_in\",\n",
    "                \"distribution\": \"uniform\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.01}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.001, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 100000, (3,))\n",
    "noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.99,\n",
    "                            tau=0.005,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback('Pendulum-v1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desired_goal_func(env):\n",
    "    return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def achieved_goal_func(env):\n",
    "    return env.get_wrapper_attr('_get_obs')()\n",
    "\n",
    "def reward_func(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='none',\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.target_critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(1,1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.state_normalizer.normalize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = her.desired_goal_func(her.agent.env)\n",
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.goal_normalizer.normalize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_renders(folder_path):\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a .mp4 or .meta.json extension\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".meta.json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Remove the file\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_renders(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/ddpg/renders/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch-Reach (Robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FetchReach-v2\", max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achieved_goal_func(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_wrapper_attr(\"_get_obs\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.2,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=256,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchReach-v2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent.critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='future',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, action, rewards, next_states, dones, achieved_goals, next_achieved_goals, desired_goals = her.agent.replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"distance_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get success\n",
    "her.agent.env.get_wrapper_attr(\"_is_success\")(achieved_goal_func(her.agent.env), desired_goal_func(her.agent.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.agent.env.get_wrapper_attr(\"goal_distance\")(next_state_achieved_goal, desired_goal, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her = rl_agents.HER.load(\"/workspaces/RL_Agents/pytorch/src/app/assets/models/her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"goal\") - pusher_her.agent.env.get_wrapper_attr(\"get_body_com\")(\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher_her.agent.replay_buffer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pusher_her.agent.replay_buffer.desired_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ENV\n",
    "env = gym.make(\"Pusher-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(\n",
    "                    env,\n",
    "                    \"/renders/training\",\n",
    "                    episode_trigger=lambda x: True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "# take action\n",
    "    next_state, reward, term, trunc, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER Fetch Push (Robitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train(num_epochs=50,\n",
    "          num_cycles=50,\n",
    "          num_episodes=16,\n",
    "          num_updates=40,\n",
    "          render=True,\n",
    "          render_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchPush-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_goal_func, achieved_goal_func, reward_func = gym_helper.get_her_goal_functions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_shape = desired_goal_func(env).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build actor\n",
    "\n",
    "dense_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "actor = models.ActorModel(env, cnn_model=None, dense_layers=dense_layers, goal_shape=goal_shape, optimizer='Adam',\n",
    "                          optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build critic\n",
    "\n",
    "state_layers = [\n",
    "    \n",
    "]\n",
    "\n",
    "merged_layers = [\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "               \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        64,\n",
    "        \"relu\",\n",
    "        {\n",
    "            \"kaiming uniform\": {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "critic = models.CriticModel(env=env, cnn_model=None, state_layers=state_layers, merged_layers=merged_layers, goal_shape=goal_shape, optimizer=\"Adam\", optimizer_params={'weight_decay':0.0}, learning_rate=0.00001, normalize_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = helper.ReplayBuffer(env, 1000000, goal_shape)\n",
    "# noise = helper.OUNoise(shape=env.action_space.shape, dt=1.0, device='cuda')\n",
    "noise = helper.NormalNoise(shape=env.action_space.shape, mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent = rl_agents.DDPG(env=env,\n",
    "                            actor_model=actor,\n",
    "                            critic_model=critic,\n",
    "                            discount=0.98,\n",
    "                            tau=0.05,\n",
    "                            action_epsilon=0.3,\n",
    "                            replay_buffer=replay_buffer,\n",
    "                            batch_size=128,\n",
    "                            noise=noise,\n",
    "                            callbacks=[rl_callbacks.WandbCallback(\"FetchPush-v2\")],\n",
    "                            save_dir=\"fetch_push/models/ddpg/\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her = rl_agents.HER(\n",
    "    agent=ddpg_agent,\n",
    "    strategy='final',\n",
    "    num_workers=4,\n",
    "    tolerance=0.05,\n",
    "    num_goals=4,\n",
    "    desired_goal=desired_goal_func,\n",
    "    achieved_goal=achieved_goal_func,\n",
    "    reward_fn=reward_func,\n",
    "    normalizer_clip=5.0,\n",
    "    save_dir=\"fetch_push/models/her/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config_path = \"/workspaces/RL_Agents/pytorch/src/app/HER_Test/her/config.json\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl_agents.HER.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callback in agent.agent.callbacks:\n",
    "    print(callback._sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/wandb_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    wandb_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'assets/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated configuration to a train config file\n",
    "os.makedirs('sweep', exist_ok=True)\n",
    "train_config_path = os.path.join(os.getcwd(), 'sweep/train_config.json')\n",
    "with open(train_config_path, 'w') as f:\n",
    "    json.dump(sweep_config, f)\n",
    "\n",
    "# Save and Set the sweep config path\n",
    "sweep_config_path = os.path.join(os.getcwd(), 'sweep/sweep_config.json')\n",
    "with open(sweep_config_path, 'w') as f:\n",
    "    json.dump(wandb_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['python', 'sweep.py']\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'\n",
    "\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable\n",
    "os.environ['WANDB_DISABLE_SERVICE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/sweep_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    sweep_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your JSON configuration file\n",
    "config_file_path = 'sweep/train_config.json'\n",
    "\n",
    "# Read the JSON configuration file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    train_config = json.load(file)\n",
    "\n",
    "# Print the configuration to verify it has been loaded correctly\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=sweep_config[\"project\"])\n",
    "# loop over num wandb agents\n",
    "num_agents = 1\n",
    "# for agent in range(num_agents):\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=lambda: wandb_support._run_sweep(sweep_config, train_config,),\n",
    "    count=train_config['num_sweeps'],\n",
    "    project=sweep_config[\"project\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Beta, Normal, kl_divergence\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid1 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_1 = ppo_agent_hybrid1.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "env_id = 'BipedalWalker-v3'\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.1\n",
    "kl_coeff = 0.01\n",
    "loss = 'kl'\n",
    "timesteps = 100_000\n",
    "num_envs = 10\n",
    "device = 'cuda'\n",
    "\n",
    "seed = 42\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, num_envs, dense_layers, learning_rate=policy_lr, distribution='Beta', device=device)\n",
    "dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=640, learning_epochs=10, num_envs=num_envs)\n",
    "\n",
    "# seed = 43\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid2 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_2 = ppo_agent_hybrid2.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "\n",
    "# seed = 44\n",
    "# env = gym.make(env_id)\n",
    "# T.manual_seed(seed)\n",
    "# T.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# gym.utils.seeding.np_random.seed = seed\n",
    "# # Build policy model\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=3e-4)\n",
    "# dense_layers = [(128,\"tanh\",{\"default\":{}}),(128,\"tanh\",{\"default\":{}})]\n",
    "# value_function = ValueModel(env, dense_layers, learning_rate=3e-4)\n",
    "# ppo_agent_hybrid3 = PPO(env, policy, value_function, distribution='Beta', discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff, kl_coefficient=kl_coeff, loss=loss)\n",
    "# hybrid_train_info_3 = ppo_agent_hybrid3.train(timesteps=timesteps, trajectory_length=2048, batch_size=64, learning_epochs=10)\n",
    "# hybrid_test_info = ppo_agent_hybrid.test(1000, 'PPO_hybrid', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: [19.]; total steps: 1000; avg scores/10 episodes: [-15.98675275]; avg total score: -15.986752752931778\n",
      "episode: [39.]; total steps: 2000; avg scores/10 episodes: [-14.79615362]; avg total score: -14.796153624697917\n",
      "learning timestep: 2000\n",
      "num batches:31\n",
      "Policy Loss: 0.007226414978504181\n",
      "Value Loss: 14.294544219970703\n",
      "Entropy: -1.6312243938446045\n",
      "KL Divergence: 0.057561248540878296\n",
      "episode: [58.]; total steps: 3000; avg scores/10 episodes: [-14.88853906]; avg total score: -14.888539056646746\n",
      "episode: [78.]; total steps: 4000; avg scores/10 episodes: [-15.54355268]; avg total score: -15.543552680952065\n",
      "learning timestep: 4000\n",
      "num batches:31\n",
      "Policy Loss: -0.11906469613313675\n",
      "Value Loss: 9.622722625732422\n",
      "Entropy: -1.6217445135116577\n",
      "KL Divergence: 0.021938078105449677\n",
      "episode: [98.]; total steps: 5000; avg scores/10 episodes: [-16.33771249]; avg total score: -16.337712492361934\n",
      "episode: [117.]; total steps: 6000; avg scores/10 episodes: [-15.8792487]; avg total score: -15.879248704515595\n",
      "learning timestep: 6000\n",
      "num batches:31\n",
      "Policy Loss: -0.2890947163105011\n",
      "Value Loss: 8.607643127441406\n",
      "Entropy: -1.6476482152938843\n",
      "KL Divergence: 0.07943661510944366\n",
      "episode: [137.]; total steps: 7000; avg scores/10 episodes: [-14.54295249]; avg total score: -14.542952490658797\n",
      "episode: [156.]; total steps: 8000; avg scores/10 episodes: [-13.98886727]; avg total score: -13.988867267603855\n",
      "learning timestep: 8000\n",
      "num batches:31\n",
      "Policy Loss: -0.010197674855589867\n",
      "Value Loss: 7.6580810546875\n",
      "Entropy: -1.6723432540893555\n",
      "KL Divergence: 0.09101390838623047\n",
      "episode: [176.]; total steps: 9000; avg scores/10 episodes: [-14.48769579]; avg total score: -14.487695792268022\n",
      "episode: [196.]; total steps: 10000; avg scores/10 episodes: [-16.2049249]; avg total score: -16.20492489779466\n",
      "learning timestep: 10000\n",
      "num batches:31\n",
      "Policy Loss: 0.14506861567497253\n",
      "Value Loss: 7.159826278686523\n",
      "Entropy: -1.71547269821167\n",
      "KL Divergence: 0.1266637146472931\n",
      "episode: [215.]; total steps: 11000; avg scores/10 episodes: [-14.29937755]; avg total score: -14.299377552587686\n",
      "episode: [235.]; total steps: 12000; avg scores/10 episodes: [-13.2560679]; avg total score: -13.25606790294683\n",
      "learning timestep: 12000\n",
      "num batches:31\n",
      "Policy Loss: -0.14767986536026\n",
      "Value Loss: 2.9531095027923584\n",
      "Entropy: -1.7361533641815186\n",
      "KL Divergence: 0.020263956859707832\n",
      "episode: [254.]; total steps: 13000; avg scores/10 episodes: [-13.43120519]; avg total score: -13.431205191489912\n",
      "episode: [274.]; total steps: 14000; avg scores/10 episodes: [-14.17502604]; avg total score: -14.17502603780608\n",
      "learning timestep: 14000\n",
      "num batches:31\n",
      "Policy Loss: 0.1461498737335205\n",
      "Value Loss: 4.967649459838867\n",
      "Entropy: -1.7636406421661377\n",
      "KL Divergence: 0.07979060709476471\n",
      "episode: [294.]; total steps: 15000; avg scores/10 episodes: [-13.541774]; avg total score: -13.541774000277552\n",
      "episode: [313.]; total steps: 16000; avg scores/10 episodes: [-13.82947926]; avg total score: -13.829479261464698\n",
      "learning timestep: 16000\n",
      "num batches:31\n",
      "Policy Loss: 0.002689272165298462\n",
      "Value Loss: 7.005257606506348\n",
      "Entropy: -1.7430928945541382\n",
      "KL Divergence: 0.14507074654102325\n",
      "episode: [333.]; total steps: 17000; avg scores/10 episodes: [-15.45266369]; avg total score: -15.452663693433411\n",
      "episode: [352.]; total steps: 18000; avg scores/10 episodes: [-14.18442682]; avg total score: -14.18442681695581\n",
      "learning timestep: 18000\n",
      "num batches:31\n",
      "Policy Loss: -0.03379012271761894\n",
      "Value Loss: 2.4583871364593506\n",
      "Entropy: -1.7070538997650146\n",
      "KL Divergence: 0.22698189318180084\n",
      "episode: [372.]; total steps: 19000; avg scores/10 episodes: [-14.85597394]; avg total score: -14.855973937310997\n",
      "episode: [392.]; total steps: 20000; avg scores/10 episodes: [-14.67820492]; avg total score: -14.678204918028626\n",
      "learning timestep: 20000\n",
      "num batches:31\n",
      "Policy Loss: 0.034541524946689606\n",
      "Value Loss: 2.757009267807007\n",
      "Entropy: -1.6683398485183716\n",
      "KL Divergence: 0.10054127871990204\n",
      "episode: [411.]; total steps: 21000; avg scores/10 episodes: [-14.99190787]; avg total score: -14.991907868876705\n",
      "episode: [431.]; total steps: 22000; avg scores/10 episodes: [-13.95797745]; avg total score: -13.957977446382591\n",
      "learning timestep: 22000\n",
      "num batches:31\n",
      "Policy Loss: -0.07920033484697342\n",
      "Value Loss: 3.2364659309387207\n",
      "Entropy: -1.7173954248428345\n",
      "KL Divergence: 0.15593639016151428\n",
      "episode: [450.]; total steps: 23000; avg scores/10 episodes: [-13.00257793]; avg total score: -13.002577931181992\n",
      "episode: [470.]; total steps: 24000; avg scores/10 episodes: [-12.44607123]; avg total score: -12.446071234559117\n",
      "learning timestep: 24000\n",
      "num batches:31\n",
      "Policy Loss: -0.10852476954460144\n",
      "Value Loss: 10.039727210998535\n",
      "Entropy: -1.7714383602142334\n",
      "KL Divergence: 0.34830015897750854\n",
      "episode: [490.]; total steps: 25000; avg scores/10 episodes: [-15.547545]; avg total score: -15.547545002282934\n",
      "episode: [509.]; total steps: 26000; avg scores/10 episodes: [-14.0973221]; avg total score: -14.09732210108827\n",
      "learning timestep: 26000\n",
      "num batches:31\n",
      "Policy Loss: -0.29179561138153076\n",
      "Value Loss: 3.39639949798584\n",
      "Entropy: -1.8928874731063843\n",
      "KL Divergence: 0.051014624536037445\n",
      "episode: [529.]; total steps: 27000; avg scores/10 episodes: [-18.09485633]; avg total score: -18.09485633410909\n",
      "episode: [549.]; total steps: 28000; avg scores/10 episodes: [-17.39291521]; avg total score: -17.392915209518375\n",
      "learning timestep: 28000\n",
      "num batches:31\n",
      "Policy Loss: -0.04880600795149803\n",
      "Value Loss: 3.110316276550293\n",
      "Entropy: -1.8886933326721191\n",
      "KL Divergence: 0.009450064972043037\n",
      "episode: [568.]; total steps: 29000; avg scores/10 episodes: [-15.2313877]; avg total score: -15.231387701412643\n",
      "episode: [588.]; total steps: 30000; avg scores/10 episodes: [-15.63572292]; avg total score: -15.635722923683733\n",
      "learning timestep: 30000\n",
      "num batches:31\n",
      "Policy Loss: -0.1627785861492157\n",
      "Value Loss: 1.4803868532180786\n",
      "Entropy: -1.8163100481033325\n",
      "KL Divergence: 0.03955258056521416\n",
      "episode: [607.]; total steps: 31000; avg scores/10 episodes: [-14.11776702]; avg total score: -14.117767018246393\n",
      "episode: [627.]; total steps: 32000; avg scores/10 episodes: [-14.20293814]; avg total score: -14.202938138643466\n",
      "learning timestep: 32000\n",
      "num batches:31\n",
      "Policy Loss: 0.07163426280021667\n",
      "Value Loss: 1.7993628978729248\n",
      "Entropy: -1.7267402410507202\n",
      "KL Divergence: 0.06920891255140305\n",
      "episode: [647.]; total steps: 33000; avg scores/10 episodes: [-12.84982105]; avg total score: -12.849821047473572\n",
      "episode: [666.]; total steps: 34000; avg scores/10 episodes: [-13.79648606]; avg total score: -13.796486057284644\n",
      "learning timestep: 34000\n",
      "num batches:31\n",
      "Policy Loss: 0.2151346057653427\n",
      "Value Loss: 5.731136322021484\n",
      "Entropy: -1.8388749361038208\n",
      "KL Divergence: 0.305937796831131\n",
      "episode: [686.]; total steps: 35000; avg scores/10 episodes: [-13.81572449]; avg total score: -13.81572449017398\n",
      "episode: [705.]; total steps: 36000; avg scores/10 episodes: [-14.13050944]; avg total score: -14.130509441209886\n",
      "learning timestep: 36000\n",
      "num batches:31\n",
      "Policy Loss: -0.3655507564544678\n",
      "Value Loss: 7.437018394470215\n",
      "Entropy: -1.8975154161453247\n",
      "KL Divergence: 0.14886395633220673\n",
      "episode: [725.]; total steps: 37000; avg scores/10 episodes: [-14.57783385]; avg total score: -14.57783385456614\n",
      "episode: [745.]; total steps: 38000; avg scores/10 episodes: [-14.8989865]; avg total score: -14.89898649690809\n",
      "learning timestep: 38000\n",
      "num batches:31\n",
      "Policy Loss: 0.2867888808250427\n",
      "Value Loss: 2.1075329780578613\n",
      "Entropy: -1.9814550876617432\n",
      "KL Divergence: 0.05376744270324707\n",
      "episode: [764.]; total steps: 39000; avg scores/10 episodes: [-19.2549369]; avg total score: -19.254936901427097\n",
      "episode: [784.]; total steps: 40000; avg scores/10 episodes: [-18.88602642]; avg total score: -18.88602641632722\n",
      "learning timestep: 40000\n",
      "num batches:31\n",
      "Policy Loss: -0.015257995575666428\n",
      "Value Loss: 1.9341988563537598\n",
      "Entropy: -1.9649842977523804\n",
      "KL Divergence: 0.03304107114672661\n",
      "episode: [803.]; total steps: 41000; avg scores/10 episodes: [-14.92520089]; avg total score: -14.925200890291398\n",
      "episode: [823.]; total steps: 42000; avg scores/10 episodes: [-16.93812594]; avg total score: -16.938125940379447\n",
      "learning timestep: 42000\n",
      "num batches:31\n",
      "Policy Loss: -0.3878265619277954\n",
      "Value Loss: 1.9093410968780518\n",
      "Entropy: -1.7780635356903076\n",
      "KL Divergence: 0.4462849497795105\n",
      "episode: [843.]; total steps: 43000; avg scores/10 episodes: [-13.03732858]; avg total score: -13.03732858426323\n",
      "episode: [862.]; total steps: 44000; avg scores/10 episodes: [-13.59705427]; avg total score: -13.597054268087902\n",
      "learning timestep: 44000\n",
      "num batches:31\n",
      "Policy Loss: 0.08020418137311935\n",
      "Value Loss: 2.143800973892212\n",
      "Entropy: -1.7673745155334473\n",
      "KL Divergence: 0.25940126180648804\n",
      "episode: [882.]; total steps: 45000; avg scores/10 episodes: [-12.20484279]; avg total score: -12.204842788814535\n",
      "episode: [901.]; total steps: 46000; avg scores/10 episodes: [-13.55690928]; avg total score: -13.556909279007773\n",
      "learning timestep: 46000\n",
      "num batches:31\n",
      "Policy Loss: -0.13922128081321716\n",
      "Value Loss: 2.1955385208129883\n",
      "Entropy: -1.8322651386260986\n",
      "KL Divergence: 0.2995017468929291\n",
      "episode: [921.]; total steps: 47000; avg scores/10 episodes: [-12.58045342]; avg total score: -12.58045341926526\n",
      "episode: [941.]; total steps: 48000; avg scores/10 episodes: [-13.60815636]; avg total score: -13.608156362934604\n",
      "learning timestep: 48000\n",
      "num batches:31\n",
      "Policy Loss: -0.06067802011966705\n",
      "Value Loss: 3.0739803314208984\n",
      "Entropy: -1.9137979745864868\n",
      "KL Divergence: 0.48097431659698486\n",
      "episode: [960.]; total steps: 49000; avg scores/10 episodes: [-13.65207582]; avg total score: -13.652075818063219\n",
      "episode: [980.]; total steps: 50000; avg scores/10 episodes: [-13.4586524]; avg total score: -13.458652401964818\n",
      "learning timestep: 50000\n",
      "num batches:31\n",
      "Policy Loss: 0.02937249094247818\n",
      "Value Loss: 3.678478956222534\n",
      "Entropy: -1.913295030593872\n",
      "KL Divergence: 0.5150741338729858\n",
      "episode: [1000.]; total steps: 51000; avg scores/10 episodes: [-14.27077372]; avg total score: -14.27077372171576\n",
      "episode: [1019.]; total steps: 52000; avg scores/10 episodes: [-15.22576451]; avg total score: -15.225764511042524\n",
      "learning timestep: 52000\n",
      "num batches:31\n",
      "Policy Loss: 0.15098828077316284\n",
      "Value Loss: 1.9387319087982178\n",
      "Entropy: -1.8807997703552246\n",
      "KL Divergence: 0.3816162347793579\n",
      "episode: [1039.]; total steps: 53000; avg scores/10 episodes: [-15.21186921]; avg total score: -15.211869212736218\n",
      "episode: [1058.]; total steps: 54000; avg scores/10 episodes: [-15.07653421]; avg total score: -15.07653420757752\n",
      "learning timestep: 54000\n",
      "num batches:31\n",
      "Policy Loss: -0.15511532127857208\n",
      "Value Loss: 3.1383016109466553\n",
      "Entropy: -1.926393747329712\n",
      "KL Divergence: 0.24915292859077454\n",
      "episode: [1078.]; total steps: 55000; avg scores/10 episodes: [-13.31605755]; avg total score: -13.316057547514003\n",
      "episode: [1098.]; total steps: 56000; avg scores/10 episodes: [-13.74511262]; avg total score: -13.745112620792828\n",
      "learning timestep: 56000\n",
      "num batches:31\n",
      "Policy Loss: -0.019811995327472687\n",
      "Value Loss: 1.829601526260376\n",
      "Entropy: -1.9554755687713623\n",
      "KL Divergence: 0.24055272340774536\n",
      "episode: [1117.]; total steps: 57000; avg scores/10 episodes: [-15.60589292]; avg total score: -15.605892923487142\n",
      "episode: [1137.]; total steps: 58000; avg scores/10 episodes: [-13.60315499]; avg total score: -13.603154994831991\n",
      "learning timestep: 58000\n",
      "num batches:31\n",
      "Policy Loss: -0.07971400022506714\n",
      "Value Loss: 2.9021663665771484\n",
      "Entropy: -1.9474002122879028\n",
      "KL Divergence: 0.1020977571606636\n",
      "episode: [1156.]; total steps: 59000; avg scores/10 episodes: [-13.29372249]; avg total score: -13.293722489496256\n",
      "episode: [1176.]; total steps: 60000; avg scores/10 episodes: [-15.31284939]; avg total score: -15.312849390464384\n",
      "learning timestep: 60000\n",
      "num batches:31\n",
      "Policy Loss: -0.25722387433052063\n",
      "Value Loss: 2.9601998329162598\n",
      "Entropy: -2.099407196044922\n",
      "KL Divergence: 0.02738557755947113\n",
      "episode: [1196.]; total steps: 61000; avg scores/10 episodes: [-21.24849346]; avg total score: -21.248493463775965\n",
      "episode: [1215.]; total steps: 62000; avg scores/10 episodes: [-21.64240689]; avg total score: -21.64240688855557\n",
      "learning timestep: 62000\n",
      "num batches:31\n",
      "Policy Loss: 0.10067003965377808\n",
      "Value Loss: 5.575283050537109\n",
      "Entropy: -2.0979135036468506\n",
      "KL Divergence: 0.011075551621615887\n",
      "episode: [1235.]; total steps: 63000; avg scores/10 episodes: [-20.61470246]; avg total score: -20.614702455627715\n",
      "episode: [1254.]; total steps: 64000; avg scores/10 episodes: [-20.88552442]; avg total score: -20.885524420233327\n",
      "learning timestep: 64000\n",
      "num batches:31\n",
      "Policy Loss: 0.15168362855911255\n",
      "Value Loss: 2.6376097202301025\n",
      "Entropy: -1.9839305877685547\n",
      "KL Divergence: 0.07006292045116425\n",
      "episode: [1274.]; total steps: 65000; avg scores/10 episodes: [-16.65934491]; avg total score: -16.659344912299108\n",
      "episode: [1294.]; total steps: 66000; avg scores/10 episodes: [-15.93162808]; avg total score: -15.931628079819788\n",
      "learning timestep: 66000\n",
      "num batches:31\n",
      "Policy Loss: 0.26821544766426086\n",
      "Value Loss: 1.307801604270935\n",
      "Entropy: -1.984665870666504\n",
      "KL Divergence: 0.03364984691143036\n",
      "episode: [1313.]; total steps: 67000; avg scores/10 episodes: [-14.3999646]; avg total score: -14.399964599266093\n",
      "episode: [1333.]; total steps: 68000; avg scores/10 episodes: [-13.31026801]; avg total score: -13.310268008411123\n",
      "learning timestep: 68000\n",
      "num batches:31\n",
      "Policy Loss: 0.02412431687116623\n",
      "Value Loss: 1.3447668552398682\n",
      "Entropy: -2.011918544769287\n",
      "KL Divergence: 0.02841930277645588\n",
      "episode: [1352.]; total steps: 69000; avg scores/10 episodes: [-13.91489877]; avg total score: -13.9148987736506\n",
      "episode: [1372.]; total steps: 70000; avg scores/10 episodes: [-15.62646165]; avg total score: -15.626461650541263\n",
      "learning timestep: 70000\n",
      "num batches:31\n",
      "Policy Loss: 0.08360233902931213\n",
      "Value Loss: 1.4135994911193848\n",
      "Entropy: -2.0408263206481934\n",
      "KL Divergence: 0.021125927567481995\n",
      "episode: [1392.]; total steps: 71000; avg scores/10 episodes: [-14.1984694]; avg total score: -14.198469400841882\n",
      "episode: [1411.]; total steps: 72000; avg scores/10 episodes: [-13.86406001]; avg total score: -13.864060014097623\n",
      "learning timestep: 72000\n",
      "num batches:31\n",
      "Policy Loss: -0.04033887758851051\n",
      "Value Loss: 1.1291358470916748\n",
      "Entropy: -1.982190728187561\n",
      "KL Divergence: 0.18410226702690125\n",
      "episode: [1431.]; total steps: 73000; avg scores/10 episodes: [-11.8054537]; avg total score: -11.805453697973158\n",
      "episode: [1450.]; total steps: 74000; avg scores/10 episodes: [-13.60792245]; avg total score: -13.60792245427831\n",
      "learning timestep: 74000\n",
      "num batches:31\n",
      "Policy Loss: 0.1642530858516693\n",
      "Value Loss: 1.8487112522125244\n",
      "Entropy: -2.0278804302215576\n",
      "KL Divergence: 0.14121145009994507\n",
      "episode: [1470.]; total steps: 75000; avg scores/10 episodes: [-11.92252514]; avg total score: -11.922525138053663\n",
      "episode: [1490.]; total steps: 76000; avg scores/10 episodes: [-12.75621754]; avg total score: -12.756217541919941\n",
      "learning timestep: 76000\n",
      "num batches:31\n",
      "Policy Loss: 0.06772543489933014\n",
      "Value Loss: 1.7548432350158691\n",
      "Entropy: -2.0388340950012207\n",
      "KL Divergence: 0.09892416000366211\n",
      "episode: [1509.]; total steps: 77000; avg scores/10 episodes: [-12.06235853]; avg total score: -12.062358526782166\n",
      "episode: [1529.]; total steps: 78000; avg scores/10 episodes: [-12.91965782]; avg total score: -12.919657817837471\n",
      "learning timestep: 78000\n",
      "num batches:31\n",
      "Policy Loss: -0.4188173711299896\n",
      "Value Loss: 2.700157403945923\n",
      "Entropy: -2.100914716720581\n",
      "KL Divergence: 0.04774994030594826\n",
      "episode: [1549.]; total steps: 79000; avg scores/10 episodes: [-14.84805776]; avg total score: -14.848057759729457\n",
      "episode: [1568.]; total steps: 80000; avg scores/10 episodes: [-12.80112214]; avg total score: -12.801122136387821\n",
      "learning timestep: 80000\n",
      "num batches:31\n",
      "Policy Loss: 0.6138560771942139\n",
      "Value Loss: 9.299018859863281\n",
      "Entropy: -2.1548757553100586\n",
      "KL Divergence: 0.02557649090886116\n",
      "episode: [1588.]; total steps: 81000; avg scores/10 episodes: [-21.41775297]; avg total score: -21.417752968749227\n",
      "episode: [1607.]; total steps: 82000; avg scores/10 episodes: [-21.80822004]; avg total score: -21.808220037375662\n",
      "learning timestep: 82000\n",
      "num batches:31\n",
      "Policy Loss: -0.07672839611768723\n",
      "Value Loss: 6.932255268096924\n",
      "Entropy: -1.8568570613861084\n",
      "KL Divergence: 0.9011319875717163\n",
      "episode: [1627.]; total steps: 83000; avg scores/10 episodes: [-15.38997785]; avg total score: -15.38997785350372\n",
      "episode: [1647.]; total steps: 84000; avg scores/10 episodes: [-14.11899931]; avg total score: -14.118999310224467\n",
      "learning timestep: 84000\n",
      "num batches:31\n",
      "Policy Loss: -0.20222902297973633\n",
      "Value Loss: 4.0539703369140625\n",
      "Entropy: -2.1327273845672607\n",
      "KL Divergence: 0.04985814541578293\n",
      "episode: [1666.]; total steps: 85000; avg scores/10 episodes: [-18.29118172]; avg total score: -18.291181724479763\n",
      "episode: [1686.]; total steps: 86000; avg scores/10 episodes: [-18.80508927]; avg total score: -18.805089268459277\n",
      "learning timestep: 86000\n",
      "num batches:31\n",
      "Policy Loss: -0.052067264914512634\n",
      "Value Loss: 1.870478868484497\n",
      "Entropy: -1.7355546951293945\n",
      "KL Divergence: 0.7246428728103638\n",
      "episode: [1705.]; total steps: 87000; avg scores/10 episodes: [-14.61097394]; avg total score: -14.610973940174372\n",
      "episode: [1725.]; total steps: 88000; avg scores/10 episodes: [-13.22925875]; avg total score: -13.229258753408496\n",
      "learning timestep: 88000\n",
      "num batches:31\n",
      "Policy Loss: 0.5478200912475586\n",
      "Value Loss: 1.4892346858978271\n",
      "Entropy: -2.0326461791992188\n",
      "KL Divergence: 0.05810311436653137\n",
      "episode: [1745.]; total steps: 89000; avg scores/10 episodes: [-13.65914363]; avg total score: -13.659143629471387\n",
      "episode: [1764.]; total steps: 90000; avg scores/10 episodes: [-14.27471506]; avg total score: -14.27471505660012\n",
      "learning timestep: 90000\n",
      "num batches:31\n",
      "Policy Loss: -0.00671599805355072\n",
      "Value Loss: 0.5990292429924011\n",
      "Entropy: -1.8617002964019775\n",
      "KL Divergence: 0.15100838243961334\n",
      "episode: [1784.]; total steps: 91000; avg scores/10 episodes: [-14.2871745]; avg total score: -14.287174504032217\n",
      "episode: [1803.]; total steps: 92000; avg scores/10 episodes: [-15.1519584]; avg total score: -15.151958402187427\n",
      "learning timestep: 92000\n",
      "num batches:31\n",
      "Policy Loss: -0.11976321786642075\n",
      "Value Loss: 0.7264337539672852\n",
      "Entropy: -1.953763484954834\n",
      "KL Divergence: 0.08857577294111252\n",
      "episode: [1823.]; total steps: 93000; avg scores/10 episodes: [-13.70375059]; avg total score: -13.703750592439485\n",
      "episode: [1843.]; total steps: 94000; avg scores/10 episodes: [-13.95295776]; avg total score: -13.952957763360711\n",
      "learning timestep: 94000\n",
      "num batches:31\n",
      "Policy Loss: 0.022257762029767036\n",
      "Value Loss: 0.6727873086929321\n",
      "Entropy: -1.9921369552612305\n",
      "KL Divergence: 0.11810718476772308\n",
      "episode: [1862.]; total steps: 95000; avg scores/10 episodes: [-13.67052346]; avg total score: -13.670523464031607\n",
      "episode: [1882.]; total steps: 96000; avg scores/10 episodes: [-13.32238826]; avg total score: -13.322388256982801\n",
      "learning timestep: 96000\n",
      "num batches:31\n",
      "Policy Loss: -0.02954094484448433\n",
      "Value Loss: 0.9795507192611694\n",
      "Entropy: -2.057292938232422\n",
      "KL Divergence: 0.06862980127334595\n",
      "episode: [1901.]; total steps: 97000; avg scores/10 episodes: [-11.83002631]; avg total score: -11.830026314075079\n",
      "episode: [1921.]; total steps: 98000; avg scores/10 episodes: [-11.37914335]; avg total score: -11.379143350690935\n",
      "learning timestep: 98000\n",
      "num batches:31\n",
      "Policy Loss: 0.35877925157546997\n",
      "Value Loss: 0.9148218035697937\n",
      "Entropy: -2.0826175212860107\n",
      "KL Divergence: 0.043911997228860855\n",
      "episode: [1941.]; total steps: 99000; avg scores/10 episodes: [-15.05120818]; avg total score: -15.051208184416396\n",
      "episode: [1960.]; total steps: 100000; avg scores/10 episodes: [-12.78185951]; avg total score: -12.781859511120969\n"
     ]
    }
   ],
   "source": [
    "## PARAMS ##\n",
    "# env_id = 'Pendulum-v1'\n",
    "# env_id = 'LunarLanderContinuous-v3'\n",
    "# env_id = 'BipedalWalker-v3'\n",
    "# env_id = 'Humanoid-v5'\n",
    "env_id = \"Reacher-v5\"\n",
    "\n",
    "timesteps = 100_000\n",
    "trajectory_length = 2000\n",
    "batch_size = 64\n",
    "learning_epochs = 10\n",
    "num_envs = 1\n",
    "policy_lr = 3e-4\n",
    "value_lr = 2e-5\n",
    "entropy_coeff = 0.01\n",
    "kl_coeff = 0.1\n",
    "loss = 'kl'\n",
    "grad_clip = 0.5\n",
    "lambda_ = 0.0\n",
    "distribution = 'Beta'\n",
    "device = 'cuda'\n",
    "\n",
    "## WANDB ##\n",
    "project_name = 'PPO-Test'\n",
    "run_name = None\n",
    "# callbacks = [WandbCallback(project_name, run_name)]\n",
    "callbacks = []\n",
    "\n",
    "seed = 43\n",
    "env = gym.make_vec(env_id, num_envs)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "# _,_ = env.reset()\n",
    "# sample = env.action_space.sample()\n",
    "# if isinstance(sample, np.int64) or isinstance(sample, np.int32):\n",
    "#     print(f'discrete action space of size {env.action_space.n}')\n",
    "# elif isinstance(sample, np.ndarray):\n",
    "#     print(f'continuous action space of size {env.action_space.shape}')\n",
    "\n",
    "T.manual_seed(seed)\n",
    "T.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gym.utils.seeding.np_random.seed = seed\n",
    "# Build policy model\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "policy = StochasticContinuousPolicy(env, dense_layers, learning_rate=policy_lr, distribution=distribution, device=device)\n",
    "dense_layers = [(64,\"tanh\",{\"default\":{}}),(64,\"tanh\",{\"default\":{}})]\n",
    "value_function = ValueModel(env, dense_layers, learning_rate=value_lr, device=device)\n",
    "ppo = PPO(env, policy, value_function, distribution=distribution, discount=0.99, gae_coefficient=0.95, policy_clip=0.2, entropy_coefficient=entropy_coeff,\n",
    "          kl_coefficient=kl_coeff, loss=loss, grad_clip=grad_clip, lambda_=lambda_, callbacks=callbacks, device=device, seed=seed)\n",
    "hybrid_train_info_2 = ppo.train(timesteps=timesteps, trajectory_length=trajectory_length, batch_size=batch_size, learning_epochs=learning_epochs, num_envs=num_envs)\n",
    "# ppo.test(10,\"ppo_test\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"Reacher-v5\"\n",
    "num_envs = 10\n",
    "env = gym.make_vec(env_id, num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = env.spec.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make_vec(gym.envs.registration.EnvSpec.from_json(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config_path = \"/workspaces/RL_Agents/pytorch/src/app/models/ppo/config.json\"\n",
    "with open(agent_config_path, 'r', encoding=\"utf-8\") as f:\n",
    "            agent_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO.load(agent_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.test(10, render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"ppo_test/renders\"\n",
    "num_episodes = 10\n",
    "render_freq = 1\n",
    "\n",
    "# Set the policy and value function models to evaluation mode\n",
    "ppo_agent_hybrid1.policy.eval()\n",
    "ppo_agent_hybrid1.value_model.eval()\n",
    "\n",
    "# Create the render directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "env = gym.make_vec(\n",
    "    ppo_agent_hybrid1.env.spec.id,\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        lambda env: gym.wrappers.RecordVideo(\n",
    "            env,\n",
    "            save_dir + \"/renders/test\",\n",
    "            episode_trigger=lambda episode_id: (episode_id + 1) % render_freq == 0\n",
    "        )\n",
    "    ],\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "scores = []\n",
    "entropy_list = []\n",
    "kl_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    state, _ = ppo_agent_hybrid1.env.reset()\n",
    "    score = 0\n",
    "    episode_entropy = 0\n",
    "    episode_kl = 0\n",
    "    steps = 0\n",
    "\n",
    "    prev_dist = None  # To track the previous distribution for KL divergence\n",
    "\n",
    "    # Video writer setup\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video_path = os.path.join(render_dir, f\"episode_{episode+1}.mp4\")\n",
    "    #     frame = self.env.render(mode='rgb_array')\n",
    "    #     height, width, layers = frame.shape\n",
    "    #     video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n",
    "\n",
    "    while not done:\n",
    "        # Render the environment and write the frame to the video file\n",
    "        # if episode % render_freq == 0:\n",
    "        #     frame = self.env.render(mode='rgb_array')\n",
    "        #     video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Get action and log probability from the current policy\n",
    "        action, log_prob = ppo_agent_hybrid1.get_action(state)\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            act = ppo_agent_hybrid1.action_adapter(action)\n",
    "        else:\n",
    "            act = action\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, term, trunc, _ = ppo_agent_hybrid1.env.step(act)\n",
    "        if term or trunc:\n",
    "            done = True\n",
    "\n",
    "        # Calculate the distribution and entropy\n",
    "        dist = ppo_agent_hybrid1.policy(T.tensor(state, dtype=T.float32, device=ppo_agent_hybrid1.policy.device))\n",
    "        entropy = dist.entropy().sum().item()  # Sum entropy over actions\n",
    "\n",
    "        # Update KL divergence\n",
    "        if prev_dist is not None:\n",
    "            kl = kl_divergence(prev_dist, dist).sum().item()  # Sum KL divergence over actions\n",
    "        else:\n",
    "            kl = 0  # No KL divergence for the first step in the episode\n",
    "\n",
    "        # Update the previous distribution to the current one\n",
    "        if ppo_agent_hybrid1.distribution == 'Beta':\n",
    "            param1_prev = dist.concentration1.clone().detach()\n",
    "            param2_prev = dist.concentration0.clone().detach()\n",
    "            prev_dist = Beta(param1_prev, param2_prev)\n",
    "        elif ppo_agent_hybrid1.distribution == 'Normal':\n",
    "            param1_prev = dist.loc.clone().detach()\n",
    "            param2_prev = dist.scale.clone().detach()\n",
    "            prev_dist = Normal(param1_prev, param2_prev)\n",
    "\n",
    "        # Accumulate the score, entropy, and KL divergence for the episode\n",
    "        score += reward\n",
    "        episode_entropy += entropy\n",
    "        episode_kl += kl\n",
    "        steps += 1\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Release the video writer\n",
    "    # if episode % render_freq == 0:\n",
    "    #     video.release()\n",
    "\n",
    "    # Append the results for the episode\n",
    "    scores.append(score)\n",
    "    entropy_list.append(episode_entropy / steps)  # Average entropy over the episode\n",
    "    kl_list.append(episode_kl / steps)  # Average KL divergence over the episode\n",
    "\n",
    "    print(f'Episode {episode+1}/{num_episodes} - Score: {score}, Avg Entropy: {entropy_list[-1]}, Avg KL Divergence: {kl_list[-1]}')\n",
    "\n",
    "# close the environment\n",
    "ppo_agent_hybrid1.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
